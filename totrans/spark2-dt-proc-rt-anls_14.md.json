["```scala\npackage spark.ml.cookbook.chapter8\n```", "```scala\nimport org.apache.log4j.{Level, Logger}import org.apache.spark.ml.clustering.KMeansimport org.apache.spark.sql.SparkSession\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession .builder.master(\"local[*]\") .appName(\"myKMeansCluster\") .config(\"spark.sql.warehouse.dir\" ...\n```", "```scala\ndef generateKMeansRDD(sc: SparkContext, numPoints: Int, k: Int, d: Int, r: Double, numPartitions: Int = 2): RDD[Array[Double]] \n```", "```scala\npackage spark.ml.cookbook.chapter8.\n```", "```scala\n import org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.clustering.GaussianMixture\n import org.apache.spark.mllib.linalg.Vectors\n import org.apache.spark.sql.SparkSession\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"myGaussianMixture\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval dataFile =\"../data/sparkml2/chapter8/socr_data.txt\"\n```", "```scala\nval trainingData = spark.sparkContext.textFile(dataFile).map { line =>\n Vectors.dense(line.trim.split(' ').map(_.toDouble))\n }.cache()\n```", "```scala\nval myGM = new GaussianMixture()\n .setK(4 ) // default value is 2, LF, LB, RF, RB\n .setConvergenceTol(0.01) // using the default value\n .setMaxIterations(100) // max 100 iteration\n```", "```scala\nval model = myGM.run(trainingData)\n```", "```scala\nprintln(\"Model ConvergenceTol: \"+ myGM.getConvergenceTol)\n println(\"Model k:\"+myGM.getK)\n println(\"maxIteration:\"+myGM.getMaxIterations)\n\n for (i <- 0 until model.k) {\n println(\"weight=%f\\nmu=%s\\nsigma=\\n%s\\n\" format\n (model.weights(i), model.gaussians(i).mu, model.gaussians(i).sigma))\n }\n```", "```scala\nprintln(\"Cluster labels (first <= 50):\")\n val clusterLabels = model.predict(trainingData)\n clusterLabels.take(50).foreach { x =>\n *print*(\" \" + x)\n }\n```", "```scala\nCluster labels (first <= 50):\n 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter8\n```", "```scala\n import org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.clustering.PowerIterationClustering\n import org.apache.spark.sql.SparkSession\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.*ERROR*)\n```", "```scala\n// setup SparkSession to use for interactions with Sparkval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"myPowerIterationClustering\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval trainingData =spark.sparkContext.parallelize(*List*(\n (0L, 1L, 1.0),\n (0L, 2L, 1.0),\n (0L, 3L, 1.0),\n (1L, 2L, 1.0),\n (1L, 3L, 1.0),\n (2L, 3L, 1.0),\n (3L, 4L, 0.1),\n (4L, 5L, 1.0),\n (4L, 15L, 1.0),\n (5L, 6L, 1.0),\n (6L, 7L, 1.0),\n (7L, 8L, 1.0),\n (8L, 9L, 1.0),\n (9L, 10L, 1.0),\n (10L,11L, 1.0),\n (11L, 12L, 1.0),\n (12L, 13L, 1.0),\n (13L,14L, 1.0),\n (14L,15L, 1.0)\n ))\n```", "```scala\nval pic = new PowerIterationClustering()\n .setK(3)\n .setMaxIterations(15)\n```", "```scala\nval model = pic.run(trainingData)\n```", "```scala\nmodel.assignments.foreach { a =>\n println(s\"${a.id} -> ${a.cluster}\")\n }\n```", "```scala\nval clusters = model.assignments.collect().groupBy(_.cluster).mapValues(_.map(_.id))\n val assignments = clusters.toList.sortBy { case (k, v) => v.length }\n val assignmentsStr = assignments\n .map { case (k, v) =>\n s\"$k -> ${v.sorted.mkString(\"[\", \",\", \"]\")}\" }.mkString(\", \")\n val sizesStr = assignments.map {\n _._2.length\n }.sorted.mkString(\"(\", \",\", \")\")\n println(s\"Cluster assignments: $assignmentsStr\\ncluster sizes: $sizesStr\")\n```", "```scala\nCluster assignments: 1 -> [12,14], 2 -> [4,6,8,10], 0 -> [0,1,2,3,5,7,9,11,13,15]\n cluster sizes: (2,4,10)\n```", "```scala\nspark.stop()\n```", "```scala\nnew PowerIterationClustering().setK(3).setMaxIterations(15)\n```", "```scala\nval model = pic.run(trainingData)\n```", "```scala\npackage spark.ml.cookbook.chapter8\n```", "```scala\nimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SparkSessionimport org.apache.spark.ml.clustering.LDA\n```", "```scala\nval spark = SparkSession .builder.master(\"local[*]\") .appName(\"MyLDA\") .config(\"spark.sql.warehouse.dir\", \".\") .getOrCreate()\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.clustering.StreamingKMeans\n import org.apache.spark.mllib.linalg.Vectors\n import org.apache.spark.mllib.regression.LabeledPoint\n import org.apache.spark.sql.SparkSession\n import org.apache.spark.streaming.{Seconds, StreamingContext}\n```", "```scala\nval trainingDir = \"../data/sparkml2/chapter8/trainingDir\" val testDir = \"../data/sparkml2/chapter8/testDir\" val batchDuration = 10\n val numClusters = 2\n val numDimensions = 3\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"myStreamingKMeans\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval ssc = new StreamingContext(spark.sparkContext, Seconds(batchDuration.toLong))\n```", "```scala\nval trainingData = ssc.textFileStream(trainingDir).map(Vectors.parse)\n val testData = ssc.textFileStream(testDir).map(LabeledPoint.parse)\n```", "```scala\nval model = new StreamingKMeans()\n .setK(numClusters)\n .setDecayFactor(1.0)\n .setRandomCenters(numDimensions, 0.0)\n```", "```scala\nmodel.trainOn(trainingData)\n model.predictOnValues(testData.map(lp => (lp.label, lp.features))).print()\n```", "```scala\nssc.start()\n ssc.awaitTermination()\n```"]