["```scala\npackage com.chapter16.SparkTesting\nobject SimpleScalaTest {\n  def main(args: Array[String]):Unit= {\n    val a = 5\n    val b = 5\n    assert(a == b)\n      println(\"Assertion success\")       \n  }\n}\n```", "```scala\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nobject SimpleScalaTest {\n  def main(args: Array[String]):Unit= {\n    val a = 5\n    val b = 4\n    assert(a == b)\n      println(\"Assertion success\")       \n  }\n}\n```", "```scala\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nobject AssertResult {\n  def main(args: Array[String]):Unit= {\n    val x = 10\n    val y = 6\n    assertResult(3) {\n      x - y\n    }\n  }\n}\n```", "```scala\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nobject ExpectedException {\n  def main(args: Array[String]):Unit= {\n    val s = \"Hello world!\"\n    try {\n      s.charAt(0)\n      fail()\n    } catch {\n      case _: IndexOutOfBoundsException => // Expected, so continue\n    }\n  }\n}\n```", "```scala\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._\nobject ExpectedException {\n  def main(args: Array[String]):Unit= {\n    val s = \"Hello world!\"\n    try {\n      s.charAt(-1)\n      fail()\n    } catch {\n      case _: IndexOutOfBoundsException => // Expected, so continue\n    }\n  }\n}\n```", "```scala\nassertDoesNotCompile(\"val a: String = 1\")\n```", "```scala\nassertTypeError(\"val a: String = 1\")\n```", "```scala\nassertCompiles(\"val a: Int = 1\")\n```", "```scala\npackage com.chapter16.SparkTesting\nimport org.scalatest.Assertions._ \nobject CompileOrNot {\n  def main(args: Array[String]):Unit= {\n    assertDoesNotCompile(\"val a: String = 1\")\n    println(\"assertDoesNotCompile True\")\n\n    assertTypeError(\"val a: String = 1\")\n    println(\"assertTypeError True\")\n\n    assertCompiles(\"val a: Int = 1\")\n    println(\"assertCompiles True\")\n\n    assertDoesNotCompile(\"val a: Int = 1\")\n    println(\"assertDoesNotCompile True\")\n  }\n}\n```", "```scala\npackage com.chapter16.SparkTestingimport org.apache.spark._import org.apache.spark.sql.SparkSessionclass wordCounterTestDemo {  val spark = SparkSession    .builder    .master(\"local[*]\")    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")    .appName(s\"OneVsRestExample\")    .getOrCreate()  def myWordCounter(fileName: String): Long = {    val input = spark.sparkContext.textFile(fileName)    val counts = input.flatMap(_.split(\" \")).distinct()    val counter = counts.count()    counter  }}\n```", "```scala\npackage com.chapter16.SparkTesting\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.SparkSession\nclass wordCountRDD {\n  def prepareWordCountRDD(file: String, spark: SparkSession): RDD[(String, Int)] = {\n    val lines = spark.sparkContext.textFile(file)\n    lines.flatMap(_.split(\" \")).map((_, 1)).reduceByKey(_ + _)\n  }\n}\n```", "```scala\npackage com.chapter16.SparkTesting\nimport org.scalatest.{ BeforeAndAfterAll, FunSuite }\nimport org.scalatest.Assertions._\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.rdd.RDD\nclass wordCountTest2 extends FunSuite with BeforeAndAfterAll {\n  var spark: SparkSession = null\n  def tokenize(line: RDD[String]) = {\n    line.map(x => x.split(' ')).collect()\n  }\n  override def beforeAll() {\n    spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n      .appName(s\"OneVsRestExample\")\n      .getOrCreate()\n  }  \n  test(\"Test if two RDDs are equal\") {\n    val input = List(\"To be,\", \"or not to be:\", \"that is the question-\", \"William Shakespeare\")\n    val expected = Array(Array(\"To\", \"be,\"), Array(\"or\", \"not\", \"to\", \"be:\"), Array(\"that\", \"is\", \"the\", \"question-\"), Array(\"William\", \"Shakespeare\"))\n    val transformed = tokenize(spark.sparkContext.parallelize(input))\n    assert(transformed === expected)\n  }  \n  test(\"Test for word count RDD\") {\n    val fileName = \"C:/Users/rezkar/Downloads/words.txt\"\n    val obj = new wordCountRDD\n    val result = obj.prepareWordCountRDD(fileName, spark)    \n    assert(result.count() === 214)\n  }\n  override def afterAll() {\n    spark.stop()\n  }\n}\n```", "```scala\ntest(\"Test for word count RDD\") { \n  val fileName = \"data/words.txt\"\n  val obj = new wordCountRDD\n  val result = obj.prepareWordCountRDD(fileName, spark)    \n  assert(result.count() === 210)\n}\ntest(\"Test if two RDDs are equal\") {\n  val input = List(\"To be\", \"or not to be:\", \"that is the question-\", \"William Shakespeare\")\n  val expected = Array(Array(\"To\", \"be,\"), Array(\"or\", \"not\", \"to\", \"be:\"), Array(\"that\", \"is\", \"the\", \"question-\"), Array(\"William\", \"Shakespeare\"))\n  val transformed = tokenize(spark.sparkContext.parallelize(input))\n  assert(transformed === expected)\n}\n```", "```scala\n17/02/26 13:22:00 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path java.io.IOException: Could not locate executable null\\bin\\winutils.exe in the Hadoop binaries.\n```", "```scala\nspark.executor.extraJavaOptions=-Dlog4j.configuration=file:/usr/local/spark-2.1.1/conf/log4j.properties spark.driver.extraJavaOptions=-Dlog4j.configuration=file:/usr/local/spark-2.1.1/conf/log4j.properties\n```", "```scala\npackage com.chapter14.Serilazition\nimport org.apache.log4j.LogManager\nimport org.apache.log4j.Level\nimport org.apache.spark.sql.SparkSession\nobject myCustomLog {\n  def main(args: Array[String]): Unit = {   \n    val log = LogManager.getRootLogger    \n    //Everything is printed as INFO once the log level is set to INFO untill you set the level to new level for example WARN. \n    log.setLevel(Level.INFO)\n    log.info(\"Let's get started!\")    \n    // Setting logger level as WARN: after that nothing prints other than WARN\n    log.setLevel(Level.WARN)    \n    // Creating Spark Session\n    val spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n      .appName(\"Logging\")\n      .getOrCreate()\n    // These will note be printed!\n    log.info(\"Get prepared!\")\n    log.trace(\"Show if there is any ERROR!\")\n    //Started the computation and printing the logging information\n    log.warn(\"Started\")\n    spark.sparkContext.parallelize(1 to 20).foreach(println)\n    log.warn(\"Finished\")\n  }\n}\n```", "```scala\n17/05/13 16:39:14 INFO root: Let's get started!\n17/05/13 16:39:15 WARN root: Started\n4 \n1 \n2 \n5 \n3 \n17/05/13 16:39:16 WARN root: Finished\n```", "```scala\nlog4j.logger.org=OFF\n```", "```scala\nobject myCustomLogger {\n  def main(args: Array[String]):Unit= {\n    // Setting logger level as WARN\n    val log = LogManager.getRootLogger\n    log.setLevel(Level.WARN)\n    // Creating Spark Context\n    val conf = new SparkConf().setAppName(\"My App\").setMaster(\"local[*]\")\n    val sc = new SparkContext(conf)\n    //Started the computation and printing the logging information\n    //log.warn(\"Started\")\n    val i = 0\n    val data = sc.parallelize(i to 100000)\n    data.map{number =>\n      log.info(“My number”+ i)\n      number.toString\n    }\n    //log.warn(\"Finished\")\n  }\n}\n```", "```scala\norg.apache.spark.SparkException: Job aborted due to stage failure: Task not serializable: java.io.NotSerializableException: ...\nException in thread \"main\" org.apache.spark.SparkException: Task not serializable \nCaused by: java.io.NotSerializableException: org.apache.log4j.spi.RootLogger\nSerialization stack: object not serializable\n```", "```scala\nclass MyMapper(n: Int) extends Serializable {\n  @transient lazy val log = org.apache.log4j.LogManager.getLogger(\"myLogger\")\n  def logMapper(rdd: RDD[Int]): RDD[String] =\n    rdd.map { i =>\n      log.warn(\"mapping: \" + i)\n      (i + n).toString\n    }\n  }\n```", "```scala\n//Companion object \nobject MyMapper {\n  def apply(n: Int): MyMapper = new MyMapper(n)\n}\n```", "```scala\n//Main object\nobject myCustomLogwithClosureSerializable {\n  def main(args: Array[String]) {\n    val log = LogManager.getRootLogger\n    log.setLevel(Level.WARN)\n    val spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n      .appName(\"Testing\")\n      .getOrCreate()\n    log.warn(\"Started\")\n    val data = spark.sparkContext.parallelize(1 to 100000)\n    val mapper = MyMapper(1)\n    val other = mapper.logMapper(data)\n    other.collect()\n    log.warn(\"Finished\")\n  }\n```", "```scala\nclass MultiplicaitonOfTwoNumber {\n  def multiply(a: Int, b: Int): Int = {\n    val product = a * b\n    product\n  }\n}\n```", "```scala\nval myRDD = spark.sparkContext.parallelize(0 to 1000)\n    myRDD.foreachPartition(s => {\n      val notSerializable = new MultiplicaitonOfTwoNumber\n      println(notSerializable.multiply(s.next(), s.next()))\n    })\n```", "```scala\npackage com.chapter16.SparkTesting\nimport org.apache.spark.sql.SparkSession\nclass MultiplicaitonOfTwoNumber {\n  def multiply(a: Int, b: Int): Int = {\n    val product = a * b\n    product\n  }\n}\n```", "```scala\n\nobject MakingTaskSerilazible {\n  def main(args: Array[String]): Unit = {\n    val spark = SparkSession\n      .builder\n      .master(\"local[*]\")\n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\")\n      .appName(\"MakingTaskSerilazible\")\n      .getOrCreate()\n val myRDD = spark.sparkContext.parallelize(0 to 1000)\n    myRDD.foreachPartition(s => {\n      val notSerializable = new MultiplicaitonOfTwoNumber\n      println(notSerializable.multiply(s.next(), s.next()))\n    })\n  }\n}\n```", "```scala\n0\n5700\n1406\n156\n4032\n7832\n2550\n650\n```", "```scala\n--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000\n```", "```scala\nYARN_OPTS=\"-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=4000 $YARN_OPTS\"\n```", "```scala\n$ export SPARK_SUBMIT_OPTS=-agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000\n```", "```scala\n$ SPARK_HOME/bin/spark-submit \\\n--class \"com.chapter13.Clustering.KMeansDemo\" \\\n--master yarn \\\n--deploy-mode cluster \\\n--driver-memory 16g \\\n--executor-memory 4g \\\n--executor-cores 4 \\\n--queue the_queue \\\n--num-executors 1\\\n--executor-cores 1 \\\n--conf \"spark.executor.extraJavaOptions=-agentlib:jdwp=transport=dt_socket,server=n,address= host_name_to_your_computer.org:4000,suspend=n\" \\\n--driver-java-options -agentlib:jdwp=transport=dt_socket,server=y,suspend=y,address=4000 \\\n KMeans-0.0.1-SNAPSHOT-jar-with-dependencies.jar \\\nSaratoga_NY_Homes.txt\n```", "```scala\n$ export SPARK_WORKER_OPTS=\"-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n\"\n$ export SPARK_MASTER_OPTS=\"-Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=4000,suspend=n\"\n```", "```scala\n$ SPARKH_HOME/sbin/start-master.sh\n```", "```scala\nobject DebugTestSBT {  def main(args: Array[String]): Unit = {    val spark = SparkSession      .builder      .master(\"local[*]\")      .config(\"spark.sql.warehouse.dir\", \"C:/Exp/\")      .appName(\"Logging\")      .getOrCreate()          spark.sparkContext.setCheckpointDir(\"C:/Exp/\")    println(\"-------------Attach debugger now!--------------\")    Thread.sleep(8000)    // code goes here, with breakpoints set on the lines you want to pause  }}\n```"]