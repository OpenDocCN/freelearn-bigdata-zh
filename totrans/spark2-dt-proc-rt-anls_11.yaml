- en: Spark's Three Data Musketeers for Machine Learning - Perfect Together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs with Spark 2.0 using internal data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating RDDs with Spark 2.0 using external data sources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with Spark 2.0 using the filter() API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with the super useful flatMap() API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with set operation APIs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RDD transformation/aggregation with groupBy() and reduceByKey()
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with the zip() API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join transformation with paired key-value RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce and grouping transformation with paired key-value RDDs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating DataFrames from Scala data structures
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Operating on DataFrames programmatically without SQL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loading DataFrames and setup from an external source ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The three workhorses of Spark for efficient processing of data at scale are
    RDD, DataFrames, and the Dataset API. While each can stand on its own merit, the
    new paradigm shift favors Dataset as the unifying data API to meet all data wrangling
    needs in a single interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'The new Spark 2.0 Dataset API is a type-safe collection of domain objects that
    can be operated on via transformation (similar to RDDs'' filter, `map`, `flatMap()`,
    and so on) in parallel using functional or relational operations. For backward
    compatibility, Dataset has a view called **DataFrame**, which is a collection
    of rows that are untyped. In this chapter, we demonstrate all three API sets.
    The figure ahead summarizes the pros and cons of the key components of Spark for
    data wrangling:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b88f07c4-3036-4fdf-8d48-86a5b1c8c63f.png)'
  prefs: []
  type: TYPE_IMG
- en: An advanced developer in machine learning must understand and be able to use
    all three API sets without any issues, for algorithmic augmentation or legacy
    reasons. While we recommend that every developer should migrate toward the high-level
    Dataset API, you will still need to know RDDs for programming against the Spark
    core system. For example, it is very common for investment banking and hedge funds
    to read leading journals in machine learning, mathematical programming, finance,
    statistics, or artificial intelligence and then code the research in low-level
    APIs to gain competitive advantage.
  prefs: []
  type: TYPE_NORMAL
- en: RDDs - what started it all...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The RDD API is a critical toolkit for Spark developers since it favors low-level
    control over the data within a functional programming paradigm. What makes RDDs
    powerful also makes it harder to work with for new programmers. While it may be
    easy to understand the RDD API and manual optimization techniques (for example,
    `filter()` before a `groupBy()` operation), writing advanced code would require
    consistent practice and fluency.
  prefs: []
  type: TYPE_NORMAL
- en: When data files, blocks, or data structures are converted to RDDs, the data
    is broken down into smaller units called **partitions** (similar to splits in
    Hadoop) and distributed among the nodes so they can be operated on in parallel
    at the same time. Spark provides this functionality right out ...
  prefs: []
  type: TYPE_NORMAL
- en: DataFrame - a natural evolution to unite API and SQL via a high-level API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Spark developer community has always strived to provide an easy-to-use high-level
    API for the community starting from the AMPlab days at Berkley. The next evolution
    in the Data API materialized when Michael Armbrust gave the community the SparkSQL
    and Catalyst optimizer, which made data virtualization possible with Spark using
    a simple and well-understood SQL interface. The DataFrame API was a natural evolution
    to take advantage of SparkSQL by organizing data into named columns like relational
    tables.
  prefs: []
  type: TYPE_NORMAL
- en: The DataFrame API made data wrangling via SQL available to a multitude of data
    scientists and developers familiar with DataFrames in R (data.frame) or Python/Pandas
    (pandas.DataFrame).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset - a high-level unifying Data API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A dataset is an immutable collection of objects which are modelled/mapped to
    a traditional relational schema. There are four attributes that distinguish it
    as the preferred method going forward. We particularly find the Dataset API appealing
    since we find it familiar to RDDs with the usual transformational operators (for
    example, `filter()`, `map()`, `flatMap()`, and so on). The Dataset will follow
    a lazy execution paradigm similar to RDD. The best way to try to reconcile DataFrames
    and Datasets is to think of a DataFrame as an alias that can be thought of as
    `Dataset[Row]`.
  prefs: []
  type: TYPE_NORMAL
- en: '**Strong type safety**: We now have both compile-time (syntax errors) and runtime
    safety in a unified Data API, which helps the ML developer ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating RDDs with Spark 2.0 using internal data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are four ways to create RDDs in Spark. They range from the `parallelize()`
    method for simple testing and debugging within the client driver code to streaming
    RDDs for near-realtime responses. In this recipe, we provide you with several
    examples to demonstrate RDD creation using internal sources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data held in the client driver is parallelized and distributed across the
    cluster using the number of portioned RDDs (the second parameter) as the guideline.
    The resulting RDD is the magic of Spark that started it all (refer to Matei Zaharia's
    original white paper).
  prefs: []
  type: TYPE_NORMAL
- en: The resulting RDDs are now fully distributed data structures with fault tolerance
    and lineage that can be operated on in parallel using Spark framework.
  prefs: []
  type: TYPE_NORMAL
- en: We read a text file `A Tale of Two Cities by Charles Dickens` from [http://www.gutenberg.org/](http://www.gutenberg.org/) into
    Spark RDDs. We then proceed to split and tokenize the data and print the number
    of total words using Spark's operators (for example, `map`, `flatMap()`, and so
    on).
  prefs: []
  type: TYPE_NORMAL
- en: Creating RDDs with Spark 2.0 using external data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we provide you with several examples to demonstrate RDD creation
    using external sources.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: We obtain the data from the Gutenberg project. This is a great source for accessing
    actual text, ranging from the complete works of *Shakespeare* to *Charles Dickens*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Download the text from the following sources and store it in your local directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Source: [http://www.gutenberg.org](http://www.gutenberg.org)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selected book: *A Tale of Two Cities by Charles Dickens*'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'URL: [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Once again, we use `SparkContext`, available via `SparkSession`, and its function
    `textFile()` to read the external data source and parallelize it across the cluster.
    Remarkably, all the work is done for the developer behind the scenes by Spark
    using one single call to load a wide variety of formats (for example, text, S3,
    and HDFS), which parallelizes the data across the cluster using the `protocol:filepath`
    combination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To demonstrate, we load the book, which is stored as ASCII, text using the `textFile()`
    method from `SparkContext` via `SparkSession`, which, in turn, goes to work behind
    the scenes and creates portioned RDDs across the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Even though we have not covered the Spark transformation operator, we'll look
    at a small code snippet which will break the file into words using blanks as a
    separator. In a real-life situation, a regular expression will be needed to cover
    all the edge cases with all the whitespace variations (refer to the *Transforming
    RDDs with Spark using filter() APIs* recipe in this chapter).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We use a lambda function to receive each line as it is read and split it into
    words using blanks as separator.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: We use a flatMap to break the array of lists of words (that is, each group of
    words from a line corresponds to a distinct array/list for that line). In short,
    what we want is a list of words and not a list of a list of words for each line.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We read a text file `A Tale of Two Cities by Charles Dickens` from [http://www.gutenberg.org/](http://www.gutenberg.org/) into
    an RDD and then proceed to tokenize the words by using whitespace as the separator
    in a lambda expression using `.split()` and `.flatmap()` of RDD itself. We then
    proceed to use the `.count()` method of RDDs to output the total number of words.
    While this is simple, you have to bear in mind that the operation takes place
    using the distributed parallel framework of Spark with only a couple of lines.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Creating RDDs with external data sources, whether it is a text file, Hadoop
    HDFS, sequence file, Casandra, or Parquet file is remarkably simple. Once again,
    we use `SparkSession` (`SparkContext` prior to Spark 2.0) to get a handle to the
    cluster. Once the function (for example, textFile Protocol: file path) is executed,
    the data is broken into smaller pieces (partitions) and automatically flows to
    the cluster, which becomes available to the computations as fault-tolerant distributed
    collections that can be operated on in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of variations that one must consider when working with real-life
    situations. The best advice based on our own experience is to consult the documentation
    before writing your own functions or connectors. Spark either supports your data
    source right out of the box, or the vendor has a connector that can be downloaded
    to do the same.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Another situation that we often see is many small files that are generated (usually
    within `HDFS` directories) that need to be parallelized as RDDs for consumption.
    `SparkContext` has a method named `wholeTextFiles()` which lets you read a directory
    containing multiple files and returns each of them as (filename, content) key-value
    pairs. We found this to be very useful in multi-stage machine learning situations
    using lambda architecture, where the model parameters are calculated as a batch
    and then updated in Spark every day.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In this example, we read multiple files and then print the first file for examination.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `spark.sparkContext.wholeTextFiles()` function is used to read a large
    number of small files and present them as (K,V), or key-value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark documentation for the `textFile()` and `wholeTextFiles()` functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
  prefs: []
  type: TYPE_NORMAL
- en: The `textFile()` API is a single abstraction for interfacing to external data
    sources. The formulation of protocol/path is enough to invoke the right decoder.
    We'll demonstrate reading from an ASCII text file, Amazon AWS S3, and HDFS with
    code snippets that the user would leverage to build their own system.
  prefs: []
  type: TYPE_NORMAL
- en: The path can be expressed as a simple path (for example, local text file) to
    a complete URI with the required protocol (for example, s3n for AWS storage buckets)
    to complete resource path with server and port configuration (for example, to
    read HDFS file from a Hadoop cluster). ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with Spark 2.0 using the filter() API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the `filter()` method of RDD which is used to select
    a subset of the base RDD and return a new filtered RDD. The format is similar
    to `map()`, but a lambda function selects which members are to be included in
    the resulting RDD.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `filter()` API is demonstrated using several examples. In the first example
    we went through an RDD and output odd numbers by using a lambda expression `.filter
    ( i => (i%2) == 1)` which takes advantage of the mod (modulus) function.
  prefs: []
  type: TYPE_NORMAL
- en: In the second example we made it a bit interesting by mapping the result to
    a square function using a lambda expression `num.map(pow(_,2)).filter(_ %2 ==
    1)`.
  prefs: []
  type: TYPE_NORMAL
- en: In the third example, we went through the text and filtered out short lines
    (for example, lines under 30 character) using the lambda expression `.filter(_.length
    < 30).filter(_.length > 0)` to print short versus total number of lines (`.count()`
    ) as output.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The `filter()` API walks through the parallelized distributed collection (that
    is, RDDs) and applies the selection criteria supplied to `filter()` as a lambda
    in order to include or exclude the element from the resulting RDD. The combination
    uses `map()`, which transforms each element and `filter()`, which selects a subset
    is a powerful combination in Spark ML programming.
  prefs: []
  type: TYPE_NORMAL
- en: We will see later with the `DataFrame` API how a similar `Filter()` API can
    be used to achieve the same effect using a higher-level framework used in R and
    Python (pandas).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `.filter()`, which is a method call of RDD, is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `BloomFilter()`--for the sake of completeness, be aware that
    there is also a bloom filter function already in existence and it is suggested
    that you avoid coding by yourselves. The link for this same is [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.util.sketch.BloomFilter).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with the super useful flatMap() API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine the `flatMap()` method which is often a source of
    confusion for beginners; however, on closer examination we demonstrate that it
    is a clear concept that applies the lambda function to each element just like
    map, and then flattens the resulting RDD as a single structure (rather than having
    a list of lists, we create a single list made of all sublist with sublist elements).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `textFile()` function to create the initial (that is, base RDD) from
    our text file that we downloaded earlier from [http://www.gutenberg.org/cache/epub/98/pg98.txt](http://www.gutenberg.org/cache/epub/98/pg98.txt):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the map function to the RDDs to demonstrate the `map()` function transformation.
    To start with, we are doing it the wrong way to make a point: we first attempt
    to separate all the words based on the regular expression *[\s\W]+]* using just
    `map()` to demonstrate that the resulting RDD is a list of lists in which each
    list corresponds to a line and the tokenized word within that line. This example
    demonstrates what could cause confusion for beginners when using `flatMap()`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The following line trims each line and then splits the line into words. The
    resulting RDD (that is, wordRDD2) will be a list of lists of words rather than
    a single list of words for the whole file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: On running the previous code, you will get the following output.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: We use the `flatMap()` method to not only map, but also flatten the list of
    lists so we end up with an RDD which is made of words themselves. We trim and
    split the words (that is, tokenize) and then filter for words greater than zero
    and then map it to upper case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In this case, after flattening the list using `flatMap()`, we can get a list
    of the words back as expected.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this short example, we read a text file and then split the words (that is,
    tokenize it) using the `flatMap(_.trim.split("""[\s\W]+""")` lambda expression
    to have a single RDD with the tokenized content. Additionally we use the `filter
    ()` API `filter(_.length > 0)` to exclude the empty lines and the lambda expression
    `.map(_.toUpperCase())` in a `.map()` API to map to uppercase before outputting
    the results.
  prefs: []
  type: TYPE_NORMAL
- en: There are cases where we do not want to get a list back for every element of
    base RDD (for example, get a list for words corresponding to a line). We sometimes
    prefer to have a single flattened list that is flat and corresponds to every word
    in the document. In short, rather than a list of lists, we want a single list
    containing ...
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The function `glom()` is a function that lets you model each partition in the
    RDD as an array rather than a row list. While it is possible to produce the results
    in most cases, `glom()` allows you to reduce the shuffling between partitions.
  prefs: []
  type: TYPE_NORMAL
- en: While at the surface, both method 1 and 2 mentioned in the text below look similar
    for calculating the minimum numbers in an RDD, the `glom()` function will cause
    much less data shuffling across the network by first applying `min()` to all the
    partitions, and then sending over the resulting data. The best way to see the
    difference is to use this on 10M+ RDDs and watch the IO and CPU usage accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The first method is to find the minimum value without using `glom()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the preceding code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The second method is to find the minimum value using `glom(`, which causes a
    local application of the min function to a partition and then sends the results
    across via a shuffle.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the preceding code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `flatMap()`, `PairFlatMap()`, and other variations under RDD
    is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the `FlatMap()` function under RDD is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.FlatMapFunction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the `PairFlatMap()` function - very handy variation for paired
    data elements is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction](http://spark.apache.org/docs/2.0.0/api/scala/index.html#org.apache.spark.api.java.function.PairFlatMapFunction)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `flatMap()` method applies the supplied function (lambda or named function
    via def) to every element, flattens the structure, and produces a new RDD.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming RDDs with set operation APIs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore set operations on RDDs, such as `intersection()`,
    `union()`, `subtract(),` and `distinct()` and `Cartesian()`. Let's implement the
    usual set operations in a distributed manner.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we started with three sets of number Arrays (odd, even, and
    their combo) and then proceeded to pass them as parameters into the set operation
    API. We covered how to use `intersection()`, `union()`, `subtract()`, `distinct()`,
    and `cartesian()` RDD operators.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the RDD set operators are easy to use, one must be careful with the data
    shuffling that Spark has to perform in the background to complete some of these
    operations (for example, intersection).
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that the union operator does not remove duplicates from the
    resulting RDD set.
  prefs: []
  type: TYPE_NORMAL
- en: RDD transformation/aggregation with groupBy() and reduceByKey()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the `groupBy()` and `reduceBy()` methods, which allow
    us to group values corresponding to a key. It is an expensive operation due to
    internal shuffling. We first demonstrate `groupby()` in more detail and then cover
    `reduceBy()` to show the similarity in coding these while stressing the advantage
    of the `reduceBy()` operator.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we created numbers one through twelve and placed them in three
    partitions. We then proceeded to break them into odd/even using a simple mod operation
    while. The `groupBy()` is used to aggregate them into two groups of odd/even.
    This is a typical aggregation problem that should look familiar to SQL users.
    Later in this chapter, we revisit this operation using `DataFrame` which also
    takes advantage of the better optimization techniques provided by the SparkSQL
    engine. In the later part, we demonstrate the similarity of `groupBy()` and `reduceByKey()`.
    We set up an array of alphabets (that is, `a` and `b`) and then convert them into
    RDD. We then proceed to aggregate them based on key (that is, unique letters -
    only two in this case) and print the total in each group.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given the direction for Spark which favors the Dataset/DataFrame paradigm over
    low-level RDD coding, one must seriously consider the reasoning for doing `groupBy()`
    on an RDD. While there are legitimate situations for which the operation is needed,
    the readers are advised to reformulate their solution to take advantage of the
    SparkSQL subsystem and its optimizer called **Catalyst**.
  prefs: []
  type: TYPE_NORMAL
- en: The Catalyst optimizer takes into account Scala's powerful features such as
    **pattern matching** and **quasiquotes** while building an optimized query plan.
  prefs: []
  type: TYPE_NORMAL
- en: The documentation on Scala pattern matching is available at [http://docs.scala-lang.org/tutorials/tour/pattern-matching.html](http://docs.scala-lang.org/tutorials/tour/pattern-matching.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The documentation on Scala quasiquotes is available at [http://docs.scala-lang.org/overviews/quasiquotes/intro.html
    ...](http://docs.scala-lang.org/overviews/quasiquotes/intro.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for `groupBy()` and `reduceByKey()` operations under RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)'
  prefs: []
  type: TYPE_NORMAL
- en: Transforming RDDs with the zip() API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe we explore the `zip()` function. For those of us working in Python
    or Scala, `zip()` is a familiar method that lets you pair items before applying
    an inline function. Using Spark, it can be used to facilitate RDD arithmetic between
    pairs. Conceptually, it combines the two RDDs in such a way that each member of
    one RDD is paired with the second RDD that occupies the same position (that is,
    it lines up the two RDDs and makes pairs out of the members).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirements.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: Set up the data structures and RDD for the example. In this example we create
    two RDDs from `Array[]` and let Spark decide on the number of partitions (that
    is, the second parameter in the `parallize()` method is not set).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `zip()` function to the RDDs to demonstrate the transformation.
    In the example, we take the partitioned RDD of ranges and label them as odd/even
    using the mod function. We use the `zip()` function to pair elements from the
    two RDDs (SignalNoiseRDD and SignalStrengthRDD) so we can apply a `map()` function
    and compute their ratio (noise to signal ratio). We can use this technique to
    perform almost all types of arithmetic or non-arithmetic operations involving
    individual members of two RDDs.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The pairing of two RDD members act as a tuple or a row. The individual members
    of the pair created by `zip()` can be accessed by their position (for example,
    `._1` and `._2`)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we first set up two arrays representing signal noise and signal
    strength. They are simply a set of measured numbers that we could have received
    from the IoT platform. We then proceeded to pair the two separate arrays so each
    member looks like they have been input originally as a pair of (x, y). We then
    proceed to divide the pair and produce the noise to signal ratio using the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: The `zip()` method has many variations that involve partitions. The developers
    should familiarize themselves with variations of the `zip()` method with partition
    (for example, `zipPartitions`).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `zip()` and `zipPartitions()` operations under RDD is available
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Join transformation with paired key-value RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we introduce the `KeyValueRDD` pair RDD and the supporting join
    operations such as `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`
    as an alternative to the more traditional and more expensive set operations available
    via the set operation API, such as `intersection()`, `union()`, `subtraction()`,
    `distinct()`, `cartesian()`, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: We'll demonstrate `join()`, `leftOuterJoin` and `rightOuterJoin()`, and `fullOuterJoin()`,
    to explain the power and flexibility of key-value pair RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Set up the data structures and RDD for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Turn the List into RDDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: We can access the `keys` and `values` inside a pair RDD.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `mapValues()` function to the pair RDDs to demonstrate the transformation.
    In this example we use the map function to lift up the value by adding 100 to
    every element. This is a popular technique to introduce noise to the data (that
    is, jittering).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `join()` function to the RDDs to demonstrate the transformation.
    We use `join()` to join the two RDDs. We join the two RDDs based on keys (that
    is, north, south, and so on).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `leftOuterJoin()` function to the RDDs to demonstrate the transformation.
    The `leftOuterjoin` acts like a relational left outer join. Spark replaces the
    absence of a membership with `None` rather than `NULL`, which is common in relational
    systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: We'll apply `rightOuterJoin()` to the RDDs to demonstrate the transformation.
    This is similar to a right outer join in relational systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: We then apply the `fullOuterJoin()` function to the RDDs to demonstrate the
    transformation. This is similar to full outer join in relational systems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we declared three lists representing typical data available
    in relational tables, which could be imported using a connector to Casandra or
    RedShift (not shown here to simplify the recipe). We used two of the three lists
    representing city names (that is, data tables) and joined them with the first
    list, which represents directions (for example, defining tables). The first step
    is to define three lists of paired values. We then parallelized them into key-value
    RDDs so we can perform join operations between the first RDD (that is, directions)
    and the other two RDDs representing city names. We applied the join function to
    the RDDs to demonstrate the transformation.
  prefs: []
  type: TYPE_NORMAL
- en: We demonstrated `join()`, `leftOuterJoin` and `rightOuterJoin() ...`
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `join()` and its variations under RDD is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  prefs: []
  type: TYPE_NORMAL
- en: Reduce and grouping transformation with paired key-value RDDs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore reduce and group by key. The `reduceByKey()` and
    `groupbyKey()` operations are much more efficient and preferred to `reduce()`
    and `groupBy()` in most cases. The functions provide convenient facilities to
    aggregate values and combine them by key with less shuffling, which is problematic
    on large data sets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the data structures and RDD for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: We apply `groupByKey()` to demonstrate the transformation. In this example,
    we group all the buy and sell signals together while operating in a distributed
    setting.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: We apply the `reduceByKey()` function to the pair of RDDs to demonstrate the
    transformation. In this example, the function is, to sum up the total volume for
    the buy and sell signals. The Scala notation of `(_+_)` simply denotes adding
    two members at the time and producing a single result from it. Just like `reduce()`,
    we can apply any function (that is, inline for simple functions and named functions
    for more complex cases).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example we declared a list of items as being sold or purchased and their
    corresponding price (that is, typical commercial transaction). We then proceeded
    to calculate the sum using Scala shorthand notation `(_+_)`. In the last step,
    we provided the total for each key group (that is, `Buy` or `Sell`). The key-value
    RDD is a powerful construct that can reduce coding while providing the functionality
    needed to group paired values into aggregated buckets. The `groupByKey()` and
    `reduceByKey()` functions mimic the same aggregation functionality, while `reduceByKey()`
    is more efficient due to less shuffling of the data while final results are being
    assembled.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `groupByKey()` and `reduceByKey()` operations under RDD is
    available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD).
  prefs: []
  type: TYPE_NORMAL
- en: Creating DataFrames from Scala data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the `DataFrame` API, which provides a higher level
    of abstraction than RDDs for working with data. The API is similar to R and Python
    data frame facilities (pandas).
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrame` simplifies coding and lets you use standard SQL to retrieve and
    manipulate data. Spark keeps additional information about DataFrames, which helps
    the API to manipulate the frames with ease. Every `DataFrame` will have a schema
    (either inferred from data or explicitly defined) which allows us to view the
    frame like an SQL table. The secret sauce of SparkSQL and DataFrame is that the
    catalyst optimizer will work behind the scenes to optimize access by rearranging
    calls in the pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the Scala data structures as two `List()` objects and a sequence
    (that is, `Seq()`). We then proceed to turn the `List` structures into RDDs for
    conversion to `DataFrames` for the next steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: We take a list which is turned into an RDD using the `parallelize()` method
    and use the `toDF()` method of the RDD to turn it into a DataFrame. The `show()`
    method allows us to view the DataFrame, which is similar to a SQL table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output.:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: In the following code snippet, we take a generic Scala **Seq** (**Sequence**)
    data structure and use `createDataFrame()` explicitly to create a DataFrame while
    naming the columns at the same time.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: In the next two steps, we use the `show()` method to see the contents and then
    proceed to use `printscheme()` to show the inferred scheme based on types. In
    this example, the DataFrame correctly identified the integer and double in the
    Seq as the valid type for the two columns of numbers.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we took two lists and a Seq data structure and converted them
    to DataFrame and used `df1.show()` and `df1.printSchema()` to display contents
    and schema for the table.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames can be created from both internal and external sources. Just like
    SQL tables, the DataFrames have schemas associated with them that can either be
    inferred or explicitly defined using Scala case classes or the `map()` function
    to explicitly convert while ingesting the data.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure completeness, we include the `import` statement that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicit import statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example code for Spark 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: Operating on DataFrames programmatically without SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how to manipulate DataFrame with code and method
    calls only (without SQL). The DataFrames have their own methods that allow you
    to perform SQL-like operations using a programmatic approach. We demonstrate some
    of these commands such as `select()`, `show()`, and `explain()` to get the point
    across that the DataFrame itself is capable of wrangling and manipulating the
    data without using SQL.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up logging level for `log4j`. This step is optional,
    but we highly recommend it (change the level appropriately as you move through
    the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and error to cut down on output. See the
    previous step for package requirement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we loaded data from a text file into an RDD and then converted
    it to a DataFrame structure using the `.toDF()` API. We then proceeded to mimic
    SQL queries using built-in methods such as `select()`, `filter()`, `show()`, and
    `explain()` that help us to programmatically explore the data (no SQL). The `explain()`
    command shows the query plan which can be awfully useful to remove the bottleneck.
  prefs: []
  type: TYPE_NORMAL
- en: DataFrames provide multiple approaches to data wrangling.
  prefs: []
  type: TYPE_NORMAL
- en: For those comfortable with the DataFrame API and packages from R ([https://cran.r-project.org](https://cran.r-project.org))
    like dplyr or an older version, we have a programmatic API with an extensive set
    of methods that lets you do all your data wrangling via the API.
  prefs: []
  type: TYPE_NORMAL
- en: For those more comfortable with SQL, you can simply use SQL to retrieve and
    manipulate data as if you were using Squirrel or Toad to query the database.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits `import` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example `import` statement for Spark 2.0:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: Loading DataFrames and setup from an external source
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine data manipulation using SQL. Spark's approach to
    provide, both a pragmatic and SQL interface works very well in production settings
    in which we not only require machine learning, but also access to existing data
    sources using SQL to ensure compatibility and familiarity with existing SQL-based
    systems. DataFrame with SQL makes for an elegant process toward integration in
    real-life settings.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the imports related to DataFrame and the required data structures and
    create the RDDs as needed for the example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the logging level to warning and `Error` to cut down on output. See
    the previous step for package requirement:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Set up the Spark context and application parameter so Spark can run.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: We create the DataFrame corresponding to the `customer` file. In this step,
    we first create an RDD and then proceed to use the `toDF()` to convert the RDD
    to DataFrame and name the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Customer data contents for reference:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the preceding code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af5a9571-9dc1-4585-aa7e-1bc87e6ccb83.png)'
  prefs: []
  type: TYPE_IMG
- en: We create the DataFrame corresponding to the `product` file. In this step, we
    first create an RDD and then proceed to use the `toDF()` to convert the RDD to
    DataFrame and name the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert `prodRDD` to DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: Using SQL select, we display the contents of the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Product data contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6567c3b-fd23-443a-a849-5029f7a1bb74.png)'
  prefs: []
  type: TYPE_IMG
- en: We create the DataFrame corresponding to the `sales` file. In this step we first
    create an RDD and then proceed to use `toDF()` to convert the RDD to DataFrame
    and name the columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'We convert the `saleRDD` to DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: We use SQL select to display the table.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Sales data contents:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e7cfde8b-1d1c-4182-a185-a32bde43e9f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We print schemas for the customer, product, and sales DataFrames to verify
    schema after column definition and type conversion:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'On running the previous code, you will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we first loaded data into an RDD and then converted it into
    a DataFrame using the `toDF()` method. The DataFrame is very good at inferring
    types, but there are occasions that require manual intervention. We used the `map()`
    function after creating the RDD (lazy initialization paradigm applies) to massage
    the data either by type conversion or calling on more complicated user-defined
    functions (referenced in the `map()` method) to do the conversion or data wrangling.
    Finally, we proceeded to examine the schema for each of the three DataFrames using
    `show()` and `printSchema()`.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To ensure completeness, we include the `import` statements that we used prior
    to Spark 2.0.0 to run the code (namely, Spark 1.5.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you see any issues with implicit conversion, double check to make sure you
    have included the implicits `import` statement.
  prefs: []
  type: TYPE_NORMAL
- en: 'Example `import` statement for Spark 1.5.2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: Using DataFrames with standard SQL language - SparkSQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to use DataFrame SQL capabilities to perform
    basic CRUD operations, but there is nothing limiting you from using the SQL interface
    provided by Spark to any level of sophistication (that is, DML) desired.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: Set up the imports related to DataFrames and the required data structures and
    create the RDDs as needed for the example
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: Import the packages for setting up the logging level for `log4j`. This step
    is optional, but we highly recommend it (change the level appropriately as you
    move through the development cycle).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: Set up the logging level to warning and `ERROR` to cut down on output. See the
    previous step for package requirement.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic workflow for DataFrame using SQL is to first populate the DataFrame
    either through internal Scala data structures or via external data sources first,
    and then use the `createOrReplaceTempView()` call to register the DataFrame as
    a SQL-like artifact.
  prefs: []
  type: TYPE_NORMAL
- en: When you use DataFrames, you have the benefit of additional metadata that Spark
    stores (whether API or SQL approach) which can benefit you during the coding and
    execution.
  prefs: []
  type: TYPE_NORMAL
- en: While RDDs are still the workhorses of core Spark, the trend is toward the DataFrame
    approach which has successfully shown its capabilities in languages such as Python/Pandas
    or R.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There has been a change for registration of a DataFrame as a table. Refer to
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'For versions prior to Spark 2.0.0: `registerTempTable()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For Spark version 2.0.0 and previous: `createOrReplaceTempView()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pre-Spark 2.0.0 to register a DataFrame as a SQL table like artifact:'
  prefs: []
  type: TYPE_NORMAL
- en: Before we can use the DataFrame for queries via SQL, we have to register the
    DataFrame as a temp table so the SQL statements can refer to it without any Scala/Spark
    syntax. This step may cause confusion for many beginners as we are not creating
    any table (temp or permanent), but the call `registerTempTable()` creates a name
    in SQL land that the SQL statements can refer to without additional UDF or without
    any domain-specific query language.
  prefs: []
  type: TYPE_NORMAL
- en: Register the ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for DataFrame is available at [https://spark.apache.org/docs/latest/sql-programming-guide.html](https://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you see any issues with implicit conversion, please double check to make
    sure you have included implicits `import` statement.
  prefs: []
  type: TYPE_NORMAL
- en: Example `import` statement for Spark 1.5.2
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: DataFrame is an extensive subsystem and deserves an entire book on its own.
    It makes complex data manipulation at scale available to SQL programmers.
  prefs: []
  type: TYPE_NORMAL
- en: Working with the Dataset API using a Scala Sequence
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine the new Dataset and how it works with the *seq* Scala
    data structure. We often see a relationship between the LabelPoint data structure
    used with ML libraries and a Scala sequence (that is, seq data structure) that
    play nicely with dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The Dataset is being positioned as a unifying API going forward. It is important
    to note that DataFrame is still available as an alias described as `Dataset[Row]`.
    We have covered the SQL examples extensively via DataFrame recipes, so we concentrate
    our efforts on other variations for dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for a Spark session to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: Define a Scala `case class` to model data for processing, and the `Car` class
    will represent electric and hybrid cars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a Scala sequence and populate it with electric and hybrid cars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: Configure output level to `ERROR` to reduce Spark's logging output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Create a SparkSession yielding access to the Spark cluster, including the underlying
    session object attributes and functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Import Spark implicits, therefore adding in behavior with only an import.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will create a Dataset from the car data sequence utilizing the Spark
    session's `createDataset()` method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: Let's print out the results as confirmation that our method invocation transformed
    the sequence into a Spark Dataset by invoking the show method.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/a252a4cf-d0d2-4e9b-ba36-ebe5a76de2f0.png)'
  prefs: []
  type: TYPE_IMG
- en: Print out the Dataset's implied column names. We can now use class attribute
    names as column names.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: Let's show the automatically generated schema, and validate inferred data types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we will filter the Dataset on price referring to the `Car` class attribute
    price as a column and show results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/99ffee16-3167-451a-8586-308cf4d4321f.png)'
  prefs: []
  type: TYPE_IMG
- en: We close the program by stopping the Spark session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we introduced Spark's Dataset feature which first appeared in
    Spark 1.6 and which was further refined in subsequent releases. First, we created
    an instance of a Dataset from a Scala sequence with the help of the `createDataset()`
    method belonging to the Spark session. The next step was to print out meta information
    about the generated Datatset to establish that the creation transpired as expected.
    Finally, snippets of Spark SQL were used to filter the Dataset by the price column
    for any price greater than $50, 000.00 and show the final results of execution.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dataset has a view called [DataFrame](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/package.html#DataFrame=org.apache.spark.sql.Dataset%5Borg.apache.spark.sql.Row%5D),
    which is a Dataset of [row](https://spark.apache.org/docs/2.0.0/api/scala/org/apache/spark/sql/Row.html)s
    which is untyped. The Dataset still retains all the transformation abilities of
    RDD such as `filter()`, `map()`, `flatMap()`, and so on. This is one of the reasons
    we find Datasets easy to use if we have programmed in Spark using RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KeyValue grouped dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational grouped dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and using Datasets from RDDs and back again
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how to use RDD and interact with Dataset to build
    a multi-stage machine learning pipeline. Even though the Dataset (conceptually
    thought of as RDD with strong type-safety) is the way forward, you still have
    to be able to interact with other machine learning algorithms or codes that return/operate
    on RDD for either legacy or coding reasons. In this recipe, we also explore how
    to create and convert from Dataset to RDD and back.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for Spark session to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs: []
  type: TYPE_PRE
- en: Define a Scala case class to model data for processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE131]'
  prefs: []
  type: TYPE_PRE
- en: Let's create a Scala sequence and populate it with electric and hybrid cars.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE132]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we transformed an RDD into a Dataset and finally transformed
    it back to an RDD. We began with a Scala sequence which was changed into an RDD.
    After the creation of the RDD, invocation of Spark's session `createDataset()`
    method occurred, passing the RDD as an argument while receiving a Dataset as the
    result.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the Dataset was grouped by the make column, counting the existence of
    various makes of cars. The next step involved filtering the Dataset for makes
    of Tesla and transforming the results back to an RDD. Finally, we displayed the
    resulting RDD by way of the RDD `foreach()` method.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Dataset source file in Spark is only about 2500+ lines of Scala code. It
    is a very nice piece of code which can be leveraged for specialization under Apache
    license. We list the following URL and encourage you to at least scan the file
    and understand how buffering comes into play when using Dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Source code for Datasets hosted on GitHub is available at [https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala](https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/Dataset.scala).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Dataset can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KeyValue grouped Dataset can be found at[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relational grouped Dataset can be found at[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Working with JSON using the Dataset API and SQL together
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how to use JSON with Dataset. The JSON format has
    rapidly become the de-facto standard for data interoperability in the last 5 years.
  prefs: []
  type: TYPE_NORMAL
- en: We explore how Dataset uses JSON and executes API commands like `select()`.
    We then progress by creating a view (that is, `createOrReplaceTempView()`) and
    then execute a SQL query to demonstrate how to query against a JSON file using
    API and SQL with ease.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use a JSON data file named `cars.json` which has been created for this
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE133]'
  prefs: []
  type: TYPE_PRE
- en: Set up the package location where the program will reside
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE134]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for the Spark session to gain access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE135]'
  prefs: []
  type: TYPE_PRE
- en: Define a Scala `case class` to model data for processing.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: Set output level to `ERROR` to reduce Spark's logging output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs: []
  type: TYPE_PRE
- en: Initialize a Spark session creating an entry point for access to the Spark cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE138]'
  prefs: []
  type: TYPE_PRE
- en: Import Spark implicits, therefore adding in behavior with only an import.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: Now, we will load the JSON data file into memory, specifying the class type
    as `Car`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: Let's print out the data from our generated Dataset of type `Car`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/1aa57574-0e2f-40e9-9892-35f18eaf2b8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we will display column names of the Dataset to verify that the cars' JSON
    attribute names were processed correctly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE142]'
  prefs: []
  type: TYPE_PRE
- en: Let's see the automatically generated schema and validate the inferred data
    types.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE143]'
  prefs: []
  type: TYPE_PRE
- en: In this step, we will select the Dataset's `make` column, removing duplicates
    by applying the `distinct` method and showing the results.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/668b4ac1-6bf1-4b53-9db4-8770ce230f53.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, create a view on the cars Dataset so we can execute a literal Spark SQL
    query string against the dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs: []
  type: TYPE_PRE
- en: Finally, we execute a Spark SQL query filtering the Dataset for electric cars,
    and returning only three of the defined columns.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE146]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/ca582e1b-da30-46c7-8371-822fa08704f7.png)'
  prefs: []
  type: TYPE_IMG
- en: We close the program by stopping the Spark session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE147]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It is extremely straightforward to read a **JavaScript Object Notation** (**JSON**)
    data file and to transform it into a Dataset with Spark. JSON has become a widely
    used data format over the past several years and Spark's support for the format
    is substantial.
  prefs: []
  type: TYPE_NORMAL
- en: In the first part, we demonstrated loading JSON into a Dataset by means of built-in
    JSON parsing functionality in Spark's session. You should take note of Spark's
    built-in functionality that transforms the JSON data into the car case class.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part, we demonstrated Spark SQL being applied on the Dataset to
    wrangle the said data into a desirable state. We utilized the Dataset's select
    method to retrieve the `make` column and apply the `distinct` method for the removal
    ...
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To fully understand and master the Dataset API, be sure to understand the concept
    of `Row` and `Encoder`.
  prefs: []
  type: TYPE_NORMAL
- en: Datasets follow the *lazy execution* paradigm, meaning that execution only occurs
    by invoking actions in Spark. When we execute an action, the Catalyst query optimizer
    produces a logical plan and generates a physical plan for optimized execution
    in a parallel distributed manner. See the figure in the introduction for all the
    detailed steps.
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for `Row` is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for `Encoder` is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Encoder)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Dataset is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for KeyValue grouped Dataset is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.KeyValueGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for relational grouped Dataset [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.RelationalGroupedDataset)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Again, be sure to download and explore the Dataset source file, which is about
    2500+ lines from GitHub. Exploring the Spark source code is the best way to learn
    advanced programming in Scala, Scala Annotations, and Spark 2.0 itself.
  prefs: []
  type: TYPE_NORMAL
- en: 'Noteworthy for Pre-Spark 2.0 users:'
  prefs: []
  type: TYPE_NORMAL
- en: SparkSession is the single entry ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Functional programming with the Dataset API using domain objects
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how functional programming works with Dataset. We
    use the Dataset and functional programming to separate the cars (domain object)
    by their models.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use package instruction to provide the right path
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE148]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs: []
  type: TYPE_PRE
- en: Define a Scala case to contain our data for processing, and our car class will
    represent electric and ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we use a Scala sequence data structure to hold the original
    data, which is a series of cars and their attributes. Using `createDataset()`*,*
    we create a DataSet and populate it. We then proceed to use the 'make' attribute
    with `groupBy` and `mapGroups()` to list cars by their models using a functional
    paradigm with DataSet. Using this form of functional programming with domain objects
    was not impossible before DataSet (for example, the case class with RDD or UDF
    with DataFrame), but the DataSet construct makes this easy and intrinsic.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Be sure to include the `implicits` statement in all your DataSet coding:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE150]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The documentation for Datasets can be accessed at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  prefs: []
  type: TYPE_NORMAL
