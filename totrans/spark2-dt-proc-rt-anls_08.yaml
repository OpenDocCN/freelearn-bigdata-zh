- en: Spark Tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will dig deeper into Apache Spark internals and see that
    while Spark is great in making us feel like we are using just another Scala collection,
    we don''t have to forget that Spark actually runs in a distributed system. Therefore,
    some extra care should be taken. In a nutshell, the following topics will be covered
    in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring Spark jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark configuration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common mistakes in Spark app development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimization techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Monitoring Spark jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark provides web UI for monitoring all the jobs running or completed on computing
    nodes (drivers or executors). In this section, we will discuss in brief how to
    monitor Spark jobs using Spark web UI with appropriate examples. We will see how
    to monitor the progress of jobs (including submitted, queued, and running jobs).
    All the tabs in the Spark web UI will be discussed briefly. Finally, we will discuss
    the logging procedure in Spark for better tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Spark web interface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The web UI (also known as Spark UI) is the web interface for running Spark applications
    to monitor the execution of jobs on a web browser such as Firefox or Google Chrome.
    When a SparkContext launches, a web UI that displays useful information about
    the application gets started on port 4040 in standalone mode. The Spark web UI
    is available in different ways depending on whether the application is still running
    or has finished its execution.
  prefs: []
  type: TYPE_NORMAL
- en: Also, you can use the web UI after the application has finished its execution
    by persisting all the events using `EventLoggingListener`. The `EventLoggingListener`,
    however, cannot work alone, and the incorporation of the Spark history server
    is required. Combining these two features, the ...
  prefs: []
  type: TYPE_NORMAL
- en: Jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Depending upon the SparkContext, the Jobs tab shows the status of all the Spark
    jobs in a Spark application. When you access the Jobs tab on the Spark UI using
    a web browser at `http://localhost:4040` (for standalone mode), you should observe
    the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: 'User: This shows the active user who has submitted the Spark job'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Total Uptime: This shows the total uptime for the jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scheduling Mode: In most cases, it is first-in-first-out (aka FIFO)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Active Jobs: This shows the number of active jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Completed Jobs: This shows the number of completed jobs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Event Timeline: This shows the timeline of a job that has completed its execution'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Internally, the Jobs tab is represented by the `JobsTab` class, which is a
    custom SparkUI tab with the jobs prefix. The Jobs tab uses `JobProgressListener`
    to access statistics about the Spark jobs to display the above information on
    the page. Take a look at the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf303fd3-cd31-4810-bd34-b61193c1b848.png)**Figure 2:** The jobs tab
    in the Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you further expand the Active Jobs option in the Jobs tab, you will be able
    to see the execution plan, status, number of completed stages, and the job ID
    of that particular job as DAG Visualization, as shown in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bf74a871-39d8-4be0-b374-ef2554b65faf.png)**Figure 3:** The DAG visualization
    for task in the Spark web UI (abridged)'
  prefs: []
  type: TYPE_IMG
- en: When a user enters the code in the Spark console (for example, Spark shell or
    using Spark submit), Spark Core creates an operator graph. This is basically what
    happens when a user executes an action (for example, reduce, collect, count, first,
    take, countByKey, saveAsTextFile) or transformation (for example, map, flatMap,
    filter, mapPartitions, sample, union, intersection, distinct) on an RDD (which
    are immutable objects) at a particular node.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2f67f963-d847-4179-b308-ac80fee4bf39.png)**Figure 4:** DAG scheduler
    transforming RDD lineage into stage DAG'
  prefs: []
  type: TYPE_NORMAL
- en: During the transformation or action, **Directed Acyclic Graph** (**DAG**) information
    is used to restore the node to last transformation and actions (refer to *Figure
    4* and *Figure 5* for a clearer picture) to maintain the data resiliency. Finally,
    the graph is submitted to a DAG scheduler.
  prefs: []
  type: TYPE_NORMAL
- en: How does Spark compute the DAG from the RDD and subsequently execute the task?
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, when any action is called on the RDD, Spark creates the DAG
    and submits it to the DAG scheduler. The DAG scheduler divides operators into
    stages of tasks. A stage comprises tasks based on partitions of the input data.
    The DAG scheduler pipelines operators together. For example, many map operators
    can be scheduled in a single stage. The final result of a DAG scheduler is a set
    of stages. The stages are passed on to the task scheduler. The task scheduler
    launches tasks through the cluster manager (Spark Standalone/YARN/Mesos). The
    task scheduler doesn't know about the dependencies of the stages. The worker executes
    the tasks on the stage.
  prefs: []
  type: TYPE_NORMAL
- en: The DAG scheduler then keeps track of which RDDs the stage outputs materialized
    from. It then finds a minimal schedule to run jobs and divides the related operators
    into stages of tasks. Based on the partitions of the input data, a stage comprises
    multiple tasks. Then, operators are pipelined together with the DAG scheduler.
    Practically, more than one map or reduce operator (for example) can be scheduled
    in a single stage.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f80b883b-bc62-4898-be8f-232d5fe2755b.png)**Figure 5:** Executing action
    leads to new ResultStage and ActiveJob in DAGScheduler'
  prefs: []
  type: TYPE_NORMAL
- en: Two fundamental concepts in DAG scheduler are jobs and stages. Thus, it has
    to track them through internal registries and counters. Technically speaking,
    DAG scheduler is a part of SparkContext's initialization that works exclusively
    on the driver (immediately after the task scheduler and scheduler backend are
    ready). DAG scheduler is responsible for three major tasks in Spark execution.
    It computes an execution DAG, that is, DAG of stages, for a job. It determines
    the preferred node to run each task on and handles failures due to shuffle output
    files being lost.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/249b0e98-ac9a-4690-9542-3461582235a8.png)**Figure 6:** DAGScheduler
    as created by SparkContext with other services'
  prefs: []
  type: TYPE_NORMAL
- en: The final result of a DAG scheduler is a set of stages. Therefore, most of the
    statistics and the status of the job can be seen using this visualization, for
    example, execution plan, status, number of completed stages, and the job ID of
    that particular job.
  prefs: []
  type: TYPE_NORMAL
- en: Stages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Stages tab in Spark UI shows the current status of all stages of all jobs
    in a Spark application, including two optional pages for the tasks and statistics
    for a stage and pool details. Note that this information is available only when
    the application works in a fair scheduling mode. You should be able to access
    the Stages tab at `http://localhost:4040/stages`. Note that when there are no
    jobs submitted, the tab shows nothing but the title. The Stages tab shows the
    stages in a Spark application. The following stages can be seen in this tab:'
  prefs: []
  type: TYPE_NORMAL
- en: Active Stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pending Stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Completed Stages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, when you submit a Spark job locally, you should be able to see
    the following status:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 7:** The stages for all jobs in the Spark ...'
  prefs: []
  type: TYPE_NORMAL
- en: Storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Storage tab shows the size and memory use for each RDD, DataFrame, or Dataset.
    You should be able to see the storage-related information of RDDs, DataFrames,
    or Datasets. The following figure shows storage metadata such as RDD name, storage
    level, the number of cache partitions, the percentage of a fraction of the data
    that was cached, and the size of the RDD in the main memory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bce60273-1b2f-421d-830f-797c3cf2c647.png)**Figure 9:** Storage tab
    shows space consumed by an RDD in disk'
  prefs: []
  type: TYPE_NORMAL
- en: Note that if the RDD cannot be cached in the main memory, disk space will be
    used instead. A more detailed discussion will be carried out in a later section
    of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/79827dae-e0cc-405e-a5c0-b40830657142.png)**Figure 10:** Data distribution
    and the storage used by the RDD in disk'
  prefs: []
  type: TYPE_NORMAL
- en: Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Environment tab shows the environmental variables that are currently set
    on your machine (that is, driver). More specifically, runtime information such
    as Java Home, Java Version, and Scala Version can be seen under Runtime Information.
    Spark properties such as Spark application ID, app name, and driver host information,
    driver port, executor ID, master URL, and the schedule mode can be seen. Furthermore,
    other system-related properties and job properties such as AWT toolkit version,
    file encoding type (for example, UTF-8), and file encoding package information
    (for example, sun.io) can be seen under System Properties.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e96ff1b2-89ee-450c-ba39-90f3574b04af.png)**Figure ...**'
  prefs: []
  type: TYPE_NORMAL
- en: Executors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The Executors tab uses `ExecutorsListener` to collect information about executors
    for a Spark application. An executor is a distributed agent that is responsible
    for executing tasks. Executors are instantiated in different ways. For example,
    they can be instantiated when `CoarseGrainedExecutorBackend` receives `RegisteredExecutor`
    message for Spark Standalone and YARN. The second case is when a Spark job is
    submitted to Mesos. The Mesos''s `MesosExecutorBackend` gets registered. The third
    case is when you run your Spark jobs locally, that is, `LocalEndpoint` is created.
    An executor typically runs for the entire lifetime of a Spark application, which
    is called static allocation of executors, although you can also opt in for dynamic
    allocation. The executor backends exclusively manage all the executors in a computing
    node or clusters. An executor reports heartbeat and partial metrics for active
    tasks to the **HeartbeatReceiver** RPC endpoint on the driver periodically and
    the results are sent to the driver. They also provide in-memory storage for RDDs
    that are cached by user programs through block manager. Refer to the following
    figure for a clearer idea on this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d0402ed6-9387-4afd-92d0-ba718b425723.png)**Figure 12:** Spark driver
    instantiates an executor that is responsible for HeartbeatReceiver''s Heartbeat
    message handler'
  prefs: []
  type: TYPE_NORMAL
- en: 'When an executor starts, it first registers with the driver and communicates
    directly to execute tasks, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0a0491c0-ba58-42f2-b664-1cf715bff81a.png)**Figure 13:** Launching tasks
    on executor using TaskRunners'
  prefs: []
  type: TYPE_NORMAL
- en: You should be able to access the Executors tab at `http://localhost:4040/executors`.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7c502d08-b8ab-478f-9600-010dbf8890b8.png)**Figure 14:** Executor tab
    on Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding figure, Executor ID, Address, Status, RDD Blocks,
    Storage Memory, Disk Used, Cores, Active Tasks, Failed Tasks, Complete Tasks,
    Total Tasks, Task Time (GC Time), Input, Shuffle Read, Shuffle Write, and Thread
    Dump about the executor can be seen.
  prefs: []
  type: TYPE_NORMAL
- en: SQL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The SQL tab in the Spark UI displays all the accumulator values per operator.
    You should be able to access the SQL tab at `http://localhost:4040/SQL/`. It displays
    all the SQL query executions and underlying information by default. However, the
    SQL tab displays the details of the SQL query execution only after a query has
    been selected.
  prefs: []
  type: TYPE_NORMAL
- en: A detailed discussion on SQL is out of the scope of this chapter. Interested
    readers should refer to [http://spark.apache.org/docs/latest/sql-programming-guide.html#sql](http://spark.apache.org/docs/latest/sql-programming-guide.html#sql)
    for more on how to submit an SQL query and see its result output.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing Spark application using web UI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When a Spark job is submitted for execution, a web application UI is launched
    that displays useful information about the application. An event timeline displays
    the relative ordering and interleaving of application events. The timeline view
    is available on three levels: across all jobs, within one job, and within one
    stage. The timeline also shows executor allocation and deallocation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84681c1c-41e5-4ac1-a74a-6702e1401d2b.png)**Figure 15:** Spark jobs
    executed as DAG on Spark web UI'
  prefs: []
  type: TYPE_NORMAL
- en: Observing the running and completed Spark jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To access and observe the running and the completed Spark jobs, open `http://spark_driver_host:4040`
    in a web browser. Note that you will have to replace `spark_driver_host` with
    an IP address or hostname accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: Note that if multiple SparkContexts are running on the same host, they will
    bind to successive ports beginning with 4040, 4041, 4042, and so on. By default,
    this information will be available for the duration of your Spark application
    only. This means that when your Spark job finishes its execution, the binding
    will no longer be valid or accessible.
  prefs: []
  type: TYPE_NORMAL
- en: Now, to access the active jobs that are still executing, click on the Active
    Jobs link and you will see the related information of those ...
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark applications using logs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Seeing the information about all running Spark applications depends on which
    cluster manager you are using. You should follow these instructions while debugging
    your Spark application:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Spark Standalone**: Go to the Spark master UI at `http://master:18080`. The
    master and each worker show cluster and the related job statistics. In addition,
    a detailed log output for each job is also written to the working directory of
    each worker. We will discuss how to enable the logging manually using the `log4j`
    with Spark.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**YARN**: If your cluster manager is YARN, and suppose that you are running
    your Spark jobs on the Cloudera (or any other YARN-based platform), then go to
    the YARN applications page in the Cloudera Manager Admin Console. Now, to debug
    Spark applications running on YARN, view the logs for the Node Manager role. To
    make this happen, open the log event viewer and then filter the event stream to
    choose a time window and log level and to display the Node Manager source. You
    can access logs through the command as well. The format of the command is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'For example, the following are the valid commands for these IDs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Note that the user IDs are different. However, this is only true if `yarn.log-aggregation-enable`
    is true in `yarn-site.xml` and the application has already finished the execution.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with log4j with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark uses `log4j` for its own logging. All the operations that happen backend
    get logged to the Spark shell console (which is already configured to the underlying
    storage). Spark provides a template of `log4j` as a property file, and we can
    extend and modify that file for logging in Spark. Move to the `SPARK_HOME/conf`
    directory and you should see the `log4j.properties.template` file. This could
    help us as the starting point for our own logging system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s create our own custom logging system while running a Spark job.
    When you are done, rename the file as `log4j.properties` and put it under the
    same directory (that is, project tree). A sample snapshot of the file can be seen
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Figure 17:** A snap of the ...'
  prefs: []
  type: TYPE_NORMAL
- en: Spark configuration
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are a number of ways to configure your Spark jobs. In this section, we
    will discuss these ways. More specifically, according to Spark 2.x release, there
    are three locations to configure the system:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark properties
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark properties
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As discussed previously, Spark properties control most of the application-specific
    parameters and can be set using a `SparkConf` object of Spark. Alternatively,
    these parameters can be set through the Java system properties. `SparkConf` allows
    you to configure some of the common properties as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: An application can be configured to use a number of available cores on your
    machine. For example, we ...
  prefs: []
  type: TYPE_NORMAL
- en: Environmental variables
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Environment variables can be used to set the setting in the computing nodes
    or machine settings. For example, IP address can be set through the `conf/spark-env.sh`
    script on each computing node. The following table lists the name and the functionality
    of the environmental variables that need to be set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ed855bc-8bb8-450c-b96c-f8403758e021.png)**Figure 18:** Environmental
    variables and their meaning'
  prefs: []
  type: TYPE_NORMAL
- en: Logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Finally, logging can be configured through the `log4j.properties` file under
    your Spark application tree, as discussed in the preceding section. Spark uses
    log4j for logging. There are several valid logging levels supported by log4j with
    Spark; they are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Log Level** | **Usages** |'
  prefs: []
  type: TYPE_TB
- en: '| OFF | This is the most specific, which allows no logging at all |'
  prefs: []
  type: TYPE_TB
- en: '| FATAL | This is the most specific one that shows fatal errors with little
    data |'
  prefs: []
  type: TYPE_TB
- en: '| ERROR | This shows only the general errors |'
  prefs: []
  type: TYPE_TB
- en: '| WARN | This shows warnings that are recommended to be fixed but not mandatory
    |'
  prefs: []
  type: TYPE_TB
- en: '| INFO | This shows the information required for your Spark job |'
  prefs: []
  type: TYPE_TB
- en: '| DEBUG | While debugging, those logs will be printed |'
  prefs: []
  type: TYPE_TB
- en: '| TRACE | This provides the least specific error trace with a lot of data |'
  prefs: []
  type: TYPE_TB
- en: '| ALL ... |'
  prefs: []
  type: TYPE_TB
- en: Common mistakes in Spark app development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Common mistakes that happen often are application failure, a slow job that gets
    stuck due to numerous factors, mistakes in the aggregation, actions or transformations,
    an exception in the main thread and, of course, **Out Of Memory** (**OOM**).
  prefs: []
  type: TYPE_NORMAL
- en: Application failure
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Most of the time, application failure happens because one or more stages fail
    eventually. As discussed earlier in this chapter, Spark jobs comprise several
    stages. Stages aren''t executed independently: for instance, a processing stage
    can''t take place before the relevant input-reading stage. So, suppose that stage
    1 executes successfully but stage 2 fails to execute, the whole application fails
    eventually. This can be shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ddf18574-e882-40eb-ab58-eb8c9a11da46.png)**Figure 19:** Two stages
    in a typical Spark job'
  prefs: []
  type: TYPE_NORMAL
- en: To show an example, suppose you have the following three RDD operations as stages.
    The same can be visualized as shown in *Figure 20*, *Figure 21 ...*
  prefs: []
  type: TYPE_NORMAL
- en: Slow jobs or unresponsiveness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Sometimes, if the SparkContext cannot connect to a Spark standalone master,
    then the driver may display errors such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: At other times, the driver is able to connect to the master node but the master
    is unable to communicate back to the driver. Then, multiple attempts to connect
    are made even though the driver will report that it could not connect to the Master's
    log directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you might often experience very slow performance and progress
    in your Spark jobs. This happens because your driver program is not that fast
    to compute your jobs. As discussed earlier, sometimes a particular stage may take
    a longer time than usual because there might be a shuffle, map, join, or aggregation
    operation involved. Even if the computer is running out of disk storage or main
    memory, you may experience these issues. For example, if your master node does
    not respond or you experience unresponsiveness from the computing nodes for a
    certain period of time, you might think that your Spark job has halted and become
    stagnant at a certain stage:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/482c7d9e-b096-418d-8d15-a43d37bd08f7.png)**Figure 24:** An example
    log for executor/driver unresponsiveness'
  prefs: []
  type: TYPE_NORMAL
- en: 'Potential solutions could be several, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Check to make sure that workers and drivers are correctly configured to connect
    to the Spark master on the exact address listed in the Spark master web UI/logs.
    Then, explicitly supply the Spark cluster''s master URL when starting your Spark
    shell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Set `SPARK_LOCAL_IP` to a cluster-addressable hostname for the driver, master,
    and worker processes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Sometimes, we experience some issues due to hardware failure. For example, if
    the filesystem in a computing node closes unexpectedly, that is, an I/O exception,
    your Spark job will eventually fail too. This is obvious because your Spark job
    cannot write the resulting RDDs or data to store to the local filesystem or HDFS.
    This also implies that DAG operations cannot be performed due to the stage failures.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, this I/O exception occurs due to an underlying disk failure or other
    hardware failures. This often provides logs, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b76e59df-f159-4e9e-a174-8e8e8c7885b1.png)**Figure 25:** An example
    filesystem closed'
  prefs: []
  type: TYPE_NORMAL
- en: Nevertheless, you often experience slow job computing performance because your
    Java GC is somewhat busy with, or cannot do, the GC fast. For example, the following
    figure shows that for task 0, it took 10 hours to finish the GC! I experienced
    this issue in 2014, when I was new to Spark. Control of these types of issues,
    however, is not in our hands. Therefore, our recommendation is that you should
    make the JVM free and try submitting the jobs again.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c73396c2-4006-4cf1-8359-8a6081c4baa5.png)**Figure 26:** An example
    where GC stalled in between'
  prefs: []
  type: TYPE_NORMAL
- en: The fourth factor could be the slow response or slow job performance is due
    to the lack of data serialization. This will be discussed in the next section.
    The fifth factor could be the memory leak in the code that will tend to make your
    application consume more memory, leaving the files or logical devices open. Therefore,
    make sure that there is no option that tends to be a memory leak. For example,
    it is a good practice to finish your Spark application by calling `sc.stop()`
    or `spark.stop()`. This will make sure that one SparkContext is still open and
    active. Otherwise, you might get unwanted exceptions or issues. The sixth issue
    is that we often keep too many open files, and this sometimes creates `FileNotFoundException`
    in the shuffle or merge stage.
  prefs: []
  type: TYPE_NORMAL
- en: Optimization techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are several aspects of tuning Spark applications toward better optimization
    techniques. In this section, we will discuss how we can further optimize our Spark
    applications by applying data serialization by tuning the main memory with better
    memory management. We can also optimize performance by tuning the data structure
    in your Scala code while developing Spark applications. The storage, on the other
    hand, can be maintained well by utilizing serialized RDD storage.
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important aspects is garbage collection, and it's tuning if
    you have written your Spark application using Java or Scala. We will look at how
    we can also tune this for optimized performance. For distributed environment-
    and cluster-based ...
  prefs: []
  type: TYPE_NORMAL
- en: Data serialization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Serialization is an important tuning for performance improvement and optimization
    in any distributed computing environment. Spark is not an exception, but Spark
    jobs are often data and computing extensive. Therefore, if your data objects are
    not in a good format, then you first need to convert them into serialized data
    objects. This demands a large number of bytes of your memory. Eventually, the
    whole process will slow down the entire processing and computation drastically.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, you often experience a slow response from the computing nodes.
    This means that we sometimes fail to make 100% utilization of the computing resources.
    It is true that Spark tries to keep a balance between convenience and performance.
    This also implies that data serialization should be the first step in Spark tuning
    for better performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark provides two options for data serialization: Java serialization and Kryo
    serialization libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Java serialization:** Spark serializes objects using Java''s `ObjectOutputStream`
    framework. You handle the serialization by creating any class that implements
    `java.io.Serializable`. Java serialization is very flexible but often quite slow,
    which is not suitable for large data object serialization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kryo serialization:** You can also use Kryo library to serialize your data
    objects more quickly. Compared to Java serialization, Kryo serialization is much
    faster, with 10x speedup and is compact than that of Java. However, it has one
    issue, that is, it does not support all the serializable types, but you need to
    require your classes to be registered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can start using Kryo by initializing your Spark job with a `SparkConf`
    and calling `conf.set(spark.serializer, org.apache.spark.serializer.KryoSerializer)`.
    To register your own custom classes with Kryo, use the `registerKryoClasses` method,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: If your objects are large, you may also need to increase the `spark.kryoserializer.buffer`
    config. This value needs to be large enough to hold the largest object you serialize.
    Finally, if you don't register your custom classes, Kryo still works; however,
    the full class name with each object needs to be stored, which is wasteful indeed.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the logging subsection at the end of the monitoring Spark jobs
    section, the logging and computing can be optimized using the `Kryo` serialization.
    At first, just create the `MyMapper` class as a normal class (that is, without
    any serialization), as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s register this class as a `Kyro` serialization class and then set
    the `Kyro` serialization as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'That''s all you need. The full source code of this example is given in the
    following. You should be able to run and observe the same output, but an optimized
    one as compared to the previous example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Well done! Now let's have a quick look at how to tune the memory. We will look
    at some advanced strategies to make sure the efficient use of the main memory
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Memory tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss some advanced strategies that can be used by
    users like you to make sure that an efficient use of memory is carried out while
    executing your Spark jobs. More specifically, we will show how to calculate the
    memory usages of your objects. We will suggest some advanced ways to improve it
    by optimizing your data structures or by converting your data objects in a serialized
    format using Kryo or Java serializer. Finally, we will look at how to tune Spark's
    Java heap size, cache size, and the Java garbage collector.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three considerations in tuning memory usage:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The amount of memory used by your objects: You may even want your entire dataset
    to fit in the memory'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cost of accessing those ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory usage and management
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Memory usages by your Spark application and underlying computing nodes can be
    categorized as execution and storage. Execution memory is used during the computation
    in merge, shuffles, joins, sorts, and aggregations. On the other hand, storage
    memory is used for caching and propagating internal data across the cluster. In
    short, this is due to the large amount of I/O across the network.
  prefs: []
  type: TYPE_NORMAL
- en: Technically, Spark caches network data locally. While working with Spark iteratively
    or interactively, caching or persistence are optimization techniques in Spark.
    This two help in saving interim partial results so that they can be reused in
    subsequent stages. Then these interim results (as RDDs) can be kept in memory
    (default) or more solid storage, such as a disk, and/or replicated. Furthermore,
    RDDs can be cached using cache operations too. They can also be persisted using
    a persist operation. The difference between cache and persist operations is purely
    syntactic. The cache is a synonym of persisting or persists (`MEMORY_ONLY`), that
    is, the cache is merely persisted with the default storage level `MEMORY_ONLY`.
  prefs: []
  type: TYPE_NORMAL
- en: If you go under the Storage tab in your Spark web UI, you should observe the
    memory/storage used by an RDD, DataFrame, or Dataset object, as shown in *Figure
    10*. Although there are two relevant configurations for tuning memory in Spark,
    users do not need to readjust them. The reason is that the default values set
    in the configuration files are enough for your requirements and workloads.
  prefs: []
  type: TYPE_NORMAL
- en: spark.memory.fraction is the size of the unified region as a fraction of (JVM
    heap space - 300 MB) (default 0.6). The rest of the space (40%) is reserved for
    user data structures, internal metadata in Spark, and safeguarding against OOM
    errors in case of sparse and unusually large records. On the other hand, `spark.memory.storageFraction`
    expresses the size of R storage space as a fraction of the unified region (default
    is 0.5). The default value of this parameter is 50% of Java heap space, that is,
    300 MB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, one question might arise in your mind: which storage level to choose?
    To answer this question, Spark storage levels provide you with different trade-offs
    between memory usage and CPU efficiency. If your RDDs fit comfortably with the
    default storage level (MEMORY_ONLY), let your Spark driver or master go with it.
    This is the most memory-efficient option, allowing operations on the RDDs to run
    as fast as possible. You should let it go with this because this is the most memory-efficient
    option. This also allows numerous operations on the RDDs to be done as fast as
    possible.'
  prefs: []
  type: TYPE_NORMAL
- en: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY` does not
    work out, you should try using `MEMORY_ONLY_SER`. It is strongly recommended to
    not spill your RDDs to disk unless your **UDF** (aka **user-defined function**
    that you have defined for processing your dataset) is too expensive. This also
    applies if your UDF filters a large amount of the data during the execution stages.
    In other cases, recomputing a partition, that is, repartition may be faster for
    reading data objects from disk. Finally, if you want fast fault recovery, use
    the replicated storage levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, there are the following StorageLevels available and supported in
    Spark 2.x: (number _2 in the name denotes 2 replicas):'
  prefs: []
  type: TYPE_NORMAL
- en: '`DISK_ONLY`: This is for disk-based operation for RDDs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DISK_ONLY_2`: This is for disk-based operation for RDDs for 2 replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY`: This is the default for cache operation in memory for RDDs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY_2`: This is the default for cache operation in memory for RDDs
    with 2 replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY_SER`: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY`
    does not work out, this option particularly helps in storing data objects in a
    serialized form'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_ONLY_SER_2`: If your RDDs do not fit the main memory, that is, if `MEMORY_ONLY`
    does not work out with 2 replicas, this option also helps in storing data objects
    in a serialized form'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK`: Memory and disk (aka combined) based RDD persistence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK_2`: Memory and disk (aka combined) based RDD persistence with
    2 replicas'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK_SER`: If `MEMORY_AND_DISK` does not work, it can be used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MEMORY_AND_DISK_SER_2`: If `MEMORY_AND_DISK` does not work with 2 replicas,
    this option can be used'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`OFF_HEAP`: Does not allow writing into Java heap space'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that cache is a synonym of persist (`MEMORY_ONLY`). This means that cache
    is solely persisted with the default storage level, that is, `MEMORY_ONLY`. Detailed
    information can be found at [https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html](https://jaceklaskowski.gitbooks.io/mastering-apache-spark/content/spark-rdd-StorageLevel.html).
  prefs: []
  type: TYPE_NORMAL
- en: Tuning the data structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first way to reduce extra memory usage is to avoid some features in the
    Java data structure that impose extra overheads. For example, pointer-based data
    structures and wrapper objects contribute to nontrivial overheads. To tune your
    source code with a better data structure, we provide some suggestions here, which
    can be useful.
  prefs: []
  type: TYPE_NORMAL
- en: First, design your data structures such that you use arrays of objects and primitive
    types more. Thus, this also suggests using standard Java or Scala collection classes
    like `Set`, `List`, `Queue`, `ArrayList`, `Vector`, `LinkedList`, `PriorityQueue`,
    `HashSet`, `LinkedHashSet`, and `TreeSet` more frequently.
  prefs: []
  type: TYPE_NORMAL
- en: Second, when possible, avoid using nested structures with a lot of small objects
    and pointers so ...
  prefs: []
  type: TYPE_NORMAL
- en: Serialized RDD storage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As discussed already, despite other types of memory tuning, when your objects
    are too large to fit in the main memory or disk efficiently, a simpler and better
    way of reducing memory usage is storing them in a serialized form.
  prefs: []
  type: TYPE_NORMAL
- en: This can be done using the serialized storage levels in the RDD persistence
    API, such as `MEMORY_ONLY_SER`. For more information, refer to the previous section
    on memory management and start exploring available options.
  prefs: []
  type: TYPE_NORMAL
- en: If you specify using `MEMORY_ONLY_SER`, Spark will then store each RDD partition
    as one large byte array. However, the only downside of this approach is that it
    can slow down data access times. This is reasonable and obvious too; fairly speaking,
    there's no way to avoid it since each object needs to deserialize on the flyback
    while reusing.
  prefs: []
  type: TYPE_NORMAL
- en: As discussed previously, we highly recommend using Kryo serialization instead
    of Java serialization to make data access a bit faster.
  prefs: []
  type: TYPE_NORMAL
- en: Garbage collection tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although it is not a major problem in your Java or Scala programs that just
    read an RDD sequentially or randomly once and then execute numerous operations
    on it, **Java Virtual Machine** (**JVM**) GC can be problematic and complex if
    you have a large amount of data objects w.r.t RDDs stored in your driver program.
    When the JVM needs to remove obsolete and unused objects from the old objects
    to make space for the newer ones, it is mandatory to identify them and remove
    them from the memory eventually. However, this is a costly operation in terms
    of processing time and storage. You might be wondering,Â that the cost of GC is
    proportional to the number of Java objects stored in your main memory. Therefore,
    we strongly suggest ...
  prefs: []
  type: TYPE_NORMAL
- en: Level of parallelism
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Although you can control the number of map tasks to be executed through optional
    parameters to the `SparkContext.text` file, Spark sets the same on each file according
    to its size automatically. In addition to this, for a distributed `reduce` operation
    such as `groupByKey` and `reduceByKey`, Spark uses the largest parent RDD's number
    of partitions. However, sometimes, we make one mistake, that is, not utilizing
    the full computing resources for your nodes in a computing cluster. As a result,
    the full computing resources will not be fully exploited unless you set and specify
    the level of parallelism for your Spark job explicitly. Therefore, you should
    set the level of parallelism as the second argument.
  prefs: []
  type: TYPE_NORMAL
- en: For more on this option, please refer to [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.PairRDDFunctions)
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, you can do it by setting the config property spark.default.parallelism
    to change the default. For operations such as parallelizing with no parent RDDs,
    the level of parallelism depends on the cluster manager, that is, standalone,
    Mesos, or YARN. For the local mode, set the level of parallelism equal to the
    number of cores on the local machine. For Mesos or YARN, set the fine-grained
    mode to 8\. In other cases, the total number of cores on all executor nodes or
    2, whichever is larger, and in general, 2-3 tasks per CPU core in your cluster
    is recommended.
  prefs: []
  type: TYPE_NORMAL
- en: Broadcasting
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A broadcast variable enables a Spark developer to keep a read-only copy of an
    instance or class variable cached on each driver program, rather than transferring
    a copy of its own with the dependent tasks. However, an explicit creation of a
    broadcast variable is useful only when tasks across multiple stages need the same
    data in deserialize form.
  prefs: []
  type: TYPE_NORMAL
- en: In Spark application development, using the broadcasting option of SparkContext
    can reduce the size of each serialized task greatly. This also helps to reduce
    the cost of initiating a Spark job in a cluster. If you have a certain task in
    your Spark job that uses large objects from the driver program, you should turn
    it into a broadcast variable.
  prefs: []
  type: TYPE_NORMAL
- en: To use a broadcast variable in a Spark ...
  prefs: []
  type: TYPE_NORMAL
- en: Data locality
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Data locality means how close the data is to the code to be processed. Technically,
    data locality can have a nontrivial impact on the performance of a Spark job to
    be executed locally or in cluster mode. As a result, if the data and the code
    to be processed are tied together, computation is supposed to be much faster.
    Usually, shipping a serialized code from a driver to an executor is much faster
    since the code size is much smaller than that of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Spark application development and job execution, there are several levels
    of locality. In order from closest to farthest, the level depends on the current
    location of the data you have to process:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Data Locality** | **Meaning** | **Special Notes** |'
  prefs: []
  type: TYPE_TB
- en: '| `PROCESS_LOCAL` | Data and code are in the same location | Best locality
    possible |'
  prefs: []
  type: TYPE_TB
- en: '| `NODE_LOCAL` | Data and the code are on the same node, for example, data
    stored on HDFS | A bit slower than `PROCESS_LOCAL` since the data has to propagate
    across the processes and network |'
  prefs: []
  type: TYPE_TB
- en: '| `NO_PREF` | The data is accessed equally from somewhere else | Has no locality
    preference |'
  prefs: []
  type: TYPE_TB
- en: '| `RACK_LOCAL` | The data is on the same rack of servers over the network |
    Suitable for large-scale data processing |'
  prefs: []
  type: TYPE_TB
- en: '| `ANY` | The data is elsewhere on the network and not in the same rack | Not
    recommended unless there are no other options available |'
  prefs: []
  type: TYPE_TB
- en: '**Table 2:** Data locality and Spark'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spark is developed such that it prefers to schedule all tasks at the best locality
    level, but this is not guaranteed and not always possible either. As a result,
    based on the situation in the computing nodes, Spark switches to lower locality
    levels if available computing resources are too occupied. Moreover, if you would
    like to have the best data locality, there are two choices for you:'
  prefs: []
  type: TYPE_NORMAL
- en: Wait until a busy CPU gets free to start a task on your data on the same server
    or same node
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Immediately start a new one, which requires moving data there
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we discussed some advanced topics of Spark toward making your
    Spark job's performance better. We discussed some basic techniques to tune your
    Spark jobs. We discussed how to monitor your jobs by accessing Spark web UI. We
    discussed how to set Spark configuration parameters. We also discussed some common
    mistakes made by Spark users and provided some recommendations. Finally, we discussed
    some optimization techniques that help tune Spark applications.
  prefs: []
  type: TYPE_NORMAL
