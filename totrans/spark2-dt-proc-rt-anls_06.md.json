["```scala\nimport org.apache.sysml.api.MLOutput\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.mllib.util.LinearDataGenerator\nimport org.apache.sysml.api.MLContext\nimport org.apache.sysml.runtime.instructions.spark.utils.{RDDConverterUtilsExt => RDDConverterUtils}\nimport org.apache.sysml.runtime.matrix.MatrixCharacteristics;\n\nval sqlContext = new SQLContext(sc)\n\nval simpleScript =\n\"\"\"\nfileX = \"\";\nfileY = \"\";\nfileZ = \"\";\n\nX = read (fileX);\nY = read (fileY);\n\nZ = X %*% Y\n\nwrite (Z,fileZ);\n\"\"\"\n```", "```scala\n// Generate data\nval rawDataX = sqlContext.createDataFrame(LinearDataGenerator.generateLinearRDD(sc, 100, 10, 1))\nval rawDataY = sqlContext.createDataFrame(LinearDataGenerator.generateLinearRDD(sc, 10, 100, 1))\n\n// Repartition into a more parallelism-friendly number of partitions\nval dataX = rawDataX.repartition(64).cache()\nval dataY = rawDataY.repartition(64).cache()\n```", "```scala\n// Create SystemML context\nval ml = new MLContext(sc)\n```", "```scala\n// Convert data to proper format\nval mcX = new MatrixCharacteristics()\nval mcY = new MatrixCharacteristics()\nval X = RDDConverterUtils.vectorDataFrameToBinaryBlock(sc, dataX, mcX, false, \"features\")\nval Y = RDDConverterUtils.vectorDataFrameToBinaryBlock(sc, dataY, mcY, false, \"features\")\n```", "```scala\n// Register inputs & outputs\nml.reset()  \nml.registerInput(\"X\", X, mcX)\nml.registerInput(\"Y\", Y, mcY)\nml.registerOutput(\"Z\")\n```", "```scala\nval outputs = ml.executeScript(simpleScript)\n\n// Get outputs\nval Z = outputs.getDF(sqlContext, \"Z\")\n```"]