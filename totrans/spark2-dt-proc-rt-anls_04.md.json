["```scala\n[hadoop@hc2nn ~]# sudo su -\n[root@hc2nn ~]# cd /tmp\n[root@hc2nn ~]#wget http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm\n[root@hc2nn ~]# rpm -ivh sbt.rpm\n```", "```scala\n[hadoop@hc2nn nbayes]$ pwd\n/home/hadoop/spark/nbayes\n[hadoop@hc2nn nbayes]$ cat bayes.sbt \nname := \"Naive Bayes\"\nversion := \"1.0\"\nscalaVersion := \"2.11.2\"\nlibraryDependencies += \"org.apache.hadoop\" % \"hadoop-client\" % \"2.8.1\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.6.0\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"2.1.1\"\n```", "```scala\n[hadoop@hc2nn nbayes]$ sbt compile\n```", "```scala\n[hadoop@hc2nn nbayes]$ sbt package\n```", "```scala\n60% of emails are spam\n80% of spam emails contain the word buy\n20% of spam emails don't contain the word buy\n40% of emails are not spam\n10% of non spam emails contain the word buy\n90% of non spam emails don't contain the word buy\n```", "```scala\nP(Spam) = the probability that an email is spam = 0.6\nP(Not Spam) = the probability that an email is not spam = 0.4\nP(Buy|Spam) = the probability that an email that is spam has the word buy = 0.8\nP(Buy|Not Spam) = the probability that an email that is not spam has the word buy = 0.1\n```", "```scala\nReason,Month,Year,WeekType,TimeBand,BreathAlcohol,AgeBand,GenderSuspicion of Alcohol,Jan,2013,Weekday,12am-4am,75,30-39,MaleMoving Traffic Violation,Jan,2013,Weekday,12am-4am,0,20-24,MaleRoad Traffic Collision,Jan,2013,Weekend,12pm-4pm,0,20-24,Female\n```", "```scala\nname := \"K-Means\"\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.clustering.{KMeans,KMeansModel}\n\nobject kmeans1 extends App {\n```", "```scala\n val hdfsServer = \"hdfs://localhost:8020\"\n val hdfsPath   = \"/data/spark/kmeans/\" \n val dataFile   = hdfsServer + hdfsPath + \"DigitalBreathTestData2013-MALE2a.csv\"\n val sparkMaster = \"spark://localhost:7077\"\n val appName = \"K-Means 1\"\n val conf = new SparkConf()\n conf.setMaster(sparkMaster)\n conf.setAppName(appName)\n val sparkCxt = new SparkContext(conf)\n```", "```scala\n val csvData = sparkCxt.textFile(dataFile)\n val VectorData = csvData.map {\n   csvLine =>\n     Vectors.dense( csvLine.split(',').map(_.toDouble))\n }\n```", "```scala\n val kMeans = new KMeans\n val numClusters         = 3\n val maxIterations       = 50\n```", "```scala\n val initializationMode = KMeans.K_MEANS_PARALLEL\n val numRuns            = 1\n val numEpsilon         = 1e-4 \n kMeans.setK( numClusters )\n kMeans.setMaxIterations( maxIterations )\n kMeans.setInitializationMode( initializationMode )\n kMeans.setRuns( numRuns )\n kMeans.setEpsilon( numEpsilon )\n```", "```scala\n VectorData.cache\n val kMeansModel = kMeans.run( VectorData )\n```", "```scala\n val kMeansCost = kMeansModel.computeCost( VectorData ) \n println( \"Input data rows : \" + VectorData.count() )\n println( \"K-Means Cost   : \" + kMeansCost )\n```", "```scala\n kMeansModel.clusterCenters.foreach{ println }\n```", "```scala\n val clusterRddInt = kMeansModel.predict( VectorData ) \n val clusterCount = clusterRddInt.countByValue\n  clusterCount.toList.foreach{ println }\n} // end object kmeans1\n```", "```scala\n[hadoop@hc2nn kmeans]$ pwd\n/home/hadoop/spark/kmeans\n[hadoop@hc2nn kmeans]$ sbt package\nLoading /usr/share/sbt/bin/sbt-launch-lib.bash\n[info] Set current project to K-Means (in build file:/home/hadoop/spark/kmeans/)\n[info] Compiling 2 Scala sources to /home/hadoop/spark/kmeans/target/scala-2.10/classes...\n[info] Packaging /home/hadoop/spark/kmeans/target/scala-2.10/k-means_2.10-1.0.jar ...\n[info] Done packaging.\n[success] Total time: 20 s, completed Feb 19, 2015 5:02:07 PM\n```", "```scala\n[hadoop@hc2nn nbayes]$ hdfs dfs -ls /data/spark/kmeans\nFound 3 items\n-rw-r--r--   3 hadoop supergroup   24645166 2015-02-05 21:11 /data/spark/kmeans/DigitalBreathTestData2013-MALE2.csv\n-rw-r--r--   3 hadoop supergroup   5694226 2015-02-05 21:48 /data/spark/kmeans/DigitalBreathTestData2013-MALE2a.csv\ndrwxr-xr-x   - hadoop supergroup         0 2015-02-05 21:46 /data/spark/kmeans/result\n```", "```scala\nspark-submit \\\n --class kmeans1 \\\n --master spark://localhost:7077 \\\n --executor-memory 700M \\\n --total-executor-cores 100 \\\n /home/hadoop/spark/kmeans/target/scala-2.10/k-means_2.10-1.0.jar\n```", "```scala\nInput data rows : 467054\nK-Means Cost   : 5.40312223450789E7\n```", "```scala\n[0.24698249738061878,1.3015883142472253,0.005830116872250263,2.9173747788555207,1.156645130895448,3.4400290524342454] \n[0.3321793984152627,1.784137241326256,0.007615970459266097,2.5831987075928917,119.58366028156011,3.8379106085083468] \n[0.25247226760684494,1.702510963969387,0.006384899819416975,2.231404248000688,52.202897927594805,3.551509158139135]\n```", "```scala\n(0,407539)\n(1,12999)\n(2,46516)\n```", "```scala\n[hadoop@hc2nn ann]$ pwd\n/home/hadoop/spark/ann\n\n[hadoop@hc2nn ann]$ ls\nann.sbt   project src target\n```", "```scala\nname := \"A N N\"\nversion := \"1.0\"\nscalaVersion := \"2.11.2\"\nlibraryDependencies += \"org.apache.hadoop\" % \"hadoop-client\" % \"2.8.1\"\nlibraryDependencies += \"org.apache.spark\" % \"spark-core\" % \"2.6.0\"\nlibraryDependencies += \"org.apache.spark\" % \"spark-mllib\" % \"2.1.1\"\nlibraryDependencies += \"org.apache.spark\" % \"akka\" % \"2.5.3\"\n```", "```scala\n[hadoop@hc2nn scala]$ pwd\n/home/hadoop/spark/ann/src/main/scala \n[hadoop@hc2nn scala]$ ls\ntest_ann1.scala test_ann2.scala\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf \nimport org.apache.spark.mllib.classification.ANNClassifier\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg._\nimport org.apache.spark.rdd.RDD \n\nobject testann1 extends App {\n```", "```scala\n val server = \"hdfs://localhost:8020\"\n val path   = \"/data/spark/ann/\"\n\n val data1 = server + path + \"close_square.img\"\n val data2 = server + path + \"close_triangle.img\"\n val data3 = server + path + \"lines.img\"\n val data4 = server + path + \"open_square.img\"\n val data5 = server + path + \"open_triangle.img\"\n val data6 = server + path + \"plus.img\"\n```", "```scala\n val sparkMaster = \"spark://localhost:8077\"\n val appName = \"ANN 1\"\n val conf = new SparkConf()\n\n conf.setMaster(sparkMaster)\n conf.setAppName(appName)\n\n val sparkCxt = new SparkContext(conf)\n```", "```scala\n val rData1 = sparkCxt.textFile(data1).map(_.split(\" \").map(_.toDouble)).collect\n val rData2 = sparkCxt.textFile(data2).map(_.split(\" \").map(_.toDouble)).collect\n val rData3 = sparkCxt.textFile(data3).map(_.split(\" \").map(_.toDouble)).collect\n val rData4 = sparkCxt.textFile(data4).map(_.split(\" \").map(_.toDouble)).collect\n val rData5 = sparkCxt.textFile(data5).map(_.split(\" \").map(_.toDouble)).collect\n val rData6 = sparkCxt.textFile(data6).map(_.split(\" \").map(_.toDouble)).collect \n val inputs = Array[Array[Double]] (\n     rData1(0), rData2(0), rData3(0), rData4(0), rData5(0), rData6(0) ) \n val outputs = Array[Double]( 0.1, 0.2, 0.3, 0.4, 0.5, 0.6 )\n```", "```scala\n val ioData = inputs.zip( outputs )\n val lpData = ioData.map{ case(features,label) =>\n\n   LabeledPoint( label, Vectors.dense(features) )\n }\n val rddData = sparkCxt.parallelize( lpData )\n```", "```scala\n val hiddenTopology : Array[Int] = Array( 100, 100 )\n val maxNumIterations = 1000\n val convTolerance   = 1e-4\n val batchSize       = 6\n val annModel = ANNClassifier.train(rddData,\n                                    batchSize,\n                                    hiddenTopology,\n                                    maxNumIterations,\n                                    convTolerance)\n```", "```scala\n val rPredictData = inputs.map{ case(features) => \n   ( Vectors.dense(features) )\n }\n val rddPredictData = sparkCxt.parallelize( rPredictData )\n val predictions = annModel.predict( rddPredictData )\n```", "```scala\n predictions.toArray().foreach( value => println( \"prediction > \" + value ) )\n} // end ann1\n```", "```scala\n[hadoop@hc2nn ann]$ pwd\n/home/hadoop/spark/ann\n[hadoop@hc2nn ann]$ sbt package\n```", "```scala\n/home/hadoop/spark/spark/bin/spark-submit \\\n --class testann1 \\\n --master spark://localhost:8077 \\\n --executor-memory 700M \\\n --total-executor-cores 100 \\\n /home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar\n```", "```scala\nprediction > 0.1\nprediction > 0.2\nprediction > 0.3\nprediction > 0.4\nprediction > 0.5\nprediction > 0.6\n```", "```scala\nobject testann2 extends App\n```", "```scala\n val tData1 = server + path + \"close_square_test.img\"\n val tData2 = server + path + \"close_triangle_test.img\"\n val tData3 = server + path + \"lines_test.img\"\n val tData4 = server + path + \"open_square_test.img\"\n val tData5 = server + path + \"open_triangle_test.img\"\n val tData6 = server + path + \"plus_test.img\"\n```", "```scala\n val rtData1 = sparkCxt.textFile(tData1).map(_.split(\" \").map(_.toDouble)).collect\n val rtData2 = sparkCxt.textFile(tData2).map(_.split(\" \").map(_.toDouble)).collect\n val rtData3 = sparkCxt.textFile(tData3).map(_.split(\" \").map(_.toDouble)).collect\n val rtData4 = sparkCxt.textFile(tData4).map(_.split(\" \").map(_.toDouble)).collect\n val rtData5 = sparkCxt.textFile(tData5).map(_.split(\" \").map(_.toDouble)).collect\n val rtData6 = sparkCxt.textFile(tData6).map(_.split(\" \").map(_.toDouble)).collect \n val tInputs = Array[Array[Double]] (\n     rtData1(0), rtData2(0), rtData3(0), rtData4(0), rtData5(0), rtData6(0) )\n\n val rTestPredictData = tInputs.map{ case(features) => ( Vectors.dense(features) ) }\n val rddTestPredictData = sparkCxt.parallelize( rTestPredictData )\n```", "```scala\n val testPredictions = annModel.predict( rddTestPredictData )\n testPredictions.toArray().foreach( value => println( \"test prediction > \" + value ) )\n```", "```scala\n/home/hadoop/spark/spark/bin/spark-submit \\\n --class testann2 \\\n --master spark://localhost:8077 \\\n --executor-memory 700M \\\n --total-executor-cores 100 \\\n /home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar\n```", "```scala\ntest prediction > 0.1\ntest prediction > 0.2\ntest prediction > 0.3\ntest prediction > 0.4\ntest prediction > 0.5\ntest prediction > 0.6\n```"]