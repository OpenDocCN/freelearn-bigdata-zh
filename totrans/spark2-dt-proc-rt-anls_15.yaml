- en: Implementing Text Analytics with Spark 2.0 ML Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Doing term frequency with Spark - everything that counts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Displaying similar words with Spark using Word2Vec
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading a complete dump of Wikipedia for a real-life Spark ML project
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Latent Semantic Analysis for text analytics with Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Topic modeling with Latent Dirichlet allocation in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text analytics is at the intersection of machine learning, mathematics, linguistics,
    and natural language processing. Text analytics, referred to as text mining in
    older literature, attempts to extract information and infer higher level concepts,
    sentiment, and semantic details from unstructured and semi-structured data. It
    is important to note that the traditional keyword searches are insufficient to
    deal with noisy, ambiguous, and irrelevant tokens and concepts that need to be
    filtered out based on the actual context.
  prefs: []
  type: TYPE_NORMAL
- en: Ultimately, what we are trying to do is for a given set of documents (text,
    tweets, web, and social media), is determine what the gist of the communication
    is and what concepts it is trying to convey (topics and ...
  prefs: []
  type: TYPE_NORMAL
- en: Doing term frequency with Spark - everything that counts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For this recipe, we will download a book in text format from Project Gutenberg,
    from [http://www.gutenberg.org/cache/epub/62/pg62.txt](http://www.gutenberg.org/cache/epub/62/pg62.txt).
  prefs: []
  type: TYPE_NORMAL
- en: Project Gutenberg offers over 50,000 free eBooks in various formats for human
    consumption. Please read their terms of use; let us not use command-line tools
    to download any books.
  prefs: []
  type: TYPE_NORMAL
- en: When you look at the contents of the file, you will notice the title and author
    of the book is *The Project Gutenberg EBook of A Princess of Mars* by Edgar Rice
    Burroughs.
  prefs: []
  type: TYPE_NORMAL
- en: This eBook is for the use of anyone, anywhere, at no cost, and with almost no
    restrictions whatsoever. You may copy it, give it away, or reuse it under the
    terms of the Project Gutenberg License included with this eBook online at [http://www.gutenberg.org/](http://www.gutenberg.org/).
  prefs: []
  type: TYPE_NORMAL
- en: We then use the downloaded book to demonstrate the classic word count program
    with Scala and Spark. The example may seem somewhat simple at first, but we are
    beginning the process of feature extraction for text processing. Also, a general
    understanding of counting word occurrences in a document will go a long way to
    help us understand the concept of TF-IDF.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `package` statement for the recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Scala, Spark, and JFreeChart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We will define a function to display our JFreeChart within a window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began by loading the downloaded book and tokenizing it via a regular expression.
    The next step was to convert all tokens to lowercase and exclude stop words from
    our token list, followed by filtering out any words less than two characters long.
  prefs: []
  type: TYPE_NORMAL
- en: The removal of stop words and words of a certain length reduce the number of
    features we have to process. It may not seem obvious, but the removal of particular
    words based on various processing criteria reduce the number of dimensions our
    machine learning algorithms will later process.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we sorted the resulting word count in descending order, taking the
    top 25, which we displayed a bar chart for.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we have the base of what a keyword search would do. It is important
    to understand the difference between topic modelling and keyword search. In a
    keyword search, we try to associate a phrase with a given document based on the
    occurrences. In this case, we will point the user to a set of documents that has
    the most number of occurrences.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next step in the evolution of this algorithm, that a developer can try as
    an extension, would be to add weights and come up with a weighted average, but
    then Spark provides a facility which we explore in the upcoming recipes.
  prefs: []
  type: TYPE_NORMAL
- en: Displaying similar words with Spark using Word2Vec
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore Word2Vec, which is Spark's tool for assessing
    word similarity. The Word2Vec algorithm is inspired by the *distributional hypothesis*
    in general linguistics. At the core, what it tries to say is that the tokens which
    occur in the same context (that is, distance from the target) tend to support
    the same primitive concept/meaning.
  prefs: []
  type: TYPE_NORMAL
- en: The Word2Vec algorithm was invented by a team of researchers at Google. Please
    refer to a white paper mentioned in the *There's more...* section of this recipe
    which describes Word2Vec in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `package` statement for the recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Scala and Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us define the location of our book file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Spark session with configurations using the factory builder pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'We should set the logging level to warning, otherwise output will be difficult
    to follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We load in the book and convert it to a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We now transform each line into a bag of words utilizing Spark''s regular expression
    tokenizer, converting each term into lowercase and filtering away any term which
    has a character length of less than four:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We remove stop words by using Spark''s `StopWordRemover` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We apply the Word2Vec machine learning algorithm to extract features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We find ten synonyms from the book for *martian*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the results of ten synonyms found by the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/7d3b2158-a771-4656-af50-1a4492e191f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Word2Vec in Spark uses skip-gram and not **Continuous Bag of Words** (**CBOW**)
    which is more suitable for a **Neural Net** (**NN**). At its core, we are attempting
    to compute the representation of the words. It is highly recommended for the user
    to understand the difference between local representation versus distributed presentation,
    which is very different to the apparent meaning of the words themselves.
  prefs: []
  type: TYPE_NORMAL
- en: If we use distributed vector representation for words, it is natural that similar
    words will fall close together in the vector space, which is a desirable generalization
    technique for pattern abstraction and manipulation (that is, we reduce the problem
    to vector arithmetic).
  prefs: []
  type: TYPE_NORMAL
- en: What we want to do for a given set of words *{Word[1,] Word[2, .... ...]*
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: How would you find similar words anyhow? How many algorithms are there that
    can solve this problem, and how do they vary? The Word2Vec algorithm has been
    around for a while and has a counterpart called CBOW. Please bear in mind that
    Spark provides the skip-gram method as the implementation technique.
  prefs: []
  type: TYPE_NORMAL
- en: 'The variations of the Word2Vec algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Continuous Bag of Words (CBOW)**: Given a central word, what are the surrounding
    words?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Skip-gram**: If we know the words surrounding, can we guess the missing word?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is a variation of the algorithm that is called **skip-gram model with
    negative sampling** (**SGNS**), which seems to outperform other variants.
  prefs: []
  type: TYPE_NORMAL
- en: The co-occurrence is the fundamental concept underlying both CBOW and skip-gram.
    Even though the skip-gram does not directly use a co-occurrence matrix, it is
    using it indirectly.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we used the *stop words* techniques from NLP to have a cleaner
    corpus before running our algorithm. The stop words are English words such as
    "*the*" that need to be removed since they are not contributing to any improvement
    in the outcome.
  prefs: []
  type: TYPE_NORMAL
- en: Another important concept is* stemming*, which is not covered here but will
    be demonstrated in later recipes. Stemming removes extra language artifacts and
    reduces the word to its root (for example, *Engineering*, *Engineer*, and *Engineers*
    become *Engin* which is the root).
  prefs: []
  type: TYPE_NORMAL
- en: 'The white paper found at the following URL should provide a deeper explanation
    for Word2Vec:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://arxiv.org/pdf/1301.3781.pdf](http://arxiv.org/pdf/1301.3781.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for the Word2Vec recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Word2Vec()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2Vec)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Word2VecModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.Word2VecModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StopWordsRemover()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.StopWordsRemover)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading a complete dump of Wikipedia for a real-life Spark ML project
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be downloading and exploring a dump of Wikipedia so
    we can have a real-life example. The dataset that we will be downloading in this
    recipe is a dump of Wikipedia articles. You will either need the command-line
    tool **curl**, or a browser to retrieve a compressed file, which is about 13.6
    GB at this time. Due to the size, we recommend the curl command-line tool.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can start with downloading the dataset using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you want to decompress the ZIP file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: This should create an uncompressed file which is named `enwiki-latest-pages-articles-multistream.xml`
    and is about 56 GB.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at the Wikipedia XML file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We recommend working with the XML file in chunks, and using sampling for your
    experiments until you are ready for a final job to submit. It will save a tremendous
    amount of time and effort.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Wiki download is available at [https://en.wikipedia.org/wiki/Wikipedia:Database_download](https://en.wikipedia.org/wiki/Wikipedia:Database_download).
  prefs: []
  type: TYPE_NORMAL
- en: Using Latent Semantic Analysis for text analytics with Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore LSA utilizing a data dump of articles from Wikipedia.
    LSA translates into analyzing a corpus of documents to find hidden meaning or
    concepts in those documents.
  prefs: []
  type: TYPE_NORMAL
- en: In the first recipe of this chapter, we covered the basics of the TF (that is,
    term frequency) technique. In this recipe, we use HashingTF for calculating TF
    and use IDF to fit a model into the calculated TF. At its core, LSA uses **singular
    value decomposition** (**SVD**) on the term frequency document to reduce dimensionality
    and therefore extract the most important concepts. There are other cleanup steps
    that we need to do (for example, stop words and stemming) that will clean up the
    bag of words before we start analyzing it.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The package statement for the recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Scala and Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The example starts off by loading a dump of Wikipedia XML using Cloud9 Hadoop
    XML streaming tools to process the enormous XML document. Once we have parsed
    out the page text, the tokenization phase invokes turning our stream of Wikipedia
    page text into tokens. We used the Porter stemmer during the tokenization phase
    to help reduce words to a common base form.
  prefs: []
  type: TYPE_NORMAL
- en: More details on stemming are available at [https://en.wikipedia.org/wiki/Stemming](https://en.wikipedia.org/wiki/Stemming).
  prefs: []
  type: TYPE_NORMAL
- en: The next step was to use Spark HashingTF on each page token to compute the term
    frequency. After this phase was completed, we utilized Spark's IDF to generate
    the inverse document frequency.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we took the TF-IDF API and applied a singular value decomposition to
    handle factorization and dimensionality reduction.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following screenshot shows the steps and flow of the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ff9337d4-120e-4985-a45a-d4a273732f96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The Cloud9 Hadoop XML tools and several other required dependencies can be
    found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`hadoop-streaming-2.7.4.jar`: [http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar](http://central.maven.org/maven2/org/apache/hadoop/hadoop-streaming/2.7.4/hadoop-streaming-2.7.4.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`lucene-snowball-3.0.3.jar`: [http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar](http://central.maven.org/maven2/org/apache/lucene/lucene-snowball/3.0.3/lucene-snowball-3.0.3.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It should be obvious by now that even though Spark does not provide a direct
    LSA implementation, the combination of TF-IDF and SVD will let us construct and
    then decompose the large corpus matrix into three matrices, which can help us
    interpret the results by applying the dimensionality reduction via SVD. We can
    concentrate on the most meaningful clusters (similar to a recommendation algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: SVD will factor the term frequency document (that is, documents by attributes)
    to three distinct matrices that are much more efficient to extract to *N* concepts
    (that is, *N=27* in our example) from a large matrix that is hard and expensive
    to handle. In ML, we always prefer the tall and skinny matrices (that is, *U*
    matrix in this case) ...
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More details on `SingularValueDecomposition()` can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.SingularValueDecomposition).
  prefs: []
  type: TYPE_NORMAL
- en: Please refer to [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.linalg.distributed.RowMatrix) for
    more details on `RowMatrix()`.
  prefs: []
  type: TYPE_NORMAL
- en: Topic modeling with Latent Dirichlet allocation in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be demonstrating topic model generation by utilizing
    Latent Dirichlet Allocation to infer topics from a collection of documents.
  prefs: []
  type: TYPE_NORMAL
- en: We have covered LDA in previous chapters as it applies to clustering and topic
    modelling, but in this chapter, we demonstrate a more elaborate example to show
    its application to text analytics using more real-life and complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: We also apply NLP techniques such as stemming and stop words to provide a more
    realistic approach to LDA problem-solving. What we are trying to do is to discover
    a set of latent factors (that is, different from the original) that can solve
    and describe the solution in a more efficient way in a reduced ...
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The `package` statement for the recipe is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Scala and Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We define a function to parse a Wikipedia page and return the title and content
    text of the page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us define the location of the Wikipedia data dump:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a job configuration for Hadoop XML streaming:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the data path for Hadoop XML streaming processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` with configurations using the factory builder pattern:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We should set the logging level to warning, otherwise, output will be difficult
    to follow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin to process the huge Wikipedia data dump into article pages taking
    a sample of the file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we process our sample data into an RDD containing a tuple of title and
    page context text to finally generate a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'We now transform the text column of the DataFrame into raw words using Spark''s
    `RegexTokenizer` for each Wikipedia page:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step is to filter raw words by removing all stop words from the tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We generate term counts for the filtered tokens by using Spark''s `CountVectorizer`
    class, resulting in a new DataFrame containing the column features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: The "MinDF" specifies the minimum number of different document terms that must
    appear in order to be included in the vocabulary.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now invoke Spark''s LDA class to generate topics and the distributions of
    tokens to topics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: The "K" refers to how many topics and "MaxIter" maximum iterations to execute.
  prefs: []
  type: TYPE_NORMAL
- en: 'We finally describe the top five generated topics and display:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/65c0cc6a-8015-4bdb-85e2-c753056bb79c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now display, topics and terms associated with them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6907078-900d-4d60-9db9-7697c384eb19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began with loading the dump of Wikipedia articles and parsed the page text
    into tokens using Hadoop XML leveraging streaming facilities API. The feature
    extraction process utilized several classes to set up the final processing by
    the LDA class, letting the tokens flow from Spark's `RegexTokenize`, `StopwordsRemover`,
    and `HashingTF`. Once we had the term frequencies, the data was passed to the
    LDA class for clustering the articles together under several topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Hadoop XML tools and several other required dependencies can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '`bliki-core-3.0.19.jar`: [http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar](http://central.maven.org/maven2/info/bliki/wiki/bliki-core/3.0.19/bliki-core-3.0.19.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cloud9-2.0.1.jar`: [http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar
    ...](http://central.maven.org/maven2/edu/umd/cloud9/2.0.1/cloud9-2.0.1.jar)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please see the recipe LDA to classify documents and text into topics in Chapter
    8, *Unsupervised Clustering with Apache Spark 2.0* for a more detailed explanation
    of the LDA algorithm itself.
  prefs: []
  type: TYPE_NORMAL
- en: The following white paper from the *Journal of Machine Learning Research (JMLR)*
    provides a comprehensive treatment for those who would like to do an extensive
    analysis. It is a well-written paper, and a person with a basic background in
    stat and math should be able to follow it without any problems.
  prefs: []
  type: TYPE_NORMAL
- en: Refer to the [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf) link
    for more details of JMLR; an alternative link is [https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf](https://www.cs.colorado.edu/~mozer/Teaching/syllabi/ProbabilisticModels/readings/BleiNgJordan2003.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for LDAModel is available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also Spark''s Scala API documentation for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: DistributedLDAModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMLDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LocalLDAModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OnlineLDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
