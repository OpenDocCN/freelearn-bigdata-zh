["```scala\n[hadoop@hc2nn stream]$ hdfs dfs -ls /data/spark/checkpoint\n ls: `/data/spark/checkpoint': No such file or directory\n```", "```scala\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.streaming.StreamingContext._\n\nobject stream1 {\n```", "```scala\ndef createContext( cpDir : String ) : StreamingContext = {\n  val appName = \"Stream example 1\"\n  val conf    = new SparkConf()\n  conf.setAppName(appName)\n  val sc = new SparkContext(conf)\n  val ssc    = new StreamingContext(sc, Seconds(5) )\n  ssc.checkpoint( cpDir )\n  ssc\n}\n```", "```scala\ndef main(args: Array[String]) {\n  val hdfsDir = \"/data/spark/checkpoint\"\n  val consumerKey       = \"QQpxx\"\n  val consumerSecret    = \"0HFzxx\"\n  val accessToken       = \"323xx\"\n  val accessTokenSecret = \"IlQxx\"\n\n  System.setProperty(\"twitter4j.oauth.consumerKey\", consumerKey)\n  System.setProperty(\"twitter4j.oauth.consumerSecret\", consumerSecret)\n  System.setProperty(\"twitter4j.oauth.accessToken\", accessToken)\n  System.setProperty(\"twitter4j.oauth.accessTokenSecret\", accessTokenSecret)\n  val ssc = StreamingContext.getOrCreate(hdfsDir,\n        () => { createContext( hdfsDir ) })\n  val stream = TwitterUtils.createStream(ssc,None).window(  Seconds(60) )\n  // do some processing\n  ssc.start()\n  ssc.awaitTermination()\n} // end main\n```", "```scala\n [hadoop@hc2nn stream]$ hdfs dfs -ls /data/spark/checkpoint\n Found 1 items\n drwxr-xr-x   - hadoop supergroup          0 2015-07-02 13:41  /data/spark/checkpoint/0fc3d94e-6f53-40fb-910d-1eef044b12e9\n```", "```scala\n DStream.checkpoint( newRequiredInterval )\n```", "```scala\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\n\nobject stream2 {\n  def main(args: Array[String]) {\n```", "```scala\nif ( args.length < 2 ) {\n System.err.println(\"Usage: stream2 <host> <port>\")\n System.exit(1)\n}\n\nval hostname = args(0).trim\nval portnum  = args(1).toInt\nval appName  = \"Stream example 2\"\nval conf     = new SparkConf()\nconf.setAppName(appName)\nval sc  = new SparkContext(conf)\nval ssc = new StreamingContext(sc, Seconds(10) )\n```", "```scala\nval rawDstream = ssc.socketTextStream( hostname, portnum )\n```", "```scala\nval wordCount = rawDstream\n  .flatMap(line => line.split(\" \"))\n  .map(word => (word,1))\n  .reduceByKey(_+_)\n  .map(item => item.swap)\n  .transform(rdd => rdd.sortByKey(false))\n  .foreachRDD( rdd =>\n    { rdd.take(10).foreach(x=>println(\"List : \" + x)) }\n  )\n```", "```scala\n    ssc.start()\n      ssc.awaitTermination()\n  } // end main\n} // end stream2\n```", "```scala\n [root@hc2nn log]# pwd\n /var/log\n [root@hc2nn log]# cat ./anaconda.storage.log | nc -lk 10777\n```", "```scala\n List : (17104,)\n List : (2333,=)\n List : (1656,:)\n List : (1603,;)\n List : (1557,DEBUG)\n List : (564,True)\n List : (495,False)\n List : (411,None)\n List : (356,at)\n List : (335,object)\n```", "```scala\nval rawDstream = ssc.textFileStream( directory )\n```", "```scala\n[root@hc2nn ~]# flume-ng version\nFlume 1.5.0-cdh5.3.3\nSource code repository: https://git-wip-us.apache.org/repos/asf/flume.git\nRevision: b88ce1fd016bc873d817343779dfff6aeea07706\nCompiled by jenkins on Wed Apr  8 14:57:43 PDT 2015\nFrom source with checksum 389d91c718e03341a2367bf4ef12428e\n```", "```scala\n[hadoop@hc2nn ~]$ nc  hc2r1m1.semtech-solutions.co.nz  10777\n```", "```scala\nagent1.sources  = source1\nagent1.channels = channel1\nagent1.sinks    = sink1\n```", "```scala\nagent1.sources.source1.channels=channel1\nagent1.sources.source1.type=netcat\nagent1.sources.source1.bind=hc2r1m1.semtech-solutions.co.nz\nagent1.sources.source1.port=10777\n```", "```scala\nagent1.channels.channel1.type=memory\nagent1.channels.channel1.capacity=1000\n```", "```scala\nagent1.sinks.sink1.type=avro\nagent1.sinks.sink1.hostname=hc2r1m1.semtech-solutions.co.nz\nagent1.sinks.sink1.port=11777 agent1.sinks.sink1.channel=channel1\n```", "```scala\n[hadoop@hc2r1m1 stream]$ more flume.bash #!/bin/bash # run the bash agent flume-ng agent \\\n --conf /etc/flume-ng/conf \\\n --conf-file ./agent1.flume.cfg \\\n -Dflume.root.logger=DEBUG,INFO,console  \\\n -name agent1\n```", "```scala\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.flume._\n\nobject stream4 {\n  def main(args: Array[String]) {\n  //The host and port name arguments for the data stream are checked and extracted:\n      if ( args.length < 2 ) {\n        System.err.println(\"Usage: stream4 <host> <port>\")\n        System.exit(1)\n      }\n      val hostname = args(0).trim\n      val portnum  = args(1).toInt\n      println(\"hostname : \" + hostname)\n      println(\"portnum  : \" + portnum)\n```", "```scala\nval appName = \"Stream example 4\"\nval conf    = new SparkConf()\nconf.setAppName(appName)\nval sc  = new SparkContext(conf)\nval ssc = new StreamingContext(sc, Seconds(10) )\nval rawDstream = FlumeUtils.createStream(ssc,hostname,portnum)\n```", "```scala\n    rawDstream.count()\n           .map(cnt => \">>>> Received events : \" + cnt )\n           .print()\n    rawDstream.map(e => new String(e.event.getBody.array() ))\n           .print\n    ssc.start()\n    ssc.awaitTermination()\n  } // end main\n} // end stream4\n```", "```scala\n[hadoop@hc2r1m1 stream]$ more run_stream.bash #!/bin/bash SPARK_HOME=/usr/local/spark\nSPARK_BIN=$SPARK_HOME/bin\nSPARK_SBIN=$SPARK_HOME/sbin JAR_PATH=/home/hadoop/spark/stream/target/scala-2.10/streaming_2.10-1.0.jar\nCLASS_VAL=$1\nCLASS_PARAMS=\"${*:2}\" STREAM_JAR=/usr/local/spark/lib/spark-examples-1.3.1-hadoop2.3.0.jar cd $SPARK_BIN ./spark-submit \\\n --class $CLASS_VAL \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 100M \\\n --total-executor-cores 50 \\\n --jars $STREAM_JAR \\\n $JAR_PATH \\\n $CLASS_PARAMS\n```", "```scala\n[hadoop@hc2r1m1 stream]$ ./run_stream.bash stream4 hc2r1m1 11777\n```", "```scala\n [hadoop@hc2r1m1 stream]$ ./flume.bash\n```", "```scala\n[hadoop@hc2nn ~]$ nc  hc2r1m1.semtech-solutions.co.nz 10777\n I hope that Apache Spark will print this\n OK\n I hope that Apache Spark will print this\n OK\n I hope that Apache Spark will print this\n OK\n```", "```scala\n2015-07-06 18:13:18,699 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41\n 2015-07-06 18:13:18,700 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1\n 2015-07-06 18:13:18,990 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41\n 2015-07-06 18:13:18,991 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1\n 2015-07-06 18:13:19,270 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41\n 2015-07-06 18:13:19,271 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1\n```", "```scala\n-------------------------------------------\n Time: 1436163210000 ms\n -------------------------------------------\n >>> Received events : 3\n -------------------------------------------\n Time: 1436163210000 ms\n -------------------------------------------\n I hope that Apache Spark will print this\n I hope that Apache Spark will print this\n I hope that Apache Spark will print this\n```", "```scala\nagent1.sources.source1.type=exec\nagent1.sources.source.command=./rss.perl\n```", "```scala\n#!/usr/bin/perl\nuse strict;\nuse LWP::UserAgent;\nuse XML::XPath;\nmy $urlsource=\"http://feeds.reuters.com/reuters/scienceNews\" ;\nmy  $agent = LWP::UserAgent->new;\n#Then an infinite while loop is opened, and an HTTP GET request is carried out against  the URL. The request is configured, and the agent makes the request via  a call to the request method:\nwhile()\n{\n  my  $req = HTTP::Request->new(GET => ($urlsource));\n  $req->header('content-type' => 'application/json');\n  $req->header('Accept'       => 'application/json');\n  my $resp = $agent->request($req);\n```", "```scala\n    if ( $resp->is_success )\n    {\n      my $xmlpage = $resp -> decoded_content;\n      my $xp = XML::XPath->new( xml => $xmlpage );\n      my $nodeset = $xp->find( '/rss/channel/item/title' );\n      my @titles = () ;\n      my $index = 0 ;\n```", "```scala\n     foreach my $node ($nodeset->get_nodelist) {\n        my $xmlstring = XML::XPath::XMLParser::as_string($node) ;\n        $xmlstring =~ s/<title>//g;\n        $xmlstring =~ s/<\\/title>//g;\n        $xmlstring =~ s/\"//g;\n        $xmlstring =~ s/,//g;\n        $titles[$index] = $xmlstring ;\n        $index = $index + 1 ;\n      } # foreach find node\n```", "```scala\n    my $nodeset = $xp->find( '/rss/channel/item/description' );\n    my @desc = () ;\n    $index = 0 ;\n    foreach my $node ($nodeset->get_nodelist) {\n       my $xmlstring = XML::XPath::XMLParser::as_string($node) ;\n       $xmlstring =~ s/<img.+\\/img>//g;\n       $xmlstring =~ s/href=\".+\"//g;\n       $xmlstring =~ s/src=\"img/.+\"//g;\n       $xmlstring =~ s/src='.+'//g;\n       $xmlstring =~ s/<br.+\\/>//g;\n       $xmlstring =~ s/<\\/div>//g;\n       $xmlstring =~ s/<\\/a>//g;\n       $xmlstring =~ s/<a >\\n//g;\n       $xmlstring =~ s/<img >//g;\n       $xmlstring =~ s/<img \\/>//g;\n       $xmlstring =~ s/<div.+>//g;\n       $xmlstring =~ s/<title>//g;\n       $xmlstring =~ s/<\\/title>//g;\n       $xmlstring =~ s/<description>//g;\n       $xmlstring =~ s/<\\/description>//g;\n       $xmlstring =~ s/&lt;.+>//g;\n       $xmlstring =~ s/\"//g;\n       $xmlstring =~ s/,//g;\n       $xmlstring =~ s/\\r|\\n//g;\n       $desc[$index] = $xmlstring ;\n       $index = $index + 1 ;\n    } # foreach find node\n```", "```scala\n   my $newsitems = $index ;\n   $index = 0 ;\n   for ($index=0; $index < $newsitems; $index++) {\n      print \"{\"category\": \"science\",\"\n              . \" \"title\": \"\" .  $titles[$index] . \"\",\"\n              . \" \"summary\": \"\" .  $desc[$index] . \"\"\"\n               . \"}\\n\";\n      } # for rss items\n    } # success ?\n    sleep(30) ;\n } # while\n```", "```scala\ncase class RSSItem(category : String, title : String, summary : String) {\n  val now: Long = System.currentTimeMillis\n  val hdfsdir = \"hdfs://hc2nn:8020/data/spark/flume/rss/\"\n```", "```scala\n         rawDstream.map(record => {\n         implicit val formats = DefaultFormats\n         read[RSSItem](new String(record.event.getBody().array()))\n      }).foreachRDD(rdd => {\n              if (rdd.count() > 0) {\n                rdd.map(item => {\n                  implicit val formats = DefaultFormats\n                  write(item)\n                 }).saveAsTextFile(hdfsdir+\"file_\"+now.toString())\n               }\n      })\n```", "```scala\n2015-07-07 14:14:24,017 (agent-shutdown-hook) [DEBUG - org.apache.flume.source.ExecSource.stop(ExecSource.java:219)] Exec source with command:./news_rss_collector.py stopped. Metrics:SOURCE:source1{src.events.accepted=80, src.events.received=80, src.append.accepted=0, src.append-batch.accepted=0, src.open-connection.count=0, src.append-batch.received=0, src.append.received=0}\nThe Scala Spark application stream5 has processed 80 events in two batches:\n>>>> Received events : 73\n>>>> Received events : 7\n```", "```scala\n[hadoop@hc2r1m1 stream]$ hdfs dfs -ls /data/spark/flume/rss/\n Found 2 items\n drwxr-xr-x   - hadoop supergroup          0 2015-07-07 14:09 /data/spark/flume/rss/file_1436234439794\n drwxr-xr-x   - hadoop supergroup          0 2015-07-07 14:14 /data/spark/flume/rss/file_1436235208370\n```", "```scala\n[hadoop@hc2r1m1 stream]$  hdfs dfs -cat /data/spark/flume/rss/file_1436235208370/part-00000 | head -1 {\"category\":\"healthcare\",\"title\":\"BRIEF-Aetna CEO says has not had specific conversations with DOJ on Humana - CNBC\",\"summary\":\"* Aetna CEO Says Has Not Had Specific Conversations With Doj About Humana Acquisition - CNBC\"}\n```"]