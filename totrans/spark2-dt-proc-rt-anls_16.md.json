["```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SparkSessionimport java.io.{BufferedOutputStream, PrintWriter}import java.net.Socketimport java.net.ServerSocketimport java.util.concurrent.TimeUnitimport scala.util.Randomimport org.apache.spark.sql.streaming.ProcessingTime\n```", "```scala\nclass CountSreamThread(socket: ...\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport java.util.concurrent.TimeUnit\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.ProcessingTime\n```", "```scala\nval spark = SparkSession\n.builder.master(\"local[*]\")\n.appName(\"DataFrame Stream\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval df = spark.read .format(\"json\")\n.option(\"inferSchema\", \"true\")\n.load(\"../data/sparkml2/chapter13/person.json\")\ndf.printSchema()\n```", "```scala\nroot\n|-- age: long (nullable = true)\n|-- name: string (nullable = true)\n```", "```scala\nval stream = spark.readStream\n.schema(df.schema)\n.json(\"../data/sparkml2/chapter13/people/\")\n```", "```scala\nval people = stream.select(\"name\", \"age\").where(\"age > 60\")\n```", "```scala\nval query = people.writeStream\n.outputMode(\"append\")\n.trigger(ProcessingTime(1, TimeUnit.SECONDS))\n.format(\"console\")\n```", "```scala\nquery.start().awaitTermination()\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport java.util.concurrent.TimeUnitimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SparkSessionimport org.apache.spark.sql.streaming.ProcessingTime\n```", "```scala\ncase class StockPrice(date: String, open: Double, high: Double, low: Double, close: Double, volume: Integer, adjclose: Double)\n```", "```scala\nval spark = SparkSession.builder.master(\"local[*]\").appName(\"Dataset ...\n```", "```scala\nval streamDataset = spark.readStream\n            .schema(s.schema)\n            .option(\"sep\", \",\")\n            .option(\"header\", \"true\")\n            .csv(\"../data/sparkml2/chapter13/ge\").as[StockPrice]\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport java.time.LocalDateTime\nimport scala.util.Random._\n```", "```scala\ncase class ClickEvent(userId: String, ipAddress: String, time: String, url: String, statusCode: String)\n```", "```scala\nval statusCodeData = Seq(200, 404, 500)\n```", "```scala\nval urlData = Seq(\"http://www.fakefoo.com\",\n \"http://www.fakefoo.com/downloads\",\n \"http://www.fakefoo.com/search\",\n \"http://www.fakefoo.com/login\",\n \"http://www.fakefoo.com/settings\",\n \"http://www.fakefoo.com/news\",\n \"http://www.fakefoo.com/reports\",\n \"http://www.fakefoo.com/images\",\n \"http://www.fakefoo.com/css\",\n \"http://www.fakefoo.com/sounds\",\n \"http://www.fakefoo.com/admin\",\n \"http://www.fakefoo.com/accounts\" )\n```", "```scala\nval ipAddressData = generateIpAddress()\ndef generateIpAddress(): Seq[String] = {\n for (n <- 1 to 255) yield s\"127.0.0.$n\" }\n```", "```scala\nval timeStampData = generateTimeStamp()\n\n def generateTimeStamp(): Seq[String] = {\n val now = LocalDateTime.now()\n for (n <- 1 to 1000) yield LocalDateTime.*of*(now.toLocalDate,\n now.toLocalTime.plusSeconds(n)).toString\n }\n```", "```scala\nval userIdData = generateUserId()\n\n def generateUserId(): Seq[Int] = {\n for (id <- 1 to 1000) yield id\n }\n```", "```scala\ndef generateClicks(clicks: Int = 1): Seq[String] = {\n 0.until(clicks).map(i => {\n val statusCode = statusCodeData(nextInt(statusCodeData.size))\n val ipAddress = ipAddressData(nextInt(ipAddressData.size))\n val timeStamp = timeStampData(nextInt(timeStampData.size))\n val url = urlData(nextInt(urlData.size))\n val userId = userIdData(nextInt(userIdData.size))\n\n s\"$userId,$ipAddress,$timeStamp,$url,$statusCode\" })\n }\n```", "```scala\ndef parseClicks(data: String): ClickEvent = {\nval fields = data.split(\",\")\nnew ClickEvent(fields(0), fields(1), fields(2), fields(3), fields(4))\n }\n```", "```scala\nval spark = SparkSession\n.builder.master(\"local[*]\")\n .appName(\"Streaming App\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .config(\"spark.executor.memory\", \"2g\")\n .getOrCreate()\nval ssc = new StreamingContext(spark.sparkContext, Seconds(1))\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval rddQueue = new Queue[RDD[String]]()\n```", "```scala\nval inputStream = ssc.queueStream(rddQueue)\n```", "```scala\nval clicks = inputStream.map(data => ClickGenerator.parseClicks(data))\n val clickCounts = clicks.map(c => c.url).countByValue()\n```", "```scala\nclickCounts.print(12)\n```", "```scala\nssc.start()\n```", "```scala\nfor (i <- 1 to 10) {\n rddQueue += ssc.sparkContext.parallelize(ClickGenerator.*generateClicks*(100))\n Thread.sleep(1000)\n }\n```", "```scala\nssc.stop()\n```", "```scala\nwget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n```", "```scala\ncurl https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -o iris.data\n```", "```scala\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n```", "```scala\nhead -5 iris.data\n5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n```", "```scala\ntail -5 iris.data\n6.3,2.5,5.0,1.9,Iris-virginica\n6.5,3.0,5.2,2.0,Iris-virginica\n6.2,3.4,5.4,2.3,Iris-virginica\n5.9,3.0,5.1,1.8,Iris-virginica\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.rdd.RDDimport org.apache.spark.SparkContextimport scala.collection.mutable.Queue\n```", "```scala\ndef readFromFile(sc: SparkContext) = { sc.textFile(\"../data/sparkml2/chapter13/iris.data\") .filter(s ...\n```", "```scala\nsetDecayFactor()\nsetK()\nsetRandomCenters(,)\n```", "```scala\nwget http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n```", "```scala\ncurl http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv -o winequality-white.csv\n```", "```scala\nhttp://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n```", "```scala\nhead -5 winequality-white.csv\n\n\"fixed acidity\";\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n7;0.27;0.36;20.7;0.045;45;170;1.001;3;0.45;8.8;6\n6.3;0.3;0.34;1.6;0.049;14;132;0.994;3.3;0.49;9.5;6\n8.1;0.28;0.4;6.9;0.05;30;97;0.9951;3.26;0.44;10.1;6\n7.2;0.23;0.32;8.5;0.058;47;186;0.9956;3.19;0.4;9.9;6\n```", "```scala\ntail -5 winequality-white.csv\n6.2;0.21;0.29;1.6;0.039;24;92;0.99114;3.27;0.5;11.2;6\n6.6;0.32;0.36;8;0.047;57;168;0.9949;3.15;0.46;9.6;5\n6.5;0.24;0.19;1.2;0.041;30;111;0.99254;2.99;0.46;9.4;6\n5.5;0.29;0.3;1.1;0.022;20;110;0.98869;3.34;0.38;12.8;7\n6;0.21;0.38;0.8;0.02;22;98;0.98941;3.26;0.32;11.8;6\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.linalg.Vectors\n import org.apache.spark.mllib.regression.LabeledPoint\n import org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD\n import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.{Row, SparkSession}\n import org.apache.spark.streaming.{Seconds, StreamingContext}\n import scala.collection.mutable.Queue\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"Regression Streaming App\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .config(\"spark.executor.memory\", \"2g\")\n .getOrCreate()\n\n import spark.implicits._\n\n val ssc = new StreamingContext(spark.sparkContext, *Seconds*(2))\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval rawDF = spark.read\n .format(\"com.databricks.spark.csv\")\n .option(\"inferSchema\", \"true\")\n .option(\"header\", \"true\")\n .option(\"delimiter\", \";\")\n .load(\"../data/sparkml2/chapter13/winequality-white.csv\")\n```", "```scala\nval rdd = rawDF.rdd.zipWithUniqueId()\n```", "```scala\nval lookupQuality = rdd.map{ case (r: Row, id: Long)=> (id, r.getInt(11))}.collect().toMap\n```", "```scala\nval labelPoints = rdd.map{ case (r: Row, id: Long)=> LabeledPoint(id,\n Vectors.dense(r.getDouble(0), r.getDouble(1), r.getDouble(2), r.getDouble(3), r.getDouble(4),\n r.getDouble(5), r.getDouble(6), r.getDouble(7), r.getDouble(8), r.getDouble(9), r.getDouble(10))\n )}\n```", "```scala\nval trainQueue = new Queue[RDD[LabeledPoint]]()\nval testQueue = new Queue[RDD[LabeledPoint]]()\n```", "```scala\nval trainingStream = ssc.queueStream(trainQueue)\nval testStream = ssc.queueStream(testQueue)\n```", "```scala\nval numFeatures = 11\n val model = new StreamingLinearRegressionWithSGD()\n .setInitialWeights(Vectors.zeros(numFeatures))\n .setNumIterations(25)\n .setStepSize(0.1)\n .setMiniBatchFraction(0.25)\n```", "```scala\nmodel.trainOn(trainingStream)\nval result = model.predictOnValues(testStream.map(lp => (lp.label, lp.features)))\nresult.map{ case (id: Double, prediction: Double) => (id, prediction, lookupQuality(id.asInstanceOf[Long])) }.print()\n\n```", "```scala\nssc.start()\n```", "```scala\nval Array(trainData, test) = labelPoints.randomSplit(Array(.80, .20))\n```", "```scala\ntrainQueue += trainData\n Thread.sleep(4000)\n```", "```scala\nval testGroups = test.randomSplit(*Array*(.50, .50))\n testGroups.foreach(group => {\n testQueue += group\n Thread.sleep(2000)\n })\n```", "```scala\nssc.stop()\n```", "```scala\nhttp://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\n```", "```scala\nwget http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data -o pima-indians-diabetes.data\n```", "```scala\nhead -5 pima-indians-diabetes.data6,148,72,35,0,33.6,0.627,50,11,85,66,29,0,26.6,0.351,31,0 ...\n```", "```scala\n    Label/Class:\n               1 - tested positive\n               0 - tested negative\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{Row, SparkSession}\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport scala.collection.mutable.Queue\n\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"Logistic Regression Streaming App\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n\n import spark.implicits._\n\n val ssc = new StreamingContext(spark.sparkContext, *Seconds*(2))\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nval rawDS = spark.read\n.text(\"../data/sparkml2/chapter13/pima-indians- diabetes.data\").as[String]\n```", "```scala\nval buffer = rawDS.rdd.map(value => {\nval data = value.split(\",\")\n(data.init.toSeq, data.last)\n})\n```", "```scala\nval lps = buffer.map{ case (feature: Seq[String], label: String) =>\nval featureVector = feature.map(_.toDouble).toArray[Double]\nLabeledPoint(label.toDouble, Vectors.dense(featureVector))\n}\n\n```", "```scala\nval trainQueue = new Queue[RDD[LabeledPoint]]()\nval testQueue = new Queue[RDD[LabeledPoint]]()\n```", "```scala\nval trainingStream = ssc.queueStream(trainQueue)\nval testStream = ssc.queueStream(testQueue)\n```", "```scala\nval numFeatures = 8\nval model = new StreamingLogisticRegressionWithSGD()\n.setInitialWeights(Vectors.*zeros*(numFeatures))\n.setNumIterations(15)\n.setStepSize(0.5)\n.setMiniBatchFraction(0.25)\n```", "```scala\nmodel.trainOn(trainingStream)\nval result = model.predictOnValues(testStream.map(lp => (lp.label,\nlp.features)))\n result.map{ case (label: Double, prediction: Double) => (label, prediction) }.print()\n```", "```scala\nssc.start()\n```", "```scala\nval Array(trainData, test) = lps.randomSplit(*Array*(.80, .20))\n```", "```scala\ntrainQueue += trainData\n Thread.sleep(4000)\n```", "```scala\nval testGroups = test.randomSplit(*Array*(.50, .50))\n testGroups.foreach(group => {\n testQueue += group\n Thread.sleep(2000)\n })\n```", "```scala\n-------------------------------------------\nTime: 1488571098000 ms\n-------------------------------------------\n(1.0,1.0)\n(1.0,1.0)\n(1.0,0.0)\n(0.0,1.0)\n(1.0,0.0)\n(1.0,1.0)\n(0.0,0.0)\n(1.0,1.0)\n(0.0,1.0)\n(0.0,1.0)\n...\n-------------------------------------------\nTime: 1488571100000 ms\n-------------------------------------------\n(1.0,1.0)\n(0.0,0.0)\n(1.0,1.0)\n(1.0,0.0)\n(0.0,1.0)\n(0.0,1.0)\n(0.0,1.0)\n(1.0,0.0)\n(0.0,0.0)\n(1.0,1.0)\n...\n```", "```scala\nssc.stop()\n```"]