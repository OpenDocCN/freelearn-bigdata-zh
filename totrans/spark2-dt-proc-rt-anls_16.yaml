- en: Spark Streaming and Machine Learning Library
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Structured streaming for near real-time machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming DataFrames for real-time machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming Datasets for real-time machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data and debugging with queueStream
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and understanding the famous Iris data for unsupervised classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming KMeans for a real-time online classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading wine quality data for streaming regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming linear regression for a real-time regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading Pima Diabetes data for supervised classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming logistic regression for an on-line classifier
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark streaming is an evolving journey toward unification and structuring of
    the APIs in order to address the concerns of batch versus stream. Spark streaming
    has been available since Spark 1.3 with **Discretized Stream** (**DStream**).
    The new direction is to abstract the underlying framework using an unbounded table
    model in which the users can query the table using SQL or functional programming
    and write the output to another output table in multiple modes (complete, delta,
    and append output). The Spark SQL Catalyst optimizer and Tungsten (off-heap memory
    manager) are now an intrinsic part of the Spark streaming, which leads to a much
    efficient execution.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we not only cover the streaming facilities available in Spark's
    ...
  prefs: []
  type: TYPE_NORMAL
- en: Structured streaming for near real-time machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the new structured streaming paradigm introduced
    in Spark 2.0\. We explore real-time streaming using sockets and structured streaming
    API to vote and tabulate the votes accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: We also explore the newly introduced subsystem by simulating a stream of randomly
    generated votes to pick the most unpopular comic book villain.
  prefs: []
  type: TYPE_NORMAL
- en: There are two distinct programs (`VoteCountStream.scala` and `CountStreamproducer.scala`)
    that make up this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark context to get access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a Scala class to generate voting data onto a client socket:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we created a simple data generation server to simulate a stream
    of voting data and then counted the vote. The following figure provides a high-level
    depiction of this concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cf9e0f2a-e5e9-4b19-af52-bd11232731a3.png)'
  prefs: []
  type: TYPE_IMG
- en: First, we began by executing the data generation server. Second, we defined
    a socket data source, which allows us to connect to the data generation server.
    Third, we constructed a simple Spark expression to group by villain (that is,
    bad superheroes) and count all currently received votes. Finally, we configured
    a threshold trigger of 10 seconds to execute our streaming query, which dumps
    the accumulated results onto the console.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two short programs involved in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '`CountStreamproducer.scala`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The producer - data generation server
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Simulates the voting for itself and broadcasts it
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`VoteCountStream.scala`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The consumer - consumes and aggregates/tabulates the data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Receives and count votes for our villain superhero
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The topic of how to program using Spark streaming and structured streaming in
    Spark is out of scope for this book, but we felt it is necessary to share some
    programs to introduce the concepts before drilling down into ML streaming offering
    for Spark.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a solid introduction to streaming, please consult the following documentation
    on Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Information of Spark 2.0+ structured streaming is available at [https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#api-using-datasets-and-dataframes)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Information of Spark 1.6 streaming is available at [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for structured streaming is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.package)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for DStream (pre-Spark 2.0) is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.streaming.dstream.DStream)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `DataStreamReader` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `DataStreamWriter` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for `StreamingQuery` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming DataFrames for real-time machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the concept of a streaming DataFrame. We create a
    DataFrame consisting of the name and age of individuals, which we will be streaming
    across a wire. A streaming DataFrame is a popular technique to use with Spark
    ML since we do not have a full integration between Spark structured ML at the
    time of writing.
  prefs: []
  type: TYPE_NORMAL
- en: We limit this recipe to only the extent of demonstrating a streaming DataFrame
    and leave it up to the reader to adapt this to their own custom ML pipelines.
    While streaming DataFrame is not available out of the box in Spark 2.1.0, it will
    be a natural evolution to see it in later versions of Spark.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` as an entry point to the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    the logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, load the person data file to infer a data schema without hand coding
    the structure types:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console, you will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Now configure a streaming DataFrame for ingestion of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Let us execute a simple data transform, by filtering on age greater than `60`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We now output the transformed streaming data to the console, which will trigger
    every second:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We start our defined streaming query and wait for data to appear in the stream:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the result of our streaming query will appear in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d7e4aeb9-e4f9-4cbf-b311-136b78be704a.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we first discover the underlying schema for a person object
    using a quick method (using a JSON object) as described in step 6\. The resulting
    DataFrame will know the schema that we subsequently impose on the streaming input
    (simulated via streaming a file) and treated as a streaming DataFrame as seen
    in step 7.
  prefs: []
  type: TYPE_NORMAL
- en: The ability to treat the stream as a DataFrame and act on it using a functional
    or SQL paradigm is a powerful concept that can be seen in step 8\. We then proceed
    to output the result using `writestream()` with `append` mode and a 1-second batch
    interval trigger.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The combination of DataFrames and structured programming is a powerful concept
    that helps us to separate the data layer from the stream, which makes the programming
    significantly easier. One of the biggest drawbacks with DStream (pre-Spark 2.0)
    was its inability to isolate the user from details of the underlying details of
    stream/RDD implementation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Documentation for DataFrames:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DataFrameReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameReader)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DataFrameWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameWriter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for Spark data stream reader and writer:'
  prefs: []
  type: TYPE_NORMAL
- en: DataStreamReader: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DataStreamWriter: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming Datasets for real-time machine learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we create a streaming Dataset to demonstrate the use of Datasets
    with a Spark 2.0 structured programming paradigm. We stream stock prices from
    a file using a Dataset and apply a filter to select the day's stock that closed
    above $100.
  prefs: []
  type: TYPE_NORMAL
- en: The recipe demonstrates how streams can be used to filter and to act on the
    incoming data using a simple structured streaming programming model. While it
    is similar to a DataFrame, there are some differences in the syntax. The recipe
    is written in a generalized manner so the user can customize it for their own
    Spark ML programming projects.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a Scala `case class` to model streaming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Create `SparkSession` to use as an entry point to the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be utilizing the market data of closing prices for **General
    Electric** (**GE**) dating back to 1972\. To simplify the data, we have preprocessed
    for the purposes of this recipe. We use the same method from the previous recipe, *Streaming
    DataFrames for real-time machine learning*, by peeking into the JSON object to
    discover the schema (step 7), which we impose on the stream in step 8.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code shows how to use the schema to make the stream look like
    a simple table that you can read from on the fly. This is a powerful concept that
    makes stream programming accessible to more programmers. The `schema(s.schema)` and
    `as[StockPrice]`from the following code snippet are required to create the streaming
    Dataset, which has a schema associated with it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for all the APIs available under Dataset at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
    website[.](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following documentation is helpful while exploring the streaming Dataset
    concept:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StreamReader`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamReader)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StreamWriter`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.DataStreamWriter)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StreamQuery`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.streaming.StreamingQuery)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming data and debugging with queueStream
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the concept of `queueStream()`*,* which is a valuable
    tool while trying to get a streaming program to work during the development cycle.
    We found the `queueStream()` API very useful and felt that other developers can
    benefit from a recipe that fully demonstrates its usage.
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by simulating a user browsing various URLs associated with different
    web pages using the program `ClickGenerator.scala` and then proceed to consume
    and tabulate the data (user behavior/visits) using the `ClickStream.scala` program:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/84401a18-69d7-490d-8946-511151c882b3.png)'
  prefs: []
  type: TYPE_IMG
- en: We use Spark's streaming API with `Dstream()`, which will require the use ...
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a Scala `case class` to model click events by users that contain user
    identifier, IP address, time of the event, URL, and HTTP status code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Define status codes for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Define URLs for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Define IP address range for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Define timestamp range for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Define user identifier range for generation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to generate one or more pseudo-random events:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a function to parse a pseudo-random `ClickEvent` from a string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and Spark streaming context with 1-second duration:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a mutable queue to append our generated data onto:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a Spark queue stream from the streaming context passing in a reference
    of our data queue:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Process any data received by the queue stream and count the total number of
    each particular link users have clicked upon:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Print out the `12` URLs and their totals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Start our streaming context to receive micro-batches:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Loop 10 times generating 100 pseudo-random events on each iteration and append
    them our mutable queue so they materialize in the streaming queue abstraction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With this recipe, we introduced Spark Streaming using a technique many overlook,
    which allows us to craft a streaming application utilizing Spark's `QueueInputDStream`
    class. The `QueueInputDStream` class is not only a beneficial tool for understanding
    Spark streaming, but also for debugging during the development cycle. In the beginning
    steps, we set up a few data structures, in order to generate pseudo-random `clickstream`
    event data for stream processing at a later stage.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that in step 12, we are creating a streaming context instead
    of a SparkContext. The streaming context is what we use for Spark streaming applications.
    Next, the creation of a queue and queue stream is done to receive streaming data.
    Now steps ...
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At its core, `queueStream()` is just a queue of RDDs that we have after the
    Spark streaming (pre-2.0) turns into RDD:'
  prefs: []
  type: TYPE_NORMAL
- en: Documentation for structured streaming (Spark 2.0+): [https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html](https://spark.apache.org/docs/2.1.0/structured-streaming-programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for streaming (pre-Spark 2.0): [https://spark.apache.org/docs/latest/streaming-programming-guide.html](https://spark.apache.org/docs/latest/streaming-programming-guide.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downloading and understanding the famous Iris data for unsupervised classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we download and inspect the well-known Iris dataset in preparation
    for the upcoming streaming KMeans recipe, which lets you see classification/clustering
    in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: The data is housed on the UCI machine learning repository, which is a great
    source of data to prototype algorithms on. You will notice that R bloggers tend
    to love this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can start by downloading the dataset using either two of the following
    commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we begin our first step of data exploration by examining how the data in
    `iris.data` is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we take a look at the iris data to know how it is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The data is made of 150 observations. Each observation is made of four numerical
    features (measured in centimeters) and a label that signifies which class each
    Iris belongs to:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Features/attributes**:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width in cm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Label/class**:'
  prefs: []
  type: TYPE_NORMAL
- en: Iris Setosa
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris Versicolour
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Iris Virginic
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following image depicts an Iris flower with Petal and Sepal marked for
    clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fec7666e-bdb6-48c5-9df0-da2283f1877b.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following link explores the Iris dataset in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Iris_flower_data_set](https://en.wikipedia.org/wiki/Iris_flower_data_set)'
  prefs: []
  type: TYPE_NORMAL
- en: Streaming KMeans for a real-time on-line classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the streaming version of KMeans in Spark used in
    unsupervised learning schemes. The purpose of streaming KMeans algorithm is to
    classify or group a set of data points into a number of clusters based on their
    similarity factor.
  prefs: []
  type: TYPE_NORMAL
- en: There are two implementations of the KMeans classification method, one for static/offline
    data and another version for continuously arriving real-time updating data.
  prefs: []
  type: TYPE_NORMAL
- en: We will be streaming iris dataset clustering as new data streams into our streaming
    context.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'We begin by defining a function to load iris data into memory, filtering out
    blank lines, attaching an identifier to each element, and finally returning a
    tuple of type string and long:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we begin by loading the iris dataset and using the `zip()` API
    to pair data with a unique identifier to the data for generating *labeled points* data
    structure for use with the KMeans algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the mutable queues and `QueueInputDStream` are created for appending data
    to simulate streaming. Once the `QueueInputDStream` starts receiving data then
    the streaming k-mean clustering begins to dynamically cluster data and printing
    out results. The interesting thing you will notice here is we are streaming the
    training dataset on one queue stream and the test data on another queue stream.
    As we append data to our queues, the KMeans clustering algorithm is processing
    our incoming data and dynamically generating clusters.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for *StreamingKMeans()*:'
  prefs: []
  type: TYPE_NORMAL
- en: '`StreamingKMeans`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StreamingKMeansModel`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The hyper parameters defined via a builder pattern or `streamingKMeans` are:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Downloading wine quality data for streaming regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we download and inspect the wine quality dataset from the UCI
    machine learning repository to prepare data for Spark's streaming linear regression
    algorithm from MLlib.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need one of the following command-line tools `curl` or `wget` to retrieve
    specified data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start by downloading the dataset using either of the following three
    commands. The first one is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'This command is the third way to do the same:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we begin our first steps of data exploration by seeing how the data in
    `winequality-white.csv` is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we take a look at the wine quality data to know how it is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The data is comprised of 1,599 red wines and 4,898 white wines with 11 features
    and an output label that can be used during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a list of features/attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: Fixed acidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Volatile acidity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Citric acid
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Residual sugar
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chlorides
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Free sulfur dioxide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Total sulfur dioxide
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Density
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pH
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sulphates
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alcohol
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following is the output label:'
  prefs: []
  type: TYPE_NORMAL
- en: quality (a numeric value between 0 to 10)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The following link lists datasets for popular machine learning algorithms. A
    new dataset can be chosen to experiment with as needed.
  prefs: []
  type: TYPE_NORMAL
- en: Alternative datasets are available at [https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research](https://en.wikipedia.org/wiki/List_of_datasets_for_machine_learning_research).
  prefs: []
  type: TYPE_NORMAL
- en: We selected the Iris dataset so we can use continuous numerical features for
    a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: Streaming linear regression for a real-time regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the wine quality dataset from UCI and Spark's streaming
    linear regression algorithm from MLlib to predict the quality of a wine based
    on a group of wine features.
  prefs: []
  type: TYPE_NORMAL
- en: The difference between this recipe and the traditional regression recipes we
    saw before is the use of Spark ML streaming to score the quality of the wine in
    real time using a linear regression model.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the wine quality CSV using the Databricks CSV API into a DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the DataFrame into an `rdd` and `zip` a unique identifier onto it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Build a lookup map to compare predicted quality against actual quality value
    later:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert wine quality into label points for use with the machine learning library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a mutable queue for appending data to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark streaming queues to receive streaming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure streaming linear regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Train regression model and predict final values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Start Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Split label point data into the training set and test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Append data to training data queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'Now split test data in half and append to queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Once data is received by the queue stream, you will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/e814ddc2-eebd-43f6-a81a-b84a2b60742c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Close the program by stopping the Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We started by loading the wine quality dataset into a DataFrame via Databrick's
    `spark-csv` library. The next step was to attach a unique identifier to each row
    in our dataset to later match the predicted quality to the actual quality. The
    raw data was converted to labeled points so it can be used as input for the streaming
    linear regression algorithm. In steps 9 and 10, we created instances of mutable
    queues and Spark's `QueueInputDStream` class to be used as a conduit into the
    regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We then created the streaming linear regression model, which will predict wine
    quality for our final results. We customarily created training and test datasets
    from the original data and appended them to the appropriate queue to start ...
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `StreamingLinearRegressionWithSGD()`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hyper parameters for `StreamingLinearRegressionWithSGD()`*:*
  prefs: []
  type: TYPE_NORMAL
- en: '`setInitialWeights(Vectors.*zeros*())`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setNumIterations()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setStepSize()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setMiniBatchFraction()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is also a `StreamingLinearRegression()` API that does not use the **stochastic
    gradient descent** (**SGD**) version:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.regression.StreamingLinearAlgorithm)'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following link provides a quick reference for linear regression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Linear_regression](https://en.wikipedia.org/wiki/Linear_regression)'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading Pima Diabetes data for supervised classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we download and inspect the Pima Diabetes dataset from the UCI
    machine learning repository. We will use the dataset later with Spark's streaming
    logistic regression algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will need one of the following command-line tools `curl` or `wget` to retrieve
    the specified data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start by downloading the dataset using either two of the following
    commands. The first command is as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'This is an alternative that you can use:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we begin our first steps of data exploration by seeing how the data in
    `pima-indians-diabetes.data` is formatted (from Mac or Linux Terminal):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have 768 observations for the dataset. Each line/record is comprised of 10
    features and a label value that can be used for a supervised learning model (that
    is, logistic regression). The label/class is either a `1`, meaning tested positive
    for diabetes and `0` if the test came back negative.
  prefs: []
  type: TYPE_NORMAL
- en: '**Features/Attributes:**'
  prefs: []
  type: TYPE_NORMAL
- en: Number of times pregnant
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Plasma glucose concentration a 2 hours in an oral glucose tolerance test
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diastolic blood pressure (mm Hg)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triceps skin fold thickness (mm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 2-hour serum insulin (mu U/ml)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Body mass index (weight in kg/(height in m)^2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Diabetes pedigree function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Age (years)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Class variable (0 or 1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We found the following alternative datasets from Princeton University very
    helpful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://data.princeton.edu/wws509/datasets](http://data.princeton.edu/wws509/datasets)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The dataset that you can use to explore this recipe has to be structured in
    a way that the label (prediction class) has to be binary (tested positive/negative
    for diabetes).
  prefs: []
  type: TYPE_NORMAL
- en: Streaming logistic regression for an on-line classifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be using the Pima Diabetes dataset we downloaded in
    the previous recipe and Spark's streaming logistic regression algorithm with SGD
    to predict whether a Pima with various features will test positive as a diabetic.
    It is an on-line classifier that learns and predicts based on the streamed data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a `SparkSession` object as an entry point to the cluster and a `StreamingContext`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard-to-read output, therefore set
    the logging level to warning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'Load the Pima data file into a Dataset of type string:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Build an RDD from our raw Dataset by generating a tuple consisting of the last
    item into a record as the label and everything else as a sequence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Convert the preprocessed data into label points for use with the machine learning
    library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'Create mutable queues for appending data to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark streaming queues to receive streaming data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Configure the streaming logistic regression model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the regression model and predict final values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Start Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Split label point data into the training set and test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Append data to training data queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'Now split test data in half and append to the queue for processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Once data is received by the queue stream, you will see the following output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Close the program by stopping the Spark streaming context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we loaded the Pima Diabetes Dataset into a Dataset and parsed it into
    a tuple by taking every element as a feature except the last one, which we used
    as a label. Second, we morphed the RDD of tuples into labeled points so it can
    be used as input to the streaming logistic regression algorithm. Third, we created
    instances of mutable queues and Spark's `QueueInputDStream` class to be used as
    a pathway into the logistic algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Fourth, we created the streaming logistic regression model, which will predict
    wine quality for our final results. Finally, we customarily created training and
    test datasets from original data and appended it to the appropriate queue to trigger
    the model's processing of streaming data. The final ...
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `StreamingLogisticRegressionWithSGD()` is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The hyper parameters for the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '`setInitialWeights()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setNumIterations()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setStepSize()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`setMiniBatchFraction()`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
