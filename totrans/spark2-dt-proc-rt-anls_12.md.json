["```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.Statistics\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"Summary Statistics\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval sc = spark.sparkContext\n```", "```scala\nval rdd = sc.parallelize(\n  Seq(\n    Vectors.dense(0, 1, 0),\n    Vectors.dense(1.0, 10.0, 100.0),\n    Vectors.dense(3.0, 30.0, 300.0),\n    Vectors.dense(5.0, 50.0, 500.0),\n    Vectors.dense(7.0, 70.0, 700.0),\n    Vectors.dense(9.0, 90.0, 900.0),\n    Vectors.dense(11.0, 110.0, 1100.0)\n  )\n)\n```", "```scala\nval summary = Statistics.colStats(rdd)\n```", "```scala\nprintln(\"mean:\" + summary.mean)\nprintln(\"variance:\" +summary.variance)\nprintln(\"none zero\" + summary.numNonzeros)\nprintln(\"min:\" + summary.min)\nprintln(\"max:\" + summary.max)\nprintln(\"count:\" + summary.count)\nmean:[5.142857142857142,51.57142857142857,514.2857142857142]\nvariance:[16.80952380952381,1663.952380952381,168095.2380952381]\nnone zero[6.0,7.0,6.0]\nmin:[0.0,1.0,0.0]\nmax:[11.0,110.0,1100.0]\ncount:7\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.ml.Pipelineimport org.apache.spark.ml.classification.LogisticRegressionimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}import org.apache.spark.sql.SparkSessionimport org.apache.log4j.{Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)Logger.getLogger(\"akka\" ...\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.feature.MinMaxScaler\n```", "```scala\ndef parseWine(str: String): (Int, Vector) = {\nval columns = str.split(\",\")\n(columns(0).toInt, Vectors.dense(columns(1).toFloat, columns(2).toFloat, columns(3).toFloat))\n }\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"My Normalize\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval data = Spark.read.text(\"../data/sparkml2/chapter4/wine.data\").as[String].map(parseWine)\n```", "```scala\nval df = data.toDF(\"id\", \"feature\")\n```", "```scala\ndf.printSchema()\ndf.show(false)\n```", "```scala\nval scale = new MinMaxScaler()\n      .setInputCol(\"feature\")\n      .setOutputCol(\"scaled\")\n      .setMax(1)\n      .setMin(-1)\nscale.fit(df).transform(df).select(\"scaled\").show(false)\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSessionimport org.apache.log4j.{ Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)Logger.getLogger(\"akka\" ...\n```", "```scala\ndef randomSplit(weights: Array[Double]): Array[JavaRDD[T]]\n```", "```scala\nname,city\nBears,Chicago\nPackers,Green Bay\nLions,Detroit\nVikings,Minnesota\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\ncase class Team(name: String, city: String)\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"My Dataset\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval champs = spark.createDataset(List(Team(\"Broncos\", \"Denver\"), Team(\"Patriots\", \"New England\")))\nchamps.show(false)\n```", "```scala\nval teams = spark.read\n .option(\"Header\", \"true\")\n .csv(\"../data/sparkml2/chapter4/teams.csv\")\n .as[Team]\n\n teams.show(false)\n```", "```scala\nval cities = teams.map(t => t.city)\ncities.show(false)\n```", "```scala\ncities.explain()\n== Physical Plan ==\n*SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#26]\n+- *MapElements <function1>, obj#25: java.lang.String\n+- *DeserializeToObject newInstance(class Team), obj#24: Team\n+- *Scan csv [name#9,city#10] Format: CSV, InputPaths: file:teams.csv, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string,city:string>\n```", "```scala\nteams.write\n.mode(SaveMode.Overwrite)\n.json(\"../data/sparkml2/chapter4/teams.json\"){\"name\":\"Bears\",\"city\":\"Chicago\"}\n{\"name\":\"Packers\",\"city\":\"Green Bay\"}\n{\"name\":\"Lions\",\"city\":\"Detroit\"}\n{\"name\":\"Vikings\",\"city\":\"Minnesota\"}\n```", "```scala\nspark.stop()\n```", "```scala\nDataset: spark.read.textFile()\nRDD: spark.sparkContext.textFile()\nDataFrame: spark.read.text()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Beatle(id: Long, name: String)\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nDataset: spark.read.textFile\nRDD: spark.sparkContext.textFile\nDataFrame: spark.read.text\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.ml.feature.LabeledPoint\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.sql._\n\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myLabeledPoint\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval myLabeledPoints = spark.createDataFrame(Seq(\n LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)),\n LabeledPoint(0.0, Vectors.dense(2.0, 1.0, -1.0)),\n LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)),\n LabeledPoint(1.0, Vectors.dense(0.0, 1.2, -0.5)),\n\n LabeledPoint(0.0, Vectors.sparse(3, Array(0,2), Array(1.0,3.0))),\n LabeledPoint(1.0, Vectors.sparse(3, Array(1,2), Array(1.2,-0.4)))\n\n ))\n```", "```scala\nmyLabeledPoints.show()\n```", "```scala\nval lr = new LogisticRegression()\n\n lr.setMaxIter(5)\n .setRegParam(0.01)\n val model = lr.fit(myLabeledPoints)\n\n println(\"Model was fit using parameters: \" + model.parent.extractParamMap())\n```", "```scala\nModel was fit using parameters: {\n logreg_6aebbb683272-elasticNetParam: 0.0,\n logreg_6aebbb683272-featuresCol: features,\n logreg_6aebbb683272-fitIntercept: true,\n logreg_6aebbb683272-labelCol: label,\n logreg_6aebbb683272-maxIter: 5,\n logreg_6aebbb683272-predictionCol: prediction,\n logreg_6aebbb683272-probabilityCol: probability,\n logreg_6aebbb683272-rawPredictionCol: rawPrediction,\n logreg_6aebbb683272-regParam: 0.01,\n logreg_6aebbb683272-standardization: true,\n logreg_6aebbb683272-threshold: 0.5,\n logreg_6aebbb683272-tol: 1.0E-6\n}\n```", "```scala\nspark.stop()\n```", "```scala\nSeq( \nLabeledPoint (Label, Vector(data, data, data)) \n...... \nLabeledPoint (Label, Vector(data, data, data)) \n) \n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\n```", "```scala\nval spark = SparkSession.builder.master(\"local[*]\") // if use cluster master(\"spark://master:7077\").appName(\"myAccesSparkCluster20\").config(\"spark.sql.warehouse.dir\", \".\").getOrCreate()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.{SparkConf, SparkContext}\n```", "```scala\nval conf = new SparkConf()\n.setAppName(\"MyAccessSparkClusterPre20\")\n.setMaster(\"local[4]\") // if cluster setMaster(\"spark://MasterHostIP:7077\")\n.set(\"spark.sql.warehouse.dir\", \".\")\n\nval sc = new SparkContext(conf)\n```", "```scala\nsetMaster(\"local\")\n```", "```scala\nsetMaster(\"spark://yourmasterhostIP:port\")\n```", "```scala\n-Dspark.master=local\n```", "```scala\nval file = sc.textFile(\"../data/sparkml2/chapter4/mySampleCSV.csv\")\nval headerAndData = file.map(line => line.split(\",\").map(_.trim))\nval header = headerAndData.first\nval data = headerAndData.filter(_(0) != header(0))\nval maps = data.map(splits => header.zip(splits).toMap)\n```", "```scala\nval result = maps.take(4)\nresult.foreach(println)\n```", "```scala\nsc.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SparkSessionimport scala.util.Random\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nval session = SparkSession ...\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.clustering.KMeans\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")   // if use cluster master(\"spark://master:7077\")\n.appName(\"myPMMLExport\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval data = spark.sparkContext.textFile(\"../data/sparkml2/chapter4/my_kmeans_data_sample.txt\")\n\nval parsedData = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble))).cache()\n```", "```scala\nval numClusters = 2\nval numIterations = 10\nval model = KMeans.train(parsedData, numClusters, numIterations)\n```", "```scala\nprintln(\"MyKMeans PMML Model:\\n\" + model.toPMML)\n```", "```scala\nMyKMeans PMML Model:\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<PMML version=\"4.2\" >\n    <Header description=\"k-means clustering\">\n        <Application name=\"Apache Spark MLlib\" version=\"2.0.0\"/>\n        <Timestamp>2016-11-06T13:34:57</Timestamp>\n    </Header>\n    <DataDictionary numberOfFields=\"3\">\n        <DataField name=\"field_0\" optype=\"continuous\" dataType=\"double\"/>\n        <DataField name=\"field_1\" optype=\"continuous\" dataType=\"double\"/>\n        <DataField name=\"field_2\" optype=\"continuous\" dataType=\"double\"/>\n    </DataDictionary>\n    <ClusteringModel modelName=\"k-means\" functionName=\"clustering\" modelClass=\"centerBased\" numberOfClusters=\"2\">\n        <MiningSchema>\n            <MiningField name=\"field_0\" usageType=\"active\"/>\n            <MiningField name=\"field_1\" usageType=\"active\"/>\n            <MiningField name=\"field_2\" usageType=\"active\"/>\n        </MiningSchema>\n        <ComparisonMeasure kind=\"distance\">\n            <squaredEuclidean/>\n        </ComparisonMeasure>\n        <ClusteringField field=\"field_0\" compareFunction=\"absDiff\"/>\n        <ClusteringField field=\"field_1\" compareFunction=\"absDiff\"/>\n        <ClusteringField field=\"field_2\" compareFunction=\"absDiff\"/>\n        <Cluster name=\"cluster_0\">\n            <Array n=\"3\" type=\"real\">9.06 9.179999999999998 9.12</Array>\n        </Cluster>\n        <Cluster name=\"cluster_1\">\n            <Array n=\"3\" type=\"real\">0.11666666666666665 0.11666666666666665 0.13333333333333333</Array>\n        </Cluster>\n    </ClusteringModel>\n</PMML>\n```", "```scala\nmodel.toPMML(\"../data/sparkml2/chapter4/myKMeansSamplePMML.xml\")\n```", "```scala\nspark.stop()\n```", "```scala\nModel_a.toPMML(\"/xyz/model-name.xml\")\n```", "```scala\nModel_a.toPMML(SparkContext, \"/xyz/model-name\")\n```", "```scala\nModel_a.toPMML(System.out)\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.mllib.evaluation.RegressionMetricsimport org.apache.spark.mllib.linalg.Vectorsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.mllib.tree.DecisionTreeimport org.apache.spark.sql.SparkSession\n```", "```scala\nval spark = SparkSession.builder.master(\"local[*]\").appName(\"myRegressionMetrics\").config(\"spark.sql.warehouse.dir\", \".\").getOrCreate()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myBinaryClassification\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\n// Load training data in LIBSVM format\n//https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\nval data = MLUtils.loadLibSVMFile(spark.sparkContext, \"../data/sparkml2/chapter4/myBinaryClassificationData.txt\")\n```", "```scala\nval Array(training, test) = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n training.cache()\n\n // Run training algorithm to build the model\nval model = new LogisticRegressionWithLBFGS()\n .setNumClasses(2)\n .run(training)\n```", "```scala\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n val prediction = model.predict(features)\n (prediction, label)\n }\n```", "```scala\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\n```", "```scala\nval precision = metrics.precisionByThreshold\n precision.foreach { case (t, p) =>\n println(s\"Threshold: $t, Precision: $p\")\n }\n```", "```scala\nThreshold: 2.9751613212299755E-210, Precision: 0.5405405405405406\nThreshold: 1.0, Precision: 0.4838709677419355\nThreshold: 1.5283665404870175E-268, Precision: 0.5263157894736842\nThreshold: 4.889258814400478E-95, Precision: 0.5\n```", "```scala\nval recall = metrics.recallByThreshold\n recall.foreach { case (t, r) =>\n println(s\"Threshold: $t, Recall: $r\")\n }\n```", "```scala\nThreshold: 1.0779893231660571E-300, Recall: 0.6363636363636364\nThreshold: 6.830452412352692E-181, Recall: 0.5151515151515151\nThreshold: 0.0, Recall: 1.0\nThreshold: 1.1547199216963482E-194, Recall: 0.5757575757575758\n```", "```scala\nval f1Score = metrics.fMeasureByThreshold\n f1Score.foreach { case (t, f) =>\n println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n }\n```", "```scala\nThreshold: 1.0, F-score: 0.46874999999999994, Beta = 1\nThreshold: 4.889258814400478E-95, F-score: 0.49230769230769234, Beta = 1\nThreshold: 2.2097791212639423E-117, F-score: 0.48484848484848486, Beta = 1\n\nval beta = 0.5\nval fScore = metrics.fMeasureByThreshold(beta)\nf1Score.foreach { case (t, f) =>\n  println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")\n}\n```", "```scala\nThreshold: 2.9751613212299755E-210, F-score: 0.5714285714285714, Beta = 0.5\nThreshold: 1.0, F-score: 0.46874999999999994, Beta = 0.5\nThreshold: 1.5283665404870175E-268, F-score: 0.5633802816901409, Beta = 0.5\nThreshold: 4.889258814400478E-95, F-score: 0.49230769230769234, Beta = 0.5\n```", "```scala\nval auPRC = metrics.areaUnderPR\nprintln(\"Area under precision-recall curve = \" + auPRC)\n```", "```scala\nArea under precision-recall curve = 0.5768388996048239\n```", "```scala\nval thresholds = precision.map(_._1)\n\nval roc = metrics.roc\n\nval auROC = metrics.areaUnderROC\nprintln(\"Area under ROC = \" + auROC)\n```", "```scala\nArea under ROC = 0.6983957219251337\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSessionimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGSimport org.apache.spark.mllib.evaluation.MulticlassMetricsimport org.apache.spark.mllib.regression.LabeledPointimport org.apache.spark.mllib.util.MLUtils\n```", "```scala\nval spark = SparkSession.builder.master(\"local[*]\").appName(\"myMulticlass\").config(\"spark.sql.warehouse.dir\", \".\").getOrCreate() ...\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.evaluation.MultilabelMetrics\nimport org.apache.spark.rdd.RDD\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myMultilabel\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval data: RDD[(Array[Double], Array[Double])] = spark.sparkContext.parallelize(\nSeq((Array(0.0, 1.0), Array(0.1, 2.0)),\n     (Array(0.0, 2.0), Array(0.1, 1.0)),\n     (Array.empty[Double], Array(0.0)),\n     (Array(2.0), Array(2.0)),\n     (Array(2.0, 0.0), Array(2.0, 0.0)),\n     (Array(0.0, 1.0, 2.0), Array(0.0, 1.0)),\n     (Array(1.0), Array(1.0, 2.0))), 2)\n```", "```scala\nval metrics = new MultilabelMetrics(data)\n```", "```scala\nprintln(s\"Recall = ${metrics.recall}\")\nprintln(s\"Precision = ${metrics.precision}\")\nprintln(s\"F1 measure = ${metrics.f1Measure}\")\nprintln(s\"Accuracy = ${metrics.accuracy}\")\n```", "```scala\nRecall = 0.5\nPrecision = 0.5238095238095238\nF1 measure = 0.4952380952380952\nAccuracy = 0.4523809523809524\n```", "```scala\nmetrics.labels.foreach(label =>\n println(s\"Class $label precision = ${metrics.precision(label)}\"))\n metrics.labels.foreach(label => println(s\"Class $label recall = ${metrics.recall(label)}\"))\n metrics.labels.foreach(label => println(s\"Class $label F1-score = ${metrics.f1Measure(label)}\"))\n```", "```scala\nClass 0.0 precision = 0.5\nClass 1.0 precision = 0.6666666666666666\nClass 2.0 precision = 0.5\nClass 0.0 recall = 0.6666666666666666\nClass 1.0 recall = 0.6666666666666666\nClass 2.0 recall = 0.5\nClass 0.0 F1-score = 0.5714285714285715\nClass 1.0 F1-score = 0.6666666666666666\nClass 2.0 F1-score = 0.5\n```", "```scala\nprintln(s\"Micro recall = ${metrics.microRecall}\")\nprintln(s\"Micro precision = ${metrics.microPrecision}\")\nprintln(s\"Micro F1 measure = ${metrics.microF1Measure}\")\nFrom the console output:\nMicro recall = 0.5\nMicro precision = 0.5454545454545454\nMicro F1 measure = 0.5217391304347826\n```", "```scala\nprintln(s\"Hamming loss = ${metrics.hammingLoss}\")\nprintln(s\"Subset accuracy = ${metrics.subsetAccuracy}\")\nFrom the console output:\nHamming loss = 0.39285714285714285\nSubset accuracy = 0.2857142857142857\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```"]