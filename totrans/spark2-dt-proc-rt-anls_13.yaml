- en: Recommendation Engine that Scales with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the required data for a scalable recommendation engine in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the movies data details for the recommendation system in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the rating data details for the recommendation system in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a scalable recommendation engine using collaborative filtering in Spark
    2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we used short recipes and extremely simplified code
    to demonstrate basic building blocks and concepts governing the Spark machine
    library. In this chapter, we present a more developed application that addresses
    specific machine learning library domains using Spark's API and facilities. The
    number of recipes is less in this chapter; however, we get into a more ML application
    setting.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we explore the recommendation system and its implementation
    using a matrix factorization technique that draws on latent factor models called
    **alternating least square** (**ALS**). In a nutshell, when we try to factorize
    a large matrix of user-item ratings into two lower ranked, skinnier matrices,
    we often face a non-linear or non-convex optimization problem that is very difficult
    to solve. It happens that we are very good at solving convex optimization problems
    by fixing one leg and partially solving the other and then going back and forth
    (hence alternating); we can solve this factorization (hence discovering a set
    of latent factors) much better using known optimization techniques in parallel.
  prefs: []
  type: TYPE_NORMAL
- en: We use a popular dataset (movie lens dataset) to implement the recommendation
    engine, but unlike in other chapters, we use two recipes to explore the data and
    also show how you can introduce graphical elements such as the JFreeChart popular
    library to your Spark machine learning toolkit.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows the flow of the concepts and recipes in this chapter
    to demonstrate an ALS recommendation application:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a13a1488-e583-429f-98da-3325557d7c56.png)'
  prefs: []
  type: TYPE_IMG
- en: Recommendation engines have been around for a long time and were used in early
    e-commerce systems of the 1990s, using techniques ranging from hardcoded product
    association to content-based recommendations driven by profiling. The modern systems
    use **collaboration filtering** (**CF**) to address the shortcomings of the early
    systems and also to address the scale and latency (for example, 100 ms max and
    less) that is necessary to compete in modern commerce systems (for example, Amazon,
    Netflix, eBay, News, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: The modern systems use CF based on historical interactions and records (page
    view, purchases, rating, and so on). These systems address two major issues, mainly
    scalability and sparseness (that is, we do not have all the ratings for all movies
    or songs). Most systems use a variation of Alternating Least Square with Weighted
    Lambda Regularization that can be parallelized on most major platforms (for example,
    Spark). Having said that, a practical system implemented for commercial purposes
    uses many augmentations to deal with bias (that is, not all movies and users are
    equal) and temporal issues (that is, users' choice will change and the inventory
    of items will change) that are present in today's ecosystem. Having worked on
    a smart and leading edge e-commerce system, building a competitive recommender
    is not a purist approach, but a practical one that uses multiple techniques, arriving
    at the affinity matrix/heat map as the context utilizing all three techniques
    (collaborative filtering, content-based filtering, and similarity) at the minimum.
  prefs: []
  type: TYPE_NORMAL
- en: The reader is encouraged to look up white papers and material that refer to
    the problem of cold start in recommendation systems.
  prefs: []
  type: TYPE_NORMAL
- en: To set the context, the following figure provides a high-level taxonomy of methods
    that are available to build recommendation systems. We briefly cover some of the
    pros and cons of each system but concentrate on matrix factorization (latent factor
    model) that is available in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: While both **single value decomposition** (**SVD**) and **alternative least
    squares** (**ALS**) are available, we concentrate on ALS implementation with MovieLens
    data due to the shortcomings of SVD in handling missing data among other things.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3e39b0ee-5c28-4270-9f09-fabd855c6286.png)'
  prefs: []
  type: TYPE_IMG
- en: The recommendation engine techniques in use are explained in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Content filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Content filtering is one of the original techniques for recommendation engines.
    It relies on user profiles to make recommendations. This approach relies mostly
    on pre-existing profiles for users (type, demographics, income, geo-location,
    ZIP code) and inventory (characteristics of a product, movie, or a song) to infer
    attribution which then can be filtered and acted upon. The main issue is that
    the pre-existing knowledge is often incomplete and expensive to source. This technique
    is more than a decade old and is still being practiced.
  prefs: []
  type: TYPE_NORMAL
- en: Collaborative filtering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Collaborative filtering is the workhorse of modern recommendation systems and
    relies on user interaction in the ecosystem rather than profiles to make recommendations.
  prefs: []
  type: TYPE_NORMAL
- en: This technique relies on past user behavior and product ratings and does not
    assume any pre-existing knowledge. In short, users rate the inventory items and
    the assumption is that customer taste will remain relatively constant over time,
    which can be exploited to provide recommendations. Having said that, an intelligent
    system will augment and reorder recommendations with any available context (for
    example, the user is a female who has logged in from China).
  prefs: []
  type: TYPE_NORMAL
- en: The main issue with this class of techniques is cold start, but its advantages
    of being domain free, with more accuracy and easy scalability, has made it a winner
    in the age of big data.
  prefs: []
  type: TYPE_NORMAL
- en: Neighborhood method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique is mostly implemented as **weighted local neighborhood**. In
    its core, it is a similarity technique and relies heavily on assumptions about
    items and users. While it is easy to understand and implement the technique, the
    algorithm suffers from a lack of scalability and accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Latent factor models techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This technique attempts to explain users' ratings of inventory items (for example,
    products on Amazon) by inferring a secondary set of latent factors which are inferred
    from ratings. The power comes from the fact that you do not need to know the factors
    ahead of time (similar to PCA techniques), but they are simply inferred from the
    ratings themselves. We derive the latent factors using matrix factorization techniques
    which are popular due to the extreme scalability, accuracy of predictions, and
    flexibility (they allow for bias and the temporal nature of the user and inventory).
  prefs: []
  type: TYPE_NORMAL
- en: '**Singular Value Decomposition (SVD)**: SVD has been available in Spark from
    the early days, but we recommend not to use it as a core technique due to the
    problem of its ability to deal with sparseness of data in real life (for example,
    a user will not usually rate everything), overfitting, and order (do we really
    need to produce the bottom 1,000 recommendations?).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stochastic Gradient Decent** (**SGD**): SGD is easy to implement and has
    faster running times due to its approach of looking at one movie and one user/item
    vector at a time (pick a movie and update the profile a little bit for that user
    versus a batch approach). We can implement this using the matrix facility and
    SGD in Spark as needed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Alternating Least Square** (**ALS**): Please see ALS before you take on this
    journey. Available in Spark, ALS can take advantage of parallelization from the
    start. Spark implements full matrix factorization under the hood, contrary to
    the common belief that Spark uses half factorization. We encourage the reader
    to refer to the source code to verify this for themselves. Spark provides API
    for both **explicit** (rating available) and **implicit** (an indirect inference
    needed--for example, the length of time a track is played rather than a rating).
    We discuss the bias and temporal issues in the recipe itself, by introducing mathematics
    and intuition to make our point.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up the required data for a scalable recommendation engine in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we examine downloading the MovieLens public dataset and take
    a first exploratory view of the data. We will use the explicit data based on customer
    ratings from the MovieLens dataset. The MovieLens dataset contains 1,000,000 ratings
    of 4,000 movies from 6,000 users.
  prefs: []
  type: TYPE_NORMAL
- en: 'You will need one of the following command line tools to retrieve the specified
    data: `curl` (recommended for Mac) or `wget` (recommended for Windows or Linux).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can start with downloading the dataset using either of the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also use the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now you need to decompress the ZIP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The command will create a directory named `ml-1m` with data files decompressed
    inside.
  prefs: []
  type: TYPE_NORMAL
- en: 'Change into the directory `m1-1m`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we begin our first steps of data exploration by verifying how the data
    in `movies.dat` is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we take a look at the ratings data to know how it is formatted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MovieLens dataset is an excellent alternative to the original Netflix KDD
    cup dataset. This dataset comes in multiple sets ranging from small (100 K set)
    to large (1 M and 20 M set). For those users interested in tweaking the source
    code to add their own augmentation (for example, the change regularization technique),
    the range of the dataset makes it easy to study the scaling effect and look at
    the performance curve versus Spark utilization per executive, as the data scales
    from 100 K to 20 M.
  prefs: []
  type: TYPE_NORMAL
- en: The URL to download is [http://grouplens.org/datasets/movielens/](http://grouplens.org/datasets/movielens/).
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Take a closer look at where we downloaded the data from because more datasets
    are available for use at [http://files.grouplens.org/datasets/](http://files.grouplens.org/datasets/).
  prefs: []
  type: TYPE_NORMAL
- en: The following figure depicts the size and extent of the data. For this chapter,
    we use the small set so it can easily run on a small laptop with limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9eaadad9-dbaa-453e-be03-8a44989566d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: MovieLens'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please read through the README file contained within the directory that you
    unzipped the data to. The README file contains information about data file formats
    and data descriptions.
  prefs: []
  type: TYPE_NORMAL
- en: There is also a MovieLens genome tag set that can be used for reference.
  prefs: []
  type: TYPE_NORMAL
- en: Computed tag-movie 11 million
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Relevance scores from a pool of 1,100 tags
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applied to 10,000 movies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For those interested in exploring the original Netflix dataset, please see the
    [http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a](http://academictorrents.com/details/9b13183dc4d60676b773c9e2cd6de5e5542cee9a) URL.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the movies data details for the recommendation system in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will begin to explore the movie data file by parsing data
    into a Scala `case` class and generating a simple metric. The key here is to acquire
    an understanding of our data, so in the later stages, if nebulous results arise,
    we will have some insight to make an informed conclusion about the correctness
    of our results.
  prefs: []
  type: TYPE_NORMAL
- en: This is the first of the two recipes which explore the movie dataset. Data exploration
    is an important first step in statistical analysis and machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: One of the best ways to understand the data quickly is to generate a data visualization
    of it, and we will use JFreeChart to do that. It is very important to make sure
    you feel comfortable with the data and understand firsthand what is in each file,
    and the story it tries to tell.
  prefs: []
  type: TYPE_NORMAL
- en: We must always explore, understand, and visualize the data before we do anything
    else. Most performances and misses with ML and others systems can be traced to
    a lack of understanding of how the data is laid out and how it changes over time.
    If we look at the chart given in step 14 in this recipe, one immediately realizes
    that the distribution of movies over the years is not uniform, but skewed with
    high kurtosis. While we are not going to explore this property for optimization
    and sampling in this book, it makes an important point about the nature of the
    movie data.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: JFreeChart JAR can be downloaded from the [https://sourceforge.net/projects/jfreechart/files/](https://sourceforge.net/projects/jfreechart/files/) site.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Please make sure that the JFreeChart library and its dependencies (JCommon)
    are on the classpath for the chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We define the package information for the Scala program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the program started to execute, we initialized a SparkContext in our driver
    program to start the task of processing the data. This implies that the data must
    fit in the driver's memory (user's station), which is not a server requirement
    in this case. Alternative methods of divide and conquer must be devised to deal
    with extreme datasets (partial retrieval and the assembly at destination).
  prefs: []
  type: TYPE_NORMAL
- en: We continued by loading and parsing the data file into a dataset with the data
    type of the movies. The movie dataset was then grouped by year, yielding a map
    of movies keyed by year, with buckets of associated movies attached.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a737f15-9212-4f61-a824-8063fff13309.png)'
  prefs: []
  type: TYPE_IMG
- en: Next, we extracted the year with the count of the number of movies associated
    with the specific year to generate our histogram. We then collected the data,
    causing the entire resulting data collection to materialize on the driver, and
    passed it to JFreeChart to build the data visualization.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You need to be cognizant of our use of Spark SQL because of its flexibility.
    More information is available at [http://spark.apache.org/docs/latest/sql-programming-guide.html#running-sql-queries-programmatically](http://spark.apache.org/docs/latest/sql-programming-guide.html#running-sql-queries-programmatically).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more on using JFreechart, refer to the JFreeChart API documentation at [http://www.jfree.org/jfreechart/api.html](http://www.jfree.org/jfreechart/api.html).
  prefs: []
  type: TYPE_NORMAL
- en: You can find a good tutorial on JFreeChart at the [http://www.tutorialspoint.com/jfreechart/](http://www.tutorialspoint.com/jfreechart/) link.
  prefs: []
  type: TYPE_NORMAL
- en: The link for the JFreeChart itself is [http://www.jfree.org/index.html](http://www.jfree.org/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the ratings data details for the recommendation system in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the data from the user/rating perspective to understand
    the nature and property of our data file. We will start to explore the ratings
    data file by parsing data into a Scala case class and generating visualization
    for insight. The ratings data will be used a little later to generate features
    for our recommendation engine. Again, we stress that the first step in any data
    science/machine learning exercise should be the visualization and exploration
    of the data.
  prefs: []
  type: TYPE_NORMAL
- en: Once again, the best way of understanding data quickly is to generate a data
    visualization of it, and we will use a JFreeChart scatterplot to do this. A quick
    look at the chart of ...
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We define the package information for the Scala program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We now define a Scala `case class` to model the ratings data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s define a function to display a JFreeChart within a window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'In this step, we define a function for parsing a single line of data from the
    `ratings.dat` file into the rating `case class`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We are ready to begin building our `main` function, so let''s start with the
    location of our `ratings.dat` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration, SparkSession. In this example, we show for the
    first time how to set the Spark executor memory (for example, 2 gig) on a small
    laptop. You must increase this allocation if you want to use the large dataset
    (the 144 MB set):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The interleaving of log messages leads to hard to-read output; therefore, set
    the logging level to `ERROR`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Create a dataset of all the ratings from the data file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we convert the ratings dataset into a memory table view, where we can execute
    the Spark SQL query:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'We now produce a list of all user ratings grouped by user, with their totals:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/df32964e-28ee-4bd9-9231-99a9f81160da.png)'
  prefs: []
  type: TYPE_IMG
- en: Display a scatterplot chart with ratings per user. We choose a scatterplot to
    demonstrate a different way to look at the data from the previous recipe. We encourage
    readers to explore standardization techniques (for example, remove mean) or a
    volatility varying regime (for example, GARCH) to explore the autoregressive conditional
    heteroscedasticity property of this dataset (which is beyond the scope of this
    book). The reader is advised to consult any advanced time series book to develop
    an understanding of time varying volatility of the time series and how to correct
    this before usage.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Display the chart:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3ac4fbe1-8d5c-4181-af8f-3896f8636836.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began by loading and parsing the data file into a dataset with the data type
    ratings, and finally converted it to a DataFrame. The DataFrame was then used
    to execute a Spark SQL query that grouped all the ratings by user with their totals.
  prefs: []
  type: TYPE_NORMAL
- en: A full understanding of the API and its concepts (lazy instantiation, staging,
    pipelining, and caching) is critical for every Spark developer.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c455240d-f3f0-46c5-ae6f-0de99fb6a987.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, we passed the result set of data to the JFreeChart scatterplot component
    to display our chart.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Spark DataFrame is a distributed collection of data organized into named columns.
    All DataFrame operations are also automatically parallelized and distributed on
    clusters. Also, DataFrames are lazily evaluated like RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation on DataFrames can be found at [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: A good tutorial on JFreeChart can be found at the [http://www.tutorialspoint.com/jfreechart/](http://www.tutorialspoint.com/jfreechart/) linking.
  prefs: []
  type: TYPE_NORMAL
- en: JFreeChart can be downloaded from the [http://www.jfree.org/index.html](http://www.jfree.org/index.html) URL.
  prefs: []
  type: TYPE_NORMAL
- en: Building a scalable recommendation engine using collaborative filtering in Spark
    2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will be demonstrating a recommendation system that utilizes
    a technique known as collaborative filtering. At the core, collaborative filtering
    analyzes the relationship between users themselves and the dependencies between
    the inventory (for example, movies, books, news articles, or songs) to identify
    user-to-item relationships based on a set of secondary factors called **latent
    factors** (for example, female/male, happy/sad, active/passive). The key here
    is that you do not need to know the latent factors in advance.
  prefs: []
  type: TYPE_NORMAL
- en: The recommendation will be produced via the ALS algorithm which is a collaborative
    filtering technique. At a high level, collaborative filtering entails making predictions
    of what a user may be interested in based on collecting previously known preferences,
    combined with the preferences of many other users. We will be using the ratings
    data from the MovieLens dataset and will convert it into input features for the
    recommendation algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We define the package information for the Scala program:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'We now define two Scala case classes, to model movie and ratings data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: In this step, we define functions for parsing a single line of data from the
    `ratings.dat` file into the ratings `case class`, and for parsing ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Due to the complex nature of the program, we provide a conceptual explanation
    and then proceed to explain the details of the program.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a conceptual view of ALS and how it factorizes
    the user/movie/rating matrix, which is a high-ranking order matrix to a lower
    order tall and skinny matrix, and a vector of latent factors: f(users) and f(movies).'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/12112437-3df5-4c38-ba64-5a826ee53b88.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another way to think about it is that these factors can be used to place the
    movie in an *n* dimensional space that will be matched to a given recommendation
    for a given user. It is always desirable to view machine learning as a search
    query in a dimensional variable space. The point to remember is that the latent
    factor (learned geometry space) is not pre-defined and can be as low as 10 to
    100 or 1,000 depending on what is being searched or factorized. Our recommendation,
    then, can be viewed as placing a probability mass within the n-dimensional space.
    The following figure provides an extremely simplified view of a possible two-factor
    model (two-dimensional) to demonstrate the point:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c3e88ca-279e-4c95-aae2-1cf13b93737f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'While the implementation of ALS can vary a bit from system to system, at its
    core it is an iterative full-factorization method (in Spark) with weighed regularization.
    Spark''s documentation and tutorials provide an insight into the actual math and
    the nature of the algorithm. It depicts the algorithm as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7d907fdc-4cc5-4bfd-84d2-ef996cff20b1.png)'
  prefs: []
  type: TYPE_IMG
- en: The best way to understand this formula/algorithm is to think of it as an iterating
    apparatus which is trying to discover the latent factors by alternating between
    inputs (that is, fix one of the inputs and then approximate/optimize the other--and
    then back and forth), while trying to minimize the least square error (MSE) with
    respect to a regularization penalty of weighted lambda. A more detailed explanation
    is provided in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'The program flow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: The example started, by loading the ratings and movie data from the MovieLens
    dataset. The loaded data was then transformed into Scala case classes for further
    processing. The next step was to partition the ratings data into a training set
    and test set. The training set data was used to train the machine learning algorithm.
    Training is the process in machine learning used to build a model so it can provide
    the appropriate results needed. The test data will be used to validate the results
    in the final step.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The fictitious users, or user ID zero, step configured a single user not included
    in the original dataset to help lend insight to the results by creating a dataset
    on the fly with random information, and finally appending it to the training set.
    The ALS algorithm was invoked by passing the training set data to it, comprised
    of the user ID, movie ID, and rating, subsequently yielding a matrix factorization
    model from Spark. The prediction generation was performed for the user ID zero
    and test dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final results were displayed by combining rating information with the movie
    data so the results could be understood and displayed in the original rating next
    to the estimated rating. The final step was to compute the root mean squared error
    of the generated rating, with the existing rating contained within the test dataset.
    The RMSE will tell us how accurate the train model is.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: People often struggle with ALS even though at its core it is a simple linear
    algebra operation with an added regularization penalty. What makes ALS powerful
    is its ability to be parallelized and to deal with scale (for example, Spotify).
  prefs: []
  type: TYPE_NORMAL
- en: 'ALS in layman''s language involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: With ALS, you basically want to factorize a large matrix of ratings X (100 million
    plus users is not a stretch at all) and user product ratings into two matrices
    of A and B, with lower ranks (see any introductory linear algebra book). The problem
    is that it often becomes a very hard non-linear optimization problem to solve.
    To remedy with ALS, you introduce a simple solution (**A** for **Alternating**)
    in which you fix one of the matrices and partially ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark 2.0 ML documentation to explore the ALS API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html](https://spark.apache.org/docs/latest/mllib-collaborative-filtering.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.recommendation.ALS](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.recommendation.ALS)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.recommendation.ALSModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.recommendation.ALSModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark 2.0 MLlib documentation is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.ALS](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.recommendation.ALS).
  prefs: []
  type: TYPE_NORMAL
- en: 'ALS parameters and their default constructs an ALS instance with default parameters
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Dealing with implicit input for training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are times when the actual observations (ratings) are not available and
    one must deal with implied feedback parameters. This can be as simple as which
    audio track was listened to during an engagement to how long a movie was watched,
    or the context (indexed in advance) or what caused a switch (a Netflix movie abandoned
    in the beginning, middle, or near a specific scene). The example provided in the
    third recipe deals with explicit feedback via the use of `ALS.train()`.
  prefs: []
  type: TYPE_NORMAL
- en: The Spark ML library provides an alternative method, `ALS.trainImplicit()`,
    with four hyper parameters to control the algorithm and address the implicit data.
    If you are interested in testing this (it is very similar to the explicit ...
  prefs: []
  type: TYPE_NORMAL
