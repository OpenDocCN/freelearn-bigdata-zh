["```scala\nimport org.apache.spark.ml.feature.StringIndexer\nvar indexer = new StringIndexer()\n  .setInputCol(\"colors\")\n  .setOutputCol(\"colorsIndexed\")\n\nvar indexed = indexer.fit(df).transform(df)\n```", "```scala\nvar encoder = new OneHotEncoder()  .setInputCol(\"colorIndexed\")  .setOutputCol(\"colorVec\")var encoded = encoder.transform(indexed)\n```", "```scala\nimport org.apache.spark.ml.feature.VectorAssembler\nvectorAssembler = new VectorAssembler()\n        .setInputCols(Array(\"colorVec\", \"field2\", \"field3\",\"field4\"))\n        .setOutputCol(\"features\")\n```", "```scala\nvar transformers = indexer :: encoder :: vectorAssembler :: Nilvar pipeline = new Pipeline().setStages(transformers).fit(df)var transformed = pipeline.transform(df)\n```", "```scala\nimport org.apache.spark.ml.classification.RandomForestClassifiervar rf = new RandomForestClassifier()   .setLabelCol(\"label\") .setFeaturesCol(\"features\") ...\n```", "```scala\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nval evaluator = new BinaryClassificationEvaluator()\n\nimport org.apache.spark.ml.param.ParamMap\nvar evaluatorParamMap = ParamMap(evaluator.metricName -> \"areaUnderROC\")\nvar aucTraining = evaluator.evaluate(result, evaluatorParamMap)\n```", "```scala\nvar aucTraining = evaluator.evaluate(result, evaluatorParamMap)\n```", "```scala\ndef convert(filePrefix : String) = {   val basePath = \"yourBasePath\"   var df = spark              .read              .option(\"header\",true)              .option(\"inferSchema\", \"true\")              .csv(\"basePath+filePrefix+\".csv\")    df = df.repartition(1)    df.write.parquet(basePath+filePrefix+\".parquet\")}convert(\"train_numeric\")convert(\"train_date\")convert(\"train_categorical\")\n```", "```scala\nimport org.apache.spark.ml.feature.{OneHotEncoder, StringIndexer}\n\nvar indexer = new StringIndexer()\n  .setHandleInvalid(\"skip\")\n  .setInputCol(\"L0_S22_F545\")\n  .setOutputCol(\"L0_S22_F545Index\")\n\nvar indexed = indexer.fit(df_notnull).transform(df_notnull)\nindexed.printSchema\n```", "```scala\nvar encoder = new OneHotEncoder()\n  .setInputCol(\"L0_S22_F545Index\")\n  .setOutputCol(\"L0_S22_F545Vec\")\n\nvar encoded = encoder.transform(indexed)\n```", "```scala\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.linalg.Vectors\n\nvar vectorAssembler = new VectorAssembler()\n        .setInputCols(Array(\"L0_S22_F545Vec\", \"L0_S0_F0\", \"L0_S0_F2\",\"L0_S0_F4\"))\n        .setOutputCol(\"features\")\n\nvar assembled = vectorAssembler.transform(encoded)\n```", "```scala\nimport org.apache.spark.ml.Pipelineimport org.apache.spark.ml.PipelineModel//Create an array out of individual pipeline stagesvar transformers = Array(indexer,encoder,assembled)var pipeline = new Pipeline().setStages(transformers).fit(df_notnull)var transformed = pipeline.transform(df_notnull)\n```", "```scala\nimport org.apache.spark.ml.classification.RandomForestClassifier\nvar rf = new RandomForestClassifier() \n  .setLabelCol(\"label\")\n  .setFeaturesCol(\"features\")\n\nvar model = new Pipeline().setStages(transformers :+ rf).fit(df_notnull)\n\nvar result = model.transform(df_notnull)\n```", "```scala\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluatorval evaluator = new BinaryClassificationEvaluator()import org.apache.spark.ml.param.ParamMapvar evaluatorParamMap = ParamMap(evaluator.metricName -> \"areaUnderROC\")var aucTraining = evaluator.evaluate(result, evaluatorParamMap)\n```", "```scala\nimport org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder}\nvar paramGrid = new ParamGridBuilder()\n    .addGrid(rf.numTrees, 3 :: 5 :: 10 :: 30 :: 50 :: 70 :: 100 :: 150 :: Nil)\n    .addGrid(rf.featureSubsetStrategy, \"auto\" :: \"all\" :: \"sqrt\" :: \"log2\" :: \"onethird\" :: Nil)\n    .addGrid(rf.impurity, \"gini\" :: \"entropy\" :: Nil)    \n    .addGrid(rf.maxBins, 2 :: 5 :: 10 :: 15 :: 20 :: 25 :: 30 :: Nil)\n    .addGrid(rf.maxDepth, 3 :: 5 :: 10 :: 15 :: 20 :: 25 :: 30 :: Nil)\n    .build()\n\nvar crossValidator = new CrossValidator()\n      .setEstimator(new Pipeline().setStages(transformers :+ rf))\n      .setEstimatorParamMaps(paramGrid)\n      .setNumFolds(5)\n.setEvaluator(evaluator)\nvar crossValidatorModel = crossValidator.fit(df_notnull)\nvar newPredictions = crossValidatorModel.transform(df_notnull)\n```", "```scala\nvar bestPipelineModel = crossValidatorModel.bestModel.asInstanceOf[PipelineModel]    var stages = bestPipelineModel.stagesimport org.apache.spark.ml.classification.RandomForestClassificationModel    val rfStage = stages(stages.length-1).asInstanceOf[RandomForestClassificationModel]rfStage.getNumTreesrfStage.getFeatureSubsetStrategyrfStage.getImpurityrfStage.getMaxBinsrfStage.getMaxDepth\n```"]