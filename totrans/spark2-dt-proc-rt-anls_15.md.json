["```scala\npackage spark.ml.cookbook.chapter12\n```", "```scala\nimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SQLContextimport org.apache.spark.{SparkConf, SparkContext}import org.jfree.chart.axis.{CategoryAxis, CategoryLabelPositions}import org.jfree.chart.{ChartFactory, ChartFrame, JFreeChart}import org.jfree.chart.plot.{CategoryPlot, PlotOrientation}import org.jfree.data.category.DefaultCategoryDataset\n```", "```scala\ndef show(chart: JFreeChart) ...\n```", "```scala\npackage spark.ml.cookbook.chapter12\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, Word2Vec}\nimport org.apache.spark.sql.{SQLContext, SparkSession}\nimport org.apache.spark.{SparkConf, SparkContext}\n```", "```scala\nval input = \"../data/sparkml2/chapter12/pg62.txt\"\n```", "```scala\nval spark = SparkSession\n         .builder\n.master(\"local[*]\")\n         .appName(\"Word2Vec App\")\n         .config(\"spark.sql.warehouse.dir\", \".\")\n         .getOrCreate()\nimport spark.implicits._\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval df = spark.read.text(input).toDF(\"text\")\n```", "```scala\nval tokenizer = new RegexTokenizer()\n .setPattern(\"\\\\W+\")\n .setToLowercase(true)\n .setMinTokenLength(4)\n .setInputCol(\"text\")\n .setOutputCol(\"raw\")\n val rawWords = tokenizer.transform(df)\n```", "```scala\nval stopWords = new StopWordsRemover()\n .setInputCol(\"raw\")\n .setOutputCol(\"terms\")\n .setCaseSensitive(false)\n val wordTerms = stopWords.transform(rawWords)\n```", "```scala\nval word2Vec = new Word2Vec()\n .setInputCol(\"terms\")\n .setOutputCol(\"result\")\n .setVectorSize(3)\n .setMinCount(0)\nval model = word2Vec.fit(wordTerms)\n```", "```scala\nval synonyms = model.findSynonyms(\"martian\", 10)\n```", "```scala\nsynonyms.show(false)\n```", "```scala\nspark.stop()\n```", "```scala\ncurl -L -O http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2\n```", "```scala\nbunzip2 enwiki-latest-pages-articles-multistream.xml.bz2\n```", "```scala\nhead -n50 enwiki-latest-pages-articles-multistream.xml<mediawiki xmlns=http://www.mediawiki.org/xml/export-0.10/  xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" ...\n```", "```scala\npackage spark.ml.cookbook.chapter12\n```", "```scala\nimport edu.umd.cloud9.collection.wikipedia.WikipediaPage import edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage import org.apache.hadoop.fs.Path import org.apache.hadoop.io.Text import org.apache.hadoop.mapred.{FileInputFormat, JobConf} import org.apache.log4j.{Level, Logger} import org.apache.spark.mllib.feature.{HashingTF, IDF} import org.apache.spark.mllib.linalg.distributed.RowMatrix import org.apache.spark.sql.SparkSession import org.tartarus.snowball.ext.PorterStemmer ...\n```", "```scala\npackage spark.ml.cookbook.chapter12\n```", "```scala\nimport edu.umd.cloud9.collection.wikipedia.WikipediaPage\nimport edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage\nimport org.apache.hadoop.fs.Path\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.mapred.{FileInputFormat, JobConf}\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.ml.clustering.LDA\nimport org.apache.spark.ml.feature._\nimport org.apache.spark.sql.SparkSession\n```", "```scala\ndef parseWikiPage(rawPage: String): Option[(String, String)] = {\n val wikiPage = new EnglishWikipediaPage()\n WikipediaPage.*readPage*(wikiPage, rawPage)\n\n if (wikiPage.isEmpty\n || wikiPage.isDisambiguation\n || wikiPage.isRedirect\n || !wikiPage.isArticle) {\n None\n } else {\n *Some*(wikiPage.getTitle, wikiPage.getContent)\n }\n }\n```", "```scala\nval input = \"../data/sparkml2/chapter12/enwiki_dump.xml\" \n```", "```scala\nval jobConf = new JobConf()\n jobConf.set(\"stream.recordreader.class\", \"org.apache.hadoop.streaming.StreamXmlRecordReader\")\n jobConf.set(\"stream.recordreader.begin\", \"<page>\")\n jobConf.set(\"stream.recordreader.end\", \"</page>\")\n```", "```scala\nFileInputFormat.addInputPath(jobConf, new Path(input))\n```", "```scala\nval spark = SparkSession\n    .builder\n.master(\"local[*]\")\n    .appName(\"ProcessLDA App\")\n    .config(\"spark.serializer\",   \"org.apache.spark.serializer.KryoSerializer\")\n    .config(\"spark.sql.warehouse.dir\", \".\")\n    .getOrCreate()\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval wikiData = spark.sparkContext.hadoopRDD(\n jobConf,\n classOf[org.apache.hadoop.streaming.StreamInputFormat],\n classOf[Text],\n classOf[Text]).sample(false, .1)\n```", "```scala\nval df = wiki.map(_._1.toString)\n .flatMap(parseWikiPage)\n .toDF(\"title\", \"text\")\n```", "```scala\nval tokenizer = new RegexTokenizer()\n .setPattern(\"\\\\W+\")\n .setToLowercase(true)\n .setMinTokenLength(4)\n .setInputCol(\"text\")\n .setOutputCol(\"raw\")\n val rawWords = tokenizer.transform(df)\n```", "```scala\nval stopWords = new StopWordsRemover()\n .setInputCol(\"raw\")\n .setOutputCol(\"words\")\n .setCaseSensitive(false)\n\n val wordData = stopWords.transform(rawWords)\n```", "```scala\nval cvModel = new CountVectorizer()\n .setInputCol(\"words\")\n .setOutputCol(\"features\")\n .setMinDF(2)\n .fit(wordData)\n val cv = cvModel.transform(wordData)\n cv.cache()\n```", "```scala\nval lda = new LDA()\n .setK(5)\n .setMaxIter(10)\n .setFeaturesCol(\"features\")\n val model = lda.fit(tf)\n val transformed = model.transform(tf)\n```", "```scala\nval topics = model.describeTopics(5)\n topics.show(false)\n```", "```scala\nval vocaList = cvModel.vocabulary\ntopics.collect().foreach { r => {\n println(\"\\nTopic: \" + r.get(r.fieldIndex(\"topic\")))\n val y = r.getSeq[Int](r.fieldIndex(\"termIndices\")).map(vocaList(_))\n .zip(r.getSeq[Double](r.fieldIndex(\"termWeights\")))\n y.foreach(println)\n\n }\n}\n```", "```scala\nspark.stop()\n```"]