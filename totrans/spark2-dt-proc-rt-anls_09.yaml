- en: Testing and Debugging Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In an ideal world, we write perfect Spark codes and everything runs perfectly
    all the time, right? Just kidding; in practice, we know that working with large-scale
    datasets is hardly ever that easy, and there are inevitably some data points that
    will expose any corner cases with your code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering the aforementioned challenges, therefore, in this chapter, we will
    see how difficult it can be to test an application if it is distributed; then,
    we will see some ways to tackle this. In a nutshell, the following topics will
    be cover throughout this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Testing in a distributed environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing Spark application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging Spark application
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing in a distributed environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Leslie Lamport defined the term distributed system as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"A distributed system is one in which I cannot get any work done because some
    machine I have never heard of has crashed."'
  prefs: []
  type: TYPE_NORMAL
- en: Resource sharing through **World Wide Web** (aka **WWW**), a network of connected
    computers (aka a cluster), is a good example of distributed systems. These distributed
    environments are often complex and lots of heterogeneity occurs frequently. Testing
    in these kinds of heterogeneous environments is also challenging. In this section,
    at first, we will observe some commons issues that are often raised while working
    with such a system.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are numerous definitions of distributed systems. Let''s see some definition
    and then we will try to correlate the aforementioned categories afterward. Coulouris
    defines a distributed system as *a system in which hardware or software components
    located at networked computers communicate and coordinate their actions only by
    message passing*. On the other hand, Tanenbaum defines the term in several ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '*A collection of independent computers that appear to the users of the system
    as a single computer.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A system that consists of a collection of two or more independent Computers
    which coordinate their processing through the exchange of synchronous or asynchronous
    message passing.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*A distributed system is a collection of autonomous computers linked by a network
    with software designed to produce an integrated computing facility.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, based on the preceding definition, distributed systems can be categorized
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Only hardware and software are distributed: The local distributed system is
    connected through LAN.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Users are distributed, but there are computing and hardware resources that are
    running backend, for example, WWW.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both users and hardware/software are distributed: Distributed computing cluster
    that is connected through WAN. For example, you can get these types of computing
    facilities while using Amazon AWS, Microsoft Azure, Google Cloud, or Digital Ocean''s
    droplets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Issues in a distributed system
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here we will discuss some major issues that need to be taken care of during
    the software and hardware testing so that Spark jobs run smoothly in cluster computing,
    which is essentially a distributed computing environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that all the issues are unavoidable, but we can at least tune them for
    betterment. You should follow the instructions and recommendations given in the
    previous chapter. According to *Kamal Sheel Mishra* and *Anil Kumar Tripathi*,
    *Some Issues, Challenges and Problems of Distributed Software System*, in *International
    Journal of Computer Science and Information Technologies*, Vol. 5 (4), 2014, 4922-4925\.
    URL: [https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf](https://pdfs.semanticscholar.org/4c6d/c4d739bad13bcd0398e5180c1513f18275d8.pdf),
    there ...'
  prefs: []
  type: TYPE_NORMAL
- en: Challenges of software testing in a distributed environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are some common challenges associated with the tasks in an agile software
    development, and those challenges become more complex while testing the software
    in a distributed environment before deploying them eventually. Often team members
    need to merge the software components in parallel after the bugs proliferating.
    However, based on urgency, often the merging occurs before the testing phase.
    Sometimes, many stakeholders are distributed across teams. Therefore, there's
    a huge potential for misunderstanding and teams often lose in between.
  prefs: []
  type: TYPE_NORMAL
- en: For example, Cloud Foundry ([https://www.cloudfoundry.org/](https://www.cloudfoundry.org/))
    is an open source heavily distributed PaaS software system for managing deployment
    and scalability of applications in the Cloud. It promises different features such
    as scalability, reliability, and elasticity that come inherently to deployments
    on Cloud Foundry require the underlying distributed system to implement measures
    to ensure robustness, resiliency, and failover.
  prefs: []
  type: TYPE_NORMAL
- en: 'The process of software testing is long known to comprise *unit testing*, *integration
    testing*, *smoke testing*, *acceptance testing*, *scalability testing*, *performance
    testing*, and *quality of service testing*. In Cloud Foundry, the process of testing
    a distributed system is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c974ded-af48-4a29-a107-4dce0e42a32a.png)**Figure 1:** An example of
    software testing in a distributed environment like Cloud'
  prefs: []
  type: TYPE_NORMAL
- en: As shown in the preceding figure (first column), the process of testing in a
    distributed environment like Cloud starts with running unit tests against the
    smallest points of contract in the system. Following successful execution of all
    the unit tests, integration tests are run to validate the behavior of interacting
    components as part of a single coherent software system (second column) running
    on a single box (for example, a **Virtual Machine** (**VM**) or bare metal). However,
    while these tests validate the overall behavior of the system as a monolith, they
    do not guarantee system validity in a distributed deployment. Once integration
    tests pass, the next step (third column) is to validate distributed deployment
    of the system and run the smoke tests.
  prefs: []
  type: TYPE_NORMAL
- en: As you know, that the successful configuration of the software and execution
    of unit tests prepares us to validate acceptability of system behavior. This verification
    is done by running acceptance tests (fourth column). Now, to overcome the aforementioned
    issues and challenges in distributed environments, there are also other hidden
    challenges that need to be solved by researchers and big data engineers, but those
    are actually out of the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know what real challenges are for the software testing in a distributed
    environment, now let's start testing our Spark code a bit. The next section is
    dedicated to testing Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are many ways to try to test your Spark code, depending on whether it''s
    Java (you can do basic JUnit tests to test non-Spark pieces) or ScalaTest for
    your Scala code. You can also do full integration tests by running Spark locally
    or on a small test cluster. Another awesome choice from Holden Karau is using
    Spark-testing base. You probably know that there is no native library for unit
    testing in Spark as of yet. Nevertheless, we can have the following two alternatives
    to use two libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: ScalaTest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark-testing base
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, before starting to test your Spark applications written in Scala, some
    background knowledge about unit testing and testing Scala methods is a mandate.
  prefs: []
  type: TYPE_NORMAL
- en: Testing Scala methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Here, we will see some simple techniques for testing Scala methods. For Scala
    users, this is the most familiar unit testing framework (you can also use it for
    testing Java code and soon for JavaScript). ScalaTest supports a number of different
    testing styles, each designed to support a specific type of testing need. For
    details, see ScalaTest User Guide at [http://www.scalatest.org/user_guide/selecting_a_style](http://www.scalatest.org/user_guide/selecting_a_style).
    Although ScalaTest supports many styles, one of the quickest ways to get started
    is to use the following ScalaTest traits and write the tests in the **TDD** (**test-driven
    development**) style:'
  prefs: []
  type: TYPE_NORMAL
- en: '`FunSuite`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`Assertions`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`BeforeAndAfter`'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Feel free to browse the preceding URLs to learn more about these traits; that
    will make the rest of this tutorial go smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that the TDD is a programming technique to develop software,
    and it states that you should start development from tests. Hence, it doesn't
    affect how tests are written, but when tests are written. There is no trait or
    testing style to enforce or encourage TDD in `ScalaTest.FunSuite`, `Assertions`,
    and `BeforeAndAfter` are only more similar to the xUnit testing frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are three assertions available in the ScalaTest in any style trait:'
  prefs: []
  type: TYPE_NORMAL
- en: '`assert`: This is used for general assertions in your Scala program.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertResult`: This helps differentiate expected value from the actual values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertThrows`: This is used to ensure a bit of code throws an expected exception.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The ScalaTest''s assertions are defined in the trait `Assertions`, which is
    further extended by `Suite`. In brief, the `Suite` trait is the super trait for
    all the style traits. According to the ScalaTest documentation at [http://www.scalatest.org/user_guide/using_assertions](http://www.scalatest.org/user_guide/using_assertions),
    the `Assertions` trait also provides the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`assume` to conditionally cancel a test'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`fail` to fail a test unconditionally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`cancel` to cancel a test unconditionally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`succeed` to make a test succeed unconditionally'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`intercept` to ensure a bit of code throws an expected exception and then make
    assertions about the exception'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertDoesNotCompile` to ensure a bit of code does not compile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertCompiles` to ensure a bit of code does compile'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`assertTypeError` to ensure a bit of code does not compile because of a type
    (not parse) error'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`withClue` to add more information about a failure'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'From the preceding list, we will show a few of them. In your Scala program,
    you can write assertions by calling `assert` and passing a `Boolean` expression
    in. You can simply start writing your simple unit test case using `Assertions`.
    The `Predef` is an object, where this behavior of assert is defined. Note that
    all the members of the `Predef` get imported into your every Scala source file.
    The following source code will print `Assertion success` for the following case:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'However, if you make `a = 2` and `b = 1`, for example, the assertion will fail
    and you will experience the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7f37e532-3351-4ae6-80a6-4679d0579aec.png)**Figure 2:** An example of
    assertion fail'
  prefs: []
  type: TYPE_NORMAL
- en: If you pass a true expression, assert will return normally. However, assert
    will terminate abruptly with an Assertion Error if the supplied expression is
    false. Unlike the `AssertionError` and `TestFailedException` forms, the ScalaTest's
    assert provides more information that will tell you exactly in which line the
    test case failed or for which expression. Therefore, ScalaTest's assert provides
    better error messages than Scala's assert.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, for the following source code, you should experience `TestFailedException`
    that will tell that 5 did not equal 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The following figure shows the output of the preceding Scala test:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ac606f4-4805-4633-a8c5-5a99799c8f3e.png)**Figure 3:** An example of
    TestFailedException'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following source code explains the use of the `assertResult` unit test
    to test the result of your method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding assertion will be failed and Scala will throw an exception `TestFailedException`
    and prints `Expected 3 but got 4` (*Figure 4*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1b1bcd86-c72c-494a-b7b0-6ce8c22ddc47.png)**Figure 4:** Another example
    of TestFailedException'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s see a unit testing to show expected exception:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'If you try to access an array element outside the index, the preceding code
    will tell you if you''re allowed to access the first character of the preceding
    string `Hello world!`. If your Scala program can access the value in an index,
    the assertion will fail. This also means that the test case has failed. Thus,
    the preceding test case will fail naturally since the first index contains the
    character `H`, and you should experience the following error message (*Figure
    5*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/53b86f5b-c893-4daa-8061-2a9f025b803b.png)**Figure 5:** Third example
    of TestFailedException'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, now let''s try to access the index at position `-1` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the assertion should be true, and consequently, the test case will be passed.
    Finally, the code will terminate normally. Now, let''s check our code snippets
    if it will compile or not. Very often, you may wish to ensure that a certain ordering
    of the code that represents emerging "user error" does not compile at all. The
    objective is to check the strength of the library against the error to disallow
    unwanted result and behavior. ScalaTest''s `Assertions` trait includes the following
    syntax for that purpose:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to ensure that a snippet of code does not compile because of a
    type error (as opposed to a syntax error), use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'A syntax error will still result on a thrown `TestFailedException`. Finally,
    if you want to state that a snippet of code does compile, you can make that more
    obvious with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'A complete example is shown as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2fbf984e-a29a-4dda-a9ff-389d870142c4.png)**Figure 6:** Multiple tests
    together'
  prefs: []
  type: TYPE_NORMAL
- en: Now we would like to finish the Scala-based unit testing due to page limitation.
    However, for other unit test cases, you can refer the Scala test guideline at
    [http://www.scalatest.org/user_guide](http://www.scalatest.org/user_guide).
  prefs: []
  type: TYPE_NORMAL
- en: Unit testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In software engineering, often, individual units of source code are tested to
    determine whether they are fit for use or not. This way of software testing method
    is also called the unit testing. This testing ensures that the source code developed
    by a software engineer or developer meets the design specifications and works
    as intended.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, the goal of unit testing is to separate each part of the
    program (that is, in a modular way). Then try to observe if all the individual
    parts are working normally. There are several benefits of unit testing in any
    software system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Find problems early:** It finds bugs or missing parts of the specification
    early in the development cycle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facilitates change:** It helps in refactoring ...'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Testing Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have already seen how to test your Scala code using built-in `ScalaTest`
    package of Scala. However, in this subsection, we will see how we could test our
    Spark application written in Scala. The following three methods will be discussed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Method 1:** Testing Spark applications using JUnit'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method 2:** Testing Spark applications using `ScalaTest` package'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Method 3:** Testing Spark applications using Spark testing base'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Methods 1 and 2 will be discussed here with some practical codes. However, a
    detailed discussion on method 3 will be provided in the next subsection. To keep
    the understanding easy and simple, we will use the famous word counting applications
    to demonstrate methods 1 and 2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 1: Using Scala JUnit test'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Suppose you have written an application in Scala that can tell you how many
    words are there in a document or text file as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code simply parses a text file and performs a `flatMap` operation
    by simply splitting the words. Then, it performs ...
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 2: Testing Scala code using FunSuite'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s redesign the preceding test case by returning only the RDD of the
    texts in the document, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'So, the `prepareWordCountRDD()` method in the preceding class returns an RDD
    of string and integer values. Now, if we want to test the `prepareWordCountRDD()`
    method''s functionality, we can do it more explicit by extending the test class
    with `FunSuite` and `BeforeAndAfterAll` from the `ScalaTest` package of Scala.
    The testing works in the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: Extend the test class with `FunSuite` and `BeforeAndAfterAll` from the `ScalaTest`
    package of Scala
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override the `beforeAll()` that creates Spark context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perform the test using the `test()` method and use the `assert()` method inside
    the `test()` method
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Override the `afterAll()` method that stops the Spark context
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Based on the preceding steps, let''s see a class for testing the preceding
    `prepareWordCountRDD()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The first test says that if two RDDs materialize in two different ways, the
    contents should be the same. Thus, the first test should get passed. We will see
    this in the following example. Now, for the second test, as we have seen previously,
    the word count of RDD is 214, but let's assume it unknown for a while. If it's
    214 coincidentally, the test case should pass, which is its expected behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, we are expecting both tests to be passed. Now, on Eclipse, run the test
    suite as `ScalaTest-File`, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/87342913-3a2b-4a9e-8a70-45cffdb44044.png) **Figure 10:** running the
    test suite as ScalaTest-File'
  prefs: []
  type: TYPE_NORMAL
- en: Now you should observe the following output (*Figure 11*). The output shows
    how many test cases we performed and how many of them passed, failed, canceled,
    ignored, or wasÂ in pending. It also shows the time to execute the overall test.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0db3c30f-5d87-43d8-a784-4cbbf13bb513.png)**Figure 11:** Test result
    when running the two test suites as ScalaTest-file'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fantastic! The test case passed. Now, let''s try changing the compare value
    in the assertion in the two separate tests using the `test()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you should expect that the test case will be failed. Now run the earlier
    class as `ScalaTest-File` (*Figure 12*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ca0967a-280e-46f0-a6e8-934dbcc0e93f.png)**Figure 12:** Test result
    when running the preceding two test suites as ScalaTest-File'
  prefs: []
  type: TYPE_NORMAL
- en: Well done! We have learned how to perform the unit testing using Scala's FunSuite.
    However, if you evaluate the preceding method carefully, you should agree that
    there are several disadvantages. For example, you need to ensure an explicit management
    of `SparkContext` creation and destruction. As a developer or programmer, you
    have to write more lines of code for testing a sample method. Sometimes, code
    duplication occurs as the *Before* and the *After* step has to be repeated in
    all test suites. However, this is debatable since the common code could be put
    in a common trait.
  prefs: []
  type: TYPE_NORMAL
- en: Now the question is how could we improve our experience? My recommendation is
    using the Spark testing base to make life easier and more straightforward. We
    will discuss how we could perform the unit testing the Spark testing base.
  prefs: []
  type: TYPE_NORMAL
- en: 'Method 3: Making life easier with Spark testing base'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark testing base helps you to test your most of the Spark codes with ease.
    So, what are the pros of this method then? There are many in fact. For example,
    using this the code is not verbose but we can get a very succinct code. The API
    is itself richer than that of ScalaTest or JUnit. Multiple languages support,
    for example, Scala, Java, and Python. It has the support of built-in RDD comparators.
    You can also use it for testing streaming applications. And finally and most importantly,
    it supports both local and cluster mode testings. This is most important for testing
    in a distributed environment.
  prefs: []
  type: TYPE_NORMAL
- en: The GitHub repo is located at [https://github.com/holdenk/spark-testing-base](https://github.com/holdenk/spark-testing-base).
  prefs: []
  type: TYPE_NORMAL
- en: Before starting ...
  prefs: []
  type: TYPE_NORMAL
- en: Configuring Hadoop runtime on Windows
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have already seen how to test your Spark applications written in Scala on
    Eclipse or IntelliJ, but there is another potential issue that should not be overlooked.
    Although Spark works on Windows, Spark is designed to be run on the UNIX-like
    operating system. Therefore, if you are working in a Windows environment, then
    extra care needs to be taken.
  prefs: []
  type: TYPE_NORMAL
- en: 'While using Eclipse or IntelliJ to develop your Spark applications for solving
    data analytics, machine learning, data science, or deep learning applications
    on Windows, you might face an I/O exception error and your application might not
    compile successfully or may be interrupted. Actually, the thing is that Spark
    expects that there is a runtime environment for Hadoop on Windows too. For example,
    if you run a Spark application, say `KMeansDemo.scala`, on Eclipse for the first
    time, you will experience an I/O exception saying the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The reason is that by default, Hadoop is developed for the Linux environment,
    and if you are developing your Spark applications on Windows platform, a bridge
    is required that will provide an environment for the Hadoop runtime for Spark
    to be properly executed. The details of the I/O exception can be seen in the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/32afa337-6d87-4b24-859d-c30a53ab627f.png)**Figure 14:** I/O exception
    occurred due to the failure of not to locate the winutils binary in the Hadoop
    binary path'
  prefs: []
  type: TYPE_NORMAL
- en: Now, how to get rid of this problem then? The solution is straightforward. As
    the error message says, we need to have an executable, namely `winutils.exe`.
    Now download the `winutils.exe` file from [https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin](https://github.com/steveloughran/winutils/tree/master/hadoop-2.7.1/bin),
    paste it in the Spark distribution directory, and configure Eclipse. More specifically,
    suppose your Spark distribution containing Hadoop is located at `C:/Users/spark-2.1.0-bin-hadoop2.7`.
    Inside the Spark distribution, there is a directory named bin. Now, paste the
    executable there (that is, `path = C:/Users/spark-2.1.0-binhadoop2.7/bin/`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The second phase of the solution is going to Eclipse and then selecting the
    main class (that is, `KMeansDemo.scala` in this case), and then going to the Run
    menu. From the Run menu, go to the Run Configurations option and from there select
    the Environment tab, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5e41aab-8cce-4deb-8f05-446767d78561.png)**Figure 15:** Solving the
    I/O exception occurred due to the absence of winutils binary in the Hadoop binary
    path'
  prefs: []
  type: TYPE_NORMAL
- en: If you select the tab, you a will have the option to create a new environmental
    variable for Eclipse using the JVM. Now create a new environmental variable named
    `HADOOP_HOME` and put the value as `C:/Users/spark-2.1.0-bin-hadoop2.7/`. Now
    press on Apply button and rerun your application, and your problem should be resolved.
  prefs: []
  type: TYPE_NORMAL
- en: It is to be noted that while working with Spark on Windows in a PySpark, the
    `winutils.exe` file is required too.
  prefs: []
  type: TYPE_NORMAL
- en: Please make a note that the preceding solution is also applicable in debugging
    your applications. Sometimes, even if the preceding error occurs, your Spark application
    will run properly. However, if the size of the dataset is large, it is most likely
    that the preceding error will occur.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to debug Spark applications that are running
    locally (on Eclipse or IntelliJ), standalone or cluster mode in YARN or Mesos.
    However, before diving deeper, it is necessary to know about logging in the Spark
    application.
  prefs: []
  type: TYPE_NORMAL
- en: Logging with log4j with Spark recap
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As stated earlier, Spark uses log4j for its own logging. If you configured
    Spark properly, Spark gets logged all the operation to the shell console. A sample
    snapshot of the file can be seen from the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5aa3075-1e4f-4fd6-8594-01e12076c1ce.png)**Figure 16:** A snap of the
    log4j.properties file'
  prefs: []
  type: TYPE_NORMAL
- en: 'Set the default spark-shell log level to WARN. When running the spark-shell,
    the log level for this class is used to overwrite the root logger''s log level
    so that the user can have different defaults for the shell and regular Spark apps.
    We also need to append JVM arguments when launching a job executed by an executor
    and managed by the driver. For this, you should edit the `conf/spark-defaults.conf`.
    In short, the following options can be added:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'To make the discussion clearer, we need to hide all the logs generated by Spark.
    We then can redirect them to be logged in the file system. On the other hand,
    we want our own logs to be logged in the shell and a separate file so that they
    don''t get mixed up with the ones from Spark. From here, we will point Spark to
    the files where our own logs are, which in this particular case is `/var/log/sparkU.log`.
    This `log4j.properties` file is then picked up by Spark when the application starts,
    so we don''t have to do anything aside of placing it in the mentioned location:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code, everything is printed as INFO once the log level is
    set to `INFO` until you set the level to a new level for example `WARN`. However,
    after that no info or trace and so on, that will not be printed. In addition to
    that, there are several valid logging levels supported by log4j with Spark. The
    successful execution of the preceding code should generate the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also set up the default logging for Spark shell in `conf/log4j.properties`.
    Spark provides a template of the log4j as a property file, and we can extend and
    modify that file for logging in Spark. Move to the `SPARK_HOME/conf` directory
    and you should see the `log4j.properties.template` file. You should use the following
    `conf/log4j.properties.template` after renaming it to `log4j.properties`. While
    developing your Spark application, you can put the `log4j.properties` file under
    your project directory while working on an IDE-based environment such as Eclipse.
    However, to disable logging completely, just set the `log4j.logger.org` flags
    as `OFF` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'So far, everything is very easy. However, there is a problem we haven''t noticed
    yet in the preceding code segment. One drawback of the `org.apache.log4j.Logger`
    class is that it is not serializable, which implies that we cannot use it inside
    a closure while doing operations on some parts of the Spark API. For example,
    suppose we do the following in our Spark code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'You should experience an exception that says `Task` not serializable as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'At first, we can try to solve this problem in a naive way. What you can do
    is just make the Scala class (that does the actual operation) `Serializable` using
    `extends Serializable` . For example, the code looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This section is intended for carrying out a discussion on logging. However,
    we take the opportunity to make it more versatile for general purpose Spark programming
    and issues. In order to overcome the `task not serializable` error in a more efficient
    way, the compiler will try to send the whole object (not only the lambda) by making
    it serializable and forces SPark to accept that. However, it increases shuffling
    significantly, especially for big objects! The other ways are making the whole
    class `Serializable` or by declaring the instance only within the lambda function
    passed in the map operation. Sometimes, keeping the not `Serializable` objects
    across the nodes can work. Lastly, use the `forEachPartition()` or `mapPartitions()`
    instead of just `map()` and create the not `Serializable` objects. In summary,
    these are the ways to solve the problem around:'
  prefs: []
  type: TYPE_NORMAL
- en: Serializable the class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Declare the instance only within the lambda function passed in the map
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Make the NotSerializable object as a static and create it once per machine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Call the `forEachPartition ()` or `mapPartitions()` instead of `map()` and create
    the NotSerializable object
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the preceding code, we have used the annotation `@transient lazy`, which
    marks the `Logger` class to be nonpersistent. On the other hand, an object containing
    the method apply (i.e. `MyMapperObject`) that instantiate the object of the `MyMapper`
    class is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, the object containing the `main()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s see another example that provides better insight to keep fighting
    the issue we are talking about. Suppose we have the following class that computes
    the multiplication of two integers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, essentially, if you try to use this class for computing the multiplication
    in the lambda closure using `map()`, you will get the `Task Not Serializable`
    error that we described earlier. Now we simply can use `foreachPartition()` and
    the lambda inside as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you compile it, it should return the desired result. For your ease,
    the complete code with the `main()` method is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Debugging the Spark application
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will discuss how to debug Spark applications running on
    locally on Eclipse or IntelliJ, as standalone or cluster mode in YARN or Mesos.
    Before getting started, you can also read the debugging documentation at [https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/](https://hortonworks.com/hadoop-tutorial/setting-spark-development-environment-scala/).
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark application on Eclipse as Scala debug
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To make this happen, just configure your Eclipse to debug your Spark applications
    as a regular Scala code debug. To configure select Run | Debug Configuration |
    Scala Application as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/eb20cffd-d0b0-4286-96bb-2ebb36b82048.png)**Figure 17:** Configuring
    Eclipse to debug Spark applications as a regular Scala code debug'
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose we want to debug our `KMeansDemo.scala` and ask Eclipse (you can have
    similar options on InteliJ IDE) to start the execution at line 56 and set the
    breakpoint in line 95\. To do so, run your Scala code as debugging and you should
    observe the following scenario on Eclipse:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bb345bf6-ae2b-4a92-9811-37038227fc01.png)**Figure 18:** Debugging Spark
    applications on Eclipse'
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, Eclipse will pause on the line you ask it to stop the execution in line
    95, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d4e11ff-3edf-456e-be9f-fa62cacac199.png)**Figure 19:** Debugging Spark
    applications on Eclipse (breakpoint)'
  prefs: []
  type: TYPE_IMG
- en: In summary, to simplify the preceding example, if there is any error between
    line 56 and line 95, Eclipse will show where the error actually occurs. Otherwise,
    it will follow the normal workflow if not interrupted.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark jobs running as local and standalone mode
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While debugging your Spark application locally or as standalone mode, you should
    know that debugging the driver program and debugging one of the executors is different
    since using these two types of nodes requires different submission parameters
    passed to `spark-submit`. Throughout this section, I''ll use port 4000 as the
    address. For example, if you want to debug the driver program, you can add the
    following to your `spark-submit` command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: After that, you should set your remote debugger to connect to the node where
    you have submitted the driver program. For the preceding case, port number 4000
    was ...
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark applications on YARN or Mesos cluster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When you run a Spark application on YARN, there is an option that you can enable
    by modifying `yarn-env.sh`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the remote debugging will be available through port 4000 on your Eclipse
    or IntelliJ IDE. The second option is by setting the `SPARK_SUBMIT_OPTS`. You
    can use either Eclipse or IntelliJ to develop your Spark applications that can
    be submitted to be executed on remote multinode YARN clusters. What I do is that
    I create a Maven project on Eclipse or IntelliJ and package my Java or Scala application
    as a jar file and then submit it as a Spark job. However, in order to attach your
    IDE such as Eclipse or IntelliJ debugger to your Spark application, you can define
    all the submission parameters using the `SPARK_SUBMIT_OPTS` environment variable
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then submit your Spark job as follows (please change the values accordingly
    based on your requirements and setup):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding command, it will wait until you connect your debugger,
    as shown in the following: `Listening for transport dt_socket at address: 4000`.
    Now you can configure your Java remote application (Scala application will work
    too) on the IntelliJ debugger, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/15c30522-5742-4de4-885f-fc568d44748e.png)**Figure 20:** Configuring
    remote debugger on IntelliJ'
  prefs: []
  type: TYPE_NORMAL
- en: For the preceding case, 10.200.1.101 is the IP address of the remote computing
    node where your Spark job is basically running. Finally, you will have to start
    the debugger by clicking on Debug under IntelliJ's Run menu. Then, if the debugger
    connects to your remote Spark app, you will see the logging info in the application
    console on IntelliJ. Now if you can set the breakpoints and the rests of them
    are normal debugging.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure shows an example how will you see on the IntelliJ when
    pausing a Spark job with a breakpoint:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2d8b430a-9583-4009-b8ba-bebcc6dfbd13.png)**Figure 21:** An example
    how will you see on the IntelliJ when pausing a Spark job with a breakpoint'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although it works well, sometimes I experienced that using `SPARK_JAVA_OPTS`
    won''t help you much in the debug process on Eclipse or even IntelliJ. Instead,
    use and export `SPARK_WORKER_OPTS` and `SPARK_MASTER_OPTS` while running your
    Spark jobs on a real cluster (YARN, Mesos, or AWS) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Then start your Master node as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Now open an SSH connection to your remote machine where the Spark job is actually
    running and map your localhost at 4000 (aka `localhost:4000`) to `host_name_to_your_computer.org:5000`,
    assuming the cluster is at `host_name_to_your_computer.org:5000` and listening
    on port 5000\. Now that your Eclipse will consider that you''re just debugging
    your Spark application as a local Spark application or process. However, to make
    this happen, you will have to configure the remote debugger on Eclipse, as shown
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/591d8d5c-0a0a-4e5d-9fd0-d38bf0b71f89.png)**Figure 22:** Connecting
    remote host on Eclipse for debugging Spark application'
  prefs: []
  type: TYPE_NORMAL
- en: That's it! Now you can debug on your live cluster as if it were your desktop.
    The preceding examples are for running with the Spark Master set as YARN-client.
    However, it should also work when running on a Mesos cluster. If you're running
    using YARN-cluster mode, you may have to set the driver to attach to your debugger
    rather than attaching your debugger to the driver since you won't necessarily
    know in advance what mode the driver will be executing on.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging Spark application using SBT
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The preceding setting works mostly on Eclipse or IntelliJ using the Maven project.
    Suppose that you already have your application done and are working on your preferred
    IDEs such as IntelliJ or Eclipse as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: Now, if you want to get this job to the local cluster (standalone), the very
    first step is packaging ...
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you saw how difficult the testing and debugging your Spark
    applications are. These can even be more critical in a distributed environment.
    We also discussed some advanced ways to tackle them altogether. In summary, you
    learned the way of testing in a distributed environment. Then you learned a better
    way of testing your Spark application. Finally, we discussed some advanced ways
    of debugging Spark applications.
  prefs: []
  type: TYPE_NORMAL
- en: This is more or less the end of our little journey with advanced topics on Spark.
    Now, a general suggestion from our side to you as readers or if you are relatively
    newer to the data science, data analytics, machine learning, Scala, or Spark is
    that you should at first try to understand what types of analytics you want to
    perform. To be more specific, for example, if your problem is a machine learning
    problem, try to guess what type of learning algorithms should be the best fit,
    that is, classification, clustering, regression, recommendation, or frequent pattern
    mining. Then define and formulate the problem, and after that, you should generate
    or download the appropriate data based on the feature engineering concept of Spark
    that we have discussed earlier. On the other hand, if you think that you can solve
    your problem by using deep learning algorithms or APIs, you should use other third-party
    algorithms and integrate with Spark and work straight away.
  prefs: []
  type: TYPE_NORMAL
- en: Our final recommendation to the readers is to browse the Spark website (at [http://spark.apache.org/](http://spark.apache.org/))
    regularly to get the updates and also try to incorporate the regular Spark-provided
    APIs with other third-party applications or tools to get the best result of the
    collaboration.
  prefs: []
  type: TYPE_NORMAL
