- en: Common Recipes for Implementing a Robust Machine Learning System
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Spark's basic statistical API to help you build your own algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML pipelines for real-life machine learning applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing data with Spark
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Splitting data for training and testing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Common operations with the new Dataset API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating and using RDD versus DataFrame versus Dataset from a text file in Spark
    2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LabeledPoint data structure for Spark ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting access to Spark cluster in Spark 2.0+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting access to Spark cluster pre-Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting access to SparkContext vis-a-vis SparkSession object in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New model export and PMML markup in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression model evaluation using Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary classification model evaluation ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In every line of business, ranging from running a small business to creating
    and managing a mission-critical application, there are a number of tasks that
    are common and need to be included as a part of almost every workflow that is
    required during the course of executing the functions. This is true even for building
    robust machine learning systems. In Spark machine learning, some of these tasks
    range from splitting the data for model development (train, test, validate) to
    normalizing input feature vector data to creating ML pipelines via the Spark API.
    We provide a set of recipes in this chapter to enable the reader to think about
    what is actually required to implement an end-to-end machine learning system.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter attempts to demonstrate a number of common tasks which are present
    in any robust Spark machine learning system implementation. To avoid redundant
    references these common tasks in every recipe covered in this book, we have factored
    out such common tasks as short recipes in this chapter, which can be leveraged
    as needed while reading the other chapters. These recipes can either stand alone
    or be included as pipeline subtasks in a larger system. Please note that these
    common recipes are emphasized in the larger context of machine learning algorithms
    in later chapters, while also including them as independent recipes in this chapter
    for completeness.
  prefs: []
  type: TYPE_NORMAL
- en: Spark's basic statistical API to help you build your own algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we cover Spark's multivariate statistical summary (that is,
    *Statistics.colStats*) such as correlation, stratified sampling, hypothesis testing,
    random data generation, kernel density estimators, and much more, which can be
    applied to extremely large datasets while taking advantage of both parallelism
    and resiliency via RDDs.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s retrieve the Spark session underlying the SparkContext to use when generating
    RDDs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we create a RDD with the handcrafted data to illustrate usage of summary
    statistics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We use Spark''s statistics objects by invoking the method `colStats()` and
    passing the RDD as an argument:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `colStats()` method will return a `MultivariateStatisticalSummary`, which
    contains the computed summary statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created an RDD from dense vector data followed by the generation of summary
    statistics on it using the statistics object. Once the `colStats()` method returned,
    we retrieved summary statistics such as the mean, variance, minimum, maximum,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: It cannot be emphasized enough how efficient the statistical API is on large
    datasets. These APIs will provide you with basic elements to implement any statistical
    learning algorithm from scratch. Based on our research and experience with half
    versus full matrix factorization, we encourage you to first read the source code
    and make sure that there isn't an equivalent functionality already implemented
    in Spark before implementing your own.
  prefs: []
  type: TYPE_NORMAL
- en: 'While we only demonstrate a basic statistics summary here, Spark comes equipped
    out of the box with:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Correlation: `Statistics.corr(seriesX, seriesY, "type of correlation")`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pearson (default)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Spearman
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stratified sampling - RDD API:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With a replacement RDD
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Without a replacement - requires an additional pass
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hypothesis testing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector - `Statistics.chiSqTest( vector )`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix - `Statistics.chiSqTest( dense matrix )`
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Kolmogorov-Smirnov** (**KS**) test for equality - one or two-sided:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Statistics.kolmogorovSmirnovTest(RDD, "norm", 0, 1)`'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Random data generator - `normalRDD()`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normal - can specify a parameter
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Lots of option plus `map()`s to generate any distribution
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel density estimator - `KernelDensity().estimate( data )`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick reference to the *Goodness of fit* concept in statistics can be found
    atÂ [https://en.wikipedia.org/wiki/Goodness_of_fit](https://en.wikipedia.org/wiki/Goodness_of_fit)
    link.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for more multivariate statistical summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.MultivariateStatisticalSummary)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ML pipelines for real-life machine learning applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is the first of two recipes which cover the ML pipeline in Spark 2.0\.
    For a more advanced treatment of ML pipelines with additional details such as
    API calls and parameter extraction, see later chapters in this book.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we attempt to have a single pipeline that can tokenize text,
    use HashingTF (an old trick) to map term frequencies, run a regression to fit
    a model, and then predict which group a new term belongs to (for example, news
    filtering, gesture classification, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we investigated constructing a simple machine learning pipeline
    with Spark. We began with creating a DataFrame comprised of two groups of text
    documents and then proceeded to set up a pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: First, we created a tokenizer to parse text documents into terms followed by
    the creation of the HashingTF to convert the terms into features. Then, we created
    a logistic regression object to predict which group a new text document belongs
    to.
  prefs: []
  type: TYPE_NORMAL
- en: Second, we constructed the pipeline by passing an array of arguments to it,
    specifying three stages of execution. You will notice each subsequent stage provides
    the result as a specified column while using the previous stage's output column
    as the input.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we trained the model by invoking `fit()` on the pipeline object and
    defining a set of test data for verification. Next, we transformed the test set
    with the model, producing which of the defined two groups the text documents in
    the test set belong to.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The pipeline in Spark ML was inspired by scikit-learn in Python, which is referenced
    here for completeness:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://scikit-learn.org/stable/](http://scikit-learn.org/stable/)'
  prefs: []
  type: TYPE_NORMAL
- en: ML pipelines make it easy to combine multiple algorithms used to implement a
    production task in Spark. It would be unusual to see a use case in a real-life
    situation that is made of a single algorithm. Often a number of cooperating ML
    algorithms work together to achieve a complex use case. For example, in LDA-based
    systems (for example, news briefings) or human emotion detection, there are a
    number of steps before and after the core system to be implemented as a single
    pipe to produce any meaningful and production-worthy system. See the following
    link for a real-life use case requiring ...
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for more multivariate statistical summary:'
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline docs are available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.Pipeline)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pipeline model that is useful when we load and save the `.load()`, `.save()
    methods`: [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pipeline stage information is available at [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.PipelineStage)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: HashingTF, a nice old trick to map a sequence to their term frequency in text
    analytics is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.feature.HashingTF)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Normalizing data with Spark
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate normalizing (scaling) the data prior to importing
    the data into an ML algorithm. There are a good number of ML algorithms such as
    **Support Vector Machine** (**SVM**) that work better with scaled input vectors
    rather than with the raw values.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to the UCI Machine Learning Repository and download theÂ [http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)Â file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a method to parse wine data into a tuple:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `spark.implicits`, therefore adding in behavior with only an `import`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s load the wine data into memory, taking only the first four columns and
    converting the latter three into a new feature vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we generate a DataFrame with two columns:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we will print out the DataFrame schema and display data contained within
    the DataFrame:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/da1f5304-1a7d-40c1-9695-cd40675ce2df.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, we generate the scaling model and transform the feature into a common
    range between a negative and positive one displaying the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/b0cc65be-4e1c-4114-bab7-19ce12c42628.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we explored feature scaling which is a critical step in most
    machine learning algorithms such as **classifiers**. We started out by loading
    the wine data files, extracted an identifier, and used the next three columns
    to create a feature vector.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we created a `MinMaxScaler` object, configuring a minimum and maximum
    range to scale our values into. We invoked the scaling model by executing the
    `fit()` method on the scaler class, and then we used the model to scale the values
    in our DataFrame.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we displayed the resulting DataFrame and we noticed feature vector
    values ranges are between negative 1 and positive 1.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The roots of normalizing and scaling can be better understood by examining
    the concept of **unit vectors**Â in introductory linear algebra. Please see the
    following links for some common references for unit vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: You can refer to unit vectors atÂ [https://en.wikipedia.org/wiki/Unit_vector](https://en.wikipedia.org/wiki/Unit_vector)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For scalar, you can refer toÂ [https://en.wikipedia.org/wiki/Scalar_(mathematics)](https://en.wikipedia.org/wiki/Scalar_(mathematics))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of input sensitive algorithms, such as SVM, it is recommended that
    the algorithm is trained on scaled values (for example, range from 0 to 1) of
    the features rather than the absolute values as represented by the original vector.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `MinMaxScaler` is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.MinMaxScaler)
  prefs: []
  type: TYPE_NORMAL
- en: We want to emphasize that `MinMaxScaler` is an extensive API that extends the
    `Estimator` (a concept from the ML pipeline) and when used correctly can lead
    to achieving coding efficiency and high accuracy results.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting data for training and testing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, you will learn to use Spark's API to split your available input
    data into different datasets that can be used for training and validation phases.
    It is common to use an 80/20 split, but other variations of splitting the data
    can be considered as well based on your preference.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Go to the UCI Machine Learning Repository and download theÂ [http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip](http://archive.ics.uci.edu/ml/machine-learning-databases/00359/NewsAggregatorDataset.zip)Â file.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We began by loading the data file `newsCorpora.csv`Â and then by way of the `randomSplit()`
    method attached to the dataset object, we split the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To validate the result, we must set up a Delphi technique in which the test
    data is absolutely unknown to the model. See Kaggle competitions for details atÂ [https://www.kaggle.com/competitions](https://www.kaggle.com/competitions).
  prefs: []
  type: TYPE_NORMAL
- en: 'Three types of datasets are needed for a robust ML system:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training dataset**: This is used to fit a model to sample'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation dataset**: This is used to estimate the delta or prediction error
    for the fitted model (trained by training set)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test dataset**: This is used to assess the model generalization error once
    a final model is selected'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `randomSplit()` is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.api.java.JavaRDD@randomSplit(weights:Array%5BDouble%5D):Array%5Borg.apache.spark.api.java.JavaRDD%5BT%5D%5D).
  prefs: []
  type: TYPE_NORMAL
- en: The `randomSplit()` is a method call within an RDD. While the number of RDD
    method calls can be overwhelming, mastering this Spark concept and API is a must.
  prefs: []
  type: TYPE_NORMAL
- en: 'API signature is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Randomly splits this RDD with the provided weights.
  prefs: []
  type: TYPE_NORMAL
- en: Common operations with the new Dataset API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we cover the Dataset API, which is the way forward for data
    wrangling in Spark 2.0 and beyond. In this chapter ,we cover some of the common,
    repetitive operations that are required to work with these new API sets. Additionally,
    we demonstrate the query plan generated by the Spark SQL Catalyst optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We will use a JSON data file named `cars.json`, which has been created for
    this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to get access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Define a Scala `case class` to model the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'Import `spark.implicits`, therefore adding in behavior with only an `import`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s create a dataset from a Scala list and print out the results:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/fcc65713-e335-4952-b943-5c7b600b1f5f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we will load a CSV into memory and transform it into a dataset of type
    `Team`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/90651fad-883b-4167-84fa-d17beec4469e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we demonstrate a transversal of the teams dataset by use of the `map` function,
    yielding a new dataset of city names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/3e7b1c1e-6b07-4c24-a5b9-6546ec45201d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Display the execution plan for retrieving city names:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we save the `teams` dataset to a JSON file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we created a dataset from a Scala list and displayed the output to validate
    the creation of the dataset as expected. Second, we loaded a **comma-separated
    value** (**CSV**) file into memory, transforming it into a dataset of type `Team`.
    Third, we executed the `map()` function over our dataset to build a list of team
    city names and printed out the execution plan used to generate the dataset. Finally,
    we persisted the `teams` dataset we previously loaded into a JSON formatted file
    for future use.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Please take a note of some interesting points on datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: Datasets use *lazy* evaluation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets take advantage of the Spark SQL Catalyst optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Datasets take advantage of the tungsten off-heap memory management
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are plenty of systems that will remain pre-Spark 2.0 for the next 2 year
    so you must still learn and master RDDs and DataFrame for practical reasons.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Dataset is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset).
  prefs: []
  type: TYPE_NORMAL
- en: Creating and using RDD versus DataFrame versus Dataset from a text file in Spark
    2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this recipe, we explore the subtle differences in creating RDD, DataFrame,
    and Dataset from a text file and their relationship to each other via a short
    sample code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: Assume `spark`Â is the session name
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'We also define a `case class` to host the data used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Initialize a Spark session specifying configurations with the builder pattern,
    thus making ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We create an RDD, DataFrame, and Dataset object using a similar method from
    the same text file and confirm the type using the `getClass` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Please note that they are very similar and sometimes confusing. Spark 2.0 has
    transformed DataFrame into an alias for `Dataset[Row]`, making it truly a dataset.
    We showed the preceding methods to let the user pick an example to create their
    own datatype flavor.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for datatypes is available atÂ [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: If you are unsure as to what kind of data structure you have at hand (sometimes
    the difference is not obvious), use the `getClass`Â method to verify.
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0 has transformed DataFrame into an alias for `Dataset[Row]`. While
    RDD and Dataram remain fully viable for near future, it is best to learn and code
    new projects using the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for RDD and Dataset is available at the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.rdd.RDD)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.Dataset)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LabeledPoint data structure for Spark ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LabeledPoint** is a data structure that has been around since the early days
    for packaging a feature vector along with a label so it can be used in unsupervised
    learning algorithms. We demonstrate a short recipe that uses LabeledPoint, the
    **Seq** data structure, and DataFrame to run a logistic regression for binary
    classification of the data.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the LabeledPoint, using the `SparseVector` and `DenseVector`. In
    the following code blocks, the first four LabeledPoints are created by the `DenseVector`,
    the last two LabeledPoints are created by the `SparseVector`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: The DataFrame objects are created from the preceding LabeledPoint.
  prefs: []
  type: TYPE_NORMAL
- en: We verify the raw data count and process data count.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'You can operate a `show()` function call to the DataFrame created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: You will see the following in the console:![](img/9cb90b58-293a-46c8-b6db-8461984c9b9f.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We create a simple LogisticRegression model from the data structure we just
    created:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'In the console, it will show the following `model` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We used a LabeledPoint data structure to model features and drive training of
    a logistics regression model. We began by defining a group of LabeledPoints, which
    are used to create a DataFrame for further processing. Then, we created a logistic
    regression object and passed LabeledPoint DataFrame as an argument to it so we
    could train our model. Spark ML APIs are designed to work well with the LabeledPoint
    format and require minimal intervention.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A LabeledPoint is a popular structure used to package data as a `Vector` +
    a `Label` which can be purposed for supervised machine learning algorithms. A
    typical layout of the LabeledPoint is given here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: Please note that not only dense but also sparse vectors can be used with LabeledPoint,
    which will make a huge difference in efficiency especially if you have a large
    and sparse dataset housed in the driver during testing and development.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LabeledPoint API documentation is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.feature.LabeledPoint)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DenseVector API documentation is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.DenseVector)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SparseVector API documentation is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.linalg.SparseVector)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting access to Spark cluster in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to get access to a Spark cluster using a
    single point access named `SparkSession`. Spark 2.0 abstracts multiple contexts
    (such as SQLContext, HiveContext) into a single entry point, `SparkSession`, which
    allows you to get access to all Spark subsystems in a unified way.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: Import the necessary packages for SparkContext to get access to the cluster.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Spark 2.x, `SparkSession` is more commonly used instead.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and `SparkSession` so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code utilizes the `master()` function to set the cluster type
    ...
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we show how to connect to a Spark cluster using local and remote
    options for an application. First, we create a `SparkSession` object which will
    grant us access to a Spark cluster by specifying whether the cluster is local
    or remote using the `master()` function. You can also specify the master location
    by passing a JVM argument when starting your client program. In addition, you
    can configure an application name and a working data directory. Next, you invoked
    the `getOrCreate()` method to create a new `SparkSession` or hand you a reference
    to an already existing session. Finally, we execute a small sample program to
    prove our `SparkSession` object creation is valid.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Spark session has numerous parameters and APIs that can be set and exercised,
    but it is worth consulting the Spark documentation since some of the methods/parameters
    are marked with the status Experimental or left blank - for non-experimental statuses
    (15 minimum as of our last examination).
  prefs: []
  type: TYPE_NORMAL
- en: Another change to be aware of is to use `spark.sql.warehouse.dir` for the location
    of the tables. Spark 2.0 uses `spark.sql.warehouse.dir` to set warehouse locations
    to store tables rather than `hive.metastore.warehouse.dir`. The default value
    for `spark.sql.warehouse.dir` is `System.getProperty("user.dir")`.
  prefs: []
  type: TYPE_NORMAL
- en: Also see `spark-defaults.conf` for more details.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also noteworthy are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Some of our favorite and interesting APIs from the Spark ...
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `SparkSession` API documents is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession).
  prefs: []
  type: TYPE_NORMAL
- en: Getting access to Spark cluster pre-Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a *pre-Spark 2.0 recipe*, but it will be helpful for developers who
    want to quickly compare and contrast the cluster access for porting pre-Spark
    2.0 programs to Spark 2.0's new paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext so we can have access to the
    cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code utilizes the `setMaster()` function to set the cluster master
    location. As you can see, we are running the code in `local` mode.
  prefs: []
  type: TYPE_NORMAL
- en: The `-D` option value will be overridden by the cluster master parameter set
    in the code if both exist).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following are the three sample ways to connect to the cluster in different
    modes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Running in local mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Running in cluster mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Passing the master value in:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/51cd6bc9-0345-4cd3-b26b-5ed79aa77840.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We use the preceding SparkContext to read a CSV file in and parse the CSV file
    into Spark using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'We take the sample result and print them in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: And you will see the following in the console:![](img/803b10ae-f8c7-4b57-9e86-2b96c2dd6712.png)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We then close the program by stopping the SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this example, we show how to connect to a Spark cluster using the local and
    remote modes prior to Spark 2.0\. First, we create a `SparkConf` object and configure
    all the required parameters. We will specify the master location, application
    name, and working data directory. Next, we create a SparkContext passing the `SparkConf`
    as an argument to access a Spark cluster. Also, you can specify the master location
    my passing a JVM argument when starting your client program. Finally, we execute
    a small sample program to prove our SparkContext is functioning correctly.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prior to Spark 2.0, getting access to a Spark cluster was done via **SparkContext**.
  prefs: []
  type: TYPE_NORMAL
- en: The access to the subsystems such as SQL was per-specific names context (for
    example, SQLContext**)**.
  prefs: []
  type: TYPE_NORMAL
- en: Spark 2.0 changed how we gain access to a cluster by creating a single unified
    access point (namely, `SparkSession`).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for SparkContext is available atÂ [https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext).
  prefs: []
  type: TYPE_NORMAL
- en: Getting access to SparkContext vis-a-vis SparkSession object in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate how to get hold of SparkContext using a SparkSession
    object in Spark 2.0\. This recipe will demonstrate the creation, usage, and back
    and forth conversion of RDD to Dataset. The reason this is important is that even
    though we prefer Dataset going forward, we must still be able to use and augment
    the legacy (pre-Spark 2.0) code mostly utilizing RDD.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for the Spark session to gain access to the cluster
    and `log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: 'Initialize a Spark session specifying configurations with the builder pattern,
    thus making an entry point available for the Spark cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We created RDD using the SparkContext; this was widely used in Spark 1.x. We
    also demonstrated a way to create Dataset in Spark 2.0 using the Session object.
    The conversion back and forth is necessary to deal with pre-Spark 2.0 code in
    production today.
  prefs: []
  type: TYPE_NORMAL
- en: The technical message from this recipe is that while DataSet is the preferred
    method of data wrangling going forward, we can always use the API to go back and
    forth to RDD and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: More about the datatypes can be found atÂ [http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for SparkContext and SparkSession is available at the following
    websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.SparkContext)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.SparkSession)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: New model export and PMML markup in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore the model export facility available in Spark 2.0
    to use **Predictive Model Markup Language** (**PMML**). This standard XML-based
    language allows you to export and run your models on other systems (some limitations
    apply). You can explore the *There's more...*Â section for more information.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'We read the data from a text file; the data file contains a sample dataset
    for a KMeans model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the parameters for the KMeans model, and train the model using the
    preceding datasets and parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: We have effectively created a simple KMeans model (by setting the number of
    clusters to 2) from the data structure we just created.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'In the console, it will show the following model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'We then export the PMML to an XML file in the data directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/f04f499e-1d28-43c3-84dc-4829281503f9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After you spend the time to train a model, the next step will be to persist
    the model for future use. In this recipe, we began by training a KMeans model
    to generate model info for persistence in later steps. Once we have the trained
    model, we invoke the `toPMML()` method on the model converting it into PMML for
    storage. The invocation of the method generates an XML document, then the XML
    document text can easily be persisted to a file.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'PMML is a standard developed by the **Data Mining Group** (**DMG**). The standard
    enables inter-platform interoperability by letting you build on one system and
    then deploy to another system in production. The PMML standard has gained momentum
    and has been adopted by most vendors. At its core, the standard is based on an
    XML document with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Header with general information
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dictionary describing field level definitions used by the third component (the
    model)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model structure and parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As of this writing, the Spark 2.0 Machine Library support for PMML exporting
    is currently limited to:'
  prefs: []
  type: TYPE_NORMAL
- en: Linear Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Logistic Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ridge Regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lasso
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SVM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: KMeans
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'You can export the model to the following file types in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Local filesystem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Distributed filesystem:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'Output stream--acting as a pipe:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for `PMMLExportable` API documents atÂ [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.pmml.PMMLExportable).
  prefs: []
  type: TYPE_NORMAL
- en: Regression model evaluation using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore how to evaluate a regression model (a regression
    decision tree in this example). Spark provides the **RegressionMetrics** facility
    which has basic statistical facilities such as **Mean Squared Error** (**MSE**),
    R-Squared, and so on, right out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: The objective in this recipe is to understand the evaluation metrics provided
    by Spark out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: We utilize the ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explored the generation of regression metrics to help us
    evaluate our regression model. We began to load a breast cancer data file and
    then split it in a 70/30 ratio to create training and test datasets. Next, we
    trained a `DecisionTree` regression model and utilized it to make predictions
    on our test set. Finally, we took the predictions and generated regression metrics
    which gave us the squared error, R-squared, mean absolute error, and explained
    variance.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We can use `RegressionMetrics()` to produce the following statistical measures:'
  prefs: []
  type: TYPE_NORMAL
- en: MSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RMSE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: R-squared
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explained variance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation on regression validation is available atÂ [https://en.wikipedia.org/wiki/Regression_validation](https://en.wikipedia.org/wiki/Regression_validation).
  prefs: []
  type: TYPE_NORMAL
- en: R-Squared/coefficient of determination is available atÂ [https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Wisconsin breast cancer dataset could be downloaded atÂ [ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum](ftp://ftp.cs.wisc.edu/math-prog/cpo-dataset/machine-learn/cancer/cancer1/datacum)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regression metrics documents are available at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.RegressionMetrics)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Binary classification model evaluation using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we demonstrate the use of the `BinaryClassificationMetrics`
    facility in Spark 2.0 and its application to evaluating a model that has a binary
    outcome (for example, a logistic regression).
  prefs: []
  type: TYPE_NORMAL
- en: The purpose here is not to showcase the regression itself, but to demonstrate
    how to go about evaluating it using common metrics such as **receiver operating
    characteristic** (**ROC**), Area Under ROC Curve, thresholds, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 'We download the dataset, originally from the UCI, and modify it to fit the
    need for the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: The dataset is a modified dataset. The original adult dataset has 14 features,
    among which six are continuous and eight are categorical. In this dataset, continuous
    features are discretized into quantiles, and each quantile is represented by a
    binary feature. We modified the data to fit the purpose of the code. Details of
    the dataset feature can be found at the [http://archive.ics.uci.edu/ml/index.php](http://archive.ics.uci.edu/ml/index.php)
    UCI site.
  prefs: []
  type: TYPE_NORMAL
- en: 'We split the dataset into training and test parts in a ratio of 60:40 random
    split, then get the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the prediction using the model created by the training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `BinaryClassificationMetrics` object from the predication, and
    start the evaluation on the metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the precision by `Threashold` in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the `recallByThreshold` in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the `fmeasureByThreshold` in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the `Area Under Precision Recall Curve` in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the Area Under ROC curve in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark session:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we investigated the evaluation of metrics for binary classification.
    First, we loaded the data, which is in the `libsvm` format, and split it in the
    ratio of 60:40, resulting in the creation of a training and a test set of data.
    Next, we trained a logistic regression model followed by generating predictions
    from our test set.
  prefs: []
  type: TYPE_NORMAL
- en: Once we had our predictions, we created a binary classification metrics object.
    Finally, we retrieved the true positive rate, positive predictive value, receiver
    operating curve, the area under receiver operating curve, the area under precision
    recall curve, and F-measure to evaluate our model for fitness.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Spark provides the following metrics to facilitate evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: TPR - True Positive Rate
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PPV - Positive Predictive Value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F - F-Measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: ROC - Receiver Operating Curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUROC - Area Under Receiver Operating Curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AUORC - Area Under Precision-Recall Curve
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following links should provide a good introductory material for the metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Receiver_operating_characteristic](https://en.wikipedia.org/wiki/Receiver_operating_characteristic)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Sensitivity_and_specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/F1_score](https://en.wikipedia.org/wiki/F1_score)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for the original dataset information is available at the following
    links:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/datasets.html](http://archive.ics.uci.edu/ml/datasets.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for binary classification metrics is available atÂ [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.BinaryClassificationMetrics).
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass classification model evaluation using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore `MulticlassMetrics`, which allows you to evaluate
    a model that classifies the output to more than two labels (for example, red,
    blue, green, purple, do-not-know). It highlights the use of a confusion matrix
    (`confusionMatrix`) and model accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explored generating evaluation metrics for a multi-classification
    model. First, we loaded the Iris data into memory and split it in a ratio of 60:40\.
    Second, we trained a logistic regression model with the number of classifications
    set to three. Third, we made predictions with the test dataset and utilized `MultiClassMetric`
    to generate evaluation measurements. Finally, we evaluated metrics such as the
    model accuracy, weighted precision, weighted recall, weighted F1 score, weighted
    false positive rate, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: While the scope of the book does not allow for a complete treatment of the confusion
    matrix, a short explanation and a link are provided as a quick reference.
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix is just a fancy name for an error matrix. It is mostly
    used in unsupervised learning to visualize the performance. It is a layout that
    captures actual versus predicted outcomes with an identical set of labels in two
    dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Confusion Matrix**'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2517d73f-07b5-4fcf-9750-bd8c0d6a4469.png)'
  prefs: []
  type: TYPE_IMG
- en: To get a quick introduction to the confusion matrix in unsupervised and supervised
    statistical learning systems, seeÂ [https://en.wikipedia.org/wiki/Confusion_matrix](https://en.wikipedia.org/wiki/Confusion_matrix).
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for original dataset information is available at the following
    websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://archive.ics.uci.edu/ml/datasets/Iris](http://archive.ics.uci.edu/ml/datasets/Iris)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Documentation for multiclass classification metrics is available at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MulticlassMetrics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilabel classification model evaluation using Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we explore multilabel classification `MultilabelMetrics` in
    Spark 2.0 which should not be mixed up with the previous recipe dealing with multiclass
    classification `MulticlassMetrics`. The key to exploring this recipe is to concentrate
    on evaluation metrics such as Hamming loss, accuracy, f1-measure, and so on, and
    what they measure.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure that
    the necessary JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for SparkContext to get access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SparkContext:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the dataset for the evaluation model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `MultilabelMetrics` object from the predication, and start the
    evaluation on the metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the overall statistics summary in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the individual label value in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs: []
  type: TYPE_PRE
- en: 'From the console output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the micro-statistics value in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the Hamming loss and subset accuracy from the metrics in the console:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: We then close the program by stopping the Spark session.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we investigated generating evaluation metrics for the multilabel
    classification model. We began with manually creating a dataset for the model
    evaluation. Next, we passed our dataset as an argument to the `MultilabelMetrics`
    and generated evaluation metrics. Finally, we printed out various metrics such
    as micro recall, micro precision, micro f1-measure, Hamming loss, subset accuracy,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Note that the multilabel and multiclass classifications sound similar, but they
    are two different things.
  prefs: []
  type: TYPE_NORMAL
- en: All multilabelÂ `MultilabelMetrics()` method is trying to accomplish is to map
    a number of inputs (x) to a binary vector (y) rather than numerical values in
    a typical classification system.
  prefs: []
  type: TYPE_NORMAL
- en: 'The important metrics associated with the multilabel classification are (see
    the preceding code):'
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamming loss
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Precision
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Recall
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: F1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A full explanation of each parameter is out of scope, but the following link
    provides a short treatment for the multilabel metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://en.wikipedia.org/wiki/Multi-label_classification](https://en.wikipedia.org/wiki/Multi-label_classification)'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Documentation for multilabel classification metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.evaluation.MultilabelMetrics)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the Scala Breeze library to do graphics in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will use the functions `scatter()` and `plot()` from the
    Scala Breeze linear algebra library (part of) to draw a scatter plot from a two-dimensional
    data. Once the results are computed on the Spark cluster, either the actionable
    data can be used in the driver for drawing or a JPEG or GIF can be generated in
    the backend and pushed forward for efficiency and speed (popular with GPU-based
    analytical databases such as MapD)
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First, we need to download the necessary ScalaNLP library. Download the JAR
    from the Maven repository available atÂ [https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Place the JAR in the `C:\spark-2.0.0-bin-hadoop2.7\examples\jars` directory
    on a Windows machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In macOS, please put the JAR in its correct path. For our setting examples,
    the path is `/Users/USERNAME/spark/spark-2.0.0-bin-hadoop2.7/examples/jars/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the sample screenshot showing the JARs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE120]'
  prefs: []
  type: TYPE_PRE
- en: Import the ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we created a dataset in Spark from random numbers. We then created
    a Breeze figure and set up the basic parameters. We derived *x*, *y* data from
    the created dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We used Breeze's `scatter()` and `plot()` functions to do graphics using the
    Breeze library.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One can use Breeze as an alternative to more complicated and powerful charting
    libraries such as JFreeChart, demonstrated in the previous chapter. The ScalaNLP
    project tends to be optimized with Scala goodies such as implicit conversions
    that make the coding relatively easier.
  prefs: []
  type: TYPE_NORMAL
- en: The Breeze graphics JAR file can be downloaded at [http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar](http://central.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12.jar).
  prefs: []
  type: TYPE_NORMAL
- en: More about Breeze graphics can be found atÂ [https://github.com/scalanlp/breeze/wiki/Quickstart](https://github.com/scalanlp/breeze/wiki/Quickstart).
  prefs: []
  type: TYPE_NORMAL
- en: The API document (please note, the API documentation is not necessarily up-to-date)
    can be found atÂ [http://www.scalanlp.org/api/breeze/#package](http://www.scalanlp.org/api/breeze/#package).
  prefs: []
  type: TYPE_NORMAL
- en: Note that once you are in the root package, you need click on BreezeÂ to ...
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For more information on Breeze, see the original material on GitHub atÂ [https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze).
  prefs: []
  type: TYPE_NORMAL
- en: Note that once you are in the root package, you need to click on Breeze to see
    the details.
  prefs: []
  type: TYPE_NORMAL
- en: For more information regarding the Breeze API documentation, please download
    the [https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar](https://repo1.maven.org/maven2/org/scalanlp/breeze-viz_2.11/0.12/breeze-viz_2.11-0.12-javadoc.jar)
    JAR.
  prefs: []
  type: TYPE_NORMAL
