["```scala\npackage spark.ml.cookbook.chapter1 \n```", "```scala\nimport org.apache.spark.sql.SparkSession \nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n.builder \n.master(\"local[*]\")\n .appName(\"myFirstSpark20\") \n.config(\"spark.sql.warehouse.dir\", \".\") \n.getOrCreate() \n```", "```scala\nval x = Array(1.0,5.0,8.0,10.0,15.0,21.0,27.0,30.0,38.0,45.0,50.0,64.0) \nval y = Array(5.0,1.0,4.0,11.0,25.0,18.0,33.0,20.0,30.0,43.0,55.0,57.0) \n```", "```scala\nval xRDD = spark.sparkContext.parallelize(x) \nval yRDD = spark.sparkContext.parallelize(y) \n```", "```scala\nval zipedRDD = xRDD.zip(yRDD) \nzipedRDD.collect().foreach(println) \n```", "```scala\nval xSum = zipedRDD.map(_._1).sum() \nval ySum = zipedRDD.map(_._2).sum() \nval xySum= zipedRDD.map(c => c._1 * c._2).sum() \nval n= zipedRDD.count() \n```", "```scala\nprintln(\"RDD X Sum: \" +xSum) \nprintln(\"RDD Y Sum: \" +ySum) \nprintln(\"RDD X*Y Sum: \"+xySum) \nprintln(\"Total count: \"+n) \n```", "```scala\nspark.stop() \n```", "```scala\nInformation: November 18, 2016, 11:46 AM - Compilation completed successfully with 1 warning in 55s 648ms\n```", "```scala\nProcess finished with exit code 0\n```"]