["```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) ...\n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate()\n```", "```scala\nval book1 = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/a.txt\") \n```", "```scala\nNumber of lines = 16271\n```", "```scala\nval book2 = book1.flatMap(l => l.split(\" \")) \nprintln(book1.count())\n```", "```scala\nNumber of words = 143228  \n```", "```scala\nval dirKVrdd = spark.sparkContext.wholeTextFiles(\"../data/sparkml2/chapter3/*.txt\") // place a large number of small files for demo \nprintln (\"files in the directory as RDD \", dirKVrdd) \nprintln(\"total number of files \", dirKVrdd.count()) \nprintln(\"Keys \", dirKVrdd.keys.count()) \nprintln(\"Values \", dirKVrdd.values.count()) \ndirKVrdd.collect() \nprintln(\"Values \", dirKVrdd.first()) \n```", "```scala\n    files in the directory as RDD ,../data/sparkml2/chapter3/*.txt\n    WholeTextFileRDD[10] at wholeTextFiles at myRDD.scala:88)\n    total number of files 2\n    Keys ,2\n    Values ,2\n    Values ,(file:/C:/spark-2.0.0-bin-hadoop2.7/data/sparkml2/chapter3/a.txt,\n    The Project Gutenberg EBook of A Tale of Two Cities, \n    by Charles Dickens\n\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) ...\n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval book1 = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/a.txt\")\n```", "```scala\nval wordRDD2 = book1.map(_.trim.split(\"\"\"[\\s\\W]+\"\"\") ).filter(_.length > 0) \nwordRDD2.take(3)foreach(println(_)) \n```", "```scala\n[Ljava.lang.String;@1e60b459\n[Ljava.lang.String;@717d7587\n[Ljava.lang.String;@3e906375\n```", "```scala\nval wordRDD3 = book1.flatMap(_.trim.split(\"\"\"[\\s\\W]+\"\"\") ).filter(_.length > 0).map(_.toUpperCase()) \nprintln(\"Total number of lines = \", book1.count()) \nprintln(\"Number of words = \", wordRDD3.count()) \n```", "```scala\nwordRDD3.take(5)foreach(println(_)) \n```", "```scala\nTotal number of lines = 16271\nNumber of words = 141603\nTHE\nPROJECT\nGUTENBERG\nEBOOK\nOF  \n```", "```scala\nval minValue1= numRDD.reduce(_ min _) \nprintln(\"minValue1 = \", minValue1)\n```", "```scala\nminValue1 = 1.0\n```", "```scala\nval minValue2 = numRDD.glom().map(_.min).reduce(_ min _) \nprintln(\"minValue2 = \", minValue2) \n```", "```scala\nminValue1 = 1.0  \n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) ...\n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) ...\n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\n    import org.apache.spark.sql.SparkSession \n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n.builder \n.master(\"local[*]\") \n.appName(\"myRDD\") \n.config(\"Spark.sql.warehouse.dir\", \".\") \n.getOrCreate() \n```", "```scala\nval SignalNoise: Array[Double] = Array(0.2,1.2,0.1,0.4,0.3,0.3,0.1,0.3,0.3,0.9,1.8,0.2,3.5,0.5,0.3,0.3,0.2,0.4,0.5,0.9,0.1) \nval SignalStrength: Array[Double] = Array(6.2,1.2,1.2,6.4,5.5,5.3,4.7,2.4,3.2,9.4,1.8,1.2,3.5,5.5,7.7,9.3,1.1,3.1,2.1,4.1,5.1) \nval parSN=spark.sparkContext.parallelize(SignalNoise) // parallelized signal noise RDD \nval parSS=spark.sparkContext.parallelize(SignalStrength)  // parallelized signal strength \n```", "```scala\nval zipRDD= parSN.zip(parSS).map(r => r._1 / r._2).collect() \nprintln(\"zipRDD=\") \nzipRDD.foreach(println) \n```", "```scala\nzipRDD=\n0.03225806451612903\n1.0\n0.08333333333333334\n0.0625\n0.05454545454545454  \n```", "```scala\nval zipRDD= parSN.zip(parSS).map(r => r._1 / r._2) \n```", "```scala\nprintln(\"Full Joined RDD = \") \nval fullJoinedRDD = keyValueRDD.fullOuterJoin(keyValueCity2RDD) \nfullJoinedRDD.collect().foreach(println(_)) \n```", "```scala\nval keyValuePairs = List((\"north\",1),(\"south\",2),(\"east\",3),(\"west\",4)) \nval keyValueCity1 = List((\"north\",\"Madison\"),(\"south\",\"Miami\"),(\"east\",\"NYC\"),(\"west\",\"SanJose\")) \nval keyValueCity2 = List((\"north\",\"Madison\"),(\"west\",\"SanJose\"))\n```", "```scala\nval keyValueRDD = spark.sparkContext.parallelize(keyValuePairs) \nval keyValueCity1RDD = spark.sparkContext.parallelize(keyValueCity1) \nval keyValueCity2RDD = spark.sparkContext.parallelize(keyValueCity2) \n```", "```scala\nval keys=keyValueRDD.keys \nval values=keyValueRDD.values \n```", "```scala\nval kvMappedRDD = keyValueRDD.mapValues(_+100) \nkvMappedRDD.collect().foreach(println(_)) \n```", "```scala\n(north,101)\n(south,102)\n(east,103)\n(west,104)\n\n```", "```scala\nprintln(\"Joined RDD = \") \nval joinedRDD = keyValueRDD.join(keyValueCity1RDD) \njoinedRDD.collect().foreach(println(_)) \n```", "```scala\n(south,(2,Miami))\n(north,(1,Madison))\n(west,(4,SanJose))\n(east,(3,NYC))\n```", "```scala\nprintln(\"Left Joined RDD = \") \nval leftJoinedRDD = keyValueRDD.leftOuterJoin(keyValueCity2RDD) \nleftJoinedRDD.collect().foreach(println(_)) \n```", "```scala\n(south,(2,None))\n(north,(1,Some(Madison)))\n(west,(4,Some(SanJose)))\n(east,(3,None))\n\n```", "```scala\nprintln(\"Right Joined RDD = \") \nval rightJoinedRDD = keyValueRDD.rightOuterJoin(keyValueCity2RDD) \nrightJoinedRDD.collect().foreach(println(_)) \n```", "```scala\n(north,(Some(1),Madison))\n(west,(Some(4),SanJose))  \n```", "```scala\nval fullJoinedRDD = keyValueRDD.fullOuterJoin(keyValueCity2RDD) \nfullJoinedRDD.collect().foreach(println(_)) \n```", "```scala\nFull Joined RDD = \n(south,(Some(2),None))\n(north,(Some(1),Some(Madison)))\n(west,(Some(4),Some(SanJose)))\n(east,(Some(3),None))\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.spark.sql.SparkSession \n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval signaltypeRDD = spark.sparkContext.parallelize(List((\"Buy\",1000),(\"Sell\",500),(\"Buy\",600),(\"Sell\",800))) \n```", "```scala\nval signaltypeRDD = spark.sparkContext.parallelize(List((\"Buy\",1000),(\"Sell\",500),(\"Buy\",600),(\"Sell\",800))) \nval groupedRDD = signaltypeRDD.groupByKey() \ngroupedRDD.collect().foreach(println(_)) \n```", "```scala\nGroup By Key RDD = \n(Sell, CompactBuffer(500, 800))\n(Buy, CompactBuffer(1000, 600))\n```", "```scala\nprintln(\"Reduce By Key RDD = \") \nval reducedRDD = signaltypeRDD.reduceByKey(_+_) \nreducedRDD.collect().foreach(println(_))   \n```", "```scala\nReduce By Key RDD = \n(Sell,1300)\n(Buy,1600)  \n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport org.apache.spark.sql._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myDataFrame\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval signaltypeRDD = spark.sparkContext.parallelize(List((\"Buy\",1000),(\"Sell\",500),(\"Buy\",600),(\"Sell\",800))) \nval numList = List(1,2,3,4,5,6,7,8,9) \nval numRDD = spark.sparkContext.parallelize(numList) \nval myseq = Seq( (\"Sammy\",\"North\",113,46.0),(\"Sumi\",\"South\",110,41.0), (\"Sunny\",\"East\",111,51.0),(\"Safron\",\"West\",113,2.0 )) \n```", "```scala\nval numDF = numRDD.toDF(\"mylist\") \nnumDF.show \n```", "```scala\n+------+\n|mylist|\n+------+\n|     1|\n|     2|\n|     3|\n|     4|\n|     5|\n|     6|\n|     7|\n|     8|\n|     9|\n+------+\n```", "```scala\nval df1 = spark.createDataFrame(myseq).toDF(\"Name\",\"Region\",\"dept\",\"Hours\") \n```", "```scala\ndf1.show() \ndf1.printSchema() \n```", "```scala\n+------+------+----+-----+\n|  Name|Region|dept|Hours|\n+------+------+----+-----+\n| Sammy| North| 113| 46.0|\n|  Sumi| South| 110| 41.0|\n| Sunny|  East| 111| 51.0|\n|Safron|  West| 113|  2.0|\n+------+------+----+-----+\n\nroot\n|-- Name: string (nullable = true)\n|-- Region: string (nullable = true)\n|-- dept: integer (nullable = false)\n|-- Hours: double (nullable = false) \n\n```", "```scala\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD \nimport org.apache.spark.sql.SQLContext \nimport org.apache.spark.mllib.linalg \nimport org.apache.spark.util \nimport Array._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types \nimport org.apache.spark.sql.DataFrame \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.types.{ StructType, StructField, StringType}; \n```", "```scala\nimport sqlContext.implicits \n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport org.apache.spark.sql._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) ...\n```", "```scala\nimport org.apache.spark._  import org.apache.spark.rdd.RDD import org.apache.spark.sql.SQLContext import org.apache.spark.mllib.linalg._ import org.apache.spark.util._ import Array._ import org.apache.spark.sql._ import org.apache.spark.sql.types._ import org.apache.spark.sql.DataFrame import org.apache.spark.sql.Row; import org.apache.spark.sql.types.{ StructType, StructField, StringType};\n```", "```scala\nimport sqlContext.implicits._\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.spark.sql._\n```", "```scala\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myDataFrame\")\n .config(\"Spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval customersRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/customers13.txt\") //Customer file \n\nval custRDD = customersRDD.map {\n   line => val cols = line.trim.split(\",\")\n     (cols(0).toInt, cols(1), cols(2), cols(3).toInt) \n} \nval custDF = custRDD.toDF(\"custid\",\"name\",\"city\",\"age\")   \n```", "```scala\ncustDF.show()\n```", "```scala\nval productsRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/products13.txt\") //Product file\n val prodRDD = productsRDD.map {\n     line => val cols = line.trim.split(\",\")\n       (cols(0).toInt, cols(1), cols(2), cols(3).toDouble) \n}  \n```", "```scala\nval prodDF = prodRDD.toDF(\"prodid\",\"category\",\"dept\",\"priceAdvertised\")\n```", "```scala\nprodDF.show()\n```", "```scala\nval salesRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/sales13.txt\") *//Sales file* val saleRDD = salesRDD.map {\n     line => val cols = line.trim.split(\",\")\n       (cols(0).toInt, cols(1).toInt, cols(2).toDouble)\n}\n```", "```scala\nval saleDF = saleRDD.toDF(\"prodid\", \"custid\", \"priceSold\")  \n```", "```scala\nsaleDF.show()\n```", "```scala\ncustDF.printSchema()\nproductDF.printSchema()\nsalesDF. printSchema()\n```", "```scala\nroot\n |-- custid: integer (nullable = false)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- age: integer (nullable = false)\nroot\n |-- prodid: integer (nullable = false)\n |-- category: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- priceAdvertised: double (nullable = false)\nroot\n |-- prodid: integer (nullable = false)\n |-- custid: integer (nullable = false)\n |-- priceSold: double (nullable = false)\n```", "```scala\nimport org.apache.spark._\n import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.SQLContext\n import org.apache.spark.mllib.linalg._\n import org.apache.spark.util._\n import Array._\n import org.apache.spark.sql._\n import org.apache.spark.sql.types._\n import org.apache.spark.sql.DataFrame\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.types.{ StructType, StructField, StringType};\n```", "```scala\n import sqlContext.implicits._\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.spark.sql._\n```", "```scala\nimport org.apache.log4j.Logger import org.apache.log4j.Level\n```", "```scala\nLogger.getLogger( ...\n```", "```scala\n import sqlContext.implicits._\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Car(make: String, model: String, price: Double,\nstyle: String, kind: String)\n```", "```scala\nval *carData* =\n*Seq*(\n*Car*(\"Tesla\", \"Model S\", 71000.0, \"sedan\",\"electric\"),\n*Car*(\"Audi\", \"A3 E-Tron\", 37900.0, \"luxury\",\"hybrid\"),\n*Car*(\"BMW\", \"330e\", 43700.0, \"sedan\",\"hybrid\"),\n*Car*(\"BMW\", \"i3\", 43300.0, \"sedan\",\"electric\"),\n*Car*(\"BMW\", \"i8\", 137000.0, \"coupe\",\"hybrid\"),\n*Car*(\"BMW\", \"X5 xdrive40e\", 64000.0, \"suv\",\"hybrid\"),\n*Car*(\"Chevy\", \"Spark EV\", 26000.0, \"coupe\",\"electric\"),\n*Car*(\"Chevy\", \"Volt\", 34000.0, \"sedan\",\"electric\"),\n*Car*(\"Fiat\", \"500e\", 32600.0, \"coupe\",\"electric\"),\n*Car*(\"Ford\", \"C-Max Energi\", 32600.0, \"wagon/van\",\"hybrid\"),\n*Car*(\"Ford\", \"Focus Electric\", 29200.0, \"sedan\",\"electric\"),\n*Car*(\"Ford\", \"Fusion Energi\", 33900.0, \"sedan\",\"electric\"),\n*Car*(\"Hyundai\", \"Sonata\", 35400.0, \"sedan\",\"hybrid\"),\n*Car*(\"Kia\", \"Soul EV\", 34500.0, \"sedan\",\"electric\"),\n*Car*(\"Mercedes\", \"B-Class\", 42400.0, \"sedan\",\"electric\"),\n*Car*(\"Mercedes\", \"C350\", 46400.0, \"sedan\",\"hybrid\"),\n*Car*(\"Mercedes\", \"GLE500e\", 67000.0, \"suv\",\"hybrid\"),\n*Car*(\"Mitsubishi\", \"i-MiEV\", 23800.0, \"sedan\",\"electric\"),\n*Car*(\"Nissan\", \"LEAF\", 29000.0, \"sedan\",\"electric\"),\n*Car*(\"Porsche\", \"Cayenne\", 78000.0, \"suv\",\"hybrid\"),\n*Car*(\"Porsche\", \"Panamera S\", 93000.0, \"sedan\",\"hybrid\"),\n*Car*(\"Tesla\", \"Model X\", 80000.0, \"suv\",\"electric\"),\n*Car*(\"Tesla\", \"Model 3\", 35000.0, \"sedan\",\"electric\"),\n*Car*(\"Volvo\", \"XC90 T8\", 69000.0, \"suv\",\"hybrid\"),\n*Car*(\"Cadillac\", \"ELR\", 76000.0, \"coupe\",\"hybrid\")\n)\n\n```", "```scala\n   Logger.getLogger(\"org\").setLevel(Level.ERROR)\n   Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"mydatasetseq\")\n.config(\"Spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval cars = spark.createDataset(MyDatasetData.carData) \n// carData is put in a separate scala object MyDatasetData\n```", "```scala\ninfecars.show(false)\n+----------+--------------+--------+---------+--------+\n|make |model |price |style |kind |\n```", "```scala\ncars.columns.foreach(println)\nmake\nmodel\nprice\nstyle\nkind\n```", "```scala\nprintln(cars.schema)\nStructType(StructField(make,StringType,true), StructField(model,StringType,true), StructField(price,DoubleType,false), StructField(style,StringType,true), StructField(kind,StringType,true))\n```", "```scala\ncars.filter(cars(\"price\") > 50000.00).show()\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Car(make: String, model: String, price: Double,style: String, kind: String)\n```", "```scala\nval carData =Seq(Car(\"Tesla\", \"Model S\", 71000.0, \"sedan\",\"electric\"), ...\n```", "```scala\n{\"make\": \"Telsa\", \"model\": \"Model S\", \"price\": 71000.00, \"style\": \"sedan\", \"kind\": \"electric\"}\n{\"make\": \"Audi\", \"model\": \"A3 E-Tron\", \"price\": 37900.00, \"style\": \"luxury\", \"kind\": \"hybrid\"}\n{\"make\": \"BMW\", \"model\": \"330e\", \"price\": 43700.00, \"style\": \"sedan\", \"kind\": \"hybrid\"}\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Car(make: String, model: String, price: Double,\nstyle: String, kind: String)\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"mydatasmydatasetjsonetrdd\")\n.config(\"Spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval cars = spark.read.json(\"../data/sparkml2/chapter3/cars.json\").as[Car]\n```", "```scala\ncars.show(false)\n```", "```scala\ncars.columns.foreach(println)\nmake\nmodel\nprice\nstyle\nkind\n```", "```scala\nprintln(cars.schema)\nStructType(StructField(make,StringType,true), StructField(model,StringType,true), StructField(price,DoubleType,false), StructField(style,StringType,true), StructField(kind,StringType,true))\n```", "```scala\ncars.select(\"make\").distinct().show()\n```", "```scala\ncars.createOrReplaceTempView(\"cars\")\n```", "```scala\nspark.sql(\"select make, model, kind from cars where kind = 'electric'\").show()\n```", "```scala\nspark.stop() \n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.{Dataset, SparkSession}import spark.ml.cookbook.{Car, mydatasetdata}import scala.collection.mutableimport scala.collection.mutable.ListBufferimport org.apache.log4j.{Level, Logger}import org.apache.spark.sql.SparkSession\n```", "```scala\nimport spark.implicits._\n```"]