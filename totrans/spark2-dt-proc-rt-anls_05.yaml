- en: Apache SparkML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So now that you've learned a lot about MLlib, why another ML API? First of all,
    it is a common task in data science to work with multiple frameworks and ML libraries
    as there are always advantages and disadvantages; mostly, it is a trade-off between
    performance and functionality. R, for instance, is the king when it comes to functionality--there
    exist more than 6000 R add-on packages. However, R is also one of the slowest
    execution environments for data science. SparkML, on the other hand, currently
    has relatively limited functionality but is one of the fastest libraries. Why
    is this so? This brings us to the second reason why SparkML exists.
  prefs: []
  type: TYPE_NORMAL
- en: The duality between RDD on the one hand and DataFrames and Datasets on the other
    is like a red thread in this book and doesn't stop influencing the machine learning
    chapters. As MLlib is designed to work on top of RDDs, SparkML works on top of
    DataFrames and Datasets, therefore making use of all the new performance benefits
    that Catalyst and Tungsten bring.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to the SparkML API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The concept of pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers and estimators
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A working example
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What does the new API look like?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When it comes to machine learning on Apache Spark, we are used to transforming
    data into an appropriate format and data types before we actually feed them to
    our algorithms. Machine learning practitioners around the globe discovered that
    the preprocessing tasks on a machine learning project usually follow the same
    pattern:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, the new ApacheSparkML API supports this process out of the box. It
    is called **pipelines** and is inspired by scikit-learn [http://scikit-learn.org](http://scikit-learn.org),
    a very popular machine learning library for the Python programming language. The
    central data structure is a DataFrame and all operations run on top of it.
  prefs: []
  type: TYPE_NORMAL
- en: The concept of pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'ApacheSparkML pipelines have the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**DataFrame**: This is the central data store where all the original data and
    intermediate results are stored in.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**: As the name suggests, a transformer transforms one DataFrame
    into another by adding additional (feature) columns in most of the cases. Transformers
    are stateless, which means that they don''t have any internal memory and behave
    exactly the same each time they are used; this is a concept you might be familiar
    with when using the map function of RDDs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Estimator**: In most of the cases, an estimator is some sort of machine learning
    model. In contrast to a transformer, an estimator contains an internal state representation
    and is highly dependent on the history of the data that it has already seen.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pipeline**: This is the glue which is joining the preceding components, DataFrame,
    Transformer and Estimator, together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parameter**: Machine learning algorithms have many knobs to tweak. These
    are called **hyperparameters** and the values learned by a machine learning algorithm
    to fit data are called parameters. By standardizing how hyperparameters are expressed,
    ApacheSparkML opens doors to task automation, as we will see later.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's start with something simple. One of the most common tasks in machine learning
    data preparation is string indexing and one-hot encoding of categorical values.
    Let's see how this can be done.
  prefs: []
  type: TYPE_NORMAL
- en: String indexer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume that we have a DataFrame `df` containing a column called color
    of categorical labels--red, green, and blue. We want to encode them as integer
    or float values. This is where `org.apache.spark.ml.feature.StringIndexer` kicks
    in. It automatically determines the cardinality of the category set and assigns
    each one a distinct value. So in our example, a list of categories such as red,
    red, green, red, blue, green should be transformed into 1, 1, 2, 1, 3, 2:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The result of this transformation is a DataFrame called indexed that, in addition
    to the colors column of the String type, now contains a column called `colorsIndexed`
    of type double.
  prefs: []
  type: TYPE_NORMAL
- en: OneHotEncoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We are only halfway through. Although machine learning algorithms are capable
    of making use of the `colorsIndexed` column, they perform better if we one-hot
    encode it. This actually means that, instead of having a `colorsIndexed` column
    containing label indexes between one and three, it is better if we have three
    columns--one for each color--with the constraint that every row is allowed to
    set only one of these columns to one, otherwise zero. Let''s do it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Intuitively, we would expect that we get three additional columns in the encoded
    DataFrame, for example, `colorIndexedRed`, `colorIndexedGreen`, and `colorIndexedBlue
    ...`
  prefs: []
  type: TYPE_NORMAL
- en: VectorAssembler
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start with the actual machine learning algorithm, we need to apply
    one final transformation. We have to create one additional `feature` column containing
    all the information of the columns that we want the machine learning algorithm
    to consider. This is done by `org.apache.spark.ml.feature.VectorAssembler` as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: This transformer adds only one single column to the resulting DataFrame called
    **features**, which is of the `org.apache.spark.ml.linalg.Vector` type. In other
    words, this new column called features, created by the `VectorAssembler`, contains
    all the defined columns (in this case, `colorVec`, `field2`, `field3`, and `field4`)
    encoded in a single vector object for each row. This is the format the Apache
    SparkML algorithms are happy with.
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we dive into estimators--we''ve already used one in `StringIndexer`--let''s
    first understand the concept of pipelines. As you might have noticed, the transformers
    add only one single column to a DataFrame and basically omit all other columns
    not explicitly specified as input columns; they can only be used in conjunction
    with `org.apache.spark.ml.Pipeline`, which glues individual transformers (and
    estimators) together to form a complete data analysis process. So let''s do this
    for our two `Pipeline` stages:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The now obtained DataFrame called **transformed** contains all the ...
  prefs: []
  type: TYPE_NORMAL
- en: Estimators
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've used estimators before in `StringIndexer`. We've already stated that estimators
    somehow contain state that changes while looking at data, whereas this is not
    the case for transformers. So why is `StringIndexer` an estimator? This is because
    it needs to remember all the previously seen strings and maintain a mapping table
    between strings and label indexes.
  prefs: []
  type: TYPE_NORMAL
- en: In machine learning, it is common to use at least a training and testing subset
    of your available training data. It can happen that an estimator in the pipeline,
    such as `StringIndexer`, has not seen all the string labels while looking at the
    training dataset. Therefore, you'll get an exception when evaluating the model
    using the test dataset as the `StringIndexer` now encounters labels that it has
    not seen before. This is, in fact, a very rare case and basically could mean that
    the sample function you use to separate the training and testing datasets is not
    working; however, there is an option called `setHandleInvalid("skip")` and your
    problem is solved.
  prefs: []
  type: TYPE_NORMAL
- en: Another easy way to distinguish between an estimator and a transformer is the
    additional method called `fit` on the estimators. Fit actually populates the internal
    data management structure of the estimators based on a given dataset, which, in
    the case of `StringIndexer`, is the mapping table between label strings and label
    indexes. So now let's take a look at another estimator, an actual machine learning
    algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: RandomForestClassifier
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s assume that we are in a binary classification problem setting and want
    to use `RandomForestClassifier`. All SparkML algorithms have a compatible API,
    so they can be used interchangeably. So it really doesn''t matter which one we
    use, but `RandomForestClassifier` has more (hyper)parameters than more simple
    models like logistic regression. At a later stage we''ll use (hyper)parameter
    tuning which is also inbuilt in Apache SparkML. Therefore it makes sense to use
    an algorithm where more knobs can be tweaked. Adding such a binary classifier
    to our `Pipeline` is very simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As mentioned before, model evaluation is built-in to ApacheSparkML and you''ll
    find all that you need in the `org.apache.spark.ml.evaluation` package. Let''s
    continue with our binary classification. This means that we''ll have to use `org.apache.spark.ml.evaluation.BinaryClassificationEvaluator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: To code previous initialized a `BinaryClassificationEvaluator` function and
    tells it to calculate the `areaUnderROC`, one of the many possible metrics to
    assess the prediction performance of a machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have the actual label and the prediction present in a DataFrame called
    `result`, it is simple to calculate this score and is done using the following
    line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: CrossValidation and hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will be looking at one example each of `CrossValidation` and hyperparameter
    tuning. Let's take a look at `CrossValidation`.
  prefs: []
  type: TYPE_NORMAL
- en: CrossValidation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As stated before, we've used the default parameters of the machine learning
    algorithm and we don't know if they are a good choice. In addition, instead of
    simply splitting your data into training and testing, or training, testing, and
    validation sets, `CrossValidation` might be a better choice because it makes sure
    that eventually all the data is seen by the machine learning algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: '`CrossValidation` basically splits your complete available training data into
    a number of **k** folds. This parameter **k** can be specified. Then, the whole
    `Pipeline` is run once for every fold and one machine learning model is trained
    for each fold. Finally, the different machine learning models obtained are joined.
    This is done by a voting scheme for classifiers or by averaging for regression.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure illustrates ten-fold `CrossValidation`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/650d0e60-930f-48eb-b434-af3077044821.png)'
  prefs: []
  type: TYPE_IMG
- en: Hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '`CrossValidation` is often used in conjunction with so-called (hyper)parameter
    tuning. What are hyperparameters? These are the various knobs that you can tweak
    on your machine learning algorithm. For example, these are some parameters of
    the Random Forest classifier:'
  prefs: []
  type: TYPE_NORMAL
- en: Number of trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feature subset strategy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Impurity
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximal number of bins
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maximal tree depth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting these parameters can have a significant influence on the performance
    of the trained classifier. Often, there is no way of choosing them based on a
    clear recipe--of course, experience helps--but hyperparameter tuning is considered
    as black magic. Can't we just choose many different parameters and test the prediction
    performance? Of course, we can. This feature ...
  prefs: []
  type: TYPE_NORMAL
- en: Winning a Kaggle competition with Apache SparkML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Winning a Kaggle competition is an art by itself, but we just want to show you
    how the Apache SparkML tooling can be used efficiently to do so.
  prefs: []
  type: TYPE_NORMAL
- en: We'll use an archived competition for this offered by BOSCH, a German multinational
    engineering, and electronics company, on production line performance data. Details
    for the competition data can be found at [https://www.kaggle.com/c/bosch-production-line-performance/data](https://www.kaggle.com/c/bosch-production-line-performance/data).
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The challenge data comes in three ZIP packages but we only use two of them.
    One contains categorical data, one contains continuous data, and the last one
    contains timestamps of measurements, which we will ignore for now.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you extract the data, you''ll get three large CSV files. So the first thing
    that we want to do is re-encode them into parquet in order to be more space-efficient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: First, we define a function ...
  prefs: []
  type: TYPE_NORMAL
- en: Feature engineering
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it is time to run the first transformer (which is actually an estimator).
    It is `StringIndexer` and needs to keep track of an internal mapping table between
    strings and indexes. Therefore, it is not a transformer but an estimator:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see clearly in the following image, an additional column called `L0_S22_F545Index`
    has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4052e492-9d0b-4dd8-8dbd-8ad0aaca53bf.png)'
  prefs: []
  type: TYPE_IMG
- en: Finally, let's examine some content of the newly created column and compare
    it with the source column.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can clearly see how the category string gets transformed into a float index:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/08902e98-db50-473b-9f20-c83646239384.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now we want to apply `OneHotEncoder`, which is a transformer, in order to generate
    better features for our machine learning model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the following figure, the newly created column `L0_S22_F545Vec`
    contains `org.apache.spark.ml.linalg.SparseVector` objects, which is a compressed
    representation of a sparse vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7566dc66-93d4-4433-97de-b09abb41cc1f.png)**Sparse vector representations**:
    The `OneHotEncoder`, as many other algorithms, returns a sparse vector of the
    `org.apache.spark.ml.linalg.SparseVector` type as, according to the definition,
    only one element of the vector can be one, the rest has to remain zero. This gives
    a lot of opportunity for compression as only the position of the elements that
    are non-zero has to be known. Apache Spark uses a sparse vector representation
    in the following format: *(l,[p],[v])*, where *l* stands for length of the vector,
    *p* for position (this can also be an array of positions), and *v* for the actual
    values (this can be an array of values). So if we get (13,[10],[1.0]), as in our
    earlier example, the actual sparse vector looks like this: (0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0).'
  prefs: []
  type: TYPE_NORMAL
- en: 'So now that we are done with our feature engineering, we want to create one
    overall sparse vector containing all the necessary columns for our machine learner.
    This is done using `VectorAssembler`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We basically just define a list of column names and a target column, and the
    rest is done for us:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/77298190-a703-4228-b9df-e8b960a720c6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As the view of the `features` column got a bit squashed, let''s inspect one
    instance of the feature field in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c6cb5ae-1cae-4f6d-aacd-3514e383fe0a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can clearly see that we are dealing with a sparse vector of length 16 where
    positions 0, 13, 14, and 15 are non-zero and contain the following values: `1.0`,
    `0.03`, `-0.034`, and `-0.197`. Done! Let''s create a `Pipeline` out of these
    components.'
  prefs: []
  type: TYPE_NORMAL
- en: Testing the feature engineering pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s create a `Pipeline` out of our transformers and estimators:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the `setStages` method of `Pipeline` just expects an array of `transformers`
    and `estimators`, which we had created earlier. As parts of the `Pipeline` contain
    estimators, we have to run `fit` on our `DataFrame` first. The obtained `Pipeline`
    object takes a `DataFrame` in the `transform` method and returns the results of
    the transformations:'
  prefs: []
  type: TYPE_NORMAL
- en: As expected, ...
  prefs: []
  type: TYPE_NORMAL
- en: Training the machine learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now it''s time to add another component to the `Pipeline`: the actual machine
    learning algorithm--RandomForest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This code is very straightforward. First, we have to instantiate our algorithm
    and obtain it as a reference in `rf`. We could have set additional parameters
    to the model but we''ll do this later in an automated fashion in the `CrossValidation`
    step. Then, we just add the stage to our `Pipeline`, fit it, and finally transform.
    The `fit` method, apart from running all upstream stages, also calls fit on the
    `RandomForestClassifier` in order to train it. The trained model is now contained
    within the `Pipeline` and the `transform` method actually creates our predictions
    column:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b66350b-866d-4207-9ea6-db5097902fbf.png)'
  prefs: []
  type: TYPE_IMG
- en: As we can see, we've now obtained an additional column called prediction, which
    contains the output of the `RandomForestClassifier` model. Of course, we've only
    used a very limited subset of available features/columns and have also not yet
    tuned the model, so we don't expect to do very well; however, let's take a look
    at how we can evaluate our model easily with Apache SparkML.
  prefs: []
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Without evaluation, a model is worth nothing as we don''t know how accurately
    it performs. Therefore, we will now use the built-in `BinaryClassificationEvaluator`
    in order to assess prediction performance and a widely used measure called `areaUnderROC`
    (going into detail here is beyond the scope of this book):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, there is a built-in class called `org.apache.spark.ml.evaluation.BinaryClassificationEvaluator`
    and there are some other ...
  prefs: []
  type: TYPE_NORMAL
- en: CrossValidation and hyperparameter tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As explained before, a common step in machine learning is cross-validating your
    model using testing data against training data and also tweaking the knobs of
    your machine learning algorithms. Let's use Apache SparkML in order to do this
    for us, fully automated!
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we have to configure the parameter map and `CrossValidator`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The `org.apache.spark.ml.tuning.ParamGridBuilder` is used in order to define
    the hyperparameter space where the `CrossValidator` has to search and finally,
    the `org.apache.spark.ml.tuning.CrossValidator` takes our `Pipeline`, the hyperparameter
    space of our RandomForest classifier, and the number of folds for the `CrossValidation`
    as parameters. Now, as usual, we just need to call fit and transform on the `CrossValidator`
    and it will basically run our `Pipeline` multiple times and return a model that
    performs the best. Do you know how many different models are trained? Well, we
    have five folds on `CrossValidation` and five-dimensional hyperparameter space
    cardinalities between two and eight, so let''s do the math: 5 * 8 * 5 * 2 * 7
    * 7 = 19600 times!'
  prefs: []
  type: TYPE_NORMAL
- en: Using the evaluator to assess the quality of the cross-validated and tuned model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we''ve optimized our `Pipeline` in a fully automatic fashion, let''s
    see how our best model can be obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The `crossValidatorModel.bestModel` code basically returns the best `Pipeline`.
    Now we use `bestPipelineModel.stages` to obtain the individual stages and obtain
    the tuned `RandomForestClassificationModel ...`
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You've learned that, as in many other places, the introduction of `DataFrames`
    leads to the development of complementary frameworks that are not using RDDs directly
    anymore. This is also the case for machine learning but there is much more to
    it. `Pipeline` actually takes machine learning in Apache Spark to the next level
    as it improves the productivity of the data scientist dramatically.
  prefs: []
  type: TYPE_NORMAL
- en: The compatibility between all intermediate objects and well-thought-out concepts
    is just awesome.Â Great! Finally, we've applied the concepts that we discussed
    on a real dataset from a Kaggle competition, which is a very nice starting point
    for your own machine learning project with Apache SparkML. The next Chapter covers
    Apache SystemML, which is a 3rd party machine learning library for Apache Spark.
    Let's see why it is useful and what the differences are to SparkML.
  prefs: []
  type: TYPE_NORMAL
