- en: Unsupervised Clustering with Apache Spark 2.0
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: Building a KMeans classification system in Spark 2.0
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark 2.0 to classify
    data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Latent Dirichlet Allocation (LDA) to classify documents and text into
    topics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming KMeans to classify data in near real time
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised machine learning is a type of learning technique in which we try
    to draw inferences either directly or indirectly (through latent factors) from
    a set of unlabeled observations. In simple terms, we are trying to find the hidden
    knowledge or structures in a set of data without initially labeling the training
    data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
- en: While most machine learning library implementation break down when applied to
    large datasets (iterative, multi-pass, a lot of intermediate writes), the Apache
    Spark Machine Library succeeds by providing machine library algorithms designed
    for parallelism and extremely large datasets using memory for intermediate writes
    out of the box.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
- en: 'At the most abstract level, we can think of unsupervised learning as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: Building a KMeans classifying system in Spark 2.0
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will load a set of features (for example, x, y, z coordinates)
    using a LIBSVM file and then proceed to use `KMeans()` to instantiate an object.
    We will then set the number of desired clusters to three and then use `kmeans.fit()`
    to action the algorithm. Finally, we will print the centers for the three clusters
    that we found.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: It is really important to note that Spark *does not* implement KMeans++, contrary
    to popular literature, instead it implements KMeans || (pronounced as KMeans Parallel).
    See the following recipe and the sections following the code for a complete explanation
    of the algorithm as it is implemented in Spark.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Create Spark''s Session object:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: How it works...
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We read a LIBSVM file with a set of coordinates (can be interpreted as a tuple
    of three numbers) and then created a `KMean()` object, but changed the default
    number of clusters from 2 (out of the box) to 3 for demonstration purposes. We
    used the `.fit()` to create the model and then used `model.summary.predictions.show()`
    to display which tuple belongs to which cluster. In the last step, we printed
    the cost and the center of the three clusters. Conceptually, it can be thought
    of as having a set of 3D coordinates as data and then assigning each individual
    coordinate to one of the three clusters using KMeans algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: KMeans is a form of unsupervised machine learning algorithm, with its root in
    signal processing (vector quantization) and compression (grouping similar vectors
    of items together to achieve a higher compression rate). Generally speaking, the
    KMeans algorithm attempts to group a series of observations {X[1,] X[2], ....
    , X[n]} into a series of clusters {C[1,] C[2 .....] C[n]} using a form of distance
    measure (local optimization) that is optimized in an iterative manner.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: There are three main types of KMeans algorithm that are in use. In a simple
    survey, we found 12 specialized variations of the KMeans algorithm. It is important
    to note that Spark implements a version called KMeans || (KMeans Parallel) and
    *not* KMeans++ or standard KMeans as referenced in some literature or videos.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts KMeans in a nutshell:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/870bc634-f412-46c1-9857-83ed6e05183f.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
- en: 'Source: Spark documentation'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: KMeans (Lloyd Algorithm)
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps for basic KMeans implementation (Lloyd algorithm) are:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select K datacenters from observations as the initial centroids.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keep iterating till the convergence criteria is met:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the distance from a point to each centroid
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include each data point in a cluster which is the closest centroid
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate new cluster centroids based on a distance formula (proxy for dissimilarity)
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the algorithm with new center points
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The three generations are depicted in the following figure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc4b1c43-9ed0-4396-a06a-d12c2a3d181d.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
- en: KMeans++ (Arthur's algorithm)
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next improvement over standard KMeans is the KMeans++ proposed by David
    Arthur and Sergei Vassilvitskii in 2007\. Arthur's algorithm improves the initial
    Lloyd's KMeans by being more selective during the seeding process (the initial
    step).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: KMeans++, rather than picking random centres (random centroids) as starting
    points, picks the first centroid randomly and then picks the data points one by
    one and calculates `D(x)`. Then it chooses one more data point at random and,
    using proportional probability distribution `D(x)2`, it then keeps repeating the
    last two steps until all *K* numbers are picked. After the initial seeding, we
    finally run the KMeans or a variation with the newly seeded centroid. The KMeans++
    algorithm is guaranteed to find a solution in an *Omega= O(log k)* complexity.
    Even though the initial seeding takes extra steps, the accuracy improvements are
    substantial.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: KMeans|| (pronounced as KMeans Parallel)
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KMeans || is optimized to run in parallel and can result in one-two orders of
    magnitude improvement over Lloyd's original algorithm. The limitation of KMeans++
    is that it requires K-passes over the dataset, which can severely limit the performance
    and practicality of running KMeans with large or extreme datasets. Spark's KMeans||
    parallel implementation runs faster because it takes fewer passes (a lot less)
    over the data by sampling m points and oversampling in the process.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of the algorithm and the math is depicted in the following figure:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e50796d1-a612-4089-b44f-1ffb5c933bab.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: In a nutshell, the highlight of the KMeans || (Parallel ...
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is also a streaming version of KMeans implementation in Spark that allows
    you to classify the features on the fly.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a class that helps you to generate RDD data for KMeans. We found
    this to be very useful during our application development process:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This call uses Spark context to create RDDs while allowing you to specify the
    number of points, clusters, dimensions, and partitions.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: 'A useful related API is: `generateKMeansRDD()`. Documentation for `generateKMeansRDD`
    can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator%24) for
    generate an RDD containing test data for KMeans.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need two pieces of objects to be able to write, measure, and manipulate
    the parameters of the KMeans || algorithm in Spark. The details of these two pieces
    of objects can be found at the following websites:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: '`KMeans()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KMeansModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel)'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will download the glass dataset and try to identify and label
    each glass using a bisecting KMeans algorithm. The Bisecting KMeans is a hierarchical
    version of the K-Mean algorithm implemented in Spark using the `BisectingKMeans()`
    API. While this algorithm is conceptually like KMeans, it can offer considerable
    speed for some use cases where the hierarchical path is present.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we used for this recipe is the Glass Identification Database. The
    study of the classification of types of glass was motivated by criminological
    research. Glass could be considered as evidence if it is correctly identified.
    The data can be found at NTU (Taiwan), already in LIBSVM format.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We downloaded the prepared data file in LIBSVM from: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale)
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset contains 11 features and 214 rows.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: The original dataset and data dictionary is also available at the UCI website: [http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification)
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ID number: 1 to 214'
  id: totrans-67
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RI: Refractive index'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Na: Sodium (unit measurement: weight percent in corresponding oxide, as are
    attributes 4-10)'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mg: Magnesium'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al: Aluminum'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Si: Silicon'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K: Potassium'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ca: Calcium'
  id: totrans-74
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ba: Barium'
  id: totrans-75
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fe: Iron'
  id: totrans-76
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Type of glass: Will find our class attributes or clusters using `BisectingKMeans()`:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: '`building_windows_float_processed`'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`building_windows_non-_float_processed`'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vehicle_windows_float_processed`'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this session, we explored the Bisecting KMeans model, which is new in Spark
    2.0\. We utilized the glass dataset in this session and tried to assign a glass
    type using `BisectingKMeans()`, but changed k to 6 so we have sufficient clusters.
    As usual, we loaded the data into a dataset with Spark's libsvm loading mechanism.
    We split the dataset randomly into 80% and 20%, with 80% used to train the model
    and 20% used for testing the model.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: We created the `BiSectingKmeans()` object and used the `fit(x)` function to
    create the model. We then used the `transform(x)` function for the testing dataset
    to explore the model prediction and printed out the result in the console output.
    We also output the cost of computing the clusters (sum of error squared) and then
    displayed the cluster centers. Finally, we printed the features with their assigned
    cluster number and stop operation.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: 'Approaches to hierarchical clustering include:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '**Divisive**: Top down approach (Apache Spark implementation)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agglomerative**: Bottom up approach'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More about the Bisecting KMeans can be found at:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans)'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel)'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use clustering to explore the data and get a feel for what the outcome looks
    like as clusters. The bisecting KMeans is an interesting case of hierarchical
    analysis versus KMeans clustering.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: The best way to conceptualize it is to think of bisecting KMeans as a recursive
    hierarchical KMeans. The bisecting KMeans algorithm divides the data using similarity
    measurement techniques like KMeans but uses a hierarchical scheme to increase
    accuracy. It is particularly prevalent ...
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two approaches to implementing hierarchical clustering--Spark uses
    a recursive top-down approach in which a cluster is chosen and then splits are
    performed in the algorithm as it moves down the hierarchy:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: Details about the hierarchical clustering approach can be found at [https://en.wikipedia.org/wiki/Hierarchical_clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark 2.0 documentation for Bisecting K-Mean can be found at [http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means](http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means)
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A paper describing how to use Bisecting KMeans to classify web logs can be found
    at [http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf](http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf)
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark to classify
    data
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore Spark's implementation of **expectation maximization**
    (**EM**) `GaussianMixture()`*,* which calculates the maximum likelihood given
    a set of features as input. It assumes a Gaussian mixture in which each point
    can be sampled from K number of sub-distributions (cluster memberships).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Import the necessary packages for vector and matrix manipulation:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Create Spark''s session object:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Let us take a look at the dataset and examine the input file. The Simulated
    SOCR Knee Pain Centroid Location Data represents the centroid location for the
    hypothetical knee-pain locations for 1,000 subjects. The data includes the X and
    Y coordinates of the centroids.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This dataset can be used to illustrate the Gaussian Mixture and Expectation
    Maximization. The data is available at: [http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample data looks like the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '**X**: The *x* coordinate of the centroid location for one subject and one
    view.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Y**: The *y* coordinate of the centroid location for one subject and one
    view.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X, Y
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: 11 73
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 20 88
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: 19 73
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: 15 65
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: 21 57
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: 26 101
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 24 117
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 35 106
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: 37 96
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: 35 147
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 41 151
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: 42 137
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 43 127
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 41 206
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 47 213
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 49 238
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 40 229
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a knee-pain map based on the SOCR dataset from
    `wiki.stat.ucla`:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18ed269a-1079-4fd8-a2c0-6a75e4e93fcb.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
- en: We place the data file in a data directory (you can copy the data file to any
    location you prefer).
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The data file contains 8,666 entries:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We then load the data file into RDD:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We now create a GaussianMixture model and set the parameters for the model.
    We set the K value to 4, since the data was collected by four views: **Left Front**
    (**LF**), **Left Back** (**LB**), **Right Front** (**RF**), and **Right Back**
    (**RB**). We set the convergence to the default value of 0.01, and the maximum
    iteration counts to 100:'
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We run the model algorithm:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We print out the key values for the GaussianMixture model after the training:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Since we set the K value to 4, we will have four sets of values printed out
    in the console logger:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bc6b63fe-5651-462d-8966-636bec6ba119.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
- en: 'We also print out the first 50 cluster-labels based on the GaussianMixture
    model predictions:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The sample output in the console will show the following:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We then close the program by stopping the Spark context:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: How it works...
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we observed that KMeans can discover and allocate membership
    to one and only one cluster based on an iterative method using similarity (Euclidian,
    and so on). One can think of KMeans as a specialized version of a Gaussian mixture
    model with EM models in which a discrete (hard) membership is enforced.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are cases that have overlap, which is often the case in medicine
    or signal processing, as depicted in the following figure:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86a123c5-915b-40b8-ae00-55fac3375b57.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
- en: In such cases, we need a probability density function that can express the membership
    in each sub-distribution. The Gaussian Mixture models with **Expectation Maximization**
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: New GaussianMixture()
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This constructs a default instance. The default parameters that control the
    behavior of the model are:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46e18b3f-0a38-44c0-bebf-24f741ebe8f6.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: The Gaussian Mixture models with Expectation Maximization are a form of soft
    clustering in which a membership can be inferred using a log maximum likelihood
    function. In this scenario, a probability density function with mean and covariance
    is used to define the membership or likelihood of a membership to K number of
    clusters. It is flexible in the sense that the membership is not quantified which
    allows for overlapping membership based on probability (indexed to multiple sub-distributions).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure is a snapshot of the EM algorithm:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1eac50f4-acad-4b14-a479-aa390fe87a9b.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Here are the steps to the EM algorithm:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Assume *N* number of Gaussian distribution.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate until we have convergence:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each point Z drawn with conditional probability of being drawn from distribution
    Xi written as *P (Z | Xi)*
  id: totrans-166
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the parameter's mean and variance so that they fit the points that are
    assigned to the sub-distribution
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a more mathematical explanation, including detailed work on maximum likelihood,
    see the following link: [http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf](http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure provides a quick reference point to highlight some of
    the differences between hard versus soft clustering:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1c9d196-7b32-40ea-9303-66fa819f52f7.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
- en: See also
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor GaussianMixture can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture)
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for constructor GaussianMixtureModel can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel)
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a classification method for the vertices of a graph given their similarities
    as defined by their edges. It uses the GraphX library which is ships out of the
    box with Spark to implement the algorithm. Power Iteration Clustering is similar
    to other Eigen Vector/Eigen Value decomposition algorithms but without the overhead
    of matrix decomposition. It is suitable when you have a large sparse matrix (for
    example, graphs depicted as a sparse matrix).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: GraphFrames will be the replacement/interface proper for the GraphX library
    going forward ([https://databricks.com/blog/2016/03/03/introducing-graphframes.html](https://databricks.com/blog/2016/03/03/introducing-graphframes.html)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Set up the logger level to ERROR only to reduce the output:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Create Spark''s configuration and SQL context so we can have access to the
    cluster and be able to create and use a DataFrame as needed:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We create a training dataset with a list of datasets and use the Spark `sparkContext.parallelize()`
    function to create Spark RDD:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We create a `PowerIterationClustering` object and set the parameters. We set
    the `K` value to `3` and max iteration count to `15`:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We then let the model run:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We print out the cluster assignment based on the model for the training data:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The console output will show the following information:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1fbb5236-33c4-443b-b46c-c506f0c66782.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
- en: 'We also print out the model assignment data in a collection for each cluster:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The console output will display the following information (in total, we have
    three clusters which were set in the preceding parameters):'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then close the program by stopping the Spark context:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We created a list of edges and vertices for a graph and then proceeded to create
    the object and set the parameters:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The next step was the model of training data:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The clusters were then outputted for inspection. The code near the end prints
    out the model assignment data in a collection for each cluster using Spark transformation
    operators.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: At the core **PIC** (**Power Iteration Clustering**) is an eigenvalue class
    algorithm which avoids matrix decomposition by producing an Eigen Value plus an
    Eigen Vector to satisfy *Av* = λ*v.* Because PIC avoids the decomposition of the
    matrix A, it is suitable when the input matrix A (describing a graph in ...
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a more detailed mathematical treatment of the subject (power iteration),
    see the following white paper from Carnegie Mellon University: [http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf](http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf)
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: See also
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor `PowerIterationClustering()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the constructor `PowerIterationClusteringModel()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel)
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation (LDA) to classify documents and text into topics
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the **Latent Dirichlet Allocation** (**LDA**)
    algorithm in Spark 2.0\. The LDA we use in this recipe is completely different
    from linear discrimination analysis. Both Latent Dirichlet Allocation and linear
    discrimination analysis are referred to as LDA, but they are extremely different
    techniques. In this recipe, when we use the LDA, we refer to Latent Dirichlet
    Allocation. The chapter on text analytics is also relevant to understanding the
    LDA.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: LDA is often used in natural language processing which tries to classify a large
    body of the document (for example, emails from the Enron fraud case) into a discrete
    number of topics or themes so it can be understood. LDA is also a good candidate
    for selecting articles based on one's interest (for example, as you turn a page
    and spend time on a specific topic) in a given magazine article or page.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Import the necessary packages:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We set up the necessary Spark Session to gain access to the cluster:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: We have a sample LDA dataset, which is located at the following relative path
    (you can use an absolute path). The sample file is provided with any Spark distribution
    and ...
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA assumes that the document is a mixture of different topics with Dirichlet
    prior distribution. The words in the document are assumed to have an affinity
    towards a specific topic which allows LDA to classify the overall document (compose
    and assign a distribution) that best matches a topic.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: A topic model is a generative latent model for discovering abstract themes (topics)
    that occur in the body of documents (often too large for humans to handle). The
    models are a pre-cursor to summarize, search, and browse a large set of unlabeled
    documents and their contents. Generally speaking, we are trying to find a cluster
    of features (words, sub-images, and so on) that occur together.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the overall LDA scheme:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Please be sure to refer to the white paper cited here for completeness [http://ai.stanford.edu/~ang/papers/nips01-lda.pdf](http://ai.stanford.edu/~ang/papers/nips01-lda.pdf)
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a85bbb8a-37a8-4880-ac6f-e8ede49bdb67.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
- en: 'The steps for the LDA algorithm are as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the following parameters (controls concentration and smoothing):'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alpha parameter (high alpha makes documents more similar to each other and contain
    similar topics )
  id: totrans-236
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Beta parameter ( high beta means each topic is most likely to contain a mix
    of most of the words)
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly initialize the topic assignment.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document.
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each word in the document.
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Resample the topic for each word.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: With respect to all other words and their current assignment (for the current
    iteration).
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the result.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model evaluation
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In statistics, Dirichlet distribution Dir(alpha) is a family of continuous multivariate
    probability distributions parameterized by a vector α of positive real numbers.
    For a more in-depth treatment of LDA, see the original paper in the
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Journal of Machine Learning at [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: The LDA does not assign any semantics to a topic and does not care what the
    topics are called. It is only a generative model that uses the distribution of
    fine-grained items (for example, words about cats, dogs, fish, cars) to assign
    an overall topic that scores the best. It does not know, cares, or understand
    about topics called dogs or cats.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: We often have to tokenize and vectorize the document via TF-IDF prior to input
    to an LDA algorithm.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure depicts the LDA in a nutshell:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3315b50-1445-4633-a0d5-ffaf8256afd5.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
- en: 'There are two approaches to document analysis. We can simply use matrix factorization
    to decompose a large matrix of datasets to a smaller matrix (topic assignments)
    times a vector (topics themselves):'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f6891f4-b0ce-476a-bda5-d52d10745a60.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: See also
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LDA**: documentation for a constructor can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LDAModel**: documentation for a constructor can be found at ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also, via Spark''s Scala API, documentation links for the following:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: DistributedLDAModel
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMLDAOptimizer
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDAOptimizer
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LocalLDAModel
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OnlineLDAOptimizer
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming KMeans to classify data in near real-time
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark streaming is a powerful facility which lets you combine near real-time
    and batch in the same paradigm. The streaming KMeans interface lives at the intersection
    of ML clustering and Spark streaming, and takes full advantage of the core facilities
    provided by Spark streaming itself (for example, fault tolerance, exactly once
    delivery semantics, and so on).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for streaming KMeans:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`package spark.ml.cookbook.chapter14`.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages for streaming KMeans:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We set up the following parameters for the streaming KMeans program. The training
    directory will be the directory to send the training data file. The KMeans clustering
    model utilizes the training data to run algorithms and calculations. The `testDirectory`
    will be the test data for predictions. The `batchDuration` is a number in seconds
    for a batch run. In the following case, the program will check every 10 seconds
    to see if there is any new data files for recalculations.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cluster is set to `2`, and the data dimensions will be `3`:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'With the preceding settings, the sample training data will contain data like
    the following (in the format of [*X[1], X[2], ...X[n]*], where *n* is `numDimensions`:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[0.0,0.0,0.0]'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '[0.1,0.1,0.1]'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '[0.2,0.2,0.2]'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '[9.0,9.0,9.0]'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: '[9.1,9.1,9.1]'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[9.2,9.2,9.2]'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '[0.1,0.0,0.0]'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '[0.2,0.1,0.1]'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '....'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'The test data file will contain data like the following (in the format of (*y,
    [X1, X2, .. Xn]*), where *n* is `numDimensions` and `y` is an identifier):'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: (7,[0.4,0.4,0.4])
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: (8,[0.1,0.1,0.1])
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: (9,[0.2,0.2,0.2])
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: (10,[1.1,1.0,1.0])
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: (11,[9.2,9.1,9.2])
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: (12,[9.3,9.2,9.3])
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up the necessary Spark context to gain access to the cluster:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Define the streaming context and micro-batch window:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The following code will create data by parsing the data file in the preceding
    two directories into `trainingData` and `testData RDDs`:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We create the `StreamingKMeans` model and set the parameters:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The program will train the model using the training dataset and predict using
    the test dataset:'
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We start the streaming context, and the program will run the batch every 10
    seconds to see if a new dataset is available for training and if there is any
    new test dataset for prediction. The program will exit if a termination signal
    is received (exit the batch running):'
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We copy the `testKStreaming1.txt` data file into the preceding `testDir` set
    and see the following printed out in the console logs:'
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d39d25e2-be84-46b3-917a-52c3907e3122.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
- en: For a Windows machine, we copied the `testKStreaming1.txt` file into the directory: `C:\spark-2.0.0-bin-hadoop2.7\data\sparkml2\chapter8\testDir\`.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can also check the SparkUI for more information: `http://localhost:4040/`.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The job panel will display streaming jobs, as shown in the following figure:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ef1d102-cb06-48cd-b26c-9f71ac6bbb9d.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the following figure, the streaming panel will show the preceding
    Streaming KMeans matrix as the matrix displayed, the batch job running every 10
    seconds in this case:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b14bd318-6028-4597-bf45-ca1e5a3dcd9d.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
- en: 'You can get more details on the streaming batch by clicking on any of the batches,
    as shown in the following figure:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a690bac-440b-4891-a843-1270709933a4.png)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
- en: How it works...
  id: totrans-314
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In certain situations, we cannot use batch methods to load and capture the events
    and then react to them. We can use creative methods of capturing events in the
    memory or a landing DB and then rapidly marshal that over to another system for
    processing, but most of these systems fail to act as streaming systems and often
    are very expensive to build.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides a near real-time (also referred to as subjective real time) that
    can receive incoming sources, such as Twitter feeds, signals, and so, on via connectors
    (for example, a Kafka connector) and then process and present them as an RDD interface.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the elements needed to build and construct streaming KMeans in Spark:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: Use the streaming context as opposed to the ...
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  id: totrans-319
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming KMeans are special cases of KMeans implementation in which the data
    can arrive at a near real-time and be classified into a cluster (hard classification)
    as needed. For a reference to Voronoi diagrams, see the following URL: [https://en.wikipedia.org/wiki/Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram)
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, there are other algorithms besides streaming KMeans in the Spark
    Machine Library, as shown in the following figure:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e339c28-5370-47c9-bd81-086eb04f7ca0.png)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
- en: See also
  id: totrans-323
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Streaming KMeans can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式KMeans文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)找到。
- en: Documentation for Streaming KMeans Model can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest)
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式KMeans模型文档可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest)找到。
- en: Documentation for Streaming Test--very useful for data generation--can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式测试文档——对数据生成非常有用——可在[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)找到。
