- en: Unsupervised Clustering with Apache Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a KMeans classification system in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark 2.0 to classify
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Latent Dirichlet Allocation (LDA) to classify documents and text into
    topics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming KMeans to classify data in near real time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised machine learning is a type of learning technique in which we try
    to draw inferences either directly or indirectly (through latent factors) from
    a set of unlabeled observations. In simple terms, we are trying to find the hidden
    knowledge or structures in a set of data without initially labeling the training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: While most machine learning library implementation break down when applied to
    large datasets (iterative, multi-pass, a lot of intermediate writes), the Apache
    Spark Machine Library succeeds by providing machine library algorithms designed
    for parallelism and extremely large datasets using memory for intermediate writes
    out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the most abstract level, we can think of unsupervised learning as:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a KMeans classifying system in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will load a set of features (for example, x, y, z coordinates)
    using a LIBSVM file and then proceed to use `KMeans()` to instantiate an object.
    We will then set the number of desired clusters to three and then use `kmeans.fit()`
    to action the algorithm. Finally, we will print the centers for the three clusters
    that we found.
  prefs: []
  type: TYPE_NORMAL
- en: It is really important to note that Spark *does not* implement KMeans++, contrary
    to popular literature, instead it implements KMeans || (pronounced as KMeans Parallel).
    See the following recipe and the sections following the code for a complete explanation
    of the algorithm as it is implemented in Spark.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Set the output level to `ERROR` to reduce Spark''s logging output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s Session object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We read a LIBSVM file with a set of coordinates (can be interpreted as a tuple
    of three numbers) and then created a `KMean()` object, but changed the default
    number of clusters from 2 (out of the box) to 3 for demonstration purposes. We
    used the `.fit()` to create the model and then used `model.summary.predictions.show()`
    to display which tuple belongs to which cluster. In the last step, we printed
    the cost and the center of the three clusters. Conceptually, it can be thought
    of as having a set of 3D coordinates as data and then assigning each individual
    coordinate to one of the three clusters using KMeans algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: KMeans is a form of unsupervised machine learning algorithm, with its root in
    signal processing (vector quantization) and compression (grouping similar vectors
    of items together to achieve a higher compression rate). Generally speaking, the
    KMeans algorithm attempts to group a series of observations {X[1,] X[2], ....
    , X[n]} into a series of clusters {C[1,] C[2 .....] C[n]} using a form of distance
    measure (local optimization) that is optimized in an iterative manner.
  prefs: []
  type: TYPE_NORMAL
- en: There are three main types of KMeans algorithm that are in use. In a simple
    survey, we found 12 specialized variations of the KMeans algorithm. It is important
    to note that Spark implements a version called KMeans || (KMeans Parallel) and
    *not* KMeans++ or standard KMeans as referenced in some literature or videos.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts KMeans in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/870bc634-f412-46c1-9857-83ed6e05183f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Source: Spark documentation'
  prefs: []
  type: TYPE_NORMAL
- en: KMeans (Lloyd Algorithm)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The steps for basic KMeans implementation (Lloyd algorithm) are:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly select K datacenters from observations as the initial centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Keep iterating till the convergence criteria is met:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measure the distance from a point to each centroid
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Include each data point in a cluster which is the closest centroid
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Calculate new cluster centroids based on a distance formula (proxy for dissimilarity)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Update the algorithm with new center points
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The three generations are depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bc4b1c43-9ed0-4396-a06a-d12c2a3d181d.png)'
  prefs: []
  type: TYPE_IMG
- en: KMeans++ (Arthur's algorithm)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The next improvement over standard KMeans is the KMeans++ proposed by David
    Arthur and Sergei Vassilvitskii in 2007\. Arthur's algorithm improves the initial
    Lloyd's KMeans by being more selective during the seeding process (the initial
    step).
  prefs: []
  type: TYPE_NORMAL
- en: KMeans++, rather than picking random centres (random centroids) as starting
    points, picks the first centroid randomly and then picks the data points one by
    one and calculates `D(x)`. Then it chooses one more data point at random and,
    using proportional probability distribution `D(x)2`, it then keeps repeating the
    last two steps until all *K* numbers are picked. After the initial seeding, we
    finally run the KMeans or a variation with the newly seeded centroid. The KMeans++
    algorithm is guaranteed to find a solution in an *Omega= O(log k)* complexity.
    Even though the initial seeding takes extra steps, the accuracy improvements are
    substantial.
  prefs: []
  type: TYPE_NORMAL
- en: KMeans|| (pronounced as KMeans Parallel)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: KMeans || is optimized to run in parallel and can result in one-two orders of
    magnitude improvement over Lloyd's original algorithm. The limitation of KMeans++
    is that it requires K-passes over the dataset, which can severely limit the performance
    and practicality of running KMeans with large or extreme datasets. Spark's KMeans||
    parallel implementation runs faster because it takes fewer passes (a lot less)
    over the data by sampling m points and oversampling in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'The core of the algorithm and the math is depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e50796d1-a612-4089-b44f-1ffb5c933bab.png)'
  prefs: []
  type: TYPE_IMG
- en: In a nutshell, the highlight of the KMeans || (Parallel ...
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There is also a streaming version of KMeans implementation in Spark that allows
    you to classify the features on the fly.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is also a class that helps you to generate RDD data for KMeans. We found
    this to be very useful during our application development process:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This call uses Spark context to create RDDs while allowing you to specify the
    number of points, clusters, dimensions, and partitions.
  prefs: []
  type: TYPE_NORMAL
- en: 'A useful related API is: `generateKMeansRDD()`. Documentation for `generateKMeansRDD`
    can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator$](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.util.KMeansDataGenerator%24) for
    generate an RDD containing test data for KMeans.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We need two pieces of objects to be able to write, measure, and manipulate
    the parameters of the KMeans || algorithm in Spark. The details of these two pieces
    of objects can be found at the following websites:'
  prefs: []
  type: TYPE_NORMAL
- en: '`KMeans()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeans)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`KMeansModel()`: [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.KMeansModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bisecting KMeans, the new kid on the block in Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will download the glass dataset and try to identify and label
    each glass using a bisecting KMeans algorithm. The Bisecting KMeans is a hierarchical
    version of the K-Mean algorithm implemented in Spark using the `BisectingKMeans()`
    API. While this algorithm is conceptually like KMeans, it can offer considerable
    speed for some use cases where the hierarchical path is present.
  prefs: []
  type: TYPE_NORMAL
- en: The dataset we used for this recipe is the Glass Identification Database. The
    study of the classification of types of glass was motivated by criminological
    research. Glass could be considered as evidence if it is correctly identified.
    The data can be found at NTU (Taiwan), already in LIBSVM format.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We downloaded the prepared data file in LIBSVM from: [https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale](https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass/glass.scale)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The dataset contains 11 features and 214 rows.
  prefs: []
  type: TYPE_NORMAL
- en: The original dataset and data dictionary is also available at the UCI website: [http://archive.ics.uci.edu/ml/datasets/Glass+Identification](http://archive.ics.uci.edu/ml/datasets/Glass+Identification)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'ID number: 1 to 214'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'RI: Refractive index'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Na: Sodium (unit measurement: weight percent in corresponding oxide, as are
    attributes 4-10)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mg: Magnesium'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al: Aluminum'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Si: Silicon'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'K: Potassium'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ca: Calcium'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ba: Barium'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fe: Iron'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Type of glass: Will find our class attributes or clusters using `BisectingKMeans()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`building_windows_float_processed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`building_windows_non-_float_processed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`vehicle_windows_float_processed`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this session, we explored the Bisecting KMeans model, which is new in Spark
    2.0\. We utilized the glass dataset in this session and tried to assign a glass
    type using `BisectingKMeans()`, but changed k to 6 so we have sufficient clusters.
    As usual, we loaded the data into a dataset with Spark's libsvm loading mechanism.
    We split the dataset randomly into 80% and 20%, with 80% used to train the model
    and 20% used for testing the model.
  prefs: []
  type: TYPE_NORMAL
- en: We created the `BiSectingKmeans()` object and used the `fit(x)` function to
    create the model. We then used the `transform(x)` function for the testing dataset
    to explore the model prediction and printed out the result in the console output.
    We also output the cost of computing the clusters (sum of error squared) and then
    displayed the cluster centers. Finally, we printed the features with their assigned
    cluster number and stop operation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Approaches to hierarchical clustering include:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Divisive**: Top down approach (Apache Spark implementation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Agglomerative**: Bottom up approach'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'More about the Bisecting KMeans can be found at:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeans)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.BisectingKMeansModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use clustering to explore the data and get a feel for what the outcome looks
    like as clusters. The bisecting KMeans is an interesting case of hierarchical
    analysis versus KMeans clustering.
  prefs: []
  type: TYPE_NORMAL
- en: The best way to conceptualize it is to think of bisecting KMeans as a recursive
    hierarchical KMeans. The bisecting KMeans algorithm divides the data using similarity
    measurement techniques like KMeans but uses a hierarchical scheme to increase
    accuracy. It is particularly prevalent ...
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are two approaches to implementing hierarchical clustering--Spark uses
    a recursive top-down approach in which a cluster is chosen and then splits are
    performed in the algorithm as it moves down the hierarchy:'
  prefs: []
  type: TYPE_NORMAL
- en: Details about the hierarchical clustering approach can be found at [https://en.wikipedia.org/wiki/Hierarchical_clustering](https://en.wikipedia.org/wiki/Hierarchical_clustering)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Spark 2.0 documentation for Bisecting K-Mean can be found at [http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means](http://spark.apache.org/docs/latest/ml-clustering.html#bisecting-k-means)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A paper describing how to use Bisecting KMeans to classify web logs can be found
    at [http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf](http://research.ijcaonline.org/volume116/number19/pxc3902799.pdf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using Gaussian Mixture and Expectation Maximization (EM) in Spark to classify
    data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore Spark's implementation of **expectation maximization**
    (**EM**) `GaussianMixture()`*,* which calculates the maximum likelihood given
    a set of features as input. It assumes a Gaussian mixture in which each point
    can be sampled from K number of sub-distributions (cluster memberships).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for vector and matrix manipulation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s session object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Let us take a look at the dataset and examine the input file. The Simulated
    SOCR Knee Pain Centroid Location Data represents the centroid location for the
    hypothetical knee-pain locations for 1,000 subjects. The data includes the X and
    Y coordinates of the centroids.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This dataset can be used to illustrate the Gaussian Mixture and Expectation
    Maximization. The data is available at: [http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409](http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_KneePainData_041409)
  prefs: []
  type: TYPE_NORMAL
- en: 'The sample data looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**X**: The *x* coordinate of the centroid location for one subject and one
    view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Y**: The *y* coordinate of the centroid location for one subject and one
    view.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: X, Y
  prefs: []
  type: TYPE_NORMAL
- en: 11 73
  prefs: []
  type: TYPE_NORMAL
- en: 20 88
  prefs: []
  type: TYPE_NORMAL
- en: 19 73
  prefs: []
  type: TYPE_NORMAL
- en: 15 65
  prefs: []
  type: TYPE_NORMAL
- en: 21 57
  prefs: []
  type: TYPE_NORMAL
- en: 26 101
  prefs: []
  type: TYPE_NORMAL
- en: 24 117
  prefs: []
  type: TYPE_NORMAL
- en: 35 106
  prefs: []
  type: TYPE_NORMAL
- en: 37 96
  prefs: []
  type: TYPE_NORMAL
- en: 35 147
  prefs: []
  type: TYPE_NORMAL
- en: 41 151
  prefs: []
  type: TYPE_NORMAL
- en: 42 137
  prefs: []
  type: TYPE_NORMAL
- en: 43 127
  prefs: []
  type: TYPE_NORMAL
- en: 41 206
  prefs: []
  type: TYPE_NORMAL
- en: 47 213
  prefs: []
  type: TYPE_NORMAL
- en: 49 238
  prefs: []
  type: TYPE_NORMAL
- en: 40 229
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts a knee-pain map based on the SOCR dataset from
    `wiki.stat.ucla`:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18ed269a-1079-4fd8-a2c0-6a75e4e93fcb.png)'
  prefs: []
  type: TYPE_IMG
- en: We place the data file in a data directory (you can copy the data file to any
    location you prefer).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The data file contains 8,666 entries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We then load the data file into RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We now create a GaussianMixture model and set the parameters for the model.
    We set the K value to 4, since the data was collected by four views: **Left Front**
    (**LF**), **Left Back** (**LB**), **Right Front** (**RF**), and **Right Back**
    (**RB**). We set the convergence to the default value of 0.01, and the maximum
    iteration counts to 100:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We run the model algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the key values for the GaussianMixture model after the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Since we set the K value to 4, we will have four sets of values printed out
    in the console logger:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/bc6b63fe-5651-462d-8966-636bec6ba119.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also print out the first 50 cluster-labels based on the GaussianMixture
    model predictions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'The sample output in the console will show the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we observed that KMeans can discover and allocate membership
    to one and only one cluster based on an iterative method using similarity (Euclidian,
    and so on). One can think of KMeans as a specialized version of a Gaussian mixture
    model with EM models in which a discrete (hard) membership is enforced.
  prefs: []
  type: TYPE_NORMAL
- en: 'But there are cases that have overlap, which is often the case in medicine
    or signal processing, as depicted in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/86a123c5-915b-40b8-ae00-55fac3375b57.png)'
  prefs: []
  type: TYPE_IMG
- en: In such cases, we need a probability density function that can express the membership
    in each sub-distribution. The Gaussian Mixture models with **Expectation Maximization**
  prefs: []
  type: TYPE_NORMAL
- en: New GaussianMixture()
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This constructs a default instance. The default parameters that control the
    behavior of the model are:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46e18b3f-0a38-44c0-bebf-24f741ebe8f6.png)'
  prefs: []
  type: TYPE_IMG
- en: The Gaussian Mixture models with Expectation Maximization are a form of soft
    clustering in which a membership can be inferred using a log maximum likelihood
    function. In this scenario, a probability density function with mean and covariance
    is used to define the membership or likelihood of a membership to K number of
    clusters. It is flexible in the sense that the membership is not quantified which
    allows for overlapping membership based on probability (indexed to multiple sub-distributions).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure is a snapshot of the EM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1eac50f4-acad-4b14-a479-aa390fe87a9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here are the steps to the EM algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Assume *N* number of Gaussian distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate until we have convergence:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each point Z drawn with conditional probability of being drawn from distribution
    Xi written as *P (Z | Xi)*
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Adjust the parameter's mean and variance so that they fit the points that are
    assigned to the sub-distribution
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For a more mathematical explanation, including detailed work on maximum likelihood,
    see the following link: [http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf](http://www.ee.iisc.ernet.in/new/people/faculty/prasantg/downloads/GMM_Tutorial_Reynolds.pdf)'
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure provides a quick reference point to highlight some of
    the differences between hard versus soft clustering:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f1c9d196-7b32-40ea-9303-66fa819f52f7.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for constructor GaussianMixture can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixture)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for constructor GaussianMixtureModel can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.GaussianMixtureModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying the vertices of a graph using Power Iteration Clustering (PIC) in
    Spark 2.0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This is a classification method for the vertices of a graph given their similarities
    as defined by their edges. It uses the GraphX library which is ships out of the
    box with Spark to implement the algorithm. Power Iteration Clustering is similar
    to other Eigen Vector/Eigen Value decomposition algorithms but without the overhead
    of matrix decomposition. It is suitable when you have a large sparse matrix (for
    example, graphs depicted as a sparse matrix).
  prefs: []
  type: TYPE_NORMAL
- en: GraphFrames will be the replacement/interface proper for the GraphX library
    going forward ([https://databricks.com/blog/2016/03/03/introducing-graphframes.html](https://databricks.com/blog/2016/03/03/introducing-graphframes.html)).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages for Spark context to get access to the cluster
    and `Log4j.Logger` to reduce the amount of output produced by Spark:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Set up the logger level to ERROR only to reduce the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Create Spark''s configuration and SQL context so we can have access to the
    cluster and be able to create and use a DataFrame as needed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a training dataset with a list of datasets and use the Spark `sparkContext.parallelize()`
    function to create Spark RDD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'We create a `PowerIterationClustering` object and set the parameters. We set
    the `K` value to `3` and max iteration count to `15`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We then let the model run:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'We print out the cluster assignment based on the model for the training data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will show the following information:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/1fbb5236-33c4-443b-b46c-c506f0c66782.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We also print out the model assignment data in a collection for each cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The console output will display the following information (in total, we have
    three clusters which were set in the preceding parameters):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'We then close the program by stopping the Spark context:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We created a list of edges and vertices for a graph and then proceeded to create
    the object and set the parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'The next step was the model of training data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: The clusters were then outputted for inspection. The code near the end prints
    out the model assignment data in a collection for each cluster using Spark transformation
    operators.
  prefs: []
  type: TYPE_NORMAL
- en: At the core **PIC** (**Power Iteration Clustering**) is an eigenvalue class
    algorithm which avoids matrix decomposition by producing an Eigen Value plus an
    Eigen Vector to satisfy *Av* = λ*v.* Because PIC avoids the decomposition of the
    matrix A, it is suitable when the input matrix A (describing a graph in ...
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For a more detailed mathematical treatment of the subject (power iteration),
    see the following white paper from Carnegie Mellon University: [http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf](http://www.cs.cmu.edu/~wcohen/postscript/icml2010-pic-final.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for the constructor `PowerIterationClustering()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClustering)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for the constructor `PowerIterationClusteringModel()` can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.PowerIterationClusteringModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Latent Dirichlet Allocation (LDA) to classify documents and text into topics
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will explore the **Latent Dirichlet Allocation** (**LDA**)
    algorithm in Spark 2.0\. The LDA we use in this recipe is completely different
    from linear discrimination analysis. Both Latent Dirichlet Allocation and linear
    discrimination analysis are referred to as LDA, but they are extremely different
    techniques. In this recipe, when we use the LDA, we refer to Latent Dirichlet
    Allocation. The chapter on text analytics is also relevant to understanding the
    LDA.
  prefs: []
  type: TYPE_NORMAL
- en: LDA is often used in natural language processing which tries to classify a large
    body of the document (for example, emails from the Enron fraud case) into a discrete
    number of topics or themes so it can be understood. LDA is also a good candidate
    for selecting articles based on one's interest (for example, as you turn a page
    and spend time on a specific topic) in a given magazine article or page.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Set up the package location where the program will reside:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Import the necessary packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'We set up the necessary Spark Session to gain access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: We have a sample LDA dataset, which is located at the following relative path
    (you can use an absolute path). The sample file is provided with any Spark distribution
    and ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LDA assumes that the document is a mixture of different topics with Dirichlet
    prior distribution. The words in the document are assumed to have an affinity
    towards a specific topic which allows LDA to classify the overall document (compose
    and assign a distribution) that best matches a topic.
  prefs: []
  type: TYPE_NORMAL
- en: A topic model is a generative latent model for discovering abstract themes (topics)
    that occur in the body of documents (often too large for humans to handle). The
    models are a pre-cursor to summarize, search, and browse a large set of unlabeled
    documents and their contents. Generally speaking, we are trying to find a cluster
    of features (words, sub-images, and so on) that occur together.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following figure depicts the overall LDA scheme:'
  prefs: []
  type: TYPE_NORMAL
- en: Please be sure to refer to the white paper cited here for completeness [http://ai.stanford.edu/~ang/papers/nips01-lda.pdf](http://ai.stanford.edu/~ang/papers/nips01-lda.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a85bbb8a-37a8-4880-ac6f-e8ede49bdb67.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The steps for the LDA algorithm are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Initialize the following parameters (controls concentration and smoothing):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Alpha parameter (high alpha makes documents more similar to each other and contain
    similar topics )
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Beta parameter ( high beta means each topic is most likely to contain a mix
    of most of the words)
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Randomly initialize the topic assignment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Iterate:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each document.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each word in the document.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Resample the topic for each word.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: With respect to all other words and their current assignment (for the current
    iteration).
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Get the result.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In statistics, Dirichlet distribution Dir(alpha) is a family of continuous multivariate
    probability distributions parameterized by a vector α of positive real numbers.
    For a more in-depth treatment of LDA, see the original paper in the
  prefs: []
  type: TYPE_NORMAL
- en: Journal of Machine Learning at [http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf](http://www.jmlr.org/papers/volume3/blei03a/blei03a.pdf)
  prefs: []
  type: TYPE_NORMAL
- en: The LDA does not assign any semantics to a topic and does not care what the
    topics are called. It is only a generative model that uses the distribution of
    fine-grained items (for example, words about cats, dogs, fish, cars) to assign
    an overall topic that scores the best. It does not know, cares, or understand
    about topics called dogs or cats.
  prefs: []
  type: TYPE_NORMAL
- en: We often have to tokenize and vectorize the document via TF-IDF prior to input
    to an LDA algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following figure depicts the LDA in a nutshell:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f3315b50-1445-4633-a0d5-ffaf8256afd5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'There are two approaches to document analysis. We can simply use matrix factorization
    to decompose a large matrix of datasets to a smaller matrix (topic assignments)
    times a vector (topics themselves):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f6891f4-b0ce-476a-bda5-d52d10745a60.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**LDA**: documentation for a constructor can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[**LDAModel**: documentation for a constructor can be found at ](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDA)[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.ml.clustering.LDAModel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'See also, via Spark''s Scala API, documentation links for the following:'
  prefs: []
  type: TYPE_NORMAL
- en: DistributedLDAModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EMLDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LocalLDAModel
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OnlineLDAOptimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Streaming KMeans to classify data in near real-time
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Spark streaming is a powerful facility which lets you combine near real-time
    and batch in the same paradigm. The streaming KMeans interface lives at the intersection
    of ML clustering and Spark streaming, and takes full advantage of the core facilities
    provided by Spark streaming itself (for example, fault tolerance, exactly once
    delivery semantics, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Start a new project in IntelliJ or in an IDE of your choice. Make sure the necessary
    JAR files are included.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Import the necessary packages for streaming KMeans:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`package spark.ml.cookbook.chapter14`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the necessary packages for streaming KMeans:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: We set up the following parameters for the streaming KMeans program. The training
    directory will be the directory to send the training data file. The KMeans clustering
    model utilizes the training data to run algorithms and calculations. The `testDirectory`
    will be the test data for predictions. The `batchDuration` is a number in seconds
    for a batch run. In the following case, the program will check every 10 seconds
    to see if there is any new data files for recalculations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The cluster is set to `2`, and the data dimensions will be `3`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'With the preceding settings, the sample training data will contain data like
    the following (in the format of [*X[1], X[2], ...X[n]*], where *n* is `numDimensions`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[0.0,0.0,0.0]'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.1,0.1,0.1]'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.2,0.2,0.2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.0,9.0,9.0]'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.1,9.1,9.1]'
  prefs: []
  type: TYPE_NORMAL
- en: '[9.2,9.2,9.2]'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.1,0.0,0.0]'
  prefs: []
  type: TYPE_NORMAL
- en: '[0.2,0.1,0.1]'
  prefs: []
  type: TYPE_NORMAL
- en: '....'
  prefs: []
  type: TYPE_NORMAL
- en: 'The test data file will contain data like the following (in the format of (*y,
    [X1, X2, .. Xn]*), where *n* is `numDimensions` and `y` is an identifier):'
  prefs: []
  type: TYPE_NORMAL
- en: (7,[0.4,0.4,0.4])
  prefs: []
  type: TYPE_NORMAL
- en: (8,[0.1,0.1,0.1])
  prefs: []
  type: TYPE_NORMAL
- en: (9,[0.2,0.2,0.2])
  prefs: []
  type: TYPE_NORMAL
- en: (10,[1.1,1.0,1.0])
  prefs: []
  type: TYPE_NORMAL
- en: (11,[9.2,9.1,9.2])
  prefs: []
  type: TYPE_NORMAL
- en: (12,[9.3,9.2,9.3])
  prefs: []
  type: TYPE_NORMAL
- en: 'We set up the necessary Spark context to gain access to the cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'Define the streaming context and micro-batch window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The following code will create data by parsing the data file in the preceding
    two directories into `trainingData` and `testData RDDs`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'We create the `StreamingKMeans` model and set the parameters:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The program will train the model using the training dataset and predict using
    the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'We start the streaming context, and the program will run the batch every 10
    seconds to see if a new dataset is available for training and if there is any
    new test dataset for prediction. The program will exit if a termination signal
    is received (exit the batch running):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'We copy the `testKStreaming1.txt` data file into the preceding `testDir` set
    and see the following printed out in the console logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/d39d25e2-be84-46b3-917a-52c3907e3122.png)'
  prefs: []
  type: TYPE_IMG
- en: For a Windows machine, we copied the `testKStreaming1.txt` file into the directory: `C:\spark-2.0.0-bin-hadoop2.7\data\sparkml2\chapter8\testDir\`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can also check the SparkUI for more information: `http://localhost:4040/`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The job panel will display streaming jobs, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ef1d102-cb06-48cd-b26c-9f71ac6bbb9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As shown in the following figure, the streaming panel will show the preceding
    Streaming KMeans matrix as the matrix displayed, the batch job running every 10
    seconds in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b14bd318-6028-4597-bf45-ca1e5a3dcd9d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'You can get more details on the streaming batch by clicking on any of the batches,
    as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1a690bac-440b-4891-a843-1270709933a4.png)'
  prefs: []
  type: TYPE_IMG
- en: How it works...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In certain situations, we cannot use batch methods to load and capture the events
    and then react to them. We can use creative methods of capturing events in the
    memory or a landing DB and then rapidly marshal that over to another system for
    processing, but most of these systems fail to act as streaming systems and often
    are very expensive to build.
  prefs: []
  type: TYPE_NORMAL
- en: Spark provides a near real-time (also referred to as subjective real time) that
    can receive incoming sources, such as Twitter feeds, signals, and so, on via connectors
    (for example, a Kafka connector) and then process and present them as an RDD interface.
  prefs: []
  type: TYPE_NORMAL
- en: 'These are the elements needed to build and construct streaming KMeans in Spark:'
  prefs: []
  type: TYPE_NORMAL
- en: Use the streaming context as opposed to the ...
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There's more...
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Streaming KMeans are special cases of KMeans implementation in which the data
    can arrive at a near real-time and be classified into a cluster (hard classification)
    as needed. For a reference to Voronoi diagrams, see the following URL: [https://en.wikipedia.org/wiki/Voronoi_diagram](https://en.wikipedia.org/wiki/Voronoi_diagram)
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, there are other algorithms besides streaming KMeans in the Spark
    Machine Library, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e339c28-5370-47c9-bd81-086eb04f7ca0.png)'
  prefs: []
  type: TYPE_IMG
- en: See also
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Documentation for Streaming KMeans can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeans)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for Streaming KMeans Model can be found at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.stat.test.StreamingTest)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Documentation for Streaming Test--very useful for data generation--can be found
    at [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.clustering.StreamingKMeansModel)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
