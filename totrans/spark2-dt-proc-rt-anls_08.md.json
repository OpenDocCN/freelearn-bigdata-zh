["```scala\n yarn logs -applicationId <application ID> [OPTIONS]\n```", "```scala\n yarn logs -applicationId application_561453090098_0005 \n yarn logs -applicationId application_561453090070_0005 userid\n```", "```scala\nsetAppName() // App name setMaster() // Master URL setSparkHome() // Set the location where Spark is installed on worker nodes. setExecutorEnv() // Set single or multiple environment variables to be used when launching executors. setJars() // Set JAR files to distribute to the cluster. setAll() // Set multiple parameters together.\n```", "```scala\n02/05/17 12:44:45 ERROR AppClient$ClientActor: All masters are unresponsive! Giving up. \n02/05/17 12:45:31 ERROR SparkDeploySchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up. \n02/05/17 12:45:35 ERROR TaskSchedulerImpl: Exiting due to error from cluster scheduler: Spark cluster looks down\n```", "```scala\n $ bin/spark-shell --master spark://master-ip:7077\n```", "```scala\nval conf = new SparkConf()\n               .setMaster(“local[*]”)\n               .setAppName(“MyApp”)\nconf.registerKryoClasses(Array(classOf[MyOwnClass1], classOf[MyOwnClass2]))\nval sc = new SparkContext(conf)\n```", "```scala\nclass MyMapper(n: Int) { // without any serialization\n  @transient lazy val log = org.apache.log4j.LogManager.getLogger(\"myLogger\")\n  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] = rdd.map { i =>\n    log.warn(\"mapping: \" + i)\n    (i + n).toString\n  }\n}\n```", "```scala\nconf.registerKryoClasses(Array(classOf[MyMapper])) // register the class with Kyro\nconf.set(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") // set Kayro serialization\n```", "```scala\npackage com.chapter14.Serilazition\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD\nclass MyMapper(n: Int) { // without any serilization\n  @transient lazy val log = org.apache.log4j.LogManager.getLogger\n                                (\"myLogger\")\n  def MyMapperDosomething(rdd: RDD[Int]): RDD[String] = rdd.map { i =>\n    log.warn(\"mapping: \" + i)\n    (i + n).toString\n  }\n}\n//Companion object\nobject MyMapper {\n  def apply(n: Int): MyMapper = new MyMapper(n)\n}\n//Main object\nobject KyroRegistrationDemo {\n  def main(args: Array[String]) {\n    val log = LogManager.getRootLogger\n    log.setLevel(Level.WARN)\n    val conf = new SparkConf()\n      .setAppName(\"My App\")\n      .setMaster(\"local[*]\")\n    conf.registerKryoClasses(Array(classOf[MyMapper2]))\n     // register the class with Kyro\n    conf.set(\"spark.serializer\", \"org.apache.spark.serializer\n             .KryoSerializer\") // set Kayro serilazation\n    val sc = new SparkContext(conf)\n    log.warn(\"Started\")\n    val data = sc.parallelize(1 to 100000)\n    val mapper = MyMapper(1)\n    val other = mapper.MyMapperDosomething(data)\n    other.collect()\n    log.warn(\"Finished\")\n  }\n}\n```", "```scala\n17/04/29 15:33:43 WARN root: Started \n.\n.\n17/04/29 15:31:51 WARN myLogger: mapping: 1 \n17/04/29 15:31:51 WARN myLogger: mapping: 49992\n17/04/29 15:31:51 WARN myLogger: mapping: 49999\n17/04/29 15:31:51 WARN myLogger: mapping: 50000 \n.\n.                                                                                \n17/04/29 15:31:51 WARN root: Finished\n```"]