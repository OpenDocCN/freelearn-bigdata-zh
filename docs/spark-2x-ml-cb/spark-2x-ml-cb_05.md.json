["```scala\npackage spark.ml.cookbook.chapter5\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport scala.math._\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[4]\")\n .appName(\"myRegress01_20\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval x = Array(1.0,5.0,8.0,10.0,15.0,21.0,27.0,30.0,38.0,45.0,50.0,64.0)\nval y = Array(5.0,1.0,4.0,11.0,25.0,18.0,33.0,20.0,30.0,43.0,55.0,57.0)\n```", "```scala\nval xRDD = sc.parallelize(x)\nval yRDD = sc.parallelize(y)\n```", "```scala\nval zipedRDD = xRDD.zip(yRDD)\n```", "```scala\nval xSum = zipedRDD.map(_._1).sum()\nval ySum = zipedRDD.map(_._2).sum()\nval xySum= zipedRDD.map(c => c._1 * c._2).sum()\n```", "```scala\nval n= zipedRDD.count() \nval xMean = zipedRDD.map(_._1).mean()\nval yMean = zipedRDD.map(_._2).mean()\nval xyMean = zipedRDD.map(c => c._1 * c._2).mean()\n```", "```scala\nval xSquaredMean = zipedRDD.map(_._1).map(x => x * x).mean()\nval ySquaredMean = zipedRDD.map(_._2).map(y => y * y).mean()\n```", "```scala\nprintln(\"xMean yMean xyMean\", xMean, yMean, xyMean) \nxMean yMean xyMean ,26.16,25.16,989.08 \n```", "```scala\nval numerator = xMean * yMean  - xyMean\nval denominator = xMean * xMean - xSquaredMean\n```", "```scala\nval slope = numerator / denominator\nprintln(\"slope %f5\".format(slope))\n\nslope 0.9153145 \n```", "```scala\nval b_intercept = yMean - (slope*xMean)\nprintln(\"Intercept\", b_intercept) \n\nIntercept,1.21\n```", "```scala\nY = 1.21 + .9153145 * X\n```", "```scala\n(Y, X)\n(5.0,    1.0) \n(8.0,    4.0) \n(10.0,   11.0) \n(15.0,   25.0) \n(21.0,   18.0) \n(27.0,   33.0) \n(30.0,   20.0) \n(38.0,   30.0) \n(45.0,   43.0) \n(50.0,   55.0) \n(64.0,   57.0) \n```", "```scala\n../data/sparkml2/chapter5/housing8.csv\n```", "```scala\npackage spark.ml.cookbook.chapter5.\n```", "```scala\nimport org.apache.spark.ml.feature.LabeledPoint\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.regression.GeneralizedLinearRegression\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"GLR\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval data = spark.read.textFile( \"../data/sparkml2/ /chapter5/housing8.csv\" ).as[ String ]\n```", "```scala\nval regressionData = data.map { line =>\nval columns = line.split(',')\nLabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,\ncolumns(5).toDouble,columns(6).toDouble, columns(7).toDouble))\n}\n```", "```scala\nregressionData.show(false)\n```", "```scala\nval glr = new GeneralizedLinearRegression()\n.setMaxIter(1000)\n.setRegParam(0.03) //the value ranges from 0.0 to 1.0\\. Experimentation required to identify the right value.\n.setFamily(\"gaussian\")\n.setLink( \"identity\" )\n```", "```scala\nval glrModel = glr.fit(regressionData)\n```", "```scala\nval summary = glrModel.summary\n```", "```scala\nval summary = glrModel.summary\nsummary.residuals().show()\nprintln(\"Residual Degree Of Freedom: \" + summary.residualDegreeOfFreedom)\nprintln(\"Residual Degree Of Freedom Null: \" + summary.residualDegreeOfFreedomNull)\nprintln(\"AIC: \" + summary.aic)\nprintln(\"Dispersion: \" + summary.dispersion)\nprintln(\"Null Deviance: \" + summary.nullDeviance)\nprintln(\"Deviance: \" +summary.deviance)\nprintln(\"p-values: \" + summary.pValues.mkString(\",\"))\nprintln(\"t-values: \" + summary.tValues.mkString(\",\"))\nprintln(\"Coefficient Standard Error: \" + summary.coefficientStandardErrors.mkString(\",\"))\n}\n\n```", "```scala\nspark.stop()\n```", "```scala\n../data/sparkml2/chapter5/housing8.csv\n```", "```scala\npackage spark.ml.cookbook.chapter5.\n```", "```scala\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.feature.LabeledPoint\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myRegress02\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval data = spark.read.text(\n  \"../data/sparkml2/chapter5/housing8.csv\"\n).as[\n  String\n]\n\n```", "```scala\nval RegressionDataSet = data.map { line =>\nval columns = line.split(',')\nLabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,\ncolumns(5).toDouble,columns(6).toDouble, columns(7).toDouble\n))\n}\n```", "```scala\nRegressionDataSet.show(false)\n```", "```scala\nval numIterations = 10\nval lr = new LinearRegression()\n.setMaxIter(numIterations)\n.setSolver(\"l-bfgs\")\n```", "```scala\nval myModel = lr.fit(RegressionDataSet)\n```", "```scala\nval summary = myModel.summary\n```", "```scala\nprintln ( \"training Mean Squared Error = \" + summary. meanSquaredError )\nprintln(\"training Root Mean Squared Error = \" + summary.rootMeanSquaredError) }\ntraining Mean Squared Error = 13.608987362865541\ntraining Root Mean Squared Error = 3.689036102136375\n```", "```scala\nspark.stop()\n```", "```scala\n ../data/sparkml2/chapter5/housing8.csv\n```", "```scala\npackage spark.ml.cookbook.chapter5.\n```", "```scala\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.ml.feature.LabeledPoint\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myRegress03\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval data = spark.read.text( \"../data/sparkml2/chapter5/housing8.csv\" ).as[ String ]\n```", "```scala\nval RegressionDataSet = data.map { line =>\nval columns = line.split(',')\nLabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,\ncolumns(5).toDouble,columns(6).toDouble, columns(7).toDouble\n))\n}\n```", "```scala\nval lr = new LinearRegression()\n.setMaxIter(1000)\n.setElasticNetParam(0.0)\n.setRegParam(0.01)\n.setSolver( \"auto\" )\n```", "```scala\nval myModel = lr.fit(RegressionDataSet)\n```", "```scala\nval summary = myModel.summary\n```", "```scala\nprintln ( \"training Mean Squared Error = \" + summary. meanSquaredError )\nprintln(\"training Root Mean Squared Error = \" + summary.rootMeanSquaredError) }\ntraining Mean Squared Error = 13.609079490110766\ntraining Root Mean Squared Error = 3.6890485887435482\n```", "```scala\nspark.stop()\n```", "```scala\nval lr = new LinearRegression()\n.setMaxIter(1000)\n.setElasticNetParam(0.0)\n.setRegParam(0.01)\n.setSolver( \"auto\" )\n```", "```scala\n ../data/sparkml2/chapter5/housing8.csv\n```", "```scala\npackage spark.ml.cookbook.chapter5.\n```", "```scala\nimport org.apache.spark.ml.feature.LabeledPoint\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myRegress04\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval data = spark.read.text( \"../data/sparkml2/chapter5/housing8.csv\" ).as[ String ]\n```", "```scala\nval RegressionDataSet = data.map { line =>\nval columns = line.split(',')\nLabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,\ncolumns(5).toDouble,columns(6).toDouble, columns(7).toDouble\n))\n}\n```", "```scala\nval lr = new LinearRegression()\n.setMaxIter(1000)\n.setElasticNetParam(1.0)\n.setRegParam(0.01)\n.setSolver( \"auto\" )\n```", "```scala\nval myModel = lr.fit(RegressionDataSet)\n```", "```scala\nval summary = myModel.summary\n```", "```scala\nprintln ( \"training Mean Squared Error = \" + summary. meanSquaredError )\nprintln(\"training Root Mean Squared Error = \" + summary.rootMeanSquaredError) }\ntraining Mean Squared Error = 13.61187856748311\ntraining Root Mean Squared Error = 3.6894279458315906\n```", "```scala\nspark.stop()\n```", "```scala\nval lr = new LinearRegression()\n.setMaxIter(1000)\n.setElasticNetParam(1.0)\n.setRegParam(0.01)\n.setSolver( \"auto\" )\n```", "```scala\npackage spark.ml.cookbook.chapter5\n```", "```scala\nimport org.apache.spark.sql.SparkSession\n import org.apache.spark.ml.regression.IsotonicRegression\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[4]\")\n .appName(\"myIsoTonicRegress\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval data = spark.read.format(\"libsvm\")\n .load(\"../data/sparkml2/chapter5/iris.scale.txt\")\n data.printSchema()\n data.show(false)\n```", "```scala\nval Array(training, test) = data.randomSplit(Array(0.7, 0.3), seed = System.currentTimeMillis())\n```", "```scala\nval itr = new IsotonicRegression()\n\n val itrModel = itr.fit(training)\n```", "```scala\nprintln(s\"Boundaries in increasing order: ${itrModel.boundaries}\")\n println(s\"Predictions associated with the boundaries: ${itrModel.predictions}\")\n```", "```scala\nBoundaries in increasing order: [-1.0,-0.666667,-0.666667,-0.5,-0.5,-0.388889,-0.388889,-0.333333,-0.333333,-0.222222,-0.222222,-0.166667,-0.166667,0.111111,0.111111,0.333333,0.333333,0.5,0.555555,1.0]\nPredictions associated with the boundaries: [1.0,1.0,1.1176470588235294,1.1176470588235294,1.1666666666666663,1.1666666666666663,1.3333333333333333,1.3333333333333333,1.9,1.9,2.0,2.0,2.3571428571428577,2.3571428571428577,2.5333333333333314,2.5333333333333314,2.7777777777777786,2.7777777777777786,3.0,3.0]\n```", "```scala\nitrModel.transform(test).show()\n```", "```scala\nspark.stop()\n```", "```scala\ndef setFeaturesCol(value: String): IsotonicRegression.this.type\n```", "```scala\npackage spark.ml.cookbook.chapter5\n```", "```scala\nimport org.apache.spark.ml.classification\n.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.evaluation.\nMulticlassClassificationEvaluator\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{ Level, Logger}\n```", "```scala\n Logger.getLogger(\"org\").setLevel(Level.ERROR)\n Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"MLP\")\n .getOrCreate()\n```", "```scala\nval data = spark.read.format( \"libsvm\" )\n.load(\"../data/sparkml2/chapter5/iris.scale.txt\")\n```", "```scala\ndata.show(false)\n```", "```scala\nval splitData = data.randomSplit(Array( 0.8 , 0.2 ), seed = System.currentTimeMillis())\n```", "```scala\nval train = splitData(0)\n val test = splitData(1)\n```", "```scala\nval layers = Array[Int](4, 5, 4)\nval mlp = new MultilayerPerceptronClassifier()\n.setLayers(layers)\n.setBlockSize(110)\n.setSeed(System.currentTimeMillis())\n.setMaxIter(145)\n```", "```scala\nsetDefault(maxIter->100, tol -> 1e-6, blockSize ->128, solver -> MultilayerPerceptronClassifier.LBFGS, stepSize ->0.03)\n```", "```scala\nval mlpModel = mlp.fit(train)\n```", "```scala\nval result = mlpModel.transform(test)\nresult.show(false)\n```", "```scala\nval predictions = result.select(\"prediction\", \"label\")\nval eval = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\")\nprintln(\"Accuracy: \" + eval.evaluate(predictions))\nAccuracy: 0.967741935483871\n```", "```scala\nspark.stop()\n```", "```scala\nval layers = Array[Int](4, 5, 4)\nval mlp = new MultilayerPerceptronClassifier()\n.setLayers(layers)\n.setBlockSize(110)\n.setSeed(System.currentTimeMillis())\n.setMaxIter(145)\n```", "```scala\npackage spark.ml.cookbook.chapter5\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.classification\n.{LogisticRegression, OneVsRest}\nimport org.apache.spark.ml.evaluation\n.MulticlassClassificationEvaluator\nimport org.apache.log4j.{ Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"One-vs-Rest\")\n .getOrCreate()\n```", "```scala\n val data = spark.read.format(\"libsvm\")\n .load(\"../data/sparkml2/chapter5/iris.scale.txt\")\n```", "```scala\ndata.show(false)\n```", "```scala\nval Array (train, test) = data.randomSplit(Array( 0.8 , 0.2 ), seed = System.currentTimeMillis())\n```", "```scala\nval lrc = new LogisticRegression()\n.setMaxIter(15)\n.setTol(1E-3)\n.setFitIntercept(true)\n```", "```scala\nval ovr = new OneVsRest().setClassifier(lrc)\n```", "```scala\nval ovrModel = ovr.fit(train)\n```", "```scala\nval eval = new MulticlassClassificationEvaluator()\n.setMetricName(\"accuracy\")\nval accuracy = eval.evaluate(predictions)\nprintln(\"Accuracy: \" + eval.evaluate(predictions))\nAccuracy: 0.9583333333333334\n```", "```scala\nspark.stop()\n```", "```scala\ndata.randomSplit(Array( 0.8 , 0.2 ), seed = System.currentTimeMillis())\n```", "```scala\nLogisticRegression()\n.setMaxIter(15)\n.setTol(1E-3)\n.setFitIntercept(true)\n```", "```scala\nval ovr = new OneVsRest().setClassifier(lrc)\n```", "```scala\npackage spark.ml.cookbook.chapter5\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\n import org.apache.spark.ml.linalg.Vectors\n import org.apache.spark.ml.regression.AFTSurvivalRegression\n import org.apache.spark.sql.SparkSession\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[4]\")\n .appName(\"myAFTSurvivalRegression\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval file = spark.sparkContext.textFile(\"../data/sparkml2/chapter5/hmohiv.csv\")\n val headerAndData = file.map(line => line.split(\",\").map(_.trim))\n val header = headerAndData.first\n val rawData = headerAndData.filter(_(0) != header(0))\n```", "```scala\nval df = spark.createDataFrame(rawData\n .map { line =>\n val id = line(0).toDouble\n val time =line(1).toDouble\n val age = line(2).toDouble\n val censor = line(4).toDouble\n (id, censor,Vectors.dense(time,age))\n }).toDF(\"label\", \"censor\", \"features\")\n```", "```scala\ndf.show()\n```", "```scala\n @Since(\"1.6.0\")\ndef getQuantileProbabilities: Array[Double] = $(quantileProbabilities)\nsetDefault(quantileProbabilities -> Array(0.01, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99)) \n```", "```scala\nval aft = new AFTSurvivalRegression()\n .setQuantileProbabilities(Array(0.3, 0.6))\n .setQuantilesCol(\"quantiles\")\n```", "```scala\nval aftmodel = aft.fit(df)\n```", "```scala\nprintln(s\"Coefficients: ${aftmodel.coefficients} \")\n println(s\"Intercept: ${aftmodel.intercept}\" )\n println(s\"Scale: ${aftmodel.scale}\")\n```", "```scala\nCoefficients: [6.601321816135838E-4,-0.02053601452465816]\nIntercept: 4.887746420937845\nScale: 0.572288831706005\n```", "```scala\naftmodel.transform(df).show(false)\n```", "```scala\nspark.stop()\n```"]