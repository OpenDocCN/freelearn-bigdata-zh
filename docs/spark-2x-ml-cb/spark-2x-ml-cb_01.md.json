["```scala\nobject HelloWorld extends App { \n   println(\"Hello World!\") \n } \n```", "```scala\npackage spark.ml.cookbook.chapter1 \n```", "```scala\nimport org.apache.spark.sql.SparkSession \nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n.builder \n.master(\"local[*]\")\n .appName(\"myFirstSpark20\") \n.config(\"spark.sql.warehouse.dir\", \".\") \n.getOrCreate() \n```", "```scala\nval x = Array(1.0,5.0,8.0,10.0,15.0,21.0,27.0,30.0,38.0,45.0,50.0,64.0) \nval y = Array(5.0,1.0,4.0,11.0,25.0,18.0,33.0,20.0,30.0,43.0,55.0,57.0) \n```", "```scala\nval xRDD = spark.sparkContext.parallelize(x) \nval yRDD = spark.sparkContext.parallelize(y) \n```", "```scala\nval zipedRDD = xRDD.zip(yRDD) \nzipedRDD.collect().foreach(println) \n```", "```scala\nval xSum = zipedRDD.map(_._1).sum() \nval ySum = zipedRDD.map(_._2).sum() \nval xySum= zipedRDD.map(c => c._1 * c._2).sum() \nval n= zipedRDD.count() \n```", "```scala\nprintln(\"RDD X Sum: \" +xSum) \nprintln(\"RDD Y Sum: \" +ySum) \nprintln(\"RDD X*Y Sum: \"+xySum) \nprintln(\"Total count: \"+n) \n```", "```scala\nspark.stop() \n```", "```scala\nInformation: November 18, 2016, 11:46 AM - Compilation completed successfully with 1 warning in 55s 648ms\n```", "```scala\nProcess finished with exit code 0\n```", "```scala\n  package spark.ml.cookbook.chapter1\n```", "```scala\nimport java.awt.Color \nimport org.apache.log4j.{Level, Logger} \nimport org.apache.spark.sql.SparkSession \nimport org.jfree.chart.plot.{PlotOrientation, XYPlot} \nimport org.jfree.chart.{ChartFactory, ChartFrame, JFreeChart} \nimport org.jfree.data.xy.{XYSeries, XYSeriesCollection} \nimport scala.util.Random \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myChart\") \n  .config(\"spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval data = spark.sparkContext.parallelize(Random.shuffle(1 to 15).zipWithIndex) \n```", "```scala\ndata.foreach(println) \n```", "```scala\nval xy = new XYSeries(\"\") \ndata.collect().foreach{ case (y: Int, x: Int) => xy.add(x,y) } \nval dataset = new XYSeriesCollection(xy) \n```", "```scala\nval chart = ChartFactory.createXYLineChart( \n  \"MyChart\",  // chart title \n  \"x\",               // x axis label \n  \"y\",                   // y axis label \n  dataset,                   // data \n  PlotOrientation.VERTICAL, \n  false,                    // include legend \n  true,                     // tooltips \n  false                     // urls \n)\n```", "```scala\nval plot = chart.getXYPlot() \n```", "```scala\nconfigurePlot(plot) \n```", "```scala\ndef configurePlot(plot: XYPlot): Unit = { \n  plot.setBackgroundPaint(Color.WHITE) \n  plot.setDomainGridlinePaint(Color.BLACK) \n  plot.setRangeGridlinePaint(Color.BLACK) \n  plot.setOutlineVisible(false) \n} \n```", "```scala\nshow(chart) \n```", "```scala\ndef show(chart: JFreeChart) { \n  val frame = new ChartFrame(\"plot\", chart) \n  frame.pack() \n  frame.setVisible(true) \n}\n```", "```scala\nspark.stop() \n```"]