["```scala\norg.apache.spark.mllib.tree\norg.apache.spark.mllib.tree.configuration\norg.apache.spark.mllib.tree.impurity\norg.apache.spark.mllib.tree.model\n```", "```scala\n   org.apache.spark.mllib.evaluation.MulticlassMetrics \n```", "```scala\n   org.apache.spark.mllib.evaluation.RegressionMetrics \n```", "```scala\n1000025,5,1,1,1,2,1,3,1,1,2\n1002945,5,4,4,5,7,10,3,2,1,2\n1015425,3,1,1,1,2,2,3,1,1,2\n1016277,6,8,8,1,3,4,3,7,1,2\n1017023,4,1,1,3,2,1,3,1,1,2\n1017122,8,10,10,8,7,10,9,7,1,4\n...\n```", "```scala\npackage spark.ml.cookbook.chapter10\n```", "```scala\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger} \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n\n val spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"MyDecisionTreeClassification\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval rawData = spark.sparkContext.textFile(\"../data/sparkml2/chapter10/breast-cancer-wisconsin.data\")\n```", "```scala\nval data = rawData.map(_.trim)\n .filter(text => !(text.isEmpty || text.startsWith(\"#\") || text.indexOf(\"?\") > -1))\n .map { line =>\n val values = line.split(',').map(_.toDouble)\n val slicedValues = values.slice(1, values.size)\n val featureVector = Vectors.dense(slicedValues.init)\n val label = values.last / 2 -1\n LabeledPoint(label, featureVector)\n }\n```", "```scala\nval slicedValues = values.slice(1, values.size)\n```", "```scala\nval label = values.last / 2 -1\n```", "```scala\n    Raw data: 1000025,5,1,1,1,2,1,3,1,1,2\n    Processed Data: 5,1,1,1,2,1,3,1,1,0\n    Labeled Points: (0.0, [5.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0])\n```", "```scala\nprintln(rawData.count())\nprintln(data.count())\n```", "```scala\n699\n683\n```", "```scala\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n```", "```scala\ndef getMetrics(model: DecisionTreeModel, data: RDD[LabeledPoint]): MulticlassMetrics = {\n val predictionsAndLabels = data.map(example =>\n (model.predict(example.features), example.label)\n )\n new MulticlassMetrics(predictionsAndLabels)\n }\n```", "```scala\ndef evaluate(\n trainingData: RDD[LabeledPoint],\n testData: RDD[LabeledPoint],\n numClasses: Int,\n categoricalFeaturesInfo: Map[Int,Int],\n\n impurity: String,\n maxDepth: Int,\n maxBins:Int\n ) :Unit = {\n\n val model = DecisionTree.*trainClassifier*(trainingData, numClasses,\n categoricalFeaturesInfo,\n impurity, maxDepth, maxBins)\n val metrics = getMetrics(model, testData)\n println(\"Using Impurity :\"+ impurity)\n println(\"Confusion Matrix :\")\n println(metrics.confusionMatrix)\n println(\"Decision Tree Accuracy: \"+metrics.*precision*)\n println(\"Decision Tree Error: \"+ (1-metrics.*precision*))\n\n }\n```", "```scala\nval numClasses = 2\n val categoricalFeaturesInfo = *Map*[Int, Int]()\n val maxDepth = 5\n val maxBins = 32\n```", "```scala\nevaluate(trainingData, testData, numClasses, categoricalFeaturesInfo,\n\"gini\", maxDepth, maxBins)\n```", "```scala\nUsing Impurity :gini\nConfusion Matrix :\n115.0 5.0\n0 88.0\nDecision Tree Accuracy: 0.9620853080568721\nDecision Tree Error: 0.03791469194312791\nTo interpret the above Confusion metrics, Accuracy is equal to (115+ 88)/ 211 all test cases, and error is equal to 1 -accuracy\n```", "```scala\nevaluate(trainingData, testData, numClasses, categoricalFeaturesInfo,\n\"entropy\", maxDepth, maxBins)\n```", "```scala\nUsing Impurity:entropy\nConfusion Matrix:\n116.0 4.0\n9.0 82.0\nDecision Tree Accuracy: 0.9383886255924171\nDecision Tree Error: 0.06161137440758291\nTo interpret the preceding confusion metrics, accuracy is equal to (116+ 82)/ 211 for all test cases, and error is equal to 1 - accuracy\n```", "```scala\nspark.stop()\n```", "```scala\n     Raw data: 1000025,5,1,1,1,2,1,3,1,1,2\n     Processed Data: 5,1,1,1,2,1,3,1,1,0\n     Labeled Points: (0.0, [5.0,1.0,1.0,1.0,2.0,1.0,3.0,1.0,1.0])\n```", "```scala\npackage spark.ml.cookbook.chapter10\n```", "```scala\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.mllib.tree.model.DecisionTreeModel\nimport org.apache.spark.rdd.RDD\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.*ERROR*)\n\n val spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"MyDecisionTreeRegression\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval rawData = spark.sparkContext.textFile(\"../data/sparkml2/chapter10/breast-cancer-wisconsin.data\")\n```", "```scala\nval data = rawData.map(_.trim)\n .filter(text => !(text.isEmpty || text.startsWith(\"#\") || text.indexOf(\"?\") > -1))\n .map { line =>\n val values = line.split(',').map(_.toDouble)\n val slicedValues = values.slice(1, values.size)\n val featureVector = Vectors.dense(slicedValues.init)\n val label = values.last / 2 -1\n LabeledPoint(label, featureVector)\n }\n```", "```scala\nprintln(rawData.count())\nprintln(data.count())\n```", "```scala\n699\n683\n```", "```scala\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n```", "```scala\ndef getMetrics(model: DecisionTreeModel, data: RDD[LabeledPoint]): RegressionMetrics = {\n val predictionsAndLabels = data.map(example =>\n (model.predict(example.features), example.label)\n )\n new RegressionMetrics(predictionsAndLabels)\n }\n```", "```scala\nval categoricalFeaturesInfo = Map[Int, Int]()\nval impurity = \"variance\" val maxDepth = 5\nval maxBins = 32\n```", "```scala\nval model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity, maxDepth, maxBins)\nval metrics = getMetrics(model, testData)\nprintln(\"Test Mean Squared Error = \" + metrics.meanSquaredError)\nprintln(\"My regression tree model:\\n\" + model.toDebugString)\n\n```", "```scala\nTest Mean Squared Error = 0.037363769271664016\nMy regression tree model:\nDecisionTreeModel regressor of depth 5 with 37 nodes\nIf (feature 1 <= 3.0)\n   If (feature 5 <= 3.0)\n    If (feature 0 <= 6.0)\n     If (feature 7 <= 3.0)\n      Predict: 0.0\n     Else (feature 7 > 3.0)\n      If (feature 0 <= 4.0)\n       Predict: 0.0\n      Else (feature 0 > 4.0)\n       Predict: 1.0\n    Else (feature 0 > 6.0)\n     If (feature 2 <= 2.0)\n      Predict: 0.0\n     Else (feature 2 > 2.0)\n      If (feature 4 <= 2.0)\n       Predict: 0.0\n      Else (feature 4 > 2.0)\n       Predict: 1.0\n   Else (feature 5 > 3.0)\n    If (feature 1 <= 1.0)\n     If (feature 0 <= 5.0)\n      Predict: 0.0\n     Else (feature 0 > 5.0)\n      Predict: 1.0\n    Else (feature 1 > 1.0)\n     If (feature 0 <= 6.0)\n      If (feature 7 <= 4.0)\n       Predict: 0.875\n      Else (feature 7 > 4.0)\n       Predict: 0.3333333333333333\n     Else (feature 0 > 6.0)\n      Predict: 1.0\n  Else (feature 1 > 3.0)\n   If (feature 1 <= 4.0)\n    If (feature 4 <= 6.0)\n     If (feature 5 <= 7.0)\n      If (feature 0 <= 8.0)\n       Predict: 0.3333333333333333\n      Else (feature 0 > 8.0)\n       Predict: 1.0\n     Else (feature 5 > 7.0)\n      Predict: 1.0\n    Else (feature 4 > 6.0)\n     Predict: 0.0\n   Else (feature 1 > 4.0)\n    If (feature 3 <= 1.0)\n     If (feature 0 <= 6.0)\n      If (feature 0 <= 5.0)\n       Predict: 1.0\n      Else (feature 0 > 5.0)\n       Predict: 0.0\n     Else (feature 0 > 6.0)\n      Predict: 1.0\n    Else (feature 3 > 1.0)\n     Predict: 1.0\n```", "```scala\nspark.stop()\n```", "```scala\ndef getMetrics(model: DecisionTreeModel, data: RDD[LabeledPoint]): RegressionMetrics = {\n val predictionsAndLabels = data.map(example =>\n (model.predict(example.features), example.label)\n )\n new RegressionMetrics(predictionsAndLabels)\n }\n```", "```scala\nIf (feature 0 <= 4.0)\n       Predict: 0.0\n      Else (feature 0 > 4.0)\n       Predict: 1.0\n    Else (feature 0 > 6.0)\n     If (feature 2 <= 2.0)\n      Predict: 0.0\n     Else (feature 2 > 2.0)\n      If (feature 4 <= 2.0)\n........\n........\n.......\n```", "```scala\npackage spark.ml.cookbook.chapter10\n```", "```scala\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.tree.RandomForest\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.*ERROR*)\n\n val spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"MyRandomForestClassification\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval rawData = spark.sparkContext.textFile(\"../data/sparkml2/chapter10/breast-cancer-wisconsin.data\")\n```", "```scala\nval data = rawData.map(_.trim)\n .filter(text => !(text.isEmpty || text.startsWith(\"#\") || text.indexOf(\"?\") > -1))\n .map { line =>\n val values = line.split(',').map(_.toDouble)\n val slicedValues = values.slice(1, values.size)\n val featureVector = Vectors.*dense*(slicedValues.init)\n val label = values.last / 2 -1\n LabeledPoint(label, featureVector)\n }\n```", "```scala\nprintln(\"Training Data count:\"+trainingData.count())\nprintln(\"Test Data Count:\"+testData.count())\n```", "```scala\nTraining Data count: 501 \nTest Data Count: 182\n```", "```scala\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n```", "```scala\ndef getMetrics(model: RandomForestModel, data: RDD[LabeledPoint]): MulticlassMetrics = {\n val predictionsAndLabels = data.map(example =>\n (model.predict(example.features), example.label)\n )\n new MulticlassMetrics(predictionsAndLabels)\n }\n```", "```scala\ndef evaluate(\n trainingData: RDD[LabeledPoint],\n testData: RDD[LabeledPoint],\n numClasses: Int,\n categoricalFeaturesInfo: Map[Int,Int],\n numTrees: Int,\n featureSubsetStrategy: String,\n impurity: String,\n maxDepth: Int,\n maxBins:Int\n ) :Unit = {\nval model = RandomForest.*trainClassifier*(trainingData, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy,impurity, maxDepth, maxBins)\nval metrics = *getMetrics*(model, testData)\nprintln(\"Using Impurity :\"+ impurity)\nprintln(\"Confusion Matrix :\")\nprintln(metrics.confusionMatrix)\nprintln(\"Model Accuracy: \"+metrics.*precision*)\nprintln(\"Model Error: \"+ (1-metrics.*precision*))\n }\n\n```", "```scala\nval numClasses = 2\n val categoricalFeaturesInfo = *Map*[Int, Int]()\n val numTrees = 3 *// Use more in practice.* val featureSubsetStrategy = \"auto\" *// Let the algorithm choose.\n\n* val maxDepth = 4\n val maxBins = 32\n```", "```scala\nevaluate(trainingData, testData, numClasses,categoricalFeaturesInfo,numTrees,\nfeatureSubsetStrategy, \"gini\", maxDepth, maxBins)\n```", "```scala\nUsing Impurity :gini\nConfusion Matrix :\n118.0 1.0\n4.0 59.0\nModel Accuracy: 0.9725274725274725\nModel Error: 0.027472527472527486\nTo interpret the above Confusion metrics, Accuracy is equal to (118+ 59)/ 182 all test cases, and error is equal to 1 -accuracy\n```", "```scala\nevaluate(trainingData, testData, numClasses, categoricalFeaturesInfo,\n \"entropy\", maxDepth, maxBins)\n```", "```scala\nUsing Impurity :entropy\nConfusion Matrix :\n115.0  4.0   \n0.0    63.0\nModel Accuracy: 0.978021978021978\nModel Error: 0.02197802197802201             \nTo interpret the above Confusion metrics, Accuracy is equal to (115+ 63)/ 182 all test cases, and error is equal to 1 -accuracy\n```", "```scala\nspark.stop()\n```", "```scala\n val numClasses = 2\n val categoricalFeaturesInfo = *Map*[Int, Int]()\n val numTrees = 3 // Use more in practice. val featureSubsetStrategy = \"auto\" // Let the algorithm choose. val maxDepth = 4\n val maxBins = 32\n```", "```scala\nConfusion Matrix :\n118.0 1.0\n4.0 59.0\nModel Accuracy: 0.9725274725274725\nModel Error: 0.027472527472527486\n```", "```scala\nval impurity = \"variance\" // USE variance for regression\n```", "```scala\npackage spark.ml.cookbook.chapter10\n```", "```scala\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.tree.RandomForest\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.*ERROR*)\n\nval spark = SparkSession\n.builder.master(\"local[*]\")\n.appName(\"MyRandomForestRegression\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval rawData = spark.sparkContext.textFile(\"../data/sparkml2/chapter10/breast-cancer-wisconsin.data\")\n```", "```scala\nval data = rawData.map(_.trim)\n .filter(text => !(text.isEmpty || text.startsWith(\"#\") || text.indexOf(\"?\") > -1))\n .map { line =>\n val values = line.split(',').map(_.toDouble)\n val slicedValues = values.slice(1, values.size)\n val featureVector = Vectors.dense(slicedValues.init)\n val label = values.last / 2 -1\n LabeledPoint(label, featureVector)\n }\n```", "```scala\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\nprintln(\"Training Data count:\"+trainingData.count())\nprintln(\"Test Data Count:\"+testData.count())\n```", "```scala\nTraining Data count:473\nTest Data Count:210\n```", "```scala\ndef getMetrics(model: RandomForestModel, data: RDD[LabeledPoint]): RegressionMetrics = {\nval predictionsAndLabels = data.map(example =>\n (model.predict(example.features), example.label)\n )\nnew RegressionMetrics(predictionsAndLabels)\n }\n\n```", "```scala\nval numClasses = 2\nval categoricalFeaturesInfo = Map[Int, Int]()\nval numTrees = 3 // Use more in practice.val featureSubsetStrategy = \"auto\" // Let the algorithm choose.val impurity = \"variance\"\n val maxDepth = 4\nval maxBins = 32\nval model = RandomForest.trainRegressor(trainingData, categoricalFeaturesInfo,\nnumTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\nval metrics = getMetrics(model, testData)\nprintln(\"Test Mean Squared Error = \" + metrics.meanSquaredError)\nprintln(\"My Random Forest model:\\n\" + model.toDebugString)\n```", "```scala\nTest Mean Squared Error = 0.028681825568809653\nMy Random Forest model:\nTreeEnsembleModel regressor with 3 trees\n  Tree 0:\n    If (feature 2 <= 3.0)\n     If (feature 7 <= 3.0)\n      If (feature 4 <= 5.0)\n       If (feature 0 <= 8.0)\n        Predict: 0.006825938566552901\n       Else (feature 0 > 8.0)\n        Predict: 1.0\n      Else (feature 4 > 5.0)\n       Predict: 1.0\n     Else (feature 7 > 3.0)\n      If (feature 6 <= 3.0)\n       If (feature 0 <= 6.0)\n        Predict: 0.0\n       Else (feature 0 > 6.0)\n        Predict: 1.0\n      Else (feature 6 > 3.0)\n       Predict: 1.0\n    Else (feature 2 > 3.0)\n     If (feature 5 <= 3.0)\n      If (feature 4 <= 3.0)\n       If (feature 7 <= 3.0)\n        Predict: 0.1\n       Else (feature 7 > 3.0)\n        Predict: 1.0\n      Else (feature 4 > 3.0)\n       If (feature 3 <= 3.0)\n        Predict: 0.8571428571428571\n       Else (feature 3 > 3.0)\n        Predict: 1.0\n     Else (feature 5 > 3.0)\n      If (feature 5 <= 5.0)\n       If (feature 1 <= 4.0)\n        Predict: 0.75\n       Else (feature 1 > 4.0)\n        Predict: 1.0\n      Else (feature 5 > 5.0)\n       Predict: 1.0\n  Tree 1:\n...\n```", "```scala\nspark.stop()\n```", "```scala\ndef getMetrics(model: RandomForestModel, data: RDD[LabeledPoint]): RegressionMetrics = {\nval predictionsAndLabels = data.map(example =>\n (model.predict(example.features), example.label)\n )\nnew RegressionMetrics(predictionsAndLabels)\n}\n\n```", "```scala\nval impurity = \"variance\" // use variance for regression\n```", "```scala\npackage spark.ml.cookbook.chapter10\n```", "```scala\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.model.GradientBoostedTreesModel\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.*ERROR*)\n\nval spark = SparkSession\n   .builder.master(\"local[*]\")\n   .appName(\"MyGradientBoostedTreesClassification\")\n   .config(\"spark.sql.warehouse.dir\", \".\")\n   .getOrCreate()\n```", "```scala\nval rawData = spark.sparkContext.textFile(\"../data/sparkml2/chapter10/breast-cancer-wisconsin.data\")\n```", "```scala\nval data = rawData.map(_.trim)\n .filter(text => !(text.isEmpty || text.startsWith(\"#\") || text.indexOf(\"?\") > -1))\n .map { line =>\n val values = line.split(',').map(_.toDouble)\n val slicedValues = values.slice(1, values.size)\n val featureVector = Vectors.*dense*(slicedValues.init)\n val label = values.last / 2 -1\n LabeledPoint(label, featureVector)\n }\n```", "```scala\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\nprintln(\"Training Data count:\"+trainingData.count())\nprintln(\"Test Data Count:\"+testData.count())\n```", "```scala\nTraining Data count:491\nTest Data Count:192\n```", "```scala\ndef getMetrics(model: GradientBoostedTreesModel, data: RDD[LabeledPoint]): MulticlassMetrics = {\n val predictionsAndLabels = data.map(example =>\n (model.predict(example.features), example.label)\n )\n new MulticlassMetrics(predictionsAndLabels)\n }\n```", "```scala\ndef evaluate(\n trainingData: RDD[LabeledPoint],\n testData: RDD[LabeledPoint],\n boostingStrategy : BoostingStrategy\n ) :Unit = {\n\n val model = GradientBoostedTrees.*train*(trainingData, boostingStrategy)\n\n val metrics = getMetrics(model, testData)\n println(\"Confusion Matrix :\")\n println(metrics.confusionMatrix)\n println(\"Model Accuracy: \"+metrics.*precision*)\n println(\"Model Error: \"+ (1-metrics.*precision*))\n }\n```", "```scala\nval algo = \"Classification\" val numIterations = 3\nval numClasses = 2\nval maxDepth = 5\nval maxBins = 32\nval categoricalFeatureInfo = *Map*[Int,Int]()\nval boostingStrategy = BoostingStrategy.*defaultParams*(algo)\nboostingStrategy.setNumIterations(numIterations)\nboostingStrategy.treeStrategy.setNumClasses(numClasses) \nboostingStrategy.treeStrategy.setMaxDepth(maxDepth)\nboostingStrategy.treeStrategy.setMaxBins(maxBins) boostingStrategy.treeStrategy.categoricalFeaturesInfo = categoricalFeatureInfo\n```", "```scala\nevaluate(trainingData, testData, boostingStrategy)\n```", "```scala\nConfusion Matrix :\n124.0 2.0\n2.0 64.0\nModel Accuracy: 0.9791666666666666\nModel Error: 0.02083333333333337\n\nTo interpret the above Confusion metrics, Accuracy is equal to (124+ 64)/ 192 all test cases, and error is equal to 1 -accuracy\n```", "```scala\nspark.stop()\n```", "```scala\nval algo = \"Classification\"\n val numIterations = 3\n val numClasses = 2\n val maxDepth = 5\n val maxBins = 32\n val categoricalFeatureInfo = Map[Int,Int]()\n\n val boostingStrategy = BoostingStrategy.*defaultParams*(algo)\n```", "```scala\nevaluate(trainingData, testData, boostingStrategy)\n```", "```scala\nConfusion Matrix :\n124.0 2.0\n2.0 64.0\nModel Accuracy: 0.9791666666666666\nModel Error: 0.02083333333333337\n```", "```scala\nalgo = \"Regression\" val boostingStrategy = BoostingStrategy.defaultParams(algo)\n```", "```scala\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.model.GradientBoostedTreesModel\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\nimport org.apache.spark.mllib.tree.configuration.BoostingStrategy\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n\nval spark = SparkSession\n   .builder   .master(\"local[*]\")\n   .appName(\"MyGradientBoostedTreesRegression\")\n   .config(\"spark.sql.warehouse.dir\", \".\")\n   .getOrCreate()\n```", "```scala\nval rawData = spark.sparkContext.textFile(\"../data/sparkml2/chapter10/breast-cancer-wisconsin.data\")\n```", "```scala\nval data = rawData.map(_.trim)\n .filter(text => !(text.isEmpty || text.startsWith(\"#\") || text.indexOf(\"?\") > -1))\n .map { line =>\n val values = line.split(',').map(_.toDouble)\n val slicedValues = values.slice(1, values.size)\n val featureVector = Vectors.*dense*(slicedValues.init)\n val label = values.last / 2 -1\n *LabeledPoint*(label, featureVector)\n }\n```", "```scala\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\nprintln(\"Training Data count:\"+trainingData.count())\nprintln(\"Test Data Count:\"+testData.count())\n```", "```scala\nTraining Data count:469\nTest Data Count:214\n```", "```scala\ndef getMetrics(model: GradientBoostedTreesModel, data: RDD[LabeledPoint]): RegressionMetrics = {\n val predictionsAndLabels = data.map(example =>\n (model.predict(example.features), example.label)\n )\n new RegressionMetrics(predictionsAndLabels)\n }\n```", "```scala\nval algo = \"Regression\" val numIterations = 3\nval maxDepth = 5\nval maxBins = 32\nval categoricalFeatureInfo = Map[Int,Int]()\nval boostingStrategy = BoostingStrategy.defaultParams(algo)\nboostingStrategy.setNumIterations(numIterations)\nboostingStrategy.treeStrategy.setMaxDepth(maxDepth) \nboostingStrategy.treeStrategy.setMaxBins(maxBins) boostingStrategy.treeStrategy.categoricalFeaturesInfo = categoricalFeatureInfo\n```", "```scala\nval model = GradientBoostedTrees.train(trainingData, boostingStrategy)\nval metrics = getMetrics(model, testData)\n\n println(\"Test Mean Squared Error = \" + metrics.meanSquaredError)\n println(\"My regression GBT model:\\n\" + model.toDebugString)\n```", "```scala\nTest Mean Squared Error = 0.05370763765769276\nMy regression GBT model:\nTreeEnsembleModel regressor with 3 trees\nTree 0:\nIf (feature 1 <= 2.0)\nIf (feature 0 <= 6.0)\nIf (feature 5 <= 5.0)\nIf (feature 5 <= 4.0)\nPredict: 0.0\nElse (feature 5 > 4.0)\n...\n```", "```scala\nspark.stop()\n```", "```scala\n val algo = \"Regression\"\n val numIterations = 3\n val maxDepth = 5\n val maxBins = 32\n val categoricalFeatureInfo = *Map*[Int,Int]()\n\n val boostingStrategy = BoostingStrategy.*defaultParams*(algo)\n```", "```scala\nTest Mean Squared Error = 0.05370763765769276\nMy regression GBT model:\nTree 0:\nIf (feature 1 <= 2.0)\nIf (feature 0 <= 6.0)\nIf (feature 5 <= 5.0)\nIf (feature 5 <= 4.0)\nPredict: 0.0\nElse (feature 5 > 4.0)\n...\n```"]