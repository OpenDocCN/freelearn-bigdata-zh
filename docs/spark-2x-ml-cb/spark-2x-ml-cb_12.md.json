["```scala\npackage spark.ml.cookbook.chapter12\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.{SparkConf, SparkContext}\nimport org.jfree.chart.axis.{CategoryAxis, CategoryLabelPositions}\nimport org.jfree.chart.{ChartFactory, ChartFrame, JFreeChart}\nimport org.jfree.chart.plot.{CategoryPlot, PlotOrientation}\nimport org.jfree.data.category.DefaultCategoryDataset\n```", "```scala\ndef show(chart: JFreeChart) {\nval frame = new ChartFrame(\"\", chart)\n   frame.pack()\n   frame.setVisible(true)\n }\n```", "```scala\nval input = \"../data/sparkml2/chapter12/pg62.txt\"\n```", "```scala\nval spark = SparkSession\n .builder .master(\"local[*]\")\n .appName(\"ProcessWordCount\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\nimport spark.implicits._\n```", "```scala\nLogger.getRootLogger.setLevel(Level.*WARN*)\n```", "```scala\nval stopwords = scala.io.Source.fromFile(\"../data/sparkml2/chapter12/stopwords.txt\").getLines().toSet\n```", "```scala\nval lineOfBook = spark.sparkContext.textFile(input)\n .flatMap(line => line.split(\"\\\\W+\"))\n .map(_.toLowerCase)\n .filter( s => !stopwords.contains(s))\n .filter( s => s.length >= 2)\n .map(word => (word, 1))\n .reduceByKey(_ + _)\n .sortBy(_._2, false)\n```", "```scala\nval top25 = lineOfBook.take(25)\n```", "```scala\nval dataset = new DefaultCategoryDataset()\ntop25.foreach( {case (term: String, count: Int) => dataset.setValue(count, \"Count\", term) })\n```", "```scala\nval chart = ChartFactory.createBarChart(\"Term frequency\",\n \"Words\", \"Count\", dataset, PlotOrientation.VERTICAL,\n false, true, false)\n\n val plot = chart.getCategoryPlot()\n val domainAxis = plot.getDomainAxis();\n domainAxis.setCategoryLabelPositions(CategoryLabelPositions.DOWN_45);\nshow(chart)\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter12\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.ml.feature.{RegexTokenizer, StopWordsRemover, Word2Vec}\nimport org.apache.spark.sql.{SQLContext, SparkSession}\nimport org.apache.spark.{SparkConf, SparkContext}\n```", "```scala\nval input = \"../data/sparkml2/chapter12/pg62.txt\"\n```", "```scala\nval spark = SparkSession\n         .builder\n.master(\"local[*]\")\n         .appName(\"Word2Vec App\")\n         .config(\"spark.sql.warehouse.dir\", \".\")\n         .getOrCreate()\nimport spark.implicits._\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval df = spark.read.text(input).toDF(\"text\")\n```", "```scala\nval tokenizer = new RegexTokenizer()\n .setPattern(\"\\\\W+\")\n .setToLowercase(true)\n .setMinTokenLength(4)\n .setInputCol(\"text\")\n .setOutputCol(\"raw\")\n val rawWords = tokenizer.transform(df)\n```", "```scala\nval stopWords = new StopWordsRemover()\n .setInputCol(\"raw\")\n .setOutputCol(\"terms\")\n .setCaseSensitive(false)\n val wordTerms = stopWords.transform(rawWords)\n```", "```scala\nval word2Vec = new Word2Vec()\n .setInputCol(\"terms\")\n .setOutputCol(\"result\")\n .setVectorSize(3)\n .setMinCount(0)\nval model = word2Vec.fit(wordTerms)\n```", "```scala\nval synonyms = model.findSynonyms(\"martian\", 10)\n```", "```scala\nsynonyms.show(false)\n```", "```scala\nspark.stop()\n```", "```scala\ncurl -L -O http://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2\n```", "```scala\nbunzip2 enwiki-latest-pages-articles-multistream.xml.bz2\n```", "```scala\nhead -n50 enwiki-latest-pages-articles-multistream.xml\n<mediawiki xmlns=http://www.mediawiki.org/xml/export-0.10/  xsi:schemaLocation=\"http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd\" version=\"0.10\" xml:lang=\"en\"> \n\n  <siteinfo> \n    <sitename>Wikipedia</sitename> \n    <dbname>enwiki</dbname> \n    <base>https://en.wikipedia.org/wiki/Main_Page</base> \n    <generator>MediaWiki 1.27.0-wmf.22</generator> \n    <case>first-letter</case> \n    <namespaces> \n      <namespace key=\"-2\" case=\"first-letter\">Media</namespace> \n      <namespace key=\"-1\" case=\"first-letter\">Special</namespace> \n      <namespace key=\"0\" case=\"first-letter\" /> \n      <namespace key=\"1\" case=\"first-letter\">Talk</namespace> \n      <namespace key=\"2\" case=\"first-letter\">User</namespace> \n      <namespace key=\"3\" case=\"first-letter\">User talk</namespace> \n      <namespace key=\"4\" case=\"first-letter\">Wikipedia</namespace> \n      <namespace key=\"5\" case=\"first-letter\">Wikipedia talk</namespace> \n      <namespace key=\"6\" case=\"first-letter\">File</namespace> \n      <namespace key=\"7\" case=\"first-letter\">File talk</namespace> \n      <namespace key=\"8\" case=\"first-letter\">MediaWiki</namespace> \n      <namespace key=\"9\" case=\"first-letter\">MediaWiki talk</namespace> \n      <namespace key=\"10\" case=\"first-letter\">Template</namespace> \n      <namespace key=\"11\" case=\"first-letter\">Template talk</namespace> \n      <namespace key=\"12\" case=\"first-letter\">Help</namespace> \n      <namespace key=\"13\" case=\"first-letter\">Help talk</namespace> \n      <namespace key=\"14\" case=\"first-letter\">Category</namespace> \n      <namespace key=\"15\" case=\"first-letter\">Category talk</namespace> \n      <namespace key=\"100\" case=\"first-letter\">Portal</namespace> \n      <namespace key=\"101\" case=\"first-letter\">Portal talk</namespace> \n      <namespace key=\"108\" case=\"first-letter\">Book</namespace> \n      <namespace key=\"109\" case=\"first-letter\">Book talk</namespace> \n      <namespace key=\"118\" case=\"first-letter\">Draft</namespace> \n      <namespace key=\"119\" case=\"first-letter\">Draft talk</namespace> \n      <namespace key=\"446\" case=\"first-letter\">Education Program</namespace> \n      <namespace key=\"447\" case=\"first-letter\">Education Program talk</namespace> \n      <namespace key=\"710\" case=\"first-letter\">TimedText</namespace> \n      <namespace key=\"711\" case=\"first-letter\">TimedText talk</namespace> \n      <namespace key=\"828\" case=\"first-letter\">Module</namespace> \n      <namespace key=\"829\" case=\"first-letter\">Module talk</namespace> \n      <namespace key=\"2300\" case=\"first-letter\">Gadget</namespace> \n      <namespace key=\"2301\" case=\"first-letter\">Gadget talk</namespace> \n      <namespace key=\"2302\" case=\"case-sensitive\">Gadget definition</namespace> \n      <namespace key=\"2303\" case=\"case-sensitive\">Gadget definition talk</namespace> \n      <namespace key=\"2600\" case=\"first-letter\">Topic</namespace> \n    </namespaces> \n  </siteinfo> \n  <page> \n    <title>AccessibleComputing</title> \n    <ns>0</ns> \n    <id>10</id> \n    <redirect title=\"Computer accessibility\" />\n```", "```scala\npackage spark.ml.cookbook.chapter12\n```", "```scala\nimport edu.umd.cloud9.collection.wikipedia.WikipediaPage\n import edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage\n import org.apache.hadoop.fs.Path\n import org.apache.hadoop.io.Text\n import org.apache.hadoop.mapred.{FileInputFormat, JobConf}\n import org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.feature.{HashingTF, IDF}\n import org.apache.spark.mllib.linalg.distributed.RowMatrix\n import org.apache.spark.sql.SparkSession\n import org.tartarus.snowball.ext.PorterStemmer\n```", "```scala\nimport edu.umd.cloud9.collection.wikipedia.WikipediaPage\nimport edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage\n```", "```scala\ndef parseWikiPage(rawPage: String): Option[(String, String)] = {\n val wikiPage = new EnglishWikipediaPage()\n WikipediaPage.*readPage*(wikiPage, rawPage)\n\n if (wikiPage.isEmpty\n || wikiPage.isDisambiguation\n || wikiPage.isRedirect\n || !wikiPage.isArticle) {\n None\n } else {\n Some(wikiPage.getTitle, wikiPage.getContent)\n }\n }\n```", "```scala\ndef wordStem(stem: PorterStemmer, term: String): String = {\n stem.setCurrent(term)\n stem.stem()\n stem.getCurrent\n }\n```", "```scala\ndef tokenizePage(rawPageText: String, stopWords: Set[String]): Seq[String] = {\n val stem = new PorterStemmer()\n\n rawPageText.split(\"\\\\W+\")\n .map(_.toLowerCase)\n .filterNot(s => stopWords.contains(s))\n .map(s => wordStem(stem, s))\n .filter(s => s.length > 3)\n .distinct\n .toSeq\n }\n```", "```scala\nval input = \"../data/sparkml2/chapter12/enwiki_dump.xml\"\n```", "```scala\nval jobConf = new JobConf()\n jobConf.set(\"stream.recordreader.class\", \"org.apache.hadoop.streaming.StreamXmlRecordReader\")\n jobConf.set(\"stream.recordreader.begin\", \"<page>\")\n jobConf.set(\"stream.recordreader.end\", \"</page>\")\n```", "```scala\nFileInputFormat.addInputPath(jobConf, new Path(input))\n```", "```scala\nval spark = SparkSession\n   .builder.master(\"local[*]\")\n   .appName(\"ProcessLSA App\")\n   .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\")\n   .config(\"spark.sql.warehouse.dir\", \".\")\n   .getOrCreate()\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval wikiData = spark.sparkContext.hadoopRDD(\n jobConf,\n classOf[org.apache.hadoop.streaming.StreamInputFormat],\n classOf[Text],\n classOf[Text]).sample(false, .1)\n```", "```scala\nval wikiPages = wikiData.map(_._1.toString).flatMap(*parseWikiPage*)\n```", "```scala\nprintln(\"Wiki Page Count: \" + wikiPages.count())\n```", "```scala\nval stopwords = scala.io.Source.fromFile(\"../data/sparkml2/chapter12/stopwords.txt\").getLines().toSet\n```", "```scala\nval wikiTerms = wikiPages.map{ case(title, text) => tokenizePage(text, stopwords) }\n```", "```scala\nval hashtf = new HashingTF()\n val tf = hashtf.transform(wikiTerms)\n```", "```scala\nval idf = new IDF(minDocFreq=2)\n val idfModel = idf.fit(tf)\n val tfidf = idfModel.transform(tf)\n```", "```scala\ntfidf.cache()\n val rowMatrix = new RowMatrix(tfidf)\n val svd = rowMatrix.computeSVD(k=25, computeU = true)\n\n println(svd)\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter12\n```", "```scala\nimport edu.umd.cloud9.collection.wikipedia.WikipediaPage\n import edu.umd.cloud9.collection.wikipedia.language.EnglishWikipediaPage\n import org.apache.hadoop.fs.Path\n import org.apache.hadoop.io.Text\n import org.apache.hadoop.mapred.{FileInputFormat, JobConf}\n import org.apache.log4j.{Level, Logger}\n import org.apache.spark.ml.clustering.LDA\n import org.apache.spark.ml.feature._\n import org.apache.spark.sql.SparkSession\n```", "```scala\ndef parseWikiPage(rawPage: String): Option[(String, String)] = {\n val wikiPage = new EnglishWikipediaPage()\n WikipediaPage.*readPage*(wikiPage, rawPage)\n\n if (wikiPage.isEmpty\n || wikiPage.isDisambiguation\n || wikiPage.isRedirect\n || !wikiPage.isArticle) {\n None\n } else {\n *Some*(wikiPage.getTitle, wikiPage.getContent)\n }\n }\n```", "```scala\nval input = \"../data/sparkml2/chapter12/enwiki_dump.xml\" \n```", "```scala\nval jobConf = new JobConf()\n jobConf.set(\"stream.recordreader.class\", \"org.apache.hadoop.streaming.StreamXmlRecordReader\")\n jobConf.set(\"stream.recordreader.begin\", \"<page>\")\n jobConf.set(\"stream.recordreader.end\", \"</page>\")\n```", "```scala\nFileInputFormat.addInputPath(jobConf, new Path(input))\n```", "```scala\nval spark = SparkSession\n    .builder\n.master(\"local[*]\")\n    .appName(\"ProcessLDA App\")\n    .config(\"spark.serializer\",   \"org.apache.spark.serializer.KryoSerializer\")\n    .config(\"spark.sql.warehouse.dir\", \".\")\n    .getOrCreate()\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval wikiData = spark.sparkContext.hadoopRDD(\n jobConf,\n classOf[org.apache.hadoop.streaming.StreamInputFormat],\n classOf[Text],\n classOf[Text]).sample(false, .1)\n```", "```scala\nval df = wiki.map(_._1.toString)\n .flatMap(parseWikiPage)\n .toDF(\"title\", \"text\")\n```", "```scala\nval tokenizer = new RegexTokenizer()\n .setPattern(\"\\\\W+\")\n .setToLowercase(true)\n .setMinTokenLength(4)\n .setInputCol(\"text\")\n .setOutputCol(\"raw\")\n val rawWords = tokenizer.transform(df)\n```", "```scala\nval stopWords = new StopWordsRemover()\n .setInputCol(\"raw\")\n .setOutputCol(\"words\")\n .setCaseSensitive(false)\n\n val wordData = stopWords.transform(rawWords)\n```", "```scala\nval cvModel = new CountVectorizer()\n .setInputCol(\"words\")\n .setOutputCol(\"features\")\n .setMinDF(2)\n .fit(wordData)\n val cv = cvModel.transform(wordData)\n cv.cache()\n```", "```scala\nval lda = new LDA()\n .setK(5)\n .setMaxIter(10)\n .setFeaturesCol(\"features\")\n val model = lda.fit(tf)\n val transformed = model.transform(tf)\n```", "```scala\nval topics = model.describeTopics(5)\n topics.show(false)\n```", "```scala\nval vocaList = cvModel.vocabulary\ntopics.collect().foreach { r => {\n println(\"\\nTopic: \" + r.get(r.fieldIndex(\"topic\")))\n val y = r.getSeq[Int](r.fieldIndex(\"termIndices\")).map(vocaList(_))\n .zip(r.getSeq[Double](r.fieldIndex(\"termWeights\")))\n y.foreach(println)\n\n }\n}\n```", "```scala\nspark.stop()\n```"]