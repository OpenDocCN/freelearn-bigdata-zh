["```scala\nimport scala.collection.mutable.ArrayBuffer\nimport scala.util.control.Breaks._\n```", "```scala\ndef quadratic_function_itself(x:Double):Double = {\n// the function being differentiated\n// f(x) = 2x^2 - 8x + 9\nreturn 2 * math.pow(x,2) - (8*x) + 9\n}\n```", "```scala\ndef derivative_of_function(x:Double):Double = {\n// The derivative of f(x)\nreturn 4 * x - 8\n}\n```", "```scala\nvar currentMinimumValue = 13.0 // just pick up a random value\n```", "```scala\nval actualMinima = 2.0 // proxy for a label in training phase\n```", "```scala\nvar oldMinimumValue = 0.0\nvar iteration = 0;\nvar minimumVector = ArrayBuffer[Double]()\nvar costVector = ArrayBuffer[Double]()\n```", "```scala\nval stepSize = .01\nval tolerance = 0.0001\n```", "```scala\nwhile (math.abs(currentMinimumValue - oldMinimumValue) > tolerance) {\niteration +=1 //= iteration + 1 for debugging when non-convergence\n```", "```scala\noldMinimumValue = currentMinimumValue\nval gradient_value_at_point = derivative_of_function(oldMinimumValue)\n```", "```scala\nval move_by_amount = gradient_value_at_point * stepSize\ncurrentMinimumValue = oldMinimumValue - move_by_amount\n```", "```scala\ncostVector += math.pow(actualMinima - currentMinimumValue, 2)\nminimumVector += currentMinimumValue\n```", "```scala\nprint(\"Iteration= \",iteration,\" currentMinimumValue= \", currentMinimumValue)\nprint(\"\\n\")\n```", "```scala\n(Iteration= ,1, currentMinimumValue= ,12.56)\n(Iteration= ,2, currentMinimumValue= ,12.1376)\n(Iteration= ,3, currentMinimumValue= ,11.732096)\n(Iteration= ,4, currentMinimumValue= ,11.342812160000001)\n(Iteration= ,5, currentMinimumValue= ,10.9690996736)\n(Iteration= ,6, currentMinimumValue= ,10.610335686656)\n(Iteration= ,7, currentMinimumValue= ,10.265922259189761)\n(Iteration= ,8, currentMinimumValue= ,9.935285368822171)\n..........\n..........\n..........\n(Iteration= ,203, currentMinimumValue= ,2.0027698292180602)\n(Iteration= ,204, currentMinimumValue= ,2.0026590360493377)\n(Iteration= ,205, currentMinimumValue= ,2.0025526746073643)\n(Iteration= ,206, currentMinimumValue= ,2.00245056762307)\n(Iteration= ,207, currentMinimumValue= ,2.002352544918147)\n```", "```scala\nif (iteration == 1000000) break //break if non-convergence - debugging\n}\n```", "```scala\nprint(\"\\n Cost Vector: \"+ costVector)\nprint(\"\\n Minimum Vactor\" + minimumVector)\n```", "```scala\nCost vector: ArrayBuffer(111.51360000000001, 102.77093376000002, 94.713692553216, 87.28813905704389, ........7.0704727116774655E-6, 6.516147651082496E-6, 6.005281675238673E-6, 5.534467591900128E-6)\n\nMinimum VactorArrayBuffer(12.56, 12.1376, 11.732096, 11.342812160000001, 10.9690996736, 10.610335686656, 10.265922259189761, 9.935285368822171, ........2.0026590360493377, 2.0025526746073643, 2.00245056762307, 2.002352544918147)\n\n```", "```scala\nvar minimaXvalue= currentMinimumValue\nvar minimaYvalue= quadratic_function_itself(currentMinimumValue)\n```", "```scala\nprint(\"\\n\\nGD Algo: Local minimum found at X=\"+f\"$minimaXvalue%1.2f\")\nprint(\"\\nGD Algo: Y=f(x)= : \"+f\"$minimaYvalue%1.2f\")\n}\n```", "```scala\nGD Algo: Local minimum found at X = : 2.00 GD Algo: Y=f(x)= : 1.00\n```", "```scala\nimport java.awt.Color\nimport org.jfree.chart.plot.{XYPlot, PlotOrientation}\nimport org.jfree.chart.{ChartFactory, ChartFrame, JFreeChart}\nimport org.jfree.data.xy.{XYSeries, XYSeriesCollection}\nimport scala.collection.mutable.ArrayBuffer\n```", "```scala\nval gradientStepError = ArrayBuffer[(Int, Double)]()\nval bStep = ArrayBuffer[(Int, Double)]()\nval mStep = ArrayBuffer[(Int, Double)]()\n```", "```scala\ndef show(chart: JFreeChart) {\nval frame = new ChartFrame(\"plot\", chart)\nframe.pack()\nframe.setVisible(true)\n}\ndef configurePlot(plot: XYPlot): Unit = {\nplot.setBackgroundPaint(Color.WHITE)\nplot.setDomainGridlinePaint(Color.BLACK)\nplot.setRangeGridlinePaint(Color.BLACK)\nplot.setOutlineVisible(false)\n}\n```", "```scala\nBeta : Slope (m variable)\nAlpha : Intercept b variable)\n\ndef compute_error_for_line_given_points(b:Double, m:Double, points: Array[Array[Double]]):Double = {\nvar totalError = 0.0\nfor( point <- points ) {\nvar x = point(0)\nvar y = point(1)\ntotalError += math.pow(y - (m * x + b), 2)\n}\nreturn totalError / points.length\n}\n```", "```scala\ndef step_gradient(b_current:Double, m_current:Double, points:Array[Array[Double]], learningRate:Double): Array[Double]= {\nvar b_gradient= 0.0\nvar m_gradient= 0.0\nvar N = points.length.toDouble\nfor (point <- points) {\nvar x = point(0)\nvar y = point(1)\nb_gradient += -(2 / N) * (y - ((m_current * x) + b_current))\nm_gradient += -(2 / N) * x * (y - ((m_current * x) + b_current))\n}\nvar result = new Array[Double](2)\nresult(0) = b_current - (learningRate * b_gradient)\nresult(1) = m_current - (learningRate * m_gradient)\nreturn result\n}\n```", "```scala\ndef readCSV(inputFile: String) : Array[Array[Double]] = {scala.io.Source.fromFile(inputFile)\n.getLines()\n.map(_.split(\",\").map(_.trim.toDouble))\n.toArray\n}\n```", "```scala\ndef gradient_descent_runner(points:Array[Array[Double]], starting_b:Double, starting_m:Double, learning_rate:Double, num_iterations:Int):Array[Double]= {\nvar b = starting_b\nvar m = starting_m\nvar result = new Array[Double](2)\nvar error = 0.0\nresult(0) =b\nresult(1) =m\nfor (i <-0 to num_iterations) {\nresult = step_gradient(result(0), result(1), points, learning_rate)\nbStep += Tuple2(i, result(0))\nmStep += Tuple2(i, result(1))\nerror = compute_error_for_line_given_points(result(0), result(1), points)\ngradientStepError += Tuple2(i, error)\n}\n```", "```scala\ndef main(args: Array[String]): Unit = {\nval input = \"../data/sparkml2/chapter9/Year_Salary.csv\"\nval points = readCSV(input)\nval learning_rate = 0.001\nval initial_b = 0\nval initial_m = 0\nval num_iterations = 30000\nprintln(s\"Starting gradient descent at b = $initial_b, m =$initial_m, error = \"+ compute_error_for_line_given_points(initial_b, initial_m, points))\nprintln(\"Running...\")\nval result= gradient_descent_runner(points, initial_b, initial_m, learning_rate, num_iterations)\nvar b= result(0)\nvar m = result(1)\nprintln( s\"After $num_iterations iterations b = $b, m = $m, error = \"+ compute_error_for_line_given_points(b, m, points))\nval xy = new XYSeries(\"\")\ngradientStepError.foreach{ case (x: Int,y: Double) => xy.add(x,y) }\nval dataset = new XYSeriesCollection(xy)\nval chart = ChartFactory.createXYLineChart(\n\"Gradient Descent\", // chart title\n\"Iteration\", // x axis label\n\"Error\", // y axis label\ndataset, // data\nPlotOrientation.VERTICAL,\nfalse, // include legend\ntrue, // tooltips\nfalse // urls)\nval plot = chart.getXYPlot()\nconfigurePlot(plot)\nshow(chart)\nval bxy = new XYSeries(\"b\")\nbStep.foreach{ case (x: Int,y: Double) => bxy.add(x,y) }\nval mxy = new XYSeries(\"m\")\nmStep.foreach{ case (x: Int,y: Double) => mxy.add(x,y) }\nval stepDataset = new XYSeriesCollection()\nstepDataset.addSeries(bxy)\nstepDataset.addSeries(mxy)\nval stepChart = ChartFactory.createXYLineChart(\n\"Gradient Descent Steps\", // chart title\n\"Iteration\", // x axis label\n\"Steps\", // y axis label\nstepDataset, // data\nPlotOrientation.VERTICAL,\ntrue, // include legend\ntrue, // tooltips\nfalse // urls\n)\nval stepPlot = stepChart.getXYPlot()\nconfigurePlot(stepPlot)\nshow(stepChart)\n}\n```", "```scala\nr (i <-0 to num_iterations) {\nstep_gradient()\n...\ncompute_error_for_line_given_points()\n...\n}\n```", "```scala\npackage spark.ml.cookbook.chapter9\n```", "```scala\nimport org.apache.spark.ml.feature.LabeledPoint\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.regression.LinearRegression\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\nimport spark.implicits._\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myRegressNormal\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval data = spark.read.text(\"../data/sparkml2/housing8.csv\").as[String]\nval RegressionDataSet = data.map { line => val columns = line.split(',')\nLabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,\ncolumns(5).toDouble,columns(6).toDouble, columns(7).toDouble\n))\n}\n```", "```scala\nval lr = new LinearRegression()                                      \n  .setMaxIter(1000)                                   \n  .setElasticNetParam(0.0)  \n  .setRegParam(0.01)                                    \n  .setSolver(\"normal\")\n```", "```scala\nval myModel = lr.fit(RegressionDataSet)\nExtract the model summary:\nval summary = myModel.summary\n```", "```scala\ntraining Mean Squared Error = 13.609079490110766\ntraining Root Mean Squared Error = 3.6890485887435482\n```"]