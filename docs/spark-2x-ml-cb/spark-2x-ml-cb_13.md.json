["```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\nimport java.io.{BufferedOutputStream, PrintWriter}\nimport java.net.Socket\nimport java.net.ServerSocket\nimport java.util.concurrent.TimeUnit\nimport scala.util.Random\nimport org.apache.spark.sql.streaming.ProcessingTime\n```", "```scala\nclass CountSreamThread(socket: Socket) extends Thread\n```", "```scala\nval villians = Array(\"Bane\", \"Thanos\", \"Loki\", \"Apocalypse\", \"Red Skull\", \"The Governor\", \"Sinestro\", \"Galactus\",\n \"Doctor Doom\", \"Lex Luthor\", \"Joker\", \"Magneto\", \"Darth Vader\")\n```", "```scala\noverride def run(): Unit = {\n\n println(\"Connection accepted\")\n val out = new PrintWriter(new BufferedOutputStream(socket.getOutputStream()))\n\n println(\"Producing Data\")\n while (true) {\n out.println(villians(Random.nextInt(villians.size)))\n Thread.sleep(10)\n }\n\n println(\"Done Producing\")\n }\n```", "```scala\nobject CountStreamProducer {\n\n def main(args: Array[String]): Unit = {\n\n val ss = new ServerSocket(9999)\n while (true) {\n println(\"Accepting Connection...\")\n new CountSreamThread(ss.accept()).start()\n }\n }\n }\n```", "```scala\n   Logger.getLogger(\"org\").setLevel(Level.ERROR)\n    Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder.master(\"local[*]\")\n.appName(\"votecountstream\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval stream = spark.readStream\n .format(\"socket\")\n .option(\"host\", \"localhost\")\n .option(\"port\", 9999)\n .load()\n```", "```scala\nval villainsVote = stream.groupBy(\"value\").count()\n```", "```scala\nval query = villainsVote.orderBy(\"count\").writeStream\n .outputMode(\"complete\")\n .format(\"console\")\n .trigger(ProcessingTime.create(10, TimeUnit.SECONDS))\n .start()\n```", "```scala\nquery.awaitTermination()\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport java.util.concurrent.TimeUnit\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.ProcessingTime\n```", "```scala\nval spark = SparkSession\n.builder.master(\"local[*]\")\n.appName(\"DataFrame Stream\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval df = spark.read .format(\"json\")\n.option(\"inferSchema\", \"true\")\n.load(\"../data/sparkml2/chapter13/person.json\")\ndf.printSchema()\n```", "```scala\nroot\n|-- age: long (nullable = true)\n|-- name: string (nullable = true)\n```", "```scala\nval stream = spark.readStream\n.schema(df.schema)\n.json(\"../data/sparkml2/chapter13/people/\")\n```", "```scala\nval people = stream.select(\"name\", \"age\").where(\"age > 60\")\n```", "```scala\nval query = people.writeStream\n.outputMode(\"append\")\n.trigger(ProcessingTime(1, TimeUnit.SECONDS))\n.format(\"console\")\n```", "```scala\nquery.start().awaitTermination()\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport java.util.concurrent.TimeUnit\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.ProcessingTime\n\n```", "```scala\ncase class StockPrice(date: String, open: Double, high: Double, low: Double, close: Double, volume: Integer, adjclose: Double)\n```", "```scala\nval spark = SparkSession\n.builder.master(\"local[*]\")\n.appName(\"Dataset Stream\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval s = spark.read\n.format(\"csv\")\n.option(\"header\", \"true\")\n.option(\"inferSchema\", \"true\")\n.load(\"../data/sparkml2/chapter13/GE.csv\")\ns.printSchema()\n```", "```scala\nroot\n|-- date: timestamp (nullable = true)\n|-- open: double (nullable = true)\n|-- high: double (nullable = true)\n|-- low: double (nullable = true)\n|-- close: double (nullable = true)\n|-- volume: integer (nullable = true)\n|-- adjclose: double (nullable = true)\n```", "```scala\nval streamDataset = spark.readStream\n            .schema(s.schema)\n            .option(\"sep\", \",\")\n            .option(\"header\", \"true\")\n            .csv(\"../data/sparkml2/chapter13/ge\").as[StockPrice]\n```", "```scala\nval ge = streamDataset.filter(\"close > 100.00\")\n```", "```scala\nval query = ge.writeStream\n.outputMode(\"append\")\n.trigger(ProcessingTime(1, TimeUnit.SECONDS))\n.format(\"console\")\n\n```", "```scala\nquery.start().awaitTermination()\n```", "```scala\nval streamDataset = spark.readStream\n            .schema(s.schema)\n            .option(\"sep\", \",\")\n            .option(\"header\", \"true\")\n            .csv(\"../data/sparkml2/chapter13/ge\").as[StockPrice]\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport java.time.LocalDateTime\nimport scala.util.Random._\n```", "```scala\ncase class ClickEvent(userId: String, ipAddress: String, time: String, url: String, statusCode: String)\n```", "```scala\nval statusCodeData = Seq(200, 404, 500)\n```", "```scala\nval urlData = Seq(\"http://www.fakefoo.com\",\n \"http://www.fakefoo.com/downloads\",\n \"http://www.fakefoo.com/search\",\n \"http://www.fakefoo.com/login\",\n \"http://www.fakefoo.com/settings\",\n \"http://www.fakefoo.com/news\",\n \"http://www.fakefoo.com/reports\",\n \"http://www.fakefoo.com/images\",\n \"http://www.fakefoo.com/css\",\n \"http://www.fakefoo.com/sounds\",\n \"http://www.fakefoo.com/admin\",\n \"http://www.fakefoo.com/accounts\" )\n```", "```scala\nval ipAddressData = generateIpAddress()\ndef generateIpAddress(): Seq[String] = {\n for (n <- 1 to 255) yield s\"127.0.0.$n\" }\n```", "```scala\nval timeStampData = generateTimeStamp()\n\n def generateTimeStamp(): Seq[String] = {\n val now = LocalDateTime.now()\n for (n <- 1 to 1000) yield LocalDateTime.*of*(now.toLocalDate,\n now.toLocalTime.plusSeconds(n)).toString\n }\n```", "```scala\nval userIdData = generateUserId()\n\n def generateUserId(): Seq[Int] = {\n for (id <- 1 to 1000) yield id\n }\n```", "```scala\ndef generateClicks(clicks: Int = 1): Seq[String] = {\n 0.until(clicks).map(i => {\n val statusCode = statusCodeData(nextInt(statusCodeData.size))\n val ipAddress = ipAddressData(nextInt(ipAddressData.size))\n val timeStamp = timeStampData(nextInt(timeStampData.size))\n val url = urlData(nextInt(urlData.size))\n val userId = userIdData(nextInt(userIdData.size))\n\n s\"$userId,$ipAddress,$timeStamp,$url,$statusCode\" })\n }\n```", "```scala\ndef parseClicks(data: String): ClickEvent = {\nval fields = data.split(\",\")\nnew ClickEvent(fields(0), fields(1), fields(2), fields(3), fields(4))\n }\n```", "```scala\nval spark = SparkSession\n.builder.master(\"local[*]\")\n .appName(\"Streaming App\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .config(\"spark.executor.memory\", \"2g\")\n .getOrCreate()\nval ssc = new StreamingContext(spark.sparkContext, Seconds(1))\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval rddQueue = new Queue[RDD[String]]()\n```", "```scala\nval inputStream = ssc.queueStream(rddQueue)\n```", "```scala\nval clicks = inputStream.map(data => ClickGenerator.parseClicks(data))\n val clickCounts = clicks.map(c => c.url).countByValue()\n```", "```scala\nclickCounts.print(12)\n```", "```scala\nssc.start()\n```", "```scala\nfor (i <- 1 to 10) {\n rddQueue += ssc.sparkContext.parallelize(ClickGenerator.*generateClicks*(100))\n Thread.sleep(1000)\n }\n```", "```scala\nssc.stop()\n```", "```scala\nwget https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n```", "```scala\ncurl https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data -o iris.data\n```", "```scala\nhttps://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n```", "```scala\nhead -5 iris.data\n5.1,3.5,1.4,0.2,Iris-setosa\n4.9,3.0,1.4,0.2,Iris-setosa\n4.7,3.2,1.3,0.2,Iris-setosa\n4.6,3.1,1.5,0.2,Iris-setosa\n5.0,3.6,1.4,0.2,Iris-setosa\n```", "```scala\ntail -5 iris.data\n6.3,2.5,5.0,1.9,Iris-virginica\n6.5,3.0,5.2,2.0,Iris-virginica\n6.2,3.4,5.4,2.3,Iris-virginica\n5.9,3.0,5.1,1.8,Iris-virginica\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport scala.collection.mutable.Queue\n```", "```scala\ndef readFromFile(sc: SparkContext) = {\n sc.textFile(\"../data/sparkml2/chapter13/iris.data\")\n .filter(s => !s.isEmpty)\n .zipWithIndex()\n }\n```", "```scala\ndef toLabelPoints(records: (String, Long)): LabeledPoint = {\n val (record, recordId) = records\n val fields = record.split(\",\")\n LabeledPoint(recordId,\n Vectors.*dense*(fields(0).toDouble, fields(1).toDouble,\n fields(2).toDouble, fields(3).toDouble))\n }\n```", "```scala\ndef buildLabelLookup(records: RDD[(String, Long)]) = {\n records.map {\n case (record: String, id: Long) => {\n val fields = record.split(\",\")\n (id, fields(4))\n }\n }.collect().toMap\n }\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"KMean Streaming App\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .config(\"spark.executor.memory\", \"2g\")\n .getOrCreate()\n\n val ssc = new StreamingContext(spark.sparkContext, *Seconds*(1))\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval irisData = IrisData.readFromFile(spark.sparkContext)\nval lookup = IrisData.buildLabelLookup(irisData)\n```", "```scala\nval trainQueue = new Queue[RDD[LabeledPoint]]()\nval testQueue = new Queue[RDD[LabeledPoint]]()\n```", "```scala\nval trainingStream = ssc.queueStream(trainQueue)\n val testStream = ssc.queueStream(testQueue)\n```", "```scala\nval model = new StreamingKMeans().setK(3)\n .setDecayFactor(1.0)\n .setRandomCenters(4, 0.0)\n```", "```scala\nmodel.trainOn(trainingStream.map(lp => lp.features))\n```", "```scala\nval values = model.predictOnValues(testStream.map(lp => (lp.label, lp.features)))\n values.foreachRDD(n => n.foreach(v => {\n println(v._2, v._1, lookup(v._1.toLong))\n }))\n```", "```scala\n  ssc.start()\n```", "```scala\nval irisLabelPoints = irisData.map(record => IrisData.toLabelPoints(record))\n```", "```scala\nval Array(trainData, test) = irisLabelPoints.randomSplit(Array(.80, .20))\n```", "```scala\ntrainQueue += irisLabelPoints\n Thread.sleep(2000)\n```", "```scala\nval testGroups = test.randomSplit(*Array*(.25, .25, .25, .25))\n testGroups.foreach(group => {\n testQueue += group\n *println*(\"-\" * 25)\n Thread.sleep(1000)\n })\n```", "```scala\n-------------------------\n(0,78.0,Iris-versicolor)\n(2,14.0,Iris-setosa)\n(1,132.0,Iris-virginica)\n(0,55.0,Iris-versicolor)\n(2,57.0,Iris-versicolor)\n-------------------------\n(2,3.0,Iris-setosa)\n(2,19.0,Iris-setosa)\n(2,98.0,Iris-versicolor)\n(2,29.0,Iris-setosa)\n(1,110.0,Iris-virginica)\n(2,39.0,Iris-setosa)\n(0,113.0,Iris-virginica)\n(1,50.0,Iris-versicolor)\n(0,63.0,Iris-versicolor)\n(0,74.0,Iris-versicolor)\n-------------------------\n(2,16.0,Iris-setosa)\n(0,106.0,Iris-virginica)\n(0,69.0,Iris-versicolor)\n(1,115.0,Iris-virginica)\n(1,116.0,Iris-virginica)\n(1,139.0,Iris-virginica)\n-------------------------\n(2,1.0,Iris-setosa)\n(2,7.0,Iris-setosa)\n(2,17.0,Iris-setosa)\n(0,99.0,Iris-versicolor)\n(2,38.0,Iris-setosa)\n(0,59.0,Iris-versicolor)\n(1,76.0,Iris-versicolor)\n```", "```scala\nssc.stop()\n```", "```scala\nsetDecayFactor()\nsetK()\nsetRandomCenters(,)\n```", "```scala\nwget http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n```", "```scala\ncurl http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv -o winequality-white.csv\n```", "```scala\nhttp://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\n```", "```scala\nhead -5 winequality-white.csv\n\n\"fixed acidity\";\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n7;0.27;0.36;20.7;0.045;45;170;1.001;3;0.45;8.8;6\n6.3;0.3;0.34;1.6;0.049;14;132;0.994;3.3;0.49;9.5;6\n8.1;0.28;0.4;6.9;0.05;30;97;0.9951;3.26;0.44;10.1;6\n7.2;0.23;0.32;8.5;0.058;47;186;0.9956;3.19;0.4;9.9;6\n```", "```scala\ntail -5 winequality-white.csv\n6.2;0.21;0.29;1.6;0.039;24;92;0.99114;3.27;0.5;11.2;6\n6.6;0.32;0.36;8;0.047;57;168;0.9949;3.15;0.46;9.6;5\n6.5;0.24;0.19;1.2;0.041;30;111;0.99254;2.99;0.46;9.4;6\n5.5;0.29;0.3;1.1;0.022;20;110;0.98869;3.34;0.38;12.8;7\n6;0.21;0.38;0.8;0.02;22;98;0.98941;3.26;0.32;11.8;6\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.linalg.Vectors\n import org.apache.spark.mllib.regression.LabeledPoint\n import org.apache.spark.mllib.regression.StreamingLinearRegressionWithSGD\n import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.{Row, SparkSession}\n import org.apache.spark.streaming.{Seconds, StreamingContext}\n import scala.collection.mutable.Queue\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"Regression Streaming App\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .config(\"spark.executor.memory\", \"2g\")\n .getOrCreate()\n\n import spark.implicits._\n\n val ssc = new StreamingContext(spark.sparkContext, *Seconds*(2))\n```", "```scala\nLogger.getRootLogger.setLevel(Level.WARN)\n```", "```scala\nval rawDF = spark.read\n .format(\"com.databricks.spark.csv\")\n .option(\"inferSchema\", \"true\")\n .option(\"header\", \"true\")\n .option(\"delimiter\", \";\")\n .load(\"../data/sparkml2/chapter13/winequality-white.csv\")\n```", "```scala\nval rdd = rawDF.rdd.zipWithUniqueId()\n```", "```scala\nval lookupQuality = rdd.map{ case (r: Row, id: Long)=> (id, r.getInt(11))}.collect().toMap\n```", "```scala\nval labelPoints = rdd.map{ case (r: Row, id: Long)=> LabeledPoint(id,\n Vectors.dense(r.getDouble(0), r.getDouble(1), r.getDouble(2), r.getDouble(3), r.getDouble(4),\n r.getDouble(5), r.getDouble(6), r.getDouble(7), r.getDouble(8), r.getDouble(9), r.getDouble(10))\n )}\n```", "```scala\nval trainQueue = new Queue[RDD[LabeledPoint]]()\nval testQueue = new Queue[RDD[LabeledPoint]]()\n```", "```scala\nval trainingStream = ssc.queueStream(trainQueue)\nval testStream = ssc.queueStream(testQueue)\n```", "```scala\nval numFeatures = 11\n val model = new StreamingLinearRegressionWithSGD()\n .setInitialWeights(Vectors.zeros(numFeatures))\n .setNumIterations(25)\n .setStepSize(0.1)\n .setMiniBatchFraction(0.25)\n```", "```scala\nmodel.trainOn(trainingStream)\nval result = model.predictOnValues(testStream.map(lp => (lp.label, lp.features)))\nresult.map{ case (id: Double, prediction: Double) => (id, prediction, lookupQuality(id.asInstanceOf[Long])) }.print()\n\n```", "```scala\nssc.start()\n```", "```scala\nval Array(trainData, test) = labelPoints.randomSplit(Array(.80, .20))\n```", "```scala\ntrainQueue += trainData\n Thread.sleep(4000)\n```", "```scala\nval testGroups = test.randomSplit(*Array*(.50, .50))\n testGroups.foreach(group => {\n testQueue += group\n Thread.sleep(2000)\n })\n```", "```scala\nssc.stop()\n```", "```scala\nhttp://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data\n```", "```scala\nwget http://archive.ics.uci.edu/ml/machine-learning-databases/pima-indians-diabetes/pima-indians-diabetes.data -o pima-indians-diabetes.data\n```", "```scala\nhead -5 pima-indians-diabetes.data\n6,148,72,35,0,33.6,0.627,50,1\n1,85,66,29,0,26.6,0.351,31,0\n8,183,64,0,0,23.3,0.672,32,1\n1,89,66,23,94,28.1,0.167,21,0\n0,137,40,35,168,43.1,2.288,33,1\n```", "```scala\ntail -5 pima-indians-diabetes.data\n10,101,76,48,180,32.9,0.171,63,0\n2,122,70,27,0,36.8,0.340,27,0\n5,121,72,23,112,26.2,0.245,30,0\n1,126,60,0,0,30.1,0.349,47,1\n1,93,70,31,0,30.4,0.315,23,0\n```", "```scala\n    Label/Class:\n               1 - tested positive\n               0 - tested negative\n```", "```scala\npackage spark.ml.cookbook.chapter13\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.mllib.classification.StreamingLogisticRegressionWithSGD\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.sql.{Row, SparkSession}\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport scala.collection.mutable.Queue\n\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"Logistic Regression Streaming App\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n\n import spark.implicits._\n\n val ssc = new StreamingContext(spark.sparkContext, *Seconds*(2))\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nval rawDS = spark.read\n.text(\"../data/sparkml2/chapter13/pima-indians- diabetes.data\").as[String]\n```", "```scala\nval buffer = rawDS.rdd.map(value => {\nval data = value.split(\",\")\n(data.init.toSeq, data.last)\n})\n```", "```scala\nval lps = buffer.map{ case (feature: Seq[String], label: String) =>\nval featureVector = feature.map(_.toDouble).toArray[Double]\nLabeledPoint(label.toDouble, Vectors.dense(featureVector))\n}\n\n```", "```scala\nval trainQueue = new Queue[RDD[LabeledPoint]]()\nval testQueue = new Queue[RDD[LabeledPoint]]()\n```", "```scala\nval trainingStream = ssc.queueStream(trainQueue)\nval testStream = ssc.queueStream(testQueue)\n```", "```scala\nval numFeatures = 8\nval model = new StreamingLogisticRegressionWithSGD()\n.setInitialWeights(Vectors.*zeros*(numFeatures))\n.setNumIterations(15)\n.setStepSize(0.5)\n.setMiniBatchFraction(0.25)\n```", "```scala\nmodel.trainOn(trainingStream)\nval result = model.predictOnValues(testStream.map(lp => (lp.label,\nlp.features)))\n result.map{ case (label: Double, prediction: Double) => (label, prediction) }.print()\n```", "```scala\nssc.start()\n```", "```scala\nval Array(trainData, test) = lps.randomSplit(*Array*(.80, .20))\n```", "```scala\ntrainQueue += trainData\n Thread.sleep(4000)\n```", "```scala\nval testGroups = test.randomSplit(*Array*(.50, .50))\n testGroups.foreach(group => {\n testQueue += group\n Thread.sleep(2000)\n })\n```", "```scala\n-------------------------------------------\nTime: 1488571098000 ms\n-------------------------------------------\n(1.0,1.0)\n(1.0,1.0)\n(1.0,0.0)\n(0.0,1.0)\n(1.0,0.0)\n(1.0,1.0)\n(0.0,0.0)\n(1.0,1.0)\n(0.0,1.0)\n(0.0,1.0)\n...\n-------------------------------------------\nTime: 1488571100000 ms\n-------------------------------------------\n(1.0,1.0)\n(0.0,0.0)\n(1.0,1.0)\n(1.0,0.0)\n(0.0,1.0)\n(0.0,1.0)\n(0.0,1.0)\n(1.0,0.0)\n(0.0,0.0)\n(1.0,1.0)\n...\n```", "```scala\nssc.stop()\n```"]