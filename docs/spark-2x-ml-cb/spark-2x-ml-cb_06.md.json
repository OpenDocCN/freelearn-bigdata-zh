["```scala\npackage spark.ml.cookbook.chapter6\n```", "```scala\nimport org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[4]\")\n .appName(\"myRegress02\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\n Logger.getLogger(\"org\").setLevel(Level.ERROR)\n Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval data = sc.textFile(\"../data/sparkml2/chapter6/housing8.csv\") \n```", "```scala\nval RegressionDataSet = data.map { line =>\n   val columns = line.split(',')\n\n   LabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,\n     columns(5).toDouble,columns(6).toDouble, columns(7).toDouble\n   ))\n }\n```", "```scala\nRegressionDataSet.collect().foreach(println(_)) \n\n(24.0,[0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09]) \n(21.6,[0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671]) \n(34.7,[0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671]) \n(33.4,[0.03237,0.0,2.18,0.0,0.458,6.998,45.8,6.0622]) \n(36.2,[0.06905,0.0,2.18,0.0,0.458,7.147,54.2,6.0622]) \n```", "```scala\nval numIterations = 1000\n val stepsSGD      = .001\n```", "```scala\n   val myModel = LinearRegressionWithSGD.train(RegressionDataSet, numIterations,stepsSGD) \n```", "```scala\nval predictedLabelValue = RegressionDataSet.map { lp => val predictedValue = myModel.predict(lp.features)\n   (lp.label, predictedValue)\n }\n```", "```scala\nprintln(\"Intercept set:\",myModel.intercept)\n println(\"Model Weights:\",myModel.weights)\n```", "```scala\nIntercept set: 0.0\n Model Weights:,[-0.03734048699612366,0.254990126659302,0.004917402413769299,\n 0.004611027094514264,0.027391067379836438,0.6401657695067162,0.1911635554630619,0.408578077994874])\n```", "```scala\npredictedLabelValue.takeSample(false,5).foreach(println(_)) \n```", "```scala\n(21.4,21.680880143786645)\n (18.4,24.04970929955823)\n (15.0,27.93421483734525)\n (41.3,23.898190127554827)\n (23.6,21.29583657363941)\n (33.3,34.58611522445151)\n (23.8,19.93920838257026)\n```", "```scala\nval MSE = predictedLabelValue.map{ case(l, p) => math.pow((l - p), 2)}.reduce(_ + _) / predictedLabelValue.count\n val RMSE = math.sqrt(MSE)println(\"training Mean Squared Error = \" + MSE)\n println(\"training Root Mean Squared Error = \" + RMSE)\n```", "```scala\ntraining Mean Squared Error = 91.45318188628684\ntraining Root Mean Squared Error = 9.563115699722912\n```", "```scala\nnewLinearRegressionWithSGD()\n```", "```scala\n  val myModel = new LinearRegressionWithSGD().setIntercept(true)\n```", "```scala\n(Model Weights:,[NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN]) \n```", "```scala\npackage spark.ml.cookbook.chapter6\n```", "```scala\nimport org.apache.spark.mllib.classification.LogisticRegressionWithSGD\n import org.apache.spark.mllib.linalg.Vectors\n import org.apache.spark.mllib.regression.{LabeledPoint, LassoWithSGD}\n import org.apache.spark.sql.{SQLContext, SparkSession}\n import org.apache.spark.{SparkConf, SparkContext}\n import org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[4]\")\n .appName(\"myRegress05\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval data = sc.textFile(\"../data/sparkml2/chapter6/admission1.csv\")\n```", "```scala\nval RegressionDataSet = data.map { line =>\n   val columns = line.split(',')\n\n   LabeledPoint(columns(0).toDouble , Vectors.dense(columns(1).toDouble,columns(2).toDouble, columns(3).toDouble ))\n\n }\n```", "```scala\nRegressionDataSet.collect().foreach(println(_)) \n\n(0.0,[380.0,3.61,3.0]) \n(1.0,[660.0,3.67,3.0]) \n(1.0,[800.0,4.0,1.0]) \n(1.0,[640.0,3.19,4.0])      \n   . . . . .  \n. . . . . \n. . . . . \n. . . . .\n```", "```scala\n// Logistic Regression with SGD r Model parameters\n\nval numIterations = 100\nval stepsSGD = .00001\nval regularizationParam = .05 // 1 is the default\n```", "```scala\nval myLogisticSGDModel = LogisticRegressionWithSGD.train(RegressionDataSet, numIterations,stepsSGD, regularizationParam) \n```", "```scala\nval predictedLabelValue = RegressionDataSet.map { lp => val predictedValue =  myLogisticSGDModel.predict(lp.features)\n   (lp.label, predictedValue)\n }\n```", "```scala\nprintln(\"Intercept set:\",myRidgeModel.intercept) \nprintln(\"Model Weights:\",myRidgeModel.weights) \n\n(Intercept set:,0.0) \n(Model Weights:,[-0.0012241832336285247,-7.351033538710254E-6,-8.625514722380274E-6])\n```", "```scala\n(0.0,0.0) \n(1.0,0.0) \n(1.0,0.0) \n(0.0,0.0) \n(1.0,0.0) \n. . . . .   \n. . . . .   \n```", "```scala\nval MSE = predictedLabelValue.map{ case(l, p) => math.pow((l - p), 2)}.reduce(_ + _) / predictedLabelValue.count\n\nval RMSE = math.sqrt(MSE)\n\nprintln(\"training Mean Squared Error = \" + MSE) \nprintln(\"training Root Mean Squared Error = \" + RMSE)\n```", "```scala\ntraining Mean Squared Error = 0.3175 \n```", "```scala\ntraining Root Mean Squared Error = 0.5634713834792322\n```", "```scala\nnewLogisticRegressionWithSGD()\n```", "```scala\npackage spark.ml.cookbook.chapter6\n```", "```scala\nimport org.apache.spark.mllib.regression.{LabeledPoint, LinearRegressionWithSGD, RidgeRegressionWithSGD}\n import org.apache.spark.sql.{SQLContext, SparkSession}\n\n import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n import org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[4]\")\n .appName(\"myRegress03\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval data = sc.textFile(\"../data/sparkml2/chapter6/housing8.csv\")\n```", "```scala\nval RegressionDataSet = data.map { line =>\n   val columns = line.split(',')\n\n   LabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,\n     columns(5).toDouble,columns(6).toDouble, columns(7).toDouble\n   ))\n\n }\n```", "```scala\nRegressionDataSet.collect().foreach(println(_)) \n\n(24.0,[0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09]) \n(21.6,[0.02731,0.0,7.07,0.0,0.469,6.421,78.9,4.9671]) \n(34.7,[0.02729,0.0,7.07,0.0,0.469,7.185,61.1,4.9671]) \n\n. . . . .  \n. . . . . \n. . . . . \n. . . . . \n\n(33.3,[0.04011,80.0,1.52,0.0,0.404,7.287,34.1,7.309]) \n(30.3,[0.04666,80.0,1.52,0.0,0.404,7.107,36.6,7.309]) \n(34.6,[0.03768,80.0,1.52,0.0,0.404,7.274,38.3,7.309]) \n(34.9,[0.0315,95.0,1.47,0.0,0.403,6.975,15.3,7.6534]) \n```", "```scala\n// Ridge regression Model parameters\n val numIterations = 1000\n val stepsSGD = .001\n val regularizationParam = 1.13 \n```", "```scala\nval myRidgeModel = RidgeRegressionWithSGD.train(RegressionDataSet, numIterations,stepsSGD, regularizationParam) \n```", "```scala\nval predictedLabelValue = RegressionDataSet.map { lp => val predictedValue = myRidgeModel.predict(lp.features)\n   (lp.label, predictedValue)\n }\n```", "```scala\nprintln(\"Intercept set:\",myRidgeModel.intercept)\n println(\"Model Weights:\",myRidgeModel.weights) \n\n(Intercept set:,0.0) \n(Model Weights:,[-0.03570346878210774,0.2577081687536239,0.005415957423129407,0.004368409890400891, 0.026279497009143078,0.6130086051124276,0.19363086562068213,0.392655338663542])\n```", "```scala\n(23.9,15.121761357965845) \n(17.0,23.11542703857021) \n(20.5,24.075526274194395) \n(28.0,19.209708926376237) \n(13.3,23.386162089812697) \n\n. . . . .   \n. . . . .\n```", "```scala\nval MSE = predictedLabelValue.map{ case(l, p) => math.pow((l - p), 2)}.reduce(_ + _) / predictedLabelValue.count\n val RMSE = math.sqrt(MSE)\n\n println(\"training Mean Squared Error = \" + MSE)\n println(\"training Root Mean Squared Error = \" + RMSE) \n```", "```scala\ntraining Mean Squared Error = 92.60723710764655\ntraining Root Mean Squared Error = 9.623265407731752  \n```", "```scala\nnew RidgeRegressionWithSGD()\n```", "```scala\nval regularizationParam = .00001\n(Model Weights:,\n[-0.0373404807799996, 0.25499013376755847, 0.0049174051853082094, 0.0046110262713086455, 0.027391063252456684, 0.6401656691002464, 0.1911635644638509, 0.4085780172461439 ]) \n\nval regularizationParam = 50\n(Model Weights:,[-0.012912409941749588, 0.2792184353165915, 0.016208621185873275, 0.0014162706383970278, 0.011205887829385417, 0.2466274224421205, 0.2261797091664634, 0.1696120633704305])\n```", "```scala\n(Model Weights:,[NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN])\n```", "```scala\npackage spark.ml.cookbook.chapter6\n```", "```scala\nimport org.apache.spark.mllib.regression.{LabeledPoint, LassoWithSGD, LinearRegressionWithSGD, RidgeRegressionWithSGD}\n import org.apache.spark.sql.{SQLContext, SparkSession}\n import org.apache.spark.ml.classification.LogisticRegression\n import org.apache.spark.ml.tuning.{ParamGridBuilder, TrainValidationSplit}\n import org.apache.spark.mllib.linalg.{Vector, Vectors}\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[4]\")\n .appName(\"myRegress04\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval data = sc.textFile(\"../data/sparkml2/chapter6/housing8.csv\")\n```", "```scala\nval RegressionDataSet = data.map { line => \nval columns = line.split(',')\n\n  LabeledPoint(columns(13).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble, columns(2).toDouble, columns(3).toDouble,columns(4).toDouble,\n\n    columns(5).toDouble,columns(6).toDouble, columns(7).toDouble\n\n  ))\n\n} \n```", "```scala\n   RegressionDataSet.collect().foreach(println(_)) \n\n(24.0,[0.00632,18.0,2.31,0.0,0.538,6.575,65.2,4.09]) \n   . . . . .  \n. . . . . \n. . . . . \n. . . . . \n   (34.6,[0.03768,80.0,1.52,0.0,0.404,7.274,38.3,7.309]) \n(34.9,[0.0315,95.0,1.47,0.0,0.403,6.975,15.3,7.6534]) \n```", "```scala\n// Lasso regression Model parameters\n\nval numIterations = 1000 \nval stepsSGD = .001 \nval regularizationParam = 1.13  \n```", "```scala\nval myRidgeModel = LassoWithSGD.train(RegressionDataSet, numIterations,stepsSGD, regularizationParam) \n```", "```scala\nval predictedLabelValue = RegressionDataSet.map { lp => val predictedValue = myRidgeModel.predict(lp.features) \n  (lp.label, predictedValue)\n\n} \n```", "```scala\nprintln(\"Intercept set:\",myRidgeModel.intercept) \nprintln(\"Model Weights:\",myRidgeModel.weights) \n\n(Intercept set:,0.0) \n(Model Weights:,[-0.0,0.2714890393052161,0.0,0.0,0.0,0.4659131582283458 ,0.2090072656520274,0.2753771238137026]) \n\n```", "```scala\n(18.0,24.145326403899134) \n(29.1,25.00830500878278) \n(23.1,10.127919006877956) \n(18.5,21.133621139346403) \n(22.2,15.755470439755092) \n. . . . .   \n. . . . .   \n```", "```scala\nval MSE = predictedLabelValue.map{ case(l, p) => math.pow((l - p), 2)}.reduce(_ + _) / predictedLabelValue.count\n\nval RMSE = math.sqrt(MSE)\n\nprintln(\"training Mean Squared Error = \" + MSE) \nprintln(\"training Root Mean Squared Error = \" + RMSE) \n```", "```scala\ntraining Mean Squared Error = 99.84312606110213\n training Root Mean Squared Error = 9.992153224460788\n```", "```scala\nnew LassoWithSGD()\n```", "```scala\nval regularizationParam = .30\n```", "```scala\n   (Model Weights:,[-0.02870908693284211,0.25634834423693936,1.707233741603369E-4, 0.0,0.01866468882602282,0.6259954005818621,0.19327180817037548,0.39741266136942227]) \n```", "```scala\nval regularizationParam = 4.13\n```", "```scala\n(Model Weights:,[-0.0,0.2714890393052161,0.0,0.0,0.0, 0.4659131582283458,0.2090072656520274,0.2753771238137026])\n```", "```scala\n(Model Weights:,[NaN,NaN,NaN,NaN,NaN,NaN,NaN,NaN])\n```", "```scala\npackage spark.ml.cookbook.chapter6\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectors\n import org.apache.spark.mllib.regression.LabeledPoint\n import org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\n import org.apache.spark.sql.{SQLContext, SparkSession}\n import org.apache.log4j.Logger\n import org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n.master(\"local[4]\")\n .appName(\"myRegress06\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval data = sc.textFile(\"../data/sparkml2/chapter6/admission1.csv\")\n```", "```scala\nval RegressionDataSet = data.map { line =>\n   val columns = line.split(',')\n\n   LabeledPoint(columns(0).toDouble , Vectors.dense(columns(1).toDouble,columns(2).toDouble, columns(3).toDouble ))\n\n }\n```", "```scala\nRegressionDataSet.collect().foreach(println(_)) \n\n(0.0,[380.0,3.61,3.0]) \n(1.0,[660.0,3.67,3.0]) \n(1.0,[800.0,4.0,1.0]) \n(1.0,[640.0,3.19,4.0])      \n   . . . . .  \n. . . . . \n. . . . . \n. . . . . \n\n```", "```scala\nval myLBFGSestimator = new LogisticRegressionWithLBFGS().setIntercept(false) \n```", "```scala\nval model1 = myLBFGSestimator.run(RegressionDataSet)\n```", "```scala\n// predict a single applicant on the go\n val singlePredict1 = model1.predict(Vectors.dense(700,3.4, 1))\n println(singlePredict1)\n\nval singlePredict2 = model1.predict(Vectors.dense(150,3.4, 1))\n println(singlePredict2) \n\n```", "```scala\n1.0   \n0.0   \n```", "```scala\n   val newApplicants=Seq(\n   (Vectors.dense(380.0, 3.61, 3.0)),\n   (Vectors.dense(660.0, 3.67, 3.0)),\n   (Vectors.dense(800.0, 1.3, 1.0)),\n   (Vectors.dense(640.0, 3.19, 4.0)),\n   (Vectors.dense(520.0, 2.93, 1.0))\n )\n```", "```scala\n val predictedLabelValue = newApplicants.map {lp => val predictedValue =  model1.predict(lp)\n   ( predictedValue)\n }\n```", "```scala\npredictedLabelValue.foreach(println(_)) \n\nOutput: \n0.0 \n0.0 \n1.0 \n0.0 \n1.0 \n```", "```scala\nLogisticRegressionWithLBFGS ()\n```", "```scala\npackage spark.ml.cookbook.chapter6\n```", "```scala\nimport org.apache.spark.mllib.util.MLUtils\n import org.apache.spark.mllib.classification.{SVMModel, SVMWithSGD}\n import org.apache.spark.mllib.evaluation.{BinaryClassificationMetrics, MultilabelMetrics, binary}\n import org.apache.spark.sql.{SQLContext, SparkSession}\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n.master(\"local[4]\")\n .appName(\"mySVM07\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval dataSetSVM = MLUtils.loadLibSVMFile(sc,\" ../data/sparkml2/chapter6/sample_libsvm_data.txt\") \n```", "```scala\nprintln(\"Top 10 rows of LibSVM data\")\n dataSetSVM.collect().take(10).foreach(println(_)) \n\nOutput: \n(0.0,(692,[127,128,129,130,131,154, .... ])) \n(1.0,(692,[158,159,160,161,185,186, .... ])) \n\n```", "```scala\nprintln(\" Total number of data vectors =\", dataSetSVM.count())\n val distinctData = dataSetSVM.distinct().count()\n println(\"Distinct number of data vectors = \", distinctData) \n\nOutput: \n( Total number of data vectors =,100) \n(Distinct number of data vectors = ,100) \n```", "```scala\nval trainingSetRatio = .20\n val populationTestSetRatio = .80\n\nval splitRatio = Array(trainingSetRatio, populationTestSetRatio) \n\n val allDataSVM = dataSetSVM.randomSplit(splitRatio)\n```", "```scala\nval numIterations = 100 \n\n val myModelSVM = SVMWithSGD.train(allDataSVM(0), numIterations,1,1)\n```", "```scala\nval predictedClassification = allDataSVM(1).map( x => (myModelSVM.predict(x.features), x.label)) \n```", "```scala\n   predictedClassification.collect().foreach(println(_)) \n\n(1.0,1.0) \n(1.0,1.0) \n(1.0,1.0) \n(1.0,1.0) \n(0.0,0.0) \n(0.0,1.0) \n(0.0,0.0) \n   ....... \n   ....... \n```", "```scala\n val falsePredictions = predictedClassification.filter(p => p._1 != p._2)\n\nprintln(allDataSVM(0).count())\n println(allDataSVM(1).count())\n\nprintln(predictedClassification.count())\n println(falsePredictions.count()) \n\nOutput: \n13 \n87 \n87 \n2 \n```", "```scala\nval metrics = new BinaryClassificationMetrics(predictedClassification) \n\n```", "```scala\nval areaUnderROCValue = metrics.areaUnderROC() \n\n  println(\"The area under ROC curve = \", areaUnderROCValue) \n\n  Output: \n  (The area under ROC curve = ,0.9743589743589743) \n```", "```scala\nnew SVMWithSGD()\n```", "```scala\n<label> <index1>:<value1> <index2>:<value2> ...\n```", "```scala\nSVMWithSGD() \n```", "```scala\nobject SVMDataGenerator() \n```", "```scala\npackage spark.ml.cookbook.chapter6\n```", "```scala\n\n import org.apache.spark.mllib.linalg.{Vector, Vectors}\n import org.apache.spark.mllib.regression.LabeledPoint\n import org.apache.spark.mllib.classification.{NaiveBayes, NaiveBayesModel}\n import org.apache.spark.mllib.evaluation.{BinaryClassificationMetrics, MulticlassMetrics, MultilabelMetrics, binary}\n import org.apache.spark.sql.{SQLContext, SparkSession}\n\n import org.apache.log4j.Logger\n import org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[4]\")\n .appName(\"myNaiveBayes08\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval data = sc.textFile(\"../data/sparkml2/chapter6/iris.data.prepared.txt\") \n```", "```scala\nval NaiveBayesDataSet = data.map { line =>\n   val columns = line.split(',')\n\n   LabeledPoint(columns(4).toDouble , Vectors.dense(columns(0).toDouble,columns(1).toDouble,columns(2).toDouble,columns(3).toDouble ))\n\n }\n```", "```scala\nprintln(\" Total number of data vectors =\", NaiveBayesDataSet.count())\n val distinctNaiveBayesData = NaiveBayesDataSet.distinct()\n println(\"Distinct number of data vectors = \", distinctNaiveBayesData.count()) \n\nOutput: \n\n(Total number of data vectors =,150) \n(Distinct number of data vectors = ,147) \n```", "```scala\ndistinctNaiveBayesData.collect().take(10).foreach(println(_)) \n\nOutput: \n(2.0,[6.3,2.9,5.6,1.8]) \n(2.0,[7.6,3.0,6.6,2.1]) \n(1.0,[4.9,2.4,3.3,1.0]) \n(0.0,[5.1,3.7,1.5,0.4]) \n(0.0,[5.5,3.5,1.3,0.2]) \n(0.0,[4.8,3.1,1.6,0.2]) \n(0.0,[5.0,3.6,1.4,0.2]) \n(2.0,[7.2,3.6,6.1,2.5]) \n.............. \n................ \n............. \n```", "```scala\nval allDistinctData = distinctNaiveBayesData.randomSplit(Array(.30,.70),13L)\n val trainingDataSet = allDistinctData(0)\n val testingDataSet = allDistinctData(1)\n```", "```scala\nprintln(\"number of training data =\",trainingDataSet.count())\n println(\"number of test data =\",testingDataSet.count()) \n\nOutput: \n(number of training data =,44) \n(number of test data =,103) \n```", "```scala\n         val myNaiveBayesModel = NaiveBayes.train(trainingDataSet) \n```", "```scala\nval predictedClassification = testingDataSet.map( x => (myNaiveBayesModel.predict(x.features), x.label)) \n```", "```scala\npredictedClassification.collect().foreach(println(_)) \n\n(2.0,2.0) \n(1.0,1.0) \n(0.0,0.0) \n(0.0,0.0) \n(0.0,0.0) \n(2.0,2.0) \n....... \n....... \n.......\n```", "```scala\nval metrics = new MulticlassMetrics(predictedClassification) \n```", "```scala\nval confusionMatrix = metrics.confusionMatrix\n println(\"Confusion Matrix= \\n\",confusionMatrix) \n\nOutput: \n   (Confusion Matrix=  \n   ,35.0  0.0   0.0    \n    0.0   34.0  0.0    \n    0.0   14.0  20.0  ) \n```", "```scala\nval myModelStat=Seq(metrics.precision,metrics.fMeasure,metrics.recall)\n myModelStat.foreach(println(_)) \n\nOutput: \n0.8640776699029126 \n0.8640776699029126 \n0.8640776699029126  \n```", "```scala\npackage spark.ml.cookbook.chapter6\n```", "```scala\nimport org.apache.spark.ml.classification.LogisticRegression\n```", "```scala\norg.apache.spark.sql.SparkSession\n```", "```scala\nimport org.apache.spark.ml.linalg.Vector\n```", "```scala\nimport org.apache.log4j.Logger\n import org.apache.log4j.Level\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myfirstlogistic\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nmaster(\"local[*]\")\n```", "```scala\nmaster(\"local[2]\")\n```", "```scala\nappName(\"myfirstlogistic\")\n```", "```scala\nconfig(\"spark.sql.warehouse.dir\", \".\")\n```", "```scala\nval trainingdata=Seq(\n (0.0, Vectors.dense(380.0, 3.61, 3.0)),\n (1.0, Vectors.dense(660.0, 3.67, 3.0)),\n (1.0, Vectors.dense(800.0, 1.3, 1.0)),\n (1.0, Vectors.dense(640.0, 3.19, 4.0)),\n (0.0, Vectors.dense(520.0, 2.93, 4.0)),\n (1.0, Vectors.dense(760.0, 3.00, 2.0)),\n (1.0, Vectors.dense(560.0, 2.98, 1.0)),\n (0.0, Vectors.dense(400.0, 3.08, 2.0)),\n (1.0, Vectors.dense(540.0, 3.39, 3.0)),\n (0.0, Vectors.dense(700.0, 3.92, 2.0)),\n (0.0, Vectors.dense(800.0, 4.0, 4.0)),\n (0.0, Vectors.dense(440.0, 3.22, 1.0)),\n (1.0, Vectors.dense(760.0, 4.0, 1.0)),\n (0.0, Vectors.dense(700.0, 3.08, 2.0)),\n (1.0, Vectors.dense(700.0, 4.0, 1.0)),\n (0.0, Vectors.dense(480.0, 3.44, 3.0)),\n (0.0, Vectors.dense(780.0, 3.87, 4.0)),\n (0.0, Vectors.dense(360.0, 2.56, 3.0)),\n (0.0, Vectors.dense(800.0, 3.75, 2.0)),\n (1.0, Vectors.dense(540.0, 3.81, 1.0))\n )\n```", "```scala\nval trainingDF = spark.createDataFrame(trainingdata).toDF(\"label\", \"features\")\n```", "```scala\nval lr_Estimator = new LogisticRegression().setMaxIter(80).setRegParam(0.01).setFitIntercept(true)\n```", "```scala\nprintln(\"LogisticRegression parameters:\\n\" + lr_Estimator.explainParams() + \"\\n\")\n```", "```scala\nAdmission_lr_Model parameters:\n{\nlogreg_34d0e7f2a3f9-elasticNetParam: 0.0,\nlogreg_34d0e7f2a3f9-featuresCol: features,\nlogreg_34d0e7f2a3f9-fitIntercept: true,\nlogreg_34d0e7f2a3f9-labelCol: label,\nlogreg_34d0e7f2a3f9-maxIter: 80,\nlogreg_34d0e7f2a3f9-predictionCol: prediction,\nlogreg_34d0e7f2a3f9-probabilityCol: probability,\nlogreg_34d0e7f2a3f9-rawPredictionCol: rawPrediction,\nlogreg_34d0e7f2a3f9-regParam: 0.01,\nlogreg_34d0e7f2a3f9-standardization: true,\nlogreg_34d0e7f2a3f9-threshold: 0.5,\nlogreg_34d0e7f2a3f9-tol: 1.0E-6\n}\n```", "```scala\nval Admission_lr_Model=lr_Estimator.fit(trainingDF)\n```", "```scala\nprintln(Admission_lr_Model.summary.predictions)\n```", "```scala\nAdmission_lr_Model Summary:\n[label: double, features: vector ... 3 more fields]\n```", "```scala\n// Build the model and predict\n val predict=Admission_lr_Model.transform(trainingDF)\n```", "```scala\n// print a schema as a guideline\npredict.printSchema()\n```", "```scala\nroot\n|-- label: double (nullable = false)\n|-- features: vector (nullable = true)\n|-- rawPrediction: vector (nullable = true)\n|-- probability: vector (nullable = true)\n|-- prediction: double (nullable = true)\n```", "```scala\n// Extract pieces that you need looking at schema and parameter\n// explanation output earlier in the program\n// Code made verbose for clarity\nval label1=predict.select(\"label\").collect()\nval features1=predict.select(\"features\").collect()\nval probability=predict.select(\"probability\").collect()\nval prediction=predict.select(\"prediction\").collect()\nval rawPrediction=predict.select(\"rawPrediction\").collect()\n```", "```scala\nprintln(\"Training Set Size=\", label1.size )\n```", "```scala\n(Training Set Size=,20)\n```", "```scala\nprintln(\"No. Original Feature Vector Predicted Outcome confidence probability\")\n println(\"--- --------------------------- ---------------------- \n ------------------------- --------------------\")\n for( i <- 0 to label1.size-1) {\n print(i, \" \", label1(i), features1(i), \" \", prediction(i), \" \", rawPrediction(i), \" \", probability(i))\n println()\n }\n```", "```scala\nNo. Original Feature Vector Predicted Outcome confidence probability\n--- --------------------------- ---------------------- ------------------------- --------------------\n(0, ,[0.0],[[380.0,3.61,3.0]], ,[0.0], ,[[1.8601472910617978,-1.8601472910617978]], ,[[0.8653141150964327,0.13468588490356728]])\n(1, ,[1.0],[[660.0,3.67,3.0]], ,[0.0], ,[[0.6331801846053525,-0.6331801846053525]], ,[[0.6532102092668394,0.34678979073316063]])\n(2, ,[1.0],[[800.0,1.3,1.0]], ,[1.0], ,[[-2.6503754234982932,2.6503754234982932]], ,[[0.06596587423646814,0.9340341257635318]])\n(3, ,[1.0],[[640.0,3.19,4.0]], ,[0.0], ,[[1.1347022244505625,-1.1347022244505625]], ,[[0.7567056336714486,0.2432943663285514]])\n(4, ,[0.0],[[520.0,2.93,4.0]], ,[0.0], ,[[1.5317564062962097,-1.5317564062962097]], ,[[0.8222631520883197,0.17773684791168035]])\n(5, ,[1.0],[[760.0,3.0,2.0]], ,[1.0], ,[[-0.8604923106990942,0.8604923106990942]], ,[[0.2972364981043905,0.7027635018956094]])\n(6, ,[1.0],[[560.0,2.98,1.0]], ,[1.0], ,[[-0.6469082170084807,0.6469082170084807]], ,[[0.3436866013868022,0.6563133986131978]])\n(7, ,[0.0],[[400.0,3.08,2.0]], ,[0.0], ,[[0.803419600659086,-0.803419600659086]], ,[[0.6907054912633392,0.30929450873666076]])\n(8, ,[1.0],[[540.0,3.39,3.0]], ,[0.0], ,[[1.0192401951528316,-1.0192401951528316]], ,[[0.7348245722723596,0.26517542772764036]])\n(9, ,[0.0],[[700.0,3.92,2.0]], ,[1.0], ,[[-0.08477122662243242,0.08477122662243242]], ,[[0.4788198754740347,0.5211801245259653]])\n(10, ,[0.0],[[800.0,4.0,4.0]], ,[0.0], ,[[0.8599949503972665,-0.8599949503972665]], ,[[0.7026595993369233,0.29734040066307665]])\n(11, ,[0.0],[[440.0,3.22,1.0]], ,[0.0], ,[[0.025000247291374955,-0.025000247291374955]], ,[[0.5062497363126953,0.49375026368730474]])\n(12, ,[1.0],[[760.0,4.0,1.0]], ,[1.0], ,[[-0.9861694953382877,0.9861694953382877]], ,[[0.27166933762974904,0.728330662370251]])\n(13, ,[0.0],[[700.0,3.08,2.0]], ,[1.0], ,[[-0.5465264211455029,0.5465264211455029]], ,[[0.3666706806887138,0.6333293193112862]])\n```", "```scala\nspark.stop()\n```"]