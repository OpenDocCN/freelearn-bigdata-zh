["```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval SignalNoise: Array[Double] = Array(0.2,1.2,0.1,0.4,0.3,0.3,0.1,0.3,0.3,0.9,1.8,0.2,3.5,0.5,0.3,0.3,0.2,0.4,0.5,0.9,0.1) \nval SignalStrength: Array[Double] = Array(6.2,1.2,1.2,6.4,5.5,5.3,4.7,2.4,3.2,9.4,1.8,1.2,3.5,5.5,7.7,9.3,1.1,3.1,2.1,4.1,5.1) \n```", "```scala\nval parSN=spark.sparkContext.parallelize(SignalNoise) // parallelized signal noise RDD \nval parSS=spark.sparkContext.parallelize(SignalStrength)  // parallelized signal strength \n```", "```scala\n    Signal Noise Local Array ,[D@2ab0702e)\n    RDD Version of Signal Noise on the cluster  \n    ,ParallelCollectionRDD[0] at parallelize at myRDD.scala:45)\n```", "```scala\nval parSN=spark.sparkContext.parallelize(SignalNoise) // parallelized signal noise RDD set with default partition \nval parSS=spark.sparkContext.parallelize(SignalStrength)  // parallelized signal strength set with default partition \nval parSN2=spark.sparkContext.parallelize(SignalNoise,4) // parallelized signal noise set with 4 partition \nval parSS2=spark.sparkContext.parallelize(SignalStrength,8)  // parallelized signal strength set with 8 partition \nprintln(\"parSN partition length \", parSN.partitions.length ) \nprintln(\"parSS partition length \", parSS.partitions.length ) \nprintln(\"parSN2 partition length \",parSN2.partitions.length ) \nprintln(\"parSS2 partition length \",parSS2.partitions.length ) \n```", "```scala\nparSN partition length ,2\nparSS partition length ,2\nparSN2 partition length ,4\nparSS2 partition length ,8\n\n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate()\n```", "```scala\nval book1 = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/a.txt\") \n```", "```scala\nNumber of lines = 16271\n```", "```scala\nval book2 = book1.flatMap(l => l.split(\" \")) \nprintln(book1.count())\n```", "```scala\nNumber of words = 143228  \n```", "```scala\nval dirKVrdd = spark.sparkContext.wholeTextFiles(\"../data/sparkml2/chapter3/*.txt\") // place a large number of small files for demo \nprintln (\"files in the directory as RDD \", dirKVrdd) \nprintln(\"total number of files \", dirKVrdd.count()) \nprintln(\"Keys \", dirKVrdd.keys.count()) \nprintln(\"Values \", dirKVrdd.values.count()) \ndirKVrdd.collect() \nprintln(\"Values \", dirKVrdd.first()) \n```", "```scala\n    files in the directory as RDD ,../data/sparkml2/chapter3/*.txt\n    WholeTextFileRDD[10] at wholeTextFiles at myRDD.scala:88)\n    total number of files 2\n    Keys ,2\n    Values ,2\n    Values ,(file:/C:/spark-2.0.0-bin-hadoop2.7/data/sparkml2/chapter3/a.txt,\n    The Project Gutenberg EBook of A Tale of Two Cities, \n    by Charles Dickens\n\n```", "```scala\nval book1 = spark.sparkContext.textFile(\"C:/xyz/dailyBuySel/*.tif\")\n```", "```scala\nval book1 = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/a.txt\", 13) \n```", "```scala\nspark.sparkContext.hadoopConfiguration.set(\"fs.s3n.awsAccessKeyId\", \"xyz\") \nspark.sparkContext.hadoopConfiguration.set(\"fs.s3n.awsSecretAccessKey\", \"....xyz...\") \nS3Rdd = spark.sparkContext.textFile(\"s3n://myBucket01/MyFile01\") \n```", "```scala\nval hdfsRDD = spark.sparkContext.textFile(\"hdfs:///localhost:9000/xyz/top10Vectors.txt\") \n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nimport breeze.numerics.pow\n```", "```scala\nval num : Array[Double] = Array(1,2,3,4,5,6,7,8,9,10,11,12,13) \n  val numRDD=sc.parallelize(num) \n  val book1 = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/a.txt\")\n```", "```scala\n  val myOdd= num.filter( i => (i%2) == 1) \n```", "```scala\nval myOdd2= num.filter(_ %2 == 1) // 2nd variation using scala notation  \nmyOdd.take(3).foreach(println) \n```", "```scala\n1.0\n3.0\n5.0\n```", "```scala\nval myOdd3= num.map(pow(_,2)).filter(_ %2 == 1) \nmyOdd3.take(3).foreach(println)  \n```", "```scala\n1.0\n9.0\n25.0\n```", "```scala\nval shortLines = book1.filter(_.length < 30).filter(_.length > 0) \n  println(\"Total number of lines = \", book1.count()) \n  println(\"Number of Short Lines = \", shortLines.count()) \n  shortLines.take(3).foreach(println) \n```", "```scala\nval theLines = book1.map(_.trim.toUpperCase()).filter(_.contains(\"TWO\")) \nprintln(\"Total number of lines = \", book1.count()) \nprintln(\"Number of lines with TWO = \", theLines.count()) \ntheLines.take(3).foreach(println) \n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval book1 = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/a.txt\")\n```", "```scala\nval wordRDD2 = book1.map(_.trim.split(\"\"\"[\\s\\W]+\"\"\") ).filter(_.length > 0) \nwordRDD2.take(3)foreach(println(_)) \n```", "```scala\n[Ljava.lang.String;@1e60b459\n[Ljava.lang.String;@717d7587\n[Ljava.lang.String;@3e906375\n```", "```scala\nval wordRDD3 = book1.flatMap(_.trim.split(\"\"\"[\\s\\W]+\"\"\") ).filter(_.length > 0).map(_.toUpperCase()) \nprintln(\"Total number of lines = \", book1.count()) \nprintln(\"Number of words = \", wordRDD3.count()) \n```", "```scala\nwordRDD3.take(5)foreach(println(_)) \n```", "```scala\nTotal number of lines = 16271\nNumber of words = 141603\nTHE\nPROJECT\nGUTENBERG\nEBOOK\nOF  \n```", "```scala\nval minValue1= numRDD.reduce(_ min _) \nprintln(\"minValue1 = \", minValue1)\n```", "```scala\nminValue1 = 1.0\n```", "```scala\nval minValue2 = numRDD.glom().map(_.min).reduce(_ min _) \nprintln(\"minValue2 = \", minValue2) \n```", "```scala\nminValue1 = 1.0  \n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval num : Array[Double]    = Array(1,2,3,4,5,6,7,8,9,10,11,12,13) \nval odd : Array[Double]    = Array(1,3,5,7,9,11,13) \nval even : Array[Double]    = Array(2,4,6,8,10,12) \n```", "```scala\nval intersectRDD = numRDD.intersection(oddRDD) \n```", "```scala\n1.0\n3.0\n5.0\n```", "```scala\n    val unionRDD = oddRDD.union(evenRDD) \n```", "```scala\n1.0\n2.0\n3.0\n4.0\n\n```", "```scala\nval subtractRDD = numRDD.subtract(oddRDD) \n```", "```scala\n2.0\n4.0\n6.0\n8.0\n\n```", "```scala\nval namesRDD = spark.sparkContext.parallelize(List(\"Ed\",\"Jain\", \"Laura\", \"Ed\")) \nval ditinctRDD = namesRDD.distinct() \n```", "```scala\n\"ED\"\n\"Jain\"\n\"Laura\"\n\n```", "```scala\nval cartesianRDD = oddRDD.cartesian(evenRDD) \ncartesianRDD.collect.foreach(println) \n```", "```scala\n(1.0,2.0)\n(1.0,4.0)\n(1.0,6.0)\n(3.0,2.0)\n(3.0,4.0)\n(3.0,6.0)   \n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport breeze.numerics.pow \nimport org.apache.spark.sql.SparkSession \nimport Array._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\n    val rangeRDD=sc.parallelize(1 to 12,3)\n```", "```scala\nval groupByRDD= rangeRDD.groupBy( i => {if (i % 2 == 1) \"Odd\" \n  else \"Even\"}).collect \ngroupByRDD.foreach(println) \n```", "```scala\nval alphabets = Array(\"a\", \"b\", \"a\", \"a\", \"a\", \"b\") // two type only to make it simple \n```", "```scala\nval alphabetsPairsRDD = spark.sparkContext.parallelize(alphabets).map(alphabets => (alphabets, 1)) \n```", "```scala\nval countsUsingGroup = alphabetsPairsRDD.groupByKey() \n  .map(c => (c._1, c._2.sum)) \n  .collect() \n```", "```scala\nval countsUsingReduce = alphabetsPairsRDD \n  .reduceByKey(_ + _) \n  .collect()\n```", "```scala\nprintln(\"Output for  groupBy\") \ncountsUsingGroup.foreach(println(_)) \nprintln(\"Output for  reduceByKey\") \ncountsUsingReduce.foreach(println(_)) \n```", "```scala\nOutput for groupBy\n(b,2)\n(a,4)\nOutput for reduceByKey\n(b,2)\n(a,4)  \n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\n    import org.apache.spark.sql.SparkSession \n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n.builder \n.master(\"local[*]\") \n.appName(\"myRDD\") \n.config(\"Spark.sql.warehouse.dir\", \".\") \n.getOrCreate() \n```", "```scala\nval SignalNoise: Array[Double] = Array(0.2,1.2,0.1,0.4,0.3,0.3,0.1,0.3,0.3,0.9,1.8,0.2,3.5,0.5,0.3,0.3,0.2,0.4,0.5,0.9,0.1) \nval SignalStrength: Array[Double] = Array(6.2,1.2,1.2,6.4,5.5,5.3,4.7,2.4,3.2,9.4,1.8,1.2,3.5,5.5,7.7,9.3,1.1,3.1,2.1,4.1,5.1) \nval parSN=spark.sparkContext.parallelize(SignalNoise) // parallelized signal noise RDD \nval parSS=spark.sparkContext.parallelize(SignalStrength)  // parallelized signal strength \n```", "```scala\nval zipRDD= parSN.zip(parSS).map(r => r._1 / r._2).collect() \nprintln(\"zipRDD=\") \nzipRDD.foreach(println) \n```", "```scala\nzipRDD=\n0.03225806451612903\n1.0\n0.08333333333333334\n0.0625\n0.05454545454545454  \n```", "```scala\nval zipRDD= parSN.zip(parSS).map(r => r._1 / r._2) \n```", "```scala\nprintln(\"Full Joined RDD = \") \nval fullJoinedRDD = keyValueRDD.fullOuterJoin(keyValueCity2RDD) \nfullJoinedRDD.collect().foreach(println(_)) \n```", "```scala\nval keyValuePairs = List((\"north\",1),(\"south\",2),(\"east\",3),(\"west\",4)) \nval keyValueCity1 = List((\"north\",\"Madison\"),(\"south\",\"Miami\"),(\"east\",\"NYC\"),(\"west\",\"SanJose\")) \nval keyValueCity2 = List((\"north\",\"Madison\"),(\"west\",\"SanJose\"))\n```", "```scala\nval keyValueRDD = spark.sparkContext.parallelize(keyValuePairs) \nval keyValueCity1RDD = spark.sparkContext.parallelize(keyValueCity1) \nval keyValueCity2RDD = spark.sparkContext.parallelize(keyValueCity2) \n```", "```scala\nval keys=keyValueRDD.keys \nval values=keyValueRDD.values \n```", "```scala\nval kvMappedRDD = keyValueRDD.mapValues(_+100) \nkvMappedRDD.collect().foreach(println(_)) \n```", "```scala\n(north,101)\n(south,102)\n(east,103)\n(west,104)\n\n```", "```scala\nprintln(\"Joined RDD = \") \nval joinedRDD = keyValueRDD.join(keyValueCity1RDD) \njoinedRDD.collect().foreach(println(_)) \n```", "```scala\n(south,(2,Miami))\n(north,(1,Madison))\n(west,(4,SanJose))\n(east,(3,NYC))\n```", "```scala\nprintln(\"Left Joined RDD = \") \nval leftJoinedRDD = keyValueRDD.leftOuterJoin(keyValueCity2RDD) \nleftJoinedRDD.collect().foreach(println(_)) \n```", "```scala\n(south,(2,None))\n(north,(1,Some(Madison)))\n(west,(4,Some(SanJose)))\n(east,(3,None))\n\n```", "```scala\nprintln(\"Right Joined RDD = \") \nval rightJoinedRDD = keyValueRDD.rightOuterJoin(keyValueCity2RDD) \nrightJoinedRDD.collect().foreach(println(_)) \n```", "```scala\n(north,(Some(1),Madison))\n(west,(Some(4),SanJose))  \n```", "```scala\nval fullJoinedRDD = keyValueRDD.fullOuterJoin(keyValueCity2RDD) \nfullJoinedRDD.collect().foreach(println(_)) \n```", "```scala\nFull Joined RDD = \n(south,(Some(2),None))\n(north,(Some(1),Some(Madison)))\n(west,(Some(4),Some(SanJose)))\n(east,(Some(3),None))\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.spark.sql.SparkSession \n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myRDD\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval signaltypeRDD = spark.sparkContext.parallelize(List((\"Buy\",1000),(\"Sell\",500),(\"Buy\",600),(\"Sell\",800))) \n```", "```scala\nval signaltypeRDD = spark.sparkContext.parallelize(List((\"Buy\",1000),(\"Sell\",500),(\"Buy\",600),(\"Sell\",800))) \nval groupedRDD = signaltypeRDD.groupByKey() \ngroupedRDD.collect().foreach(println(_)) \n```", "```scala\nGroup By Key RDD = \n(Sell, CompactBuffer(500, 800))\n(Buy, CompactBuffer(1000, 600))\n```", "```scala\nprintln(\"Reduce By Key RDD = \") \nval reducedRDD = signaltypeRDD.reduceByKey(_+_) \nreducedRDD.collect().foreach(println(_))   \n```", "```scala\nReduce By Key RDD = \n(Sell,1300)\n(Buy,1600)  \n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport org.apache.spark.sql._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myDataFrame\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval signaltypeRDD = spark.sparkContext.parallelize(List((\"Buy\",1000),(\"Sell\",500),(\"Buy\",600),(\"Sell\",800))) \nval numList = List(1,2,3,4,5,6,7,8,9) \nval numRDD = spark.sparkContext.parallelize(numList) \nval myseq = Seq( (\"Sammy\",\"North\",113,46.0),(\"Sumi\",\"South\",110,41.0), (\"Sunny\",\"East\",111,51.0),(\"Safron\",\"West\",113,2.0 )) \n```", "```scala\nval numDF = numRDD.toDF(\"mylist\") \nnumDF.show \n```", "```scala\n+------+\n|mylist|\n+------+\n|     1|\n|     2|\n|     3|\n|     4|\n|     5|\n|     6|\n|     7|\n|     8|\n|     9|\n+------+\n```", "```scala\nval df1 = spark.createDataFrame(myseq).toDF(\"Name\",\"Region\",\"dept\",\"Hours\") \n```", "```scala\ndf1.show() \ndf1.printSchema() \n```", "```scala\n+------+------+----+-----+\n|  Name|Region|dept|Hours|\n+------+------+----+-----+\n| Sammy| North| 113| 46.0|\n|  Sumi| South| 110| 41.0|\n| Sunny|  East| 111| 51.0|\n|Safron|  West| 113|  2.0|\n+------+------+----+-----+\n\nroot\n|-- Name: string (nullable = true)\n|-- Region: string (nullable = true)\n|-- dept: integer (nullable = false)\n|-- Hours: double (nullable = false) \n\n```", "```scala\nimport org.apache.spark._\nimport org.apache.spark.rdd.RDD \nimport org.apache.spark.sql.SQLContext \nimport org.apache.spark.mllib.linalg \nimport org.apache.spark.util \nimport Array._\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types \nimport org.apache.spark.sql.DataFrame \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.types.{ StructType, StructField, StringType}; \n```", "```scala\nimport sqlContext.implicits \n```", "```scala\npackage spark.ml.cookbook.chapter3 \n```", "```scala\nimport org.apache.spark.sql._\n```", "```scala\nimport org.apache.log4j.Logger \nimport org.apache.log4j.Level \n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR) \nLogger.getLogger(\"akka\").setLevel(Level.ERROR) \n```", "```scala\nval spark = SparkSession \n  .builder \n  .master(\"local[*]\") \n  .appName(\"myDataFrame\") \n  .config(\"Spark.sql.warehouse.dir\", \".\") \n  .getOrCreate() \n```", "```scala\nval customersRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/customers13.txt\") //Customer file\n```", "```scala\nCustomer data file    1101,susan,nyc,23 1204,adam,chicago,76\n1123,joe,london,65\n1109,tiffany,chicago,20\n\n```", "```scala\nval custRDD = customersRDD.map { \n  line => val cols = line.trim.split(\",\") \n    (cols(0).toInt, cols(1), cols(2), cols(3).toInt) \n} \n```", "```scala\n    val custDF = custRDD.toDF(\"custid\",\"name\",\"city\",\"age\") \n```", "```scala\ncustDF.show() \ncustDF.printSchema() \n```", "```scala\n+------+-------+-------+---+\n|custid|   name|   city|age|\n+------+-------+-------+---+\n|  1101|  susan|    nyc| 23|\n|  1204|   adam|chicago| 76|\n|  1123|    joe| london| 65|\n|  1109|tiffany|chicago| 20|\n+------+-------+-------+---+\n\nroot\n|-- custid: integer (nullable = false)\n|-- name: string (nullable = true)\n|-- city: string (nullable = true)\n|-- age: integer (nullable = false)\n```", "```scala\ncustDF.filter(\"age > 25.0\").show() \n```", "```scala\n+------+----+-------+---+ \n|custid|name|   city|age| \n+------+----+-------+---+ \n|  1204|adam|chicago| 76| \n|  1123| joe| london| 65| \n+------+----+-------+---+ \n```", "```scala\ncustDF.select(\"name\").show() \n```", "```scala\n+-------+ \n|   name| \n+-------+ \n|  susan| \n|   adam| \n|    joe| \n|tiffany| \n+-------+ \n```", "```scala\ncustDF.select(\"name\",\"city\").show() \n```", "```scala\n    +-------+-------+\n    |   name|   city|\n    +-------+-------+\n    |  susan|    nyc|\n    |   adam|chicago|\n    |    joe| london|\n    |tiffany|chicago|\n    +-------+-------+\n```", "```scala\ncustDF.select(custDF(\"name\"),custDF(\"city\"),custDF(\"age\")).show() \n```", "```scala\n+-------+-------+---+\n|   name|   city|age|\n+-------+-------+---+\n|  susan|    nyc| 23|\n|   adam|chicago| 76|\n|    joe| london| 65|\n|tiffany|chicago| 20|\n+-------+-------+---+  \n```", "```scala\ncustDF.select(custDF(\"name\"),custDF(\"city\"),custDF(\"age\") <50).show() \n```", "```scala\ncustDF.sort(\"city\").groupBy(\"city\").count().show() \n```", "```scala\ncustDF.explain()  \n```", "```scala\n== Physical Plan ==\nTungstenProject [_1#10 AS custid#14,_2#11 AS name#15,_3#12 AS city#16,_4#13 AS age#17]\n Scan PhysicalRDD[_1#10,_2#11,_3#12,_4#13]\n```", "```scala\nimport org.apache.spark._\n\n import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.SQLContext\n import org.apache.spark.mllib.linalg._\n import org.apache.spark.util._\n import Array._\n import org.apache.spark.sql._\n import org.apache.spark.sql.types._\n import org.apache.spark.sql.DataFrame\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.types.{ StructType, StructField, StringType};\n```", "```scala\nimport sqlContext.implicits._\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.spark.sql._\n```", "```scala\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myDataFrame\")\n .config(\"Spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval customersRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/customers13.txt\") //Customer file \n\nval custRDD = customersRDD.map {\n   line => val cols = line.trim.split(\",\")\n     (cols(0).toInt, cols(1), cols(2), cols(3).toInt) \n} \nval custDF = custRDD.toDF(\"custid\",\"name\",\"city\",\"age\")   \n```", "```scala\ncustDF.show()\n```", "```scala\nval productsRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/products13.txt\") //Product file\n val prodRDD = productsRDD.map {\n     line => val cols = line.trim.split(\",\")\n       (cols(0).toInt, cols(1), cols(2), cols(3).toDouble) \n}  \n```", "```scala\nval prodDF = prodRDD.toDF(\"prodid\",\"category\",\"dept\",\"priceAdvertised\")\n```", "```scala\nprodDF.show()\n```", "```scala\nval salesRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/sales13.txt\") *//Sales file* val saleRDD = salesRDD.map {\n     line => val cols = line.trim.split(\",\")\n       (cols(0).toInt, cols(1).toInt, cols(2).toDouble)\n}\n```", "```scala\nval saleDF = saleRDD.toDF(\"prodid\", \"custid\", \"priceSold\")  \n```", "```scala\nsaleDF.show()\n```", "```scala\ncustDF.printSchema()\nproductDF.printSchema()\nsalesDF. printSchema()\n```", "```scala\nroot\n |-- custid: integer (nullable = false)\n |-- name: string (nullable = true)\n |-- city: string (nullable = true)\n |-- age: integer (nullable = false)\nroot\n |-- prodid: integer (nullable = false)\n |-- category: string (nullable = true)\n |-- dept: string (nullable = true)\n |-- priceAdvertised: double (nullable = false)\nroot\n |-- prodid: integer (nullable = false)\n |-- custid: integer (nullable = false)\n |-- priceSold: double (nullable = false)\n```", "```scala\nimport org.apache.spark._\n import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.SQLContext\n import org.apache.spark.mllib.linalg._\n import org.apache.spark.util._\n import Array._\n import org.apache.spark.sql._\n import org.apache.spark.sql.types._\n import org.apache.spark.sql.DataFrame\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.types.{ StructType, StructField, StringType};\n```", "```scala\n import sqlContext.implicits._\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.spark.sql._\n```", "```scala\nimport org.apache.log4j.Logger\n import org.apache.log4j.Level\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myDataFrame\")\n .config(\"Spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\na. customerDF with columns: \"custid\",\"name\",\"city\",\"age\" b. productDF with Columns: \"prodid\",\"category\",\"dept\",\"priceAdvertised\" c. saleDF with columns: \"prodid\", \"custid\", \"priceSold\"\n\nval customersRDD =spark.sparkContext.textFile(\"../data/sparkml2/chapter3/customers13.txt\") //Customer file\n\nval custRDD = customersRDD.map {\n   line => val cols = line.trim.split(\",\")\n     (cols(0).toInt, cols(1), cols(2), cols(3).toInt)\n}\nval custDF = custRDD.toDF(\"custid\",\"name\",\"city\",\"age\") \nval productsRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/products13.txt\") //Product file\n\nval prodRDD = productsRDD.map {\n     line => val cols = line.trim.split(\",\")\n       (cols(0).toInt, cols(1), cols(2), cols(3).toDouble)       } \n\nval prodDF = prodRDD.toDF(\"prodid\",\"category\",\"dept\",\"priceAdvertised\")\n\nval salesRDD = spark.sparkContext.textFile(\"../data/sparkml2/chapter3/sales13.txt\") *//Sales file* val saleRDD = salesRDD.map {\n     line => val cols = line.trim.split(\",\")\n       (cols(0).toInt, cols(1).toInt, cols(2).toDouble)\n   }\nval saleDF = saleRDD.toDF(\"prodid\", \"custid\", \"priceSold\")\n```", "```scala\ncustDF.createOrReplaceTempView(\"customers\")\n```", "```scala\nprodDF.createOrReplaceTempView(\"products\")\n```", "```scala\nsaleDF.createOrReplaceTempView(\"sales\")\n```", "```scala\nval query1DF = spark.sql (\"select custid, name from customers\")\n query1DF.show()\n```", "```scala\nval query2DF = spark.sql(\"select prodid, priceAdvertised from products\")\n query2DF.show()\n```", "```scala\nval query3DF = spark.sql(\"select sum(priceSold) as totalSold from sales\")\nquery3DF.show()\n```", "```scala\nval query4DF = spark.sql(\"select custid, priceSold, priceAdvertised from sales s, products p where (s.priceSold/p.priceAdvertised < .80) and p.prodid = s.prodid\")\nquery4DF.show()\n```", "```scala\nquery4DF.explain()\n```", "```scala\n== Physical Plan ==\nTungstenProject [custid#30,priceSold#31,priceAdvertised#25]\n Filter ((priceSold#31 / priceAdvertised#25) < 0.8)\n SortMergeJoin [prodid#29], [prodid#22]\n TungstenSort [prodid#29 ASC], false, 0\n TungstenExchange hashpartitioning(prodid#29)\n TungstenProject [_1#26 AS prodid#29,_2#27 AS custid#30,_3#28 AS priceSold#31]\n Scan PhysicalRDD[_1#26,_2#27,_3#28]\n TungstenSort [prodid#22 ASC], false, 0\n TungstenExchange hashpartitioning(prodid#22)\n TungstenProject [_4#21 AS priceAdvertised#25,_1#18 AS prodid#22]\n Scan PhysicalRDD[_1#18,_2#19,_3#20,_4#21]\n```", "```scala\ncustDF.registerTempTable(\"customers\")\n```", "```scala\ncustDF.registerTempTable(\"customers\")\n```", "```scala\ncustDF.registerTempTable(\"customers\")\n```", "```scala\nimport org.apache.spark._\n\n import org.apache.spark.rdd.RDD\n import org.apache.spark.sql.SQLContext\n import org.apache.spark.mllib.linalg._\n import org.apache.spark.util._\n import Array._\n import org.apache.spark.sql._\n import org.apache.spark.sql.types._\n import org.apache.spark.sql.DataFrame\n import org.apache.spark.sql.Row;\n import org.apache.spark.sql.types.{ StructType, StructField, StringType};\n```", "```scala\n import sqlContext.implicits._\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Car(make: String, model: String, price: Double,\nstyle: String, kind: String)\n```", "```scala\nval *carData* =\n*Seq*(\n*Car*(\"Tesla\", \"Model S\", 71000.0, \"sedan\",\"electric\"),\n*Car*(\"Audi\", \"A3 E-Tron\", 37900.0, \"luxury\",\"hybrid\"),\n*Car*(\"BMW\", \"330e\", 43700.0, \"sedan\",\"hybrid\"),\n*Car*(\"BMW\", \"i3\", 43300.0, \"sedan\",\"electric\"),\n*Car*(\"BMW\", \"i8\", 137000.0, \"coupe\",\"hybrid\"),\n*Car*(\"BMW\", \"X5 xdrive40e\", 64000.0, \"suv\",\"hybrid\"),\n*Car*(\"Chevy\", \"Spark EV\", 26000.0, \"coupe\",\"electric\"),\n*Car*(\"Chevy\", \"Volt\", 34000.0, \"sedan\",\"electric\"),\n*Car*(\"Fiat\", \"500e\", 32600.0, \"coupe\",\"electric\"),\n*Car*(\"Ford\", \"C-Max Energi\", 32600.0, \"wagon/van\",\"hybrid\"),\n*Car*(\"Ford\", \"Focus Electric\", 29200.0, \"sedan\",\"electric\"),\n*Car*(\"Ford\", \"Fusion Energi\", 33900.0, \"sedan\",\"electric\"),\n*Car*(\"Hyundai\", \"Sonata\", 35400.0, \"sedan\",\"hybrid\"),\n*Car*(\"Kia\", \"Soul EV\", 34500.0, \"sedan\",\"electric\"),\n*Car*(\"Mercedes\", \"B-Class\", 42400.0, \"sedan\",\"electric\"),\n*Car*(\"Mercedes\", \"C350\", 46400.0, \"sedan\",\"hybrid\"),\n*Car*(\"Mercedes\", \"GLE500e\", 67000.0, \"suv\",\"hybrid\"),\n*Car*(\"Mitsubishi\", \"i-MiEV\", 23800.0, \"sedan\",\"electric\"),\n*Car*(\"Nissan\", \"LEAF\", 29000.0, \"sedan\",\"electric\"),\n*Car*(\"Porsche\", \"Cayenne\", 78000.0, \"suv\",\"hybrid\"),\n*Car*(\"Porsche\", \"Panamera S\", 93000.0, \"sedan\",\"hybrid\"),\n*Car*(\"Tesla\", \"Model X\", 80000.0, \"suv\",\"electric\"),\n*Car*(\"Tesla\", \"Model 3\", 35000.0, \"sedan\",\"electric\"),\n*Car*(\"Volvo\", \"XC90 T8\", 69000.0, \"suv\",\"hybrid\"),\n*Car*(\"Cadillac\", \"ELR\", 76000.0, \"coupe\",\"hybrid\")\n)\n\n```", "```scala\n   Logger.getLogger(\"org\").setLevel(Level.ERROR)\n   Logger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"mydatasetseq\")\n.config(\"Spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval cars = spark.createDataset(MyDatasetData.carData) \n// carData is put in a separate scala object MyDatasetData\n```", "```scala\ninfecars.show(false)\n+----------+--------------+--------+---------+--------+\n|make |model |price |style |kind |\n```", "```scala\ncars.columns.foreach(println)\nmake\nmodel\nprice\nstyle\nkind\n```", "```scala\nprintln(cars.schema)\nStructType(StructField(make,StringType,true), StructField(model,StringType,true), StructField(price,DoubleType,false), StructField(style,StringType,true), StructField(kind,StringType,true))\n```", "```scala\ncars.filter(cars(\"price\") > 50000.00).show()\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Car(make: String, model: String, price: Double,\nstyle: String, kind: String)\n```", "```scala\nval carData =\nSeq(\nCar(\"Tesla\", \"Model S\", 71000.0, \"sedan\",\"electric\"),\nCar(\"Audi\", \"A3 E-Tron\", 37900.0, \"luxury\",\"hybrid\"),\nCar(\"BMW\", \"330e\", 43700.0, \"sedan\",\"hybrid\"),\nCar(\"BMW\", \"i3\", 43300.0, \"sedan\",\"electric\"),\nCar(\"BMW\", \"i8\", 137000.0, \"coupe\",\"hybrid\"),\nCar(\"BMW\", \"X5 xdrive40e\", 64000.0, \"suv\",\"hybrid\"),\nCar(\"Chevy\", \"Spark EV\", 26000.0, \"coupe\",\"electric\"),\nCar(\"Chevy\", \"Volt\", 34000.0, \"sedan\",\"electric\"),\nCar(\"Fiat\", \"500e\", 32600.0, \"coupe\",\"electric\"),\nCar(\"Ford\", \"C-Max Energi\", 32600.0, \"wagon/van\",\"hybrid\"),\nCar(\"Ford\", \"Focus Electric\", 29200.0, \"sedan\",\"electric\"),\nCar(\"Ford\", \"Fusion Energi\", 33900.0, \"sedan\",\"electric\"),\nCar(\"Hyundai\", \"Sonata\", 35400.0, \"sedan\",\"hybrid\"),\nCar(\"Kia\", \"Soul EV\", 34500.0, \"sedan\",\"electric\"),\nCar(\"Mercedes\", \"B-Class\", 42400.0, \"sedan\",\"electric\"),\nCar(\"Mercedes\", \"C350\", 46400.0, \"sedan\",\"hybrid\"),\nCar(\"Mercedes\", \"GLE500e\", 67000.0, \"suv\",\"hybrid\"),\nCar(\"Mitsubishi\", \"i-MiEV\", 23800.0, \"sedan\",\"electric\"),\nCar(\"Nissan\", \"LEAF\", 29000.0, \"sedan\",\"electric\"),\nCar(\"Porsche\", \"Cayenne\", 78000.0, \"suv\",\"hybrid\"),\nCar(\"Porsche\", \"Panamera S\", 93000.0, \"sedan\",\"hybrid\"),\nCar(\"Tesla\", \"Model X\", 80000.0, \"suv\",\"electric\"),\nCar(\"Tesla\", \"Model 3\", 35000.0, \"sedan\",\"electric\"),\nCar(\"Volvo\", \"XC90 T8\", 69000.0, \"suv\",\"hybrid\"),\nCar(\"Cadillac\", \"ELR\", 76000.0, \"coupe\",\"hybrid\")\n)\n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"mydatasetrdd\")\n.config(\"Spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval sc = spark.sparkContext\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval rdd = spark.makeRDD(MyDatasetData.carData)\n```", "```scala\nval cars = spark.createDataset(*rdd*)\n```", "```scala\ncars.show(false)\n```", "```scala\ncars.columns.foreach(println)\nmake\nmodel\nprice\nstyle\nkind\n```", "```scala\n*println*(cars.schema)\nStructType(StructField(make,StringType,true), StructField(model,StringType,true), StructField(price,DoubleType,false), StructField(style,StringType,true), StructField(kind,StringType,true))\n```", "```scala\ncars.groupBy(\"make\").count().show()\n```", "```scala\nval carRDD = cars.where(\"make = 'Tesla'\").rdd\nCar(Tesla,Model X,80000.0,suv,electric)\nCar(Tesla,Model 3,35000.0,sedan,electric)\nCar(Tesla,Model S,71000.0,sedan,electric)\n```", "```scala\ncarRDD.foreach(println)\nCar(Tesla,Model X,80000.0,suv,electric)\nCar(Tesla,Model 3,35000.0,sedan,electric)\nCar(Tesla,Model S,71000.0,sedan,electric)\n```", "```scala\nspark.stop() \n```", "```scala\n{\"make\": \"Telsa\", \"model\": \"Model S\", \"price\": 71000.00, \"style\": \"sedan\", \"kind\": \"electric\"}\n{\"make\": \"Audi\", \"model\": \"A3 E-Tron\", \"price\": 37900.00, \"style\": \"luxury\", \"kind\": \"hybrid\"}\n{\"make\": \"BMW\", \"model\": \"330e\", \"price\": 43700.00, \"style\": \"sedan\", \"kind\": \"hybrid\"}\n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Car(make: String, model: String, price: Double,\nstyle: String, kind: String)\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"mydatasmydatasetjsonetrdd\")\n.config(\"Spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval cars = spark.read.json(\"../data/sparkml2/chapter3/cars.json\").as[Car]\n```", "```scala\ncars.show(false)\n```", "```scala\ncars.columns.foreach(println)\nmake\nmodel\nprice\nstyle\nkind\n```", "```scala\nprintln(cars.schema)\nStructType(StructField(make,StringType,true), StructField(model,StringType,true), StructField(price,DoubleType,false), StructField(style,StringType,true), StructField(kind,StringType,true))\n```", "```scala\ncars.select(\"make\").distinct().show()\n```", "```scala\ncars.createOrReplaceTempView(\"cars\")\n```", "```scala\nspark.sql(\"select make, model, kind from cars where kind = 'electric'\").show()\n```", "```scala\nspark.stop() \n```", "```scala\npackage spark.ml.cookbook.chapter3\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.{Dataset, SparkSession}\nimport spark.ml.cookbook.{Car, mydatasetdata}\nimport scala.collection.mutable\nimport scala.collection.mutable.ListBuffer\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Car(make: String, model: String, price: Double,\nstyle: String, kind: String)\n```", "```scala\nval carData =\nSeq(\nCar(\"Tesla\", \"Model S\", 71000.0, \"sedan\",\"electric\"),\nCar(\"Audi\", \"A3 E-Tron\", 37900.0, \"luxury\",\"hybrid\"),\nCar(\"BMW\", \"330e\", 43700.0, \"sedan\",\"hybrid\"),\nCar(\"BMW\", \"i3\", 43300.0, \"sedan\",\"electric\"),\nCar(\"BMW\", \"i8\", 137000.0, \"coupe\",\"hybrid\"),\nCar(\"BMW\", \"X5 xdrive40e\", 64000.0, \"suv\",\"hybrid\"),\nCar(\"Chevy\", \"Spark EV\", 26000.0, \"coupe\",\"electric\"),\nCar(\"Chevy\", \"Volt\", 34000.0, \"sedan\",\"electric\"),\nCar(\"Fiat\", \"500e\", 32600.0, \"coupe\",\"electric\"),\nCar(\"Ford\", \"C-Max Energi\", 32600.0, \"wagon/van\",\"hybrid\"),\nCar(\"Ford\", \"Focus Electric\", 29200.0, \"sedan\",\"electric\"),\nCar(\"Ford\", \"Fusion Energi\", 33900.0, \"sedan\",\"electric\"),\nCar(\"Hyundai\", \"Sonata\", 35400.0, \"sedan\",\"hybrid\"),\nCar(\"Kia\", \"Soul EV\", 34500.0, \"sedan\",\"electric\"),\nCar(\"Mercedes\", \"B-Class\", 42400.0, \"sedan\",\"electric\"),\nCar(\"Mercedes\", \"C350\", 46400.0, \"sedan\",\"hybrid\"),\nCar(\"Mercedes\", \"GLE500e\", 67000.0, \"suv\",\"hybrid\"),\nCar(\"Mitsubishi\", \"i-MiEV\", 23800.0, \"sedan\",\"electric\"),\nCar(\"Nissan\", \"LEAF\", 29000.0, \"sedan\",\"electric\"),\nCar(\"Porsche\", \"Cayenne\", 78000.0, \"suv\",\"hybrid\"),\nCar(\"Porsche\", \"Panamera S\", 93000.0, \"sedan\",\"hybrid\"),\nCar(\"Tesla\", \"Model X\", 80000.0, \"suv\",\"electric\"),\nCar(\"Tesla\", \"Model 3\", 35000.0, \"sedan\",\"electric\"),\nCar(\"Volvo\", \"XC90 T8\", 69000.0, \"suv\",\"hybrid\"),\nCar(\"Cadillac\", \"ELR\", 76000.0, \"coupe\",\"hybrid\")\n)\n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"mydatasetseq\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval cars = spark.createDataset(MyDatasetData.carData)\n```", "```scala\ncars.show(false)\n```", "```scala\nval modelData = cars.groupByKey(_.make).mapGroups({\ncase (make, car) => {\nval carModel = new ListBuffer[String]()\n           car.map(_.model).foreach({\n               c =>  carModel += c\n         })\n         (make, carModel)\n        }\n      })\n```", "```scala\n  modelData.show(false)\n```", "```scala\nspark.stop()\n```", "```scala\nimport spark.implicits._\n```"]