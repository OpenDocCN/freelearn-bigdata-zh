["```scala\npackage spark.ml.cookbook.chapter8\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.ml.clustering.KMeans\nimport org.apache.spark.sql.SparkSession\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"myKMeansCluster\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval trainingData = spark.read.format(\"libsvm\").load(\"../data/sparkml2/chapter8/my_kmeans_data.txt\")\n\ntrainingData.show()\n```", "```scala\n// Trains a k-means modelval kmeans = new KMeans()\n.setK(3) // default value is 2.setFeaturesCol(\"features\")\n.setMaxIter(10) // default Max Iteration is 20.setPredictionCol(\"prediction\")\n.setSeed(1L)\n```", "```scala\nval model = kmeans.fit(trainingData)\n```", "```scala\nmodel.summary.predictions.show()\n```", "```scala\nprintln(\"KMeans Cost:\" +model.computeCost(trainingData))\n```", "```scala\nKMeans Cost:4.137499999999979\n```", "```scala\nprintln(\"KMeans Cluster Centers: \")\n model.clusterCenters.foreach(println)\n```", "```scala\nThe centers for the 3 cluster (i.e. K= 3) \nKMeans Cluster Centers:  \n[1.025,1.075,1.15] \n[9.075,9.05,9.025] \n[3.45,3.475,3.55] \n```", "```scala\nspark.stop()\n```", "```scala\ndef generateKMeansRDD(sc: SparkContext, numPoints: Int, k: Int, d: Int, r: Double, numPartitions: Int = 2): RDD[Array[Double]] \n```", "```scala\npackage spark.ml.cookbook.chapter8\n```", "```scala\nimport org.apache.spark.ml.clustering.BisectingKMeans\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"MyBisectingKMeans\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval dataset = spark.read.format(\"libsvm\").load(\"../data/sparkml2/chapter8/glass.scale\")\n dataset.show(false)\n```", "```scala\nval splitData = dataset.randomSplit(Array(80.0, 20.0))\n val training = splitData(0)\n val testing = splitData(1)\n\n println(training.count())\n println(testing.count())\n```", "```scala\n180\n34\n```", "```scala\n// Trains a k-means modelval bkmeans = new BisectingKMeans()\n   .setK(6)\n   .setMaxIter(65)\n   .setSeed(1)\n```", "```scala\nval bisectingModel = bkmeans.fit(training)\n println(\"Parameters:\")\n println(bisectingModel.explainParams())\n```", "```scala\nParameters:\nfeaturesCol: features column name (default: features)\nk: The desired number of leaf clusters. Must be > 1\\. (default: 4, current: 6)\nmaxIter: maximum number of iterations (>= 0) (default: 20, current: 65)\nminDivisibleClusterSize: The minimum number of points (if >= 1.0) or the minimum proportion of points (if < 1.0) of a divisible cluster. (default: 1.0)\npredictionCol: prediction column name (default: prediction)\nseed: random seed (default: 566573821, current: 1)\n```", "```scala\nval cost = bisectingModel.computeCost(training)\n println(\"Sum of Squared Errors = \" + cost)\n```", "```scala\nSum of Squared Errors = 70.38842983516193\n```", "```scala\nprintln(\"Cluster Centers:\")\nval centers = bisectingModel.clusterCenters\ncenters.foreach(println)\n```", "```scala\nThe centers for the 6 cluster (i.e. K= 6) \nKMeans Cluster Centers: \n```", "```scala\nval predictions = bisectingModel.transform(testing)\n predictions.show(false)\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter8.\n```", "```scala\n import org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.clustering.GaussianMixture\n import org.apache.spark.mllib.linalg.Vectors\n import org.apache.spark.sql.SparkSession\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"myGaussianMixture\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval dataFile =\"../data/sparkml2/chapter8/socr_data.txt\"\n```", "```scala\nval trainingData = spark.sparkContext.textFile(dataFile).map { line =>\n Vectors.dense(line.trim.split(' ').map(_.toDouble))\n }.cache()\n```", "```scala\nval myGM = new GaussianMixture()\n .setK(4 ) // default value is 2, LF, LB, RF, RB\n .setConvergenceTol(0.01) // using the default value\n .setMaxIterations(100) // max 100 iteration\n```", "```scala\nval model = myGM.run(trainingData)\n```", "```scala\nprintln(\"Model ConvergenceTol: \"+ myGM.getConvergenceTol)\n println(\"Model k:\"+myGM.getK)\n println(\"maxIteration:\"+myGM.getMaxIterations)\n\n for (i <- 0 until model.k) {\n println(\"weight=%f\\nmu=%s\\nsigma=\\n%s\\n\" format\n (model.weights(i), model.gaussians(i).mu, model.gaussians(i).sigma))\n }\n```", "```scala\nprintln(\"Cluster labels (first <= 50):\")\n val clusterLabels = model.predict(trainingData)\n clusterLabels.take(50).foreach { x =>\n *print*(\" \" + x)\n }\n```", "```scala\nCluster labels (first <= 50):\n 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter8\n```", "```scala\n import org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.clustering.PowerIterationClustering\n import org.apache.spark.sql.SparkSession\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.*ERROR*)\n```", "```scala\n// setup SparkSession to use for interactions with Sparkval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"myPowerIterationClustering\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval trainingData =spark.sparkContext.parallelize(*List*(\n (0L, 1L, 1.0),\n (0L, 2L, 1.0),\n (0L, 3L, 1.0),\n (1L, 2L, 1.0),\n (1L, 3L, 1.0),\n (2L, 3L, 1.0),\n (3L, 4L, 0.1),\n (4L, 5L, 1.0),\n (4L, 15L, 1.0),\n (5L, 6L, 1.0),\n (6L, 7L, 1.0),\n (7L, 8L, 1.0),\n (8L, 9L, 1.0),\n (9L, 10L, 1.0),\n (10L,11L, 1.0),\n (11L, 12L, 1.0),\n (12L, 13L, 1.0),\n (13L,14L, 1.0),\n (14L,15L, 1.0)\n ))\n```", "```scala\nval pic = new PowerIterationClustering()\n .setK(3)\n .setMaxIterations(15)\n```", "```scala\nval model = pic.run(trainingData)\n```", "```scala\nmodel.assignments.foreach { a =>\n println(s\"${a.id} -> ${a.cluster}\")\n }\n```", "```scala\nval clusters = model.assignments.collect().groupBy(_.cluster).mapValues(_.map(_.id))\n val assignments = clusters.toList.sortBy { case (k, v) => v.length }\n val assignmentsStr = assignments\n .map { case (k, v) =>\n s\"$k -> ${v.sorted.mkString(\"[\", \",\", \"]\")}\" }.mkString(\", \")\n val sizesStr = assignments.map {\n _._2.length\n }.sorted.mkString(\"(\", \",\", \")\")\n println(s\"Cluster assignments: $assignmentsStr\\ncluster sizes: $sizesStr\")\n```", "```scala\nCluster assignments: 1 -> [12,14], 2 -> [4,6,8,10], 0 -> [0,1,2,3,5,7,9,11,13,15]\n cluster sizes: (2,4,10)\n```", "```scala\nspark.stop()\n```", "```scala\nnew PowerIterationClustering().setK(3).setMaxIterations(15)\n```", "```scala\nval model = pic.run(trainingData)\n```", "```scala\npackage spark.ml.cookbook.chapter8\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.clustering.LDA\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"MyLDA\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval input = \"../data/sparkml2/chapter8/my_lda_data.txt\"\n```", "```scala\nval dataset = spark.read.format(\"libsvm\").load(input)\n dataset.show(5)\n```", "```scala\nval lda = new LDA()\n .setK(5)\n .setMaxIter(10)\n .setFeaturesCol(\"features\")\n .setOptimizer(\"online\")\n .setOptimizeDocConcentration(true)\n```", "```scala\nval ldaModel = lda.fit(dataset)\n\n val ll = ldaModel.logLikelihood(dataset)\n val lp = ldaModel.logPerplexity(dataset)\n\n println(s\"\\t Training data log likelihood: $ll\")\n println(s\"\\t Training data log Perplexity: $lp\")\n```", "```scala\nTraining data log likelihood: -762.2149142231476\n Training data log Perplexity: 2.8869048032045974\n```", "```scala\nval topics = ldaModel.describeTopics(3)\n topics.show(false) // false is Boolean value for truncation for the dataset\n```", "```scala\nval transformed = ldaModel.transform(dataset)\n transformed.show(false)\n```", "```scala\ntransformed.show(true)\n```", "```scala\nspark.stop()\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\n import org.apache.spark.mllib.clustering.StreamingKMeans\n import org.apache.spark.mllib.linalg.Vectors\n import org.apache.spark.mllib.regression.LabeledPoint\n import org.apache.spark.sql.SparkSession\n import org.apache.spark.streaming.{Seconds, StreamingContext}\n```", "```scala\nval trainingDir = \"../data/sparkml2/chapter8/trainingDir\" val testDir = \"../data/sparkml2/chapter8/testDir\" val batchDuration = 10\n val numClusters = 2\n val numDimensions = 3\n```", "```scala\nval spark = SparkSession\n .builder.master(\"local[*]\")\n .appName(\"myStreamingKMeans\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval ssc = new StreamingContext(spark.sparkContext, Seconds(batchDuration.toLong))\n```", "```scala\nval trainingData = ssc.textFileStream(trainingDir).map(Vectors.parse)\n val testData = ssc.textFileStream(testDir).map(LabeledPoint.parse)\n```", "```scala\nval model = new StreamingKMeans()\n .setK(numClusters)\n .setDecayFactor(1.0)\n .setRandomCenters(numDimensions, 0.0)\n```", "```scala\nmodel.trainOn(trainingData)\n model.predictOnValues(testData.map(lp => (lp.label, lp.features))).print()\n```", "```scala\nssc.start()\n ssc.awaitTermination()\n```", "```scala\nval ssc = new StreamingContext(conf, Seconds(batchDuration.toLong))\n```", "```scala\nmodel = new StreamingKMeans()\n```"]