["```scala\nval w3 = w1.toBreeze // spark 1.6.x code\nval w4 = w2.toBreeze //spark 1.6.x code\n```", "```scala\nval w3 = new BreezeVector(x.toArray)//x.asBreeze, spark 2.0\nval w4 = new BreezeVector(y.toArray)//y.asBreeze, spark 2.0\n```", "```scala\npackage spark.ml.cookbook.chapter2\n```", "```scala\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\nimport org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\nimport org.apache.spark.sql.{SparkSession}\nimport org.apache.spark.rdd._\nimport org.apache.spark.mllib.linalg._\nimport breeze.linalg.{DenseVector => BreezeVector}\nimport Array._\nimport org.apache.spark.mllib.linalg.DenseMatrix\nimport org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"myVectorMatrix\").setSparkHome(\"C:\\\\spark-1.5.2-bin-hadoop2.6\")\n val sc = new SparkContext(conf)\n val sqlContext = new SQLContext(sc)\n```", "```scala\nval CustomerFeatures1: Array[Double] = Array(1,3,5,7,9,1,3,2,4,5,6,1,2,5,3,7,4,3,4,1)\n val CustomerFeatures2: Array[Double] = Array(2,5,5,8,6,1,3,2,4,5,2,1,2,5,3,2,1,1,1,1)\n val ProductFeatures1: Array[Double]  = Array(0,1,1,0,1,1,1,0,0,1,1,1,1,0,1,2,0,1,1,0)\n```", "```scala\nval x = Vectors.dense(CustomerFeatures1)\n val y = Vectors.dense(CustomerFeatures2)\n val z = Vectors.dense(ProductFeatures1)\n```", "```scala\nval denseVec2 = Vectors.dense(5,3,5,8,5,3,4,2,1,6)\n```", "```scala\nval xyz = Vectors.dense(\"2\".toDouble, \"3\".toDouble, \"4\".toDouble)\n println(xyz)\n```", "```scala\n[2.0,3.0,4.0]\n```", "```scala\nDenseVector (double[] values)\n```", "```scala\ninterface class java.lang.Object\ninterface org.apache.spark.mllib.linalg. Vector\n```", "```scala\nDenseVector copy()\n```", "```scala\nSparseVector toSparse()\n```", "```scala\nInt numNonzeros()\n```", "```scala\nDouble[] toArray()\n```", "```scala\nimport org.apache.spark.sql.{SparkSession}\nimport org.apache.spark.mllib.linalg._\nimport breeze.linalg.{DenseVector => BreezeVector}\nimport Array._\nimport org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval denseVec1 = Vectors.dense(5,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,8,9)\nval sparseVec1 = Vectors.sparse(20, Array(0,2,18,19), Array(5, 3, 8,9))\n```", "```scala\n\nprintln(denseVec1.size)\nprintln(denseVec1.numActives)\nprintln(denseVec1.numNonzeros)\nprintln(\"denceVec1 presentation = \",denseVec1)\n```", "```scala\ndenseVec1.size = 20\n\ndenseVec1.numActives = 20\n\ndenseVec1.numNonzeros = 4\n\n(denseVec1 presentation = ,[5.0,0.0,3.0,0.0,0.0,\n0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.0,9.0])\n\n```", "```scala\nprintln(sparseVec1.size)\nprintln(sparseVec1.numActives)\nprintln(sparseVec1.numNonzeros)\nprintln(\"sparseVec1 presentation = \",sparseVec1)\n```", "```scala\ndenseVec1.size = 20\nprintln(sparseVec1.numActives)= 4\nsparseVec1.numNonzeros = 4\n (sparseVec1 presentation = ,(20,[0,2,18,19],[5.0,3.0,8.0,9.0]))\n```", "```scala\nval ConvertedDenseVect : DenseVector= sparseVec1.toDense\n val ConvertedSparseVect : SparseVector= denseVec1.toSparse\nprintln(\"ConvertedDenseVect =\", ConvertedDenseVect)\n println(\"ConvertedSparseVect =\", ConvertedSparseVect)\n```", "```scala\n(ConvertedDenseVect =,[5.0,0.0,3.0,0.0,0.0,0.0,0.0,0.0,\n0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,8.0,9.0])\n(ConvertedSparseVect =,(20,[0,2,18,19],[5.0,3.0,8.0,9.0])) \n```", "```scala\nSparseVector(int size, int[] indices, double[] values)\n```", "```scala\ninterface class java.lang.Object  \n```", "```scala\nSparseVector Copy() \n```", "```scala\nDenseVector toDense()\n```", "```scala\nInt numNonzeros()\n```", "```scala\nDouble[] toArray() \n```", "```scala\n import org.apache.spark.sql.{SparkSession}\n import org.apache.spark.mllib.linalg._\n import breeze.linalg.{DenseVector => BreezeVector}\n import Array._\n import org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval MyArray1= Array(10.0, 11.0, 20.0, 30.3)\nval denseMat3 = Matrices.dense(2,2,MyArray1)   \n```", "```scala\nDenseMat3=\n10.0  20.0  \n11.0  30.3 \n```", "```scala\nval denseMat1 = Matrices.dense(3,3,Array(23.0, 11.0, 17.0, 34.3, 33.0, 24.5, 21.3,22.6,22.2))\n```", "```scala\n    denseMat1=\n    23.0  34.3  21.3  \n    11.0  33.0  22.6  \n    17.0  24.5  22.2\n\n```", "```scala\nval v1 = Vectors.dense(5,6,2,5)\n val v2 = Vectors.dense(8,7,6,7)\n val v3 = Vectors.dense(3,6,9,1)\n val v4 = Vectors.dense(7,4,9,2)\n\n val Mat11 = Matrices.dense(4,4,v1.toArray ++ v2.toArray ++ v3.toArray ++ v4.toArray)\n println(\"Mat11=\\n\", Mat11)\n```", "```scala\n    Mat11=\n    5.0  8.0  3.0  7.0  \n   6.0  7.0  6.0  4.0  \n   2.0  6.0  9.0  9.0  \n   5.0  7.0  1.0  2.0\n\n```", "```scala\nDenseMatrix(int numRows, int numCols, double[] values)\nDenseMatrix(int numRows, int numCols, double[] values, boolean isTransposed)\n```", "```scala\nstatic DenseMatrix(Vector vector) \n```", "```scala\nstatic eye(int n) \n```", "```scala\nboolean isTransposed()\n```", "```scala\nstatic DenseMatrix rand(int numRows, int numCols, java.util.Random rng) \n```", "```scala\nstatic DenseMatrix randn(int numRows, int numCols, java.util.Random rng) \n```", "```scala\nDenseMatrix transpose() \n```", "```scala\nDenseVector Copy() \n```", "```scala\nSparseVector toSparse() \n```", "```scala\nInt numNonzeros()\n```", "```scala\nDouble[] Values()\n```", "```scala\nval denseMat3 = Matrices.dense(2,2, Array(10.0, 11.0, 20.0, 30.3))\n```", "```scala\n10.0  20.0 \n11.0 30.3\n```", "```scala\n 10.0  11.0 \n 20.0 30.3\n```", "```scala\n import org.apache.spark.mllib.linalg.distributed.RowMatrix\n import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n import org.apache.spark.sql.{SparkSession}\n import org.apache.spark.mllib.linalg._\n import breeze.linalg.{DenseVector => BreezeVector}\n import Array._\n import org.apache.spark.mllib.linalg.DenseMatrix\n import org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\n val sparseMat1= Matrices.sparse(3,2 ,Array(0,1,3), Array(0,1,2), Array(11,22,33))\n```", "```scala\n println(\"Number of Columns=\",sparseMat1.numCols)\n println(\"Number of Rows=\",sparseMat1.numRows)\n println(\"Number of Active elements=\",sparseMat1.numActives)\n println(\"Number of Non Zero elements=\",sparseMat1.numNonzeros)\n println(\"sparseMat1 representation of a sparse matrix and its value=\\n\",sparseMat1)\n```", "```scala\n(Number of Columns=,2)\n(Number of Rows=,3)\n(Number of Active elements=,3)\n(Number of Non Zero elements=,3)\nsparseMat1 representation of a sparse matrix and its value= 3 x 2 CSCMatrix\n(0,0) 11.0\n(1,1) 22.0\n(2,1) 33.0)\n```", "```scala\n/* from documentation page\n 1.0 0.0 4.0\n 0.0 3.0 5.0\n 2.0 0.0 6.0\n *\n */\n //[1.0, 2.0, 3.0, 4.0, 5.0, 6.0], rowIndices=[0, 2, 1, 0, 1, 2], colPointers=[0, 2, 3, 6]\n val sparseMat33= Matrices.sparse(3,3 ,Array(0, 2, 3, 6) ,Array(0, 2, 1, 0, 1, 2),Array(1.0, 2.0, 3.0, 4.0, 5.0, 6.0))\n println(sparseMat33)\n```", "```scala\n3 x 3 CSCMatrix\n(0,0) 1.0\n(2,0) 2.0\n(1,1) 3.0\n(0,2) 4.0\n(1,2) 5.0\n(2,2) 6.0\n```", "```scala\nstatic SparseMatrix spdiag(Vector vector)\n```", "```scala\nstatic speye(int n)\n```", "```scala\nboolean isTransposed()\n```", "```scala\nstatic SparseMatrix sprand(int numRows, int numCols, java.util.Random rng)\n```", "```scala\nstatic SparseMatrix sprandn(int numRows, int numCols, java.util.Random rng)\n```", "```scala\nSparseMatrix transpose()\n```", "```scala\nSparseMatrix Copy()\n```", "```scala\nDenseMatrix toDense()\n```", "```scala\nInt numNonzeros()\n```", "```scala\nDouble[] Values()\n```", "```scala\n import org.apache.spark.mllib.linalg.distributed.RowMatrix\n import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n import org.apache.spark.sql.{SparkSession}\n import org.apache.spark.mllib.linalg._\n import breeze.linalg.{DenseVector => BreezeVector}\n import Array._\n import org.apache.spark.mllib.linalg.DenseMatrix\n import org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval w1 = Vectors.dense(1,2,3)\nval w2 = Vectors.dense(4,-5,6)\n```", "```scala\nval w1 = Vectors.dense(1,2,3)\nval w2 = Vectors.dense(4,-5,6) \nval w3 = new BreezeVector(w1.toArray)//w1.asBreeze\nval w4=  new BreezeVector(w2.toArray)// w2.asBreeze\nprintln(\"w3 + w4 =\",w3+w4)\nprintln(\"w3 - w4 =\",w3+w4)\nprintln(\"w3 * w4 =\",w3.dot(w4)) \n```", "```scala\nw3 + w4 = DenseVector(5.0, -3.0, 9.0)\nw3 - w4 = DenseVector(5.0, -3.0, 9.0)\nw3 * w4 =12.0 \n```", "```scala\nval sv1 = Vectors.sparse(10, Array(0,2,9), Array(5, 3, 13))\nval sv2 = Vectors.dense(1,0,1,1,0,0,1,0,0,13)\nprintln(\"sv1 - Sparse Vector = \",sv1)\nprintln(\"sv2 - Dense Vector = \",sv2)\nprintln(\"sv1 * sv2 =\", new BreezeVector(sv1.toArray).dot(new BreezeVector(sv2.toArray)))\n```", "```scala\nprintln(\"sv1  * sve2  =\", sv1.asBreeze.dot(sv2.asBreeze))\n```", "```scala\nsv1 - Sparse Vector =  (10,[0,2,9],[5.0,3.0,13.0]) \nsv2 - Dense  Vector = [1.0,0.0,1.0,1.0,0.0,0.0,1.0,0.0,0.0,13.0] \nsv1 * sv2 = 177.0\n```", "```scala\nval w1 = Vectors.dense(1,2,3)\n val w2 = Vectors.dense(4,-5,6)\n val w3 = w1.toBreeze\n val w4= w2.toBreeze\n println(\"w3 + w4 =\",w3+w4)\n println(\"w3 - w4 =\",w3+w4)\n println(\"w3 * w4 =\",w3.dot(w4))\n```", "```scala\nprintln(\"sv1 - Sparse Vector = \",sv1)\n println(\"sv2 - Dense Vector = \",sv2)\n println(\"sv1 * sv2 =\", sv1.toBreeze.dot(sv2.toBreeze))\n```", "```scala\n import org.apache.spark.mllib.linalg.distributed.RowMatrix\n import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n import org.apache.spark.sql.{SparkSession}\n import org.apache.spark.mllib.linalg._\n import breeze.linalg.{DenseVector => BreezeVector}\n import Array._\n import org.apache.spark.mllib.linalg.DenseMatrix\n import org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval sparseMat33= Matrices.sparse(3,3 ,Array(0, 2, 3, 6) ,Array(0, 2, 1, 0, 1, 2),Array(1.0, 2.0, 3.0, 4.0, 5.0, 6.0))\nval denseFeatureVector= Vectors.dense(1,2,1)\nval denseVec13 = Vectors.dense(5,3,0)\n```", "```scala\nval result0 = sparseMat33.multiply(denseFeatureVector)\nprintln(\"SparseMat33 =\", sparseMat33)\n println(\"denseFeatureVector =\", denseFeatureVector)\n println(\"SparseMat33 * DenseFeatureVector =\", result0)\n```", "```scala\n(SparseMat33 =,3 x 3 CSCMatrix\n(0,0) 1.0\n(2,0) 2.0\n(1,1) 3.0\n(0,2) 4.0\n(1,2) 5.0\n(2,2) 6.0)\ndenseFeatureVector =,[1.0,2.0,1.0]\nSparseMat33 * DenseFeatureVector = [5.0,11.0,8.0]\n```", "```scala\nprintln(\"denseVec2 =\", denseVec13)\nprintln(\"denseMat1 =\", denseMat1)\nval result3= denseMat1.multiply(denseVec13)\nprintln(\"denseMat1 * denseVect13 =\", result3) \n```", "```scala\n    denseVec2 =,[5.0,3.0,0.0]\n    denseMat1 =  23.0  34.3  21.3  \n                          11.0  33.0  22.6  \n                          17.0  24.5  22.2 \n    denseMat1 * denseVect13 =,[217.89,154.0,158.5]\n\n```", "```scala\nval transposedMat1= sparseMat1.transpose\n println(\"transposedMat1=\\n\",transposedMat1)\n```", "```scala\n\nOriginal sparseMat1 =,3 x 2 CSCMatrix\n(0,0) 11.0\n(1,1) 22.0\n(2,1) 33.0)\n\n(transposedMat1=,2 x 3 CSCMatrix\n(0,0) 11.0\n(1,1) 22.0\n(1,2) 33.0)\n\n1.0  4.0  7.0  \n2.0  5.0  8.0  \n3.0  6.0  9.0\n\n```", "```scala\nval transposedMat1= sparseMat1.transpose\nprintln(\"transposedMat1=\\n\",transposedMat1)         println(\"Transposed twice\", denseMat33.transpose.transpose) // we get the original back\n```", "```scala\nMatrix transposed twice=\n1.0  4.0  7.0  \n2.0  5.0  8.0  \n3.0  6.0  9.0\n\n```", "```scala\nval transposedMat2= denseMat1.transpose\n println(\"Original sparseMat1 =\", denseMat1)\n println(\"transposedMat2=\" ,transposedMat2)\nOriginal sparseMat1 =\n23.0  34.3  21.3  \n11.0  33.0  22.6  \n17.0  24.5  22.2 \ntransposedMat2=\n23.0  11.0  17.0  \n34.3  33.0  24.5  \n21.3  22.6  22.2   \n```", "```scala\n// Matrix multiplication\n val dMat1: DenseMatrix= new DenseMatrix(2, 2, Array(1.0, 3.0, 2.0, 4.0))\n val dMat2: DenseMatrix = new DenseMatrix(2, 2, Array(2.0,1.0,0.0,2.0))\n\n println(\"dMat1 * dMat2 =\", dMat1.multiply(dMat2)) //A x B\n println(\"dMat2 * dMat1 =\", dMat2.multiply(dMat1)) //B x A   not the same as A xB\n```", "```scala\ndMat1 =,1.0  2.0  \n               3.0  4.0  \ndMat2 =,2.0  0.0  \n       1.0 2.0 \ndMat1 * dMat2 =,4.0   4.0  \n                              10.0  8.0\n//Note: A x B is not the same as B x A\ndMat2 * dMat1 = 2.0   4.0   \n                               7.0  10.0\n\n```", "```scala\n import org.apache.spark.mllib.linalg.distributed.RowMatrix\n import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n import org.apache.spark.sql.{SparkSession}\n import org.apache.spark.mllib.linalg._\n import breeze.linalg.{DenseVector => BreezeVector}\n import Array._\n import org.apache.spark.mllib.linalg.DenseMatrix\n import org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nimport Log4J logger and the level\nimport org.apache.log4j.Logger\n import org.apache.log4j.Level\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.WARN)\nLogger.getLogger(\"akka\").setLevel(Level.WARN)\n```", "```scala\nval dataVectors = Seq(\n   Vectors.dense(0.0, 1.0, 0.0),\n   Vectors.dense(3.0, 1.0, 5.0),\n   Vectors.dense(0.0, 7.0, 0.0)\n )\n```", "```scala\nval identityVectors = Seq(\n   Vectors.dense(1.0, 0.0, 0.0),\n   Vectors.dense(0.0, 1.0, 0.0),\n   Vectors.dense(0.0, 0.0, 1.0)\n )\n```", "```scala\nval distMat33 = new RowMatrix(sc.parallelize(dataVectors))\n\n```", "```scala\nprintln(\"distMatt33 columns - Count =\", distMat33.computeColumnSummaryStatistics().count)\n println(\"distMatt33 columns - Mean =\", distMat33.computeColumnSummaryStatistics().mean)\n println(\"distMatt33 columns - Variance =\", distMat33.computeColumnSummaryStatistics().variance)\n println(\"distMatt33 columns - CoVariance =\", distMat33.computeCovariance())\n```", "```scala\n    distMatt33 columns - Count =            3\n    distMatt33 columns - Mean =             [ 1.0, 3.0, 1.66 ]\n    (distMatt33 columns - Variance =      [ 3.0,12.0,8.33 ]\n    (distMatt33 columns - CoVariance = 3.0   -3.0  5.0                \n                                                            -3.0  12.0  -5.0               \n                                                             5.0   -5.0  8.33  \n```", "```scala\nval flatArray = identityVectors.map(x => x.toArray).flatten.toArray \n dd.foreach(println(_))\n```", "```scala\n val dmIdentity: Matrix = Matrices.dense(3, 3, flatArray)\n```", "```scala\nval distMat44 = distMat33.multiply(dmIdentity)\n println(\"distMatt44 columns - Count =\", distMat44.computeColumnSummaryStatistics().count)\n println(\"distMatt44 columns - Mean =\", distMat44.computeColumnSummaryStatistics().mean)\n println(\"distMatt44 columns - Variance =\", distMat44.computeColumnSummaryStatistics().variance)\n println(\"distMatt44 columns - CoVariance =\", distMat44.computeCovariance())\n```", "```scala\ndistMatt44 columns - Count = 3\ndistMatt44 columns - Mean = [ 1.0, 3.0, 1.66 ]\ndistMatt44 columns - Variance = [ 3.0,12.0,8.33 ]\ndistMatt44 columns - CoVariance = 3.0 -3.0 5.0 \n -3.0 12.0 -5.0 \n 5.0 -5.0 8.33\n```", "```scala\n    import org.apache.spark.mllib.linalg.distributed.RowMatrix\n      import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n      import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n      import org.apache.spark.sql.{SparkSession}\n      import org.apache.spark.mllib.linalg._\n      import breeze.linalg.{DenseVector => BreezeVector}\n      import Array._\n      import org.apache.spark.mllib.linalg.DenseMatrix\n      import org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval dataVectors = Seq(\n   Vectors.dense(0.0, 1.0, 0.0),\n   Vectors.dense(3.0, 1.0, 5.0),\n   Vectors.dense(0.0, 7.0, 0.0)\n )\n```", "```scala\n   val distInxMat1 \n = sc.parallelize( List( IndexedRow( 0L, dataVectors(0)), IndexedRow( 1L, dataVectors(1)), IndexedRow( 1L, dataVectors(2)))) \nprintln(\"distinct elements=\", distInxMat1.distinct().count()) \n```", "```scala\n(distinct elements=,3)\n```", "```scala\nList( IndexedRow( 0L, dataVectors(0)), IndexedRow( 1L, dataVectors(1)), IndexedRow( 1L, dataVectors(2)))\n```", "```scala\n import org.apache.spark.mllib.linalg.distributed.RowMatrix\n import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n import org.apache.spark.sql.{SparkSession}\n import org.apache.spark.mllib.linalg._\n import breeze.linalg.{DenseVector => BreezeVector}\n import Array._\n import org.apache.spark.mllib.linalg.DenseMatrix\n import org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval CoordinateEntries = Seq(\n   MatrixEntry(1, 6, 300),\n   MatrixEntry(3, 1, 5),\n   MatrixEntry(1, 7, 10)\n )\n```", "```scala\nval distCordMat1 = new CoordinateMatrix( sc.parallelize(CoordinateEntries.toList)) \n```", "```scala\n println(\"First Row (MatrixEntry) =\",distCordMat1.entries.first())\n```", "```scala\n    First Row (MatrixEntry) =,MatrixEntry(1,6,300.0)\n\n```", "```scala\nMatrixEntry(1, 6, 300), MatrixEntry(3, 1, 5), MatrixEntry(1, 7, 10)\n```", "```scala\nCoordinateMatrix to a BlockMatrix and then do a quick check for its validity and access one of its properties to show that it was set up properly. BlockMatrix code takes longer to set up and it needs a real life application (not enough space) to demonstrate and show its properties in action.\n```", "```scala\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\n import org.apache.spark.mllib.linalg.distributed.{IndexedRow, IndexedRowMatrix}\n import org.apache.spark.mllib.linalg.distributed.{CoordinateMatrix, MatrixEntry}\n import org.apache.spark.sql.{SparkSession}\n import org.apache.spark.mllib.linalg._\n import breeze.linalg.{DenseVector => BreezeVector}\n import Array._\n import org.apache.spark.mllib.linalg.DenseMatrix\n import org.apache.spark.mllib.linalg.SparseVector\n```", "```scala\nval spark = SparkSession\n .builder\n .master(\"local[*]\")\n .appName(\"myVectorMatrix\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\nval distCordMat1 = new CoordinateMatrix( sc.parallelize(CoordinateEntries.toList))\n```", "```scala\nval distBlkMat1 =  distCordMat1.toBlockMatrix().cache() \n```", "```scala\ndistBlkMat1.validate() \nprintln(\"Is block empty =\", distBlkMat1.blocks.isEmpty()) \n```", "```scala\nIs block empty =,false\n```"]