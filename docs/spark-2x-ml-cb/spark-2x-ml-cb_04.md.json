["```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.stat.Statistics\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.Logger\nimport org.apache.log4j.Level\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"Summary Statistics\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval sc = spark.sparkContext\n```", "```scala\nval rdd = sc.parallelize(\n  Seq(\n    Vectors.dense(0, 1, 0),\n    Vectors.dense(1.0, 10.0, 100.0),\n    Vectors.dense(3.0, 30.0, 300.0),\n    Vectors.dense(5.0, 50.0, 500.0),\n    Vectors.dense(7.0, 70.0, 700.0),\n    Vectors.dense(9.0, 90.0, 900.0),\n    Vectors.dense(11.0, 110.0, 1100.0)\n  )\n)\n```", "```scala\nval summary = Statistics.colStats(rdd)\n```", "```scala\nprintln(\"mean:\" + summary.mean)\nprintln(\"variance:\" +summary.variance)\nprintln(\"none zero\" + summary.numNonzeros)\nprintln(\"min:\" + summary.min)\nprintln(\"max:\" + summary.max)\nprintln(\"count:\" + summary.count)\nmean:[5.142857142857142,51.57142857142857,514.2857142857142]\nvariance:[16.80952380952381,1663.952380952381,168095.2380952381]\nnone zero[6.0,7.0,6.0]\nmin:[0.0,1.0,0.0]\nmax:[11.0,110.0,1100.0]\ncount:7\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"My Pipeline\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval trainset = spark.createDataFrame(Seq(\n (1L, 1, \"spark rocks\"),\n (2L, 0, \"flink is the best\"),\n (3L, 1, \"Spark rules\"),\n (4L, 0, \"mapreduce forever\"),\n (5L, 0, \"Kafka is great\")\n )).toDF(\"id\", \"label\", \"words\")\n```", "```scala\nval tokenizer = new Tokenizer()\n .setInputCol(\"words\")\n .setOutputCol(\"tokens\")\n```", "```scala\nval hashingTF = new HashingTF()\n .setNumFeatures(1000)\n .setInputCol(tokenizer.getOutputCol)\n .setOutputCol(\"features\")\n```", "```scala\nval lr = new LogisticRegression()\n .setMaxIter(15)\n .setRegParam(0.01)\n```", "```scala\nval pipeline = new Pipeline()\n .setStages(Array(tokenizer, hashingTF, lr))\n```", "```scala\nval model = pipeline.fit(trainset)\n```", "```scala\nval testSet = spark.createDataFrame(Seq(\n (10L, 1, \"use spark please\"),\n (11L, 2, \"Kafka\")\n )).toDF(\"id\", \"label\", \"words\")\n```", "```scala\nmodel.transform(testSet).select(\"probability\", \"prediction\").show(false)\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nimport org.apache.spark.ml.feature.MinMaxScaler\n```", "```scala\ndef parseWine(str: String): (Int, Vector) = {\nval columns = str.split(\",\")\n(columns(0).toInt, Vectors.dense(columns(1).toFloat, columns(2).toFloat, columns(3).toFloat))\n }\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"My Normalize\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval data = Spark.read.text(\"../data/sparkml2/chapter4/wine.data\").as[String].map(parseWine)\n```", "```scala\nval df = data.toDF(\"id\", \"feature\")\n```", "```scala\ndf.printSchema()\ndf.show(false)\n```", "```scala\nval scale = new MinMaxScaler()\n      .setInputCol(\"feature\")\n      .setOutputCol(\"scaled\")\n      .setMax(1)\n      .setMin(-1)\nscale.fit(df).transform(df).select(\"scaled\").show(false)\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{ Level, Logger}\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"Data Splitting\")\n.getOrCreate()\n```", "```scala\nval data = spark.read.csv(\"../data/sparkml2/chapter4/newsCorpora.csv\")\n```", "```scala\nval rowCount = data.count()\nprintln(\"rowCount=\" + rowCount)\n```", "```scala\nval splitData = data.randomSplit(Array(0.8, 0.2))\n```", "```scala\nval trainingSet = splitData(0)\nval testSet = splitData(1)\n```", "```scala\nval trainingSetCount = trainingSet.count()\nval testSetCount = testSet.count()\n```", "```scala\nprintln(\"trainingSetCount=\" + trainingSetCount)\nprintln(\"testSetCount=\" + testSetCount)\nprintln(\"setRowCount=\" + (trainingSetCount+testSetCount))\nrowCount=415606\ntrainingSetCount=332265\ntestSetCount=83341\nsetRowCount=415606\n```", "```scala\nspark.stop()\n```", "```scala\ndef randomSplit(weights: Array[Double]): Array[JavaRDD[T]]\n```", "```scala\nname,city\nBears,Chicago\nPackers,Green Bay\nLions,Detroit\nVikings,Minnesota\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nimport org.apache.spark.sql.SparkSession\nimport org.apache.log4j.{Level, Logger}\n```", "```scala\ncase class Team(name: String, city: String)\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nLogger.getLogger(\"akka\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"My Dataset\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n```", "```scala\nval champs = spark.createDataset(List(Team(\"Broncos\", \"Denver\"), Team(\"Patriots\", \"New England\")))\nchamps.show(false)\n```", "```scala\nval teams = spark.read\n .option(\"Header\", \"true\")\n .csv(\"../data/sparkml2/chapter4/teams.csv\")\n .as[Team]\n\n teams.show(false)\n```", "```scala\nval cities = teams.map(t => t.city)\ncities.show(false)\n```", "```scala\ncities.explain()\n== Physical Plan ==\n*SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, input[0, java.lang.String, true], true) AS value#26]\n+- *MapElements <function1>, obj#25: java.lang.String\n+- *DeserializeToObject newInstance(class Team), obj#24: Team\n+- *Scan csv [name#9,city#10] Format: CSV, InputPaths: file:teams.csv, PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string,city:string>\n```", "```scala\nteams.write\n.mode(SaveMode.Overwrite)\n.json(\"../data/sparkml2/chapter4/teams.json\"){\"name\":\"Bears\",\"city\":\"Chicago\"}\n{\"name\":\"Packers\",\"city\":\"Green Bay\"}\n{\"name\":\"Lions\",\"city\":\"Detroit\"}\n{\"name\":\"Vikings\",\"city\":\"Minnesota\"}\n```", "```scala\nspark.stop()\n```", "```scala\nDataset: spark.read.textFile()\nRDD: spark.sparkContext.textFile()\nDataFrame: spark.read.text()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\n```", "```scala\ncase class Beatle(id: Long, name: String)\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"DatasetvsRDD\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n\nval ds = spark.read.textFile(\"../data/sparkml2/chapter4/beatles.txt\").map(line => {\nval tokens = line.split(\",\")\nBeatle(tokens(0).toLong, tokens(1))\n}).as[Beatle]\n```", "```scala\nprintln(\"Dataset Type: \" + ds.getClass)\nds.show()\n```", "```scala\nDataset Type: class org.apache.spark.sql.Dataset\n```", "```scala\nval rdd = spark.sparkContext.textFile(\"../data/sparkml2/chapter4/beatles.txt\").map(line => {\nval tokens = line.split(\",\")\nBeatle(tokens(0).toLong, tokens(1))\n })\n```", "```scala\nprintln(\"RDD Type: \" + rdd.getClass)\nrdd.collect().foreach(println)\n```", "```scala\nRDD Type: class org.apache.spark.rdd.MapPartitionsRDD\nBeatle(1,John)\nBeatle(2,Paul)\nBeatle(3,George)\nBeatle(4,Ringo)\n```", "```scala\nval df = spark.read.text(\"../data/sparkml2/chapter4/beatles.txt\").map(\n row => { // Dataset[Row]\nval tokens = row.getString(0).split(\",\")\n Beatle(tokens(0).toLong, tokens(1))\n }).toDF(\"bid\", \"bname\")\n```", "```scala\n println(\"DataFrame Type: \" + df.getClass)\n df.show()\n```", "```scala\nDataFrame Type: class org.apache.spark.sql.Dataset\n```", "```scala\nspark.stop()\n```", "```scala\nDataset: spark.read.textFile\nRDD: spark.sparkContext.textFile\nDataFrame: spark.read.text\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.ml.feature.LabeledPoint\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.ml.classification.LogisticRegression\nimport org.apache.spark.sql._\n\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myLabeledPoint\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval myLabeledPoints = spark.createDataFrame(Seq(\n LabeledPoint(1.0, Vectors.dense(0.0, 1.1, 0.1)),\n LabeledPoint(0.0, Vectors.dense(2.0, 1.0, -1.0)),\n LabeledPoint(0.0, Vectors.dense(2.0, 1.3, 1.0)),\n LabeledPoint(1.0, Vectors.dense(0.0, 1.2, -0.5)),\n\n LabeledPoint(0.0, Vectors.sparse(3, Array(0,2), Array(1.0,3.0))),\n LabeledPoint(1.0, Vectors.sparse(3, Array(1,2), Array(1.2,-0.4)))\n\n ))\n```", "```scala\nmyLabeledPoints.show()\n```", "```scala\nval lr = new LogisticRegression()\n\n lr.setMaxIter(5)\n .setRegParam(0.01)\n val model = lr.fit(myLabeledPoints)\n\n println(\"Model was fit using parameters: \" + model.parent.extractParamMap())\n```", "```scala\nModel was fit using parameters: {\n logreg_6aebbb683272-elasticNetParam: 0.0,\n logreg_6aebbb683272-featuresCol: features,\n logreg_6aebbb683272-fitIntercept: true,\n logreg_6aebbb683272-labelCol: label,\n logreg_6aebbb683272-maxIter: 5,\n logreg_6aebbb683272-predictionCol: prediction,\n logreg_6aebbb683272-probabilityCol: probability,\n logreg_6aebbb683272-rawPredictionCol: rawPrediction,\n logreg_6aebbb683272-regParam: 0.01,\n logreg_6aebbb683272-standardization: true,\n logreg_6aebbb683272-threshold: 0.5,\n logreg_6aebbb683272-tol: 1.0E-6\n}\n```", "```scala\nspark.stop()\n```", "```scala\nSeq( \nLabeledPoint (Label, Vector(data, data, data)) \n...... \nLabeledPoint (Label, Vector(data, data, data)) \n) \n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\") // if use cluster master(\"spark://master:7077\")\n.appName(\"myAccesSparkCluster20\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nmaster(\"local\") \n```", "```scala\nmaster(\"spark://yourmasterhostIP:port\") \n```", "```scala\n -Dspark.master=local\n```", "```scala\nval df = spark.read\n       .option(\"header\",\"True\")\n       .csv(\"../data/sparkml2/chapter4/mySampleCSV.csv\")\n```", "```scala\ndf.show()\n```", "```scala\nspark.stop()\n```", "```scala\nDef version: String\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.{SparkConf, SparkContext}\n```", "```scala\nval conf = new SparkConf()\n.setAppName(\"MyAccessSparkClusterPre20\")\n.setMaster(\"local[4]\") // if cluster setMaster(\"spark://MasterHostIP:7077\")\n.set(\"spark.sql.warehouse.dir\", \".\")\n\nval sc = new SparkContext(conf)\n```", "```scala\nsetMaster(\"local\")\n```", "```scala\nsetMaster(\"spark://yourmasterhostIP:port\")\n```", "```scala\n-Dspark.master=local\n```", "```scala\nval file = sc.textFile(\"../data/sparkml2/chapter4/mySampleCSV.csv\")\nval headerAndData = file.map(line => line.split(\",\").map(_.trim))\nval header = headerAndData.first\nval data = headerAndData.filter(_(0) != header(0))\nval maps = data.map(splits => header.zip(splits).toMap)\n```", "```scala\nval result = maps.take(4)\nresult.foreach(println)\n```", "```scala\nsc.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\nimport scala.util.Random\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nval session = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"SessionContextRDD\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport session.implicits._\n\n // SparkContext\nval context = session.sparkContext\n```", "```scala\nprintln(\"SparkContext\")\n\nval rdd1 = context.makeRDD(Random.shuffle(1 to 10).toList)\nrdd1.collect().foreach(println)\nprintln(\"-\" * 45)\n\nval rdd2 = context.parallelize(Random.shuffle(20 to 30).toList)\nrdd2.collect().foreach(println)\nprintln(\"\\n End of SparkContext> \" + (\"-\" * 45))\n```", "```scala\nSparkContext\n4\n6\n1\n10\n5\n2\n7\n3\n9\n8\n```", "```scala\n25\n28\n30\n29\n20\n22\n27\n23\n24\n26\n21\n End of SparkContext\n```", "```scala\nval dataset1 = session.range(40, 50)\n dataset1.show()\n\nval dataset2 = session.createDataset(Random.shuffle(60 to 70).toList)\n dataset2.show()\n```", "```scala\n// retrieve underlying RDD from Dataset\nval rdd3 = dataset2.rdd\nrdd3.collect().foreach(println)\n```", "```scala\n61\n68\n62\n67\n70\n64\n69\n65\n60\n66\n63\n```", "```scala\n// convert rdd to Dataset\nval rdd4 = context.makeRDD(Random.shuffle(80 to 90).toList)\nval dataset3 = session.createDataset(rdd4)\ndataset3.show()\n```", "```scala\nsession.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.clustering.KMeans\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")   // if use cluster master(\"spark://master:7077\")\n.appName(\"myPMMLExport\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval data = spark.sparkContext.textFile(\"../data/sparkml2/chapter4/my_kmeans_data_sample.txt\")\n\nval parsedData = data.map(s => Vectors.dense(s.split(' ').map(_.toDouble))).cache()\n```", "```scala\nval numClusters = 2\nval numIterations = 10\nval model = KMeans.train(parsedData, numClusters, numIterations)\n```", "```scala\nprintln(\"MyKMeans PMML Model:\\n\" + model.toPMML)\n```", "```scala\nMyKMeans PMML Model:\n<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"yes\"?>\n<PMML version=\"4.2\" >\n    <Header description=\"k-means clustering\">\n        <Application name=\"Apache Spark MLlib\" version=\"2.0.0\"/>\n        <Timestamp>2016-11-06T13:34:57</Timestamp>\n    </Header>\n    <DataDictionary numberOfFields=\"3\">\n        <DataField name=\"field_0\" optype=\"continuous\" dataType=\"double\"/>\n        <DataField name=\"field_1\" optype=\"continuous\" dataType=\"double\"/>\n        <DataField name=\"field_2\" optype=\"continuous\" dataType=\"double\"/>\n    </DataDictionary>\n    <ClusteringModel modelName=\"k-means\" functionName=\"clustering\" modelClass=\"centerBased\" numberOfClusters=\"2\">\n        <MiningSchema>\n            <MiningField name=\"field_0\" usageType=\"active\"/>\n            <MiningField name=\"field_1\" usageType=\"active\"/>\n            <MiningField name=\"field_2\" usageType=\"active\"/>\n        </MiningSchema>\n        <ComparisonMeasure kind=\"distance\">\n            <squaredEuclidean/>\n        </ComparisonMeasure>\n        <ClusteringField field=\"field_0\" compareFunction=\"absDiff\"/>\n        <ClusteringField field=\"field_1\" compareFunction=\"absDiff\"/>\n        <ClusteringField field=\"field_2\" compareFunction=\"absDiff\"/>\n        <Cluster name=\"cluster_0\">\n            <Array n=\"3\" type=\"real\">9.06 9.179999999999998 9.12</Array>\n        </Cluster>\n        <Cluster name=\"cluster_1\">\n            <Array n=\"3\" type=\"real\">0.11666666666666665 0.11666666666666665 0.13333333333333333</Array>\n        </Cluster>\n    </ClusteringModel>\n</PMML>\n```", "```scala\nmodel.toPMML(\"../data/sparkml2/chapter4/myKMeansSamplePMML.xml\")\n```", "```scala\nspark.stop()\n```", "```scala\nModel_a.toPMML(\"/xyz/model-name.xml\")\n```", "```scala\nModel_a.toPMML(SparkContext, \"/xyz/model-name\")\n```", "```scala\nModel_a.toPMML(System.out)\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.tree.DecisionTree\nimport org.apache.spark.sql.SparkSession\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myRegressionMetrics\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval rawData = spark.sparkContext.textFile(\"../data/sparkml2/chapter4/breast-cancer-wisconsin.data\")\nval data = rawData.map(_.trim)\n   .filter(text => !(text.isEmpty || text.indexOf(\"?\") > -1))\n   .map { line =>\n val values = line.split(',').map(_.toDouble)\n val slicedValues = values.slice(1, values.size)\n val featureVector = Vectors.dense(slicedValues.init)\n val label = values.last / 2 -1\n      LabeledPoint(label, featureVector)\n\n   }\n```", "```scala\nval splits = data.randomSplit(Array(0.7, 0.3))\nval (trainingData, testData) = (splits(0), splits(1))\n```", "```scala\nval categoricalFeaturesInfo = Map[Int, Int]()\nval impurity = \"variance\" val maxDepth = 5\nval maxBins = 32\n\nval model = DecisionTree.trainRegressor(trainingData, categoricalFeaturesInfo, impurity,\nmaxDepth, maxBins)\nval predictionsAndLabels = testData.map(example =>\n(model.predict(example.features), example.label)\n)\n```", "```scala\nval metrics = new RegressionMetrics(predictionsAndLabels)\n```", "```scala\n// Squared error\nprintln(s\"MSE = ${metrics.meanSquaredError}\")\n println(s\"RMSE = ${metrics.rootMeanSquaredError}\")\n\n // R-squared\nprintln(s\"R-squared = ${metrics.r2}\")\n\n // Mean absolute error\nprintln(s\"MAE = ${metrics.meanAbsoluteError}\")\n\n // Explained variance\nprintln(s\"Explained variance = ${metrics.explainedVariance}\")\n```", "```scala\nMSE = 0.06071332254584681\nRMSE = 0.2464007356844675\nR-squared = 0.7444017305996473\nMAE = 0.0691747572815534\nExplained variance = 0.22591111058744653\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myBinaryClassification\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\n// Load training data in LIBSVM format\n//https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary.html\nval data = MLUtils.loadLibSVMFile(spark.sparkContext, \"../data/sparkml2/chapter4/myBinaryClassificationData.txt\")\n```", "```scala\nval Array(training, test) = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n training.cache()\n\n // Run training algorithm to build the model\nval model = new LogisticRegressionWithLBFGS()\n .setNumClasses(2)\n .run(training)\n```", "```scala\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n val prediction = model.predict(features)\n (prediction, label)\n }\n```", "```scala\nval metrics = new BinaryClassificationMetrics(predictionAndLabels)\n```", "```scala\nval precision = metrics.precisionByThreshold\n precision.foreach { case (t, p) =>\n println(s\"Threshold: $t, Precision: $p\")\n }\n```", "```scala\nThreshold: 2.9751613212299755E-210, Precision: 0.5405405405405406\nThreshold: 1.0, Precision: 0.4838709677419355\nThreshold: 1.5283665404870175E-268, Precision: 0.5263157894736842\nThreshold: 4.889258814400478E-95, Precision: 0.5\n```", "```scala\nval recall = metrics.recallByThreshold\n recall.foreach { case (t, r) =>\n println(s\"Threshold: $t, Recall: $r\")\n }\n```", "```scala\nThreshold: 1.0779893231660571E-300, Recall: 0.6363636363636364\nThreshold: 6.830452412352692E-181, Recall: 0.5151515151515151\nThreshold: 0.0, Recall: 1.0\nThreshold: 1.1547199216963482E-194, Recall: 0.5757575757575758\n```", "```scala\nval f1Score = metrics.fMeasureByThreshold\n f1Score.foreach { case (t, f) =>\n println(s\"Threshold: $t, F-score: $f, Beta = 1\")\n }\n```", "```scala\nThreshold: 1.0, F-score: 0.46874999999999994, Beta = 1\nThreshold: 4.889258814400478E-95, F-score: 0.49230769230769234, Beta = 1\nThreshold: 2.2097791212639423E-117, F-score: 0.48484848484848486, Beta = 1\n\nval beta = 0.5\nval fScore = metrics.fMeasureByThreshold(beta)\nf1Score.foreach { case (t, f) =>\n  println(s\"Threshold: $t, F-score: $f, Beta = 0.5\")\n}\n```", "```scala\nThreshold: 2.9751613212299755E-210, F-score: 0.5714285714285714, Beta = 0.5\nThreshold: 1.0, F-score: 0.46874999999999994, Beta = 0.5\nThreshold: 1.5283665404870175E-268, F-score: 0.5633802816901409, Beta = 0.5\nThreshold: 4.889258814400478E-95, F-score: 0.49230769230769234, Beta = 0.5\n```", "```scala\nval auPRC = metrics.areaUnderPR\nprintln(\"Area under precision-recall curve = \" + auPRC)\n```", "```scala\nArea under precision-recall curve = 0.5768388996048239\n```", "```scala\nval thresholds = precision.map(_._1)\n\nval roc = metrics.roc\n\nval auROC = metrics.areaUnderROC\nprintln(\"Area under ROC = \" + auROC)\n```", "```scala\nArea under ROC = 0.6983957219251337\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.util.MLUtils\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myMulticlass\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\n// Load training data in LIBSVM format\n//https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/multiclass.html\nval data = MLUtils.loadLibSVMFile(spark.sparkContext, \"../data/sparkml2/chapter4/myMulticlassIrisData.txt\")\n```", "```scala\nval Array(training, test) = data.randomSplit(Array(0.6, 0.4), seed = 11L)\n training.cache()\n\n // Run training algorithm to build the model\nval model = new LogisticRegressionWithLBFGS()\n .setNumClasses(3)\n .run(training)\n```", "```scala\nval predictionAndLabels = test.map { case LabeledPoint(label, features) =>\n val prediction = model.predict(features)\n (prediction, label)\n }\n```", "```scala\nval metrics = new MulticlassMetrics(predictionAndLabels)\n```", "```scala\nprintln(\"Confusion matrix:\")\nprintln(metrics.confusionMatrix)\n```", "```scala\nConfusion matrix: \n18.0 0.0 0.0 \n0.0 15.0 8.0 \n0.0 0.0 22.0\n```", "```scala\nval accuracy = metrics.accuracy\nprintln(\"Summary Statistics\")\nprintln(s\"Accuracy = $accuracy\")\n```", "```scala\nSummary Statistics\nAccuracy = 0.873015873015873\n```", "```scala\nval labels = metrics.labels\nlabels.foreach { l =>\n println(s\"Precision($l) = \" + metrics.precision(l))\n }\n```", "```scala\nPrecision(0.0) = 1.0\nPrecision(1.0) = 1.0\nPrecision(2.0) = 0.7333333333333333\n```", "```scala\nlabels.foreach { l =>\nprintln(s\"Recall($l) = \" + metrics.recall(l))\n }\n```", "```scala\nRecall(0.0) = 1.0\nRecall(1.0) = 0.6521739130434783\nRecall(2.0) = 1.0\n```", "```scala\nlabels.foreach { l =>\n println(s\"FPR($l) = \" + metrics.falsePositiveRate(l))\n }\n```", "```scala\nFPR(0.0) = 0.0\nFPR(1.0) = 0.0\nFPR(2.0) = 0.1951219512195122\n```", "```scala\nlabels.foreach { l =>\n println(s\"F1-Score($l) = \" + metrics.fMeasure(l))\n }\n```", "```scala\nF1-Score(0.0) = 1.0\nF1-Score(1.0) = 0.7894736842105263\nF1-Score(2.0) = 0.846153846153846\n```", "```scala\nprintln(s\"Weighted precision: ${metrics.weightedPrecision}\")\n println(s\"Weighted recall: ${metrics.weightedRecall}\")\n println(s\"Weighted F1 score: ${metrics.weightedFMeasure}\")\n println(s\"Weighted false positive rate: ${metrics.weightedFalsePositiveRate}\")\n```", "```scala\nWeighted precision: 0.9068783068783068\nWeighted recall: 0.873015873015873\nWeighted F1 score: 0.8694171325750273\nWeighted false positive rate: 0.06813782423538521\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.mllib.evaluation.MultilabelMetrics\nimport org.apache.spark.rdd.RDD\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myMultilabel\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval data: RDD[(Array[Double], Array[Double])] = spark.sparkContext.parallelize(\nSeq((Array(0.0, 1.0), Array(0.1, 2.0)),\n     (Array(0.0, 2.0), Array(0.1, 1.0)),\n     (Array.empty[Double], Array(0.0)),\n     (Array(2.0), Array(2.0)),\n     (Array(2.0, 0.0), Array(2.0, 0.0)),\n     (Array(0.0, 1.0, 2.0), Array(0.0, 1.0)),\n     (Array(1.0), Array(1.0, 2.0))), 2)\n```", "```scala\nval metrics = new MultilabelMetrics(data)\n```", "```scala\nprintln(s\"Recall = ${metrics.recall}\")\nprintln(s\"Precision = ${metrics.precision}\")\nprintln(s\"F1 measure = ${metrics.f1Measure}\")\nprintln(s\"Accuracy = ${metrics.accuracy}\")\n```", "```scala\nRecall = 0.5\nPrecision = 0.5238095238095238\nF1 measure = 0.4952380952380952\nAccuracy = 0.4523809523809524\n```", "```scala\nmetrics.labels.foreach(label =>\n println(s\"Class $label precision = ${metrics.precision(label)}\"))\n metrics.labels.foreach(label => println(s\"Class $label recall = ${metrics.recall(label)}\"))\n metrics.labels.foreach(label => println(s\"Class $label F1-score = ${metrics.f1Measure(label)}\"))\n```", "```scala\nClass 0.0 precision = 0.5\nClass 1.0 precision = 0.6666666666666666\nClass 2.0 precision = 0.5\nClass 0.0 recall = 0.6666666666666666\nClass 1.0 recall = 0.6666666666666666\nClass 2.0 recall = 0.5\nClass 0.0 F1-score = 0.5714285714285715\nClass 1.0 F1-score = 0.6666666666666666\nClass 2.0 F1-score = 0.5\n```", "```scala\nprintln(s\"Micro recall = ${metrics.microRecall}\")\nprintln(s\"Micro precision = ${metrics.microPrecision}\")\nprintln(s\"Micro F1 measure = ${metrics.microF1Measure}\")\nFrom the console output:\nMicro recall = 0.5\nMicro precision = 0.5454545454545454\nMicro F1 measure = 0.5217391304347826\n```", "```scala\nprintln(s\"Hamming loss = ${metrics.hammingLoss}\")\nprintln(s\"Subset accuracy = ${metrics.subsetAccuracy}\")\nFrom the console output:\nHamming loss = 0.39285714285714285\nSubset accuracy = 0.2857142857142857\n```", "```scala\nspark.stop()\n```", "```scala\npackage spark.ml.cookbook.chapter4\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\nimport breeze.plot._\n\nimport scala.util.Random\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n```", "```scala\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"myBreezeChart\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nimport spark.implicits._\n\nval fig = Figure()\nval chart = fig.subplot(0)\n\nchart.title = \"My Breeze-Viz Chart\" chart.xlim(21,100)\nchart.ylim(0,100000)\n```", "```scala\nval ages = spark.createDataset(Random.shuffle(21 to 100).toList.take(45)).as[Int]\n\n ages.show(false)\n```", "```scala\nval x = ages.collect()\nval y = Random.shuffle(20000 to 100000).toList.take(45)\n\nval x2 = ages.collect().map(xx => xx.toDouble)\nval y2 = x2.map(xx => (1000 * xx) + (xx * 2))\n\nchart += scatter(x, y, _ => 0.5)\nchart += plot(x2, y2)\n\nchart.xlabel = \"Age\" chart.ylabel = \"Income\" fig.refresh()\n```", "```scala\nspark.stop()\n```"]