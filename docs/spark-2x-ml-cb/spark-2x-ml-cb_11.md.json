["```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.sql.SparkSession\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n\n val spark = SparkSession\n .builder\n.master(\"local[*]\")\n .appName(\"MyCSV\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\n```", "```scala\n// 1\\. load the csv file as text file\nval dataFile = \"../data/sparkml2/chapter11/ratings.csv\"\nval file = spark.sparkContext.textFile(dataFile)\n```", "```scala\nval headerAndData = file.map(line => line.split(\",\").map(_.trim))\n val header = headerAndData.first\n val data = headerAndData.filter(_(0) != header(0))\n val maps = data.map(splits => header.zip(splits).toMap)\n val result = maps.take(10)\n result.foreach(println)\n```", "```scala\nMap(userId -> 1, movieId -> 16, rating -> 4.0, timestamp -> 1217897793)\nMap(userId -> 1, movieId -> 24, rating -> 1.5, timestamp -> 1217895807)\nMap(userId -> 1, movieId -> 32, rating -> 4.0, timestamp -> 1217896246)\nMap(userId -> 1, movieId -> 47, rating -> 4.0, timestamp -> 1217896556)\nMap(userId -> 1, movieId -> 50, rating -> 4.0, timestamp -> 1217896523)\nMap(userId -> 1, movieId -> 110, rating -> 4.0, timestamp -> 1217896150)\nMap(userId -> 1, movieId -> 150, rating -> 3.0, timestamp -> 1217895940)\nMap(userId -> 1, movieId -> 161, rating -> 4.0, timestamp -> 1217897864)\nMap(userId -> 1, movieId -> 165, rating -> 3.0, timestamp -> 1217897135)\nMap(userId -> 1, movieId -> 204, rating -> 0.5, timestamp -> 1217895786)\n```", "```scala\n// 2\\. load the csv file using databricks package\nval df = spark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(dataFile)\n```", "```scala\ndf.createOrReplaceTempView(\"ratings\")\n val resDF = spark.sql(\"select * from ratings\")\n resDF.show(10, false)\n```", "```scala\nspark.stop()\n```", "```scala\nval spark = SparkSession\n .builder\n.master(\"local[*]\")\n .appName(\"MyCSV\")\n .config(\"spark.sql.warehouse.dir\", \".\")\n .getOrCreate()\nspark.sparkContext.textFile(dataFile)\nspark.sparkContext.textFile(dataFile)\n```", "```scala\nspark.read.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(dataFile)\n```", "```scala\n1::1193::5::978300760\n1::661::3::978302109\n1::914::3::978301968\n1::3408::4::978300275\n1::2355::5::978824291\n1::1197::3::978302268\n1::1287::5::978302039\n1::2804::5::978300719\n1::594::4::978302268\n1::919::4::978301368\n1::595::5::978824268\n1::938::4::978301752\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.sql.SparkSession\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\n\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"MySVD\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()    \n\n```", "```scala\nval dataFile = \"../data/sparkml2/chapter11/ratings.dat\" //read data file in as a RDD, partition RDD across <partitions> cores\nval data = spark.sparkContext.textFile(dataFile)\n```", "```scala\n//parse data and create (user, item, rating) tuplesval ratingsRDD = data\n   .map(line => line.split(\"::\"))\n   .map(fields => (fields(0).toInt, fields(1).toInt, fields(2).toDouble))\n```", "```scala\nval items = ratingsRDD.map(x => x._2).distinct()\nval maxIndex = items.max + 1\n```", "```scala\nval userItemRatings = ratingsRDD.map(x => (x._1, ( x._2, x._3))).groupByKey().cache()\n userItemRatings.take(2).foreach(println)\n```", "```scala\n(4904,CompactBuffer((2054,4.0), (588,4.0), (589,5.0), (3000,5.0), (1,5.0), ..., (3788,5.0)))\n(1084,CompactBuffer((2058,3.0), (1258,4.0), (588,2.0), (589,4.0), (1,3.0), ..., (1242,4.0)))\n```", "```scala\nval sparseVectorData = userItemRatings\n .map(a=>(a._1.toLong, Vectors.sparse(maxIndex,a._2.toSeq))).sortByKey()\n\n sparseVectorData.take(2).foreach(println)\n```", "```scala\n(1,(3953,[1,48,150,260,527,531,588,...], [5.0,5.0,5.0,4.0,5.0,4.0,4.0...]))\n(2,(3953,[21,95,110,163,165,235,265,...],[1.0,2.0,5.0,4.0,3.0,3.0,4.0,...]))\n```", "```scala\nval rows = sparseVectorData.map{\n a=> a._2\n }\n```", "```scala\nval mat = new RowMatrix(rows)\nval col = 10 //number of leading singular values\nval computeU = true\nval svd = mat.computeSVD(col, computeU)\n```", "```scala\nprintln(\"Singular values are \" + svd.s)\nprintln(\"V:\" + svd.V)\n```", "```scala\nspark.stop()\n```", "```scala\nvalmat = new RowMatrix(rows)\nval col = 10 //number of leading singular values\nval computeU = true\nval svd = mat.computeSVD(col, computeU)\n```", "```scala\nimport org.apache.log4j.{Level, Logger}\nimport org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.linalg.Vectors\nimport org.apache.spark.sql.SparkSession\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.ERROR)\nval spark = SparkSession\n.builder\n.master(\"local[*]\")\n.appName(\"MyPCA\")\n.config(\"spark.sql.warehouse.dir\", \".\")\n.getOrCreate()\n```", "```scala\nval dataFile = \"../data/sparkml2/chapter11/processed.cleveland.data\"\nval rawdata = spark.sparkContext.textFile(dataFile).map(_.trim)\nprintln(rawdata.count())\n```", "```scala\n303\n```", "```scala\nval data = rawdata.filter(text => !(text.isEmpty || text.indexOf(\"?\") > -1))\n .map { line =>\n val values = line.split(',').map(_.toDouble)\n\n Vectors.dense(values)\n }\n\n println(data.count())\n\ndata.take(2).foreach(println)\n```", "```scala\n297\n```", "```scala\n[63.0,1.0,1.0,145.0,233.0,1.0,2.0,150.0,0.0,2.3,3.0,0.0,6.0,0.0]\n[67.0,1.0,4.0,160.0,286.0,0.0,2.0,108.0,1.0,1.5,2.0,3.0,3.0,2.0]\n```", "```scala\nval df = sqlContext.createDataFrame(data.map(Tuple1.apply)).toDF(\"features\")\nval pca = new PCA()\n.setInputCol(\"features\")\n.setOutputCol(\"pcaFeatures\")\n.setK(4)\n.fit(df)\n```", "```scala\nval pcaDF = pca.transform(df)\nval result = pcaDF.select(\"pcaFeatures\")\nresult.show(false)\n```", "```scala\nspark.stop()\n```", "```scala\nval pca = new PCA()\n .setInputCol(\"features\")\n .setOutputCol(\"pcaFeatures\")\n .setK(4)\n .fit(df)\n```"]