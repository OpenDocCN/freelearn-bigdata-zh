["```py\nforest_path = '../data/forest_coverage_type.csv'\n\nforest = spark.read.csv(\n    forest_path\n    , header=True\n    , inferSchema=True\n)\n```", "```py\nHorizontal_Distance_To_Hydrology column into 10 equidistant buckets:\n```", "```py\nimport pyspark.sql.functions as f\nimport pyspark.ml.feature as feat\nimport numpy as np\n\nbuckets_no = 10\n\ndist_min_max = (\n    forest.agg(\n          f.min('Horizontal_Distance_To_Hydrology')\n            .alias('min')\n        , f.max('Horizontal_Distance_To_Hydrology')\n            .alias('max')\n    )\n    .rdd\n    .map(lambda row: (row.min, row.max))\n    .collect()[0]\n)\n\nrng = dist_min_max[1] - dist_min_max[0]\n\nsplits = list(np.arange(\n    dist_min_max[0]\n    , dist_min_max[1]\n    , rng / (buckets_no + 1)))\n\nbucketizer = feat.Bucketizer(\n    splits=splits\n    , inputCol= 'Horizontal_Distance_To_Hydrology'\n    , outputCol='Horizontal_Distance_To_Hydrology_Bkt'\n)\n\n(\n    bucketizer\n    .transform(forest)\n    .select(\n         'Horizontal_Distance_To_Hydrology'\n        ,'Horizontal_Distance_To_Hydrology_Bkt'\n    ).show(5)\n)\n```", "```py\nvectorAssembler = (\n    feat.VectorAssembler(\n        inputCols=forest.columns, \n        outputCol='feat'\n    )\n)\n\npca = (\n    feat.PCA(\n        k=5\n        , inputCol=vectorAssembler.getOutputCol()\n        , outputCol='pca_feat'\n    )\n)\n\n(\n    pca\n    .fit(vectorAssembler.transform(forest))\n    .transform(vectorAssembler.transform(forest))\n    .select('feat','pca_feat')\n    .take(1)\n)\n```", "```py\nimport pyspark.ml.classification as cl\n\nvectorAssembler = feat.VectorAssembler(\n    inputCols=forest.columns[0:-1]\n    , outputCol='features')\n\nfir_dataset = (\n    vectorAssembler\n    .transform(forest)\n    .withColumn(\n        'label'\n        , (f.col('CoverType') == 1).cast('integer'))\n    .select('label', 'features')\n)\n\nsvc_obj = cl.LinearSVC(maxIter=10, regParam=0.01)\nsvc_model = svc_obj.fit(fir_dataset)\n```", "```py\nimport pyspark.ml.regression as rg\n\nvectorAssembler = feat.VectorAssembler(\n    inputCols=forest.columns[1:]\n    , outputCol='features')\n\nelevation_dataset = (\n    vectorAssembler\n    .transform(forest)\n    .withColumn(\n        'label'\n        , f.col('Elevation').cast('float'))\n    .select('label', 'features')\n)\n\nlr_obj = rg.LinearRegression(\n    maxIter=10\n    , regParam=0.01\n    , elasticNetParam=1.00)\nlr_model = lr_obj.fit(elevation_dataset)\n```", "```py\nsummary = lr_model.summary\n\nprint(\n    summary.r2\n    , summary.rootMeanSquaredError\n    , summary.meanAbsoluteError\n)\n```", "```py\nfrom pyspark.ml import Pipeline\n\nvectorAssembler = feat.VectorAssembler(\n    inputCols=forest.columns[1:]\n    , outputCol='features')\n\nlr_obj = rg.GeneralizedLinearRegression(\n    labelCol='Elevation'\n    , maxIter=10\n    , regParam=0.01\n    , link='identity'\n    , linkPredictionCol=\"p\"\n)\n\npip = Pipeline(stages=[vectorAssembler, lr_obj])\n\n(\n    pip\n    .fit(forest)\n    .transform(forest)\n    .select('Elevation', 'prediction')\n    .show(5)\n)\n```", "```py\nelevation_dataset = (\n    vectorAssembler\n    .transform(forest)\n    .withColumn(\n        'label'\n        , f.col('Elevation').cast('float'))\n    .select('label', 'features')\n)\n```", "```py\nimport matplotlib.pyplot as plt\n\ntransformed_df = forest.select('Elevation')\ntransformed_df.toPandas().hist()\n\nplt.savefig('Elevation_histogram.png')\n\nplt.close('all')\n```", "```py\nvectorAssembler = feat.VectorAssembler(\n    inputCols=forest.columns[0:-1]\n    , outputCol='features'\n)\n\nselector = feat.ChiSqSelector(\n    labelCol='CoverType'\n    , numTopFeatures=10\n    , outputCol='selected')\n\npipeline_sel = Pipeline(stages=[vectorAssembler, selector])\n```", "```py\n(\n    pipeline_sel\n    .fit(forest)\n    .transform(forest)\n    .select(selector.getOutputCol())\n    .show(5)\n)\n```", "```py\nimport pyspark.ml.stat as st\n\nfeatures_and_label = feat.VectorAssembler(\n    inputCols=forest.columns\n    , outputCol='features'\n)\n\ncorr = st.Correlation.corr(\n    features_and_label.transform(forest), \n    'features', \n    'pearson'\n)\n\nprint(str(corr.collect()[0][0]))\n```", "```py\nnum_of_features = 10\ncols = dict([\n    (i, e) \n    for i, e \n    in enumerate(forest.columns)\n])\n\ncorr_matrix = corr.collect()[0][0]\nlabel_corr_with_idx = [\n    (i[0], e) \n    for i, e \n    in np.ndenumerate(corr_matrix.toArray()[:,0])\n][1:]\n\nlabel_corr_with_idx_sorted = sorted(\n    label_corr_with_idx\n    , key=lambda el: -abs(el[1])\n)\n\nfeatures_selected = np.array([\n    cols[el[0]] \n    for el \n    in label_corr_with_idx_sorted\n])[0:num_of_features]\n```", "```py\nforest_train, forest_test = (\n    forest\n    .randomSplit([0.7, 0.3], seed=666)\n)\n\nvectorAssembler = feat.VectorAssembler(\n    inputCols=forest.columns[0:-1]\n    , outputCol='features'\n)\n\nselector = feat.ChiSqSelector(\n    labelCol='CoverType'\n    , numTopFeatures=10\n    , outputCol='selected'\n)\n\nlogReg_obj = cl.LogisticRegression(\n    labelCol='CoverType'\n    , featuresCol=selector.getOutputCol()\n    , regParam=0.01\n    , elasticNetParam=1.0\n    , family='multinomial'\n)\n\npipeline = Pipeline(\n    stages=[\n        vectorAssembler\n        , selector\n        , logReg_obj\n    ])\n\npModel = pipeline.fit(forest_train)\n```", "```py\nimport pyspark.ml.evaluation as ev\n\nresults_logReg = (\n    pModel\n    .transform(forest_test)\n    .select('CoverType', 'probability', 'prediction')\n)\n\nevaluator = ev.MulticlassClassificationEvaluator(\n    predictionCol='prediction'\n    , labelCol='CoverType')\n\n(\n    evaluator.evaluate(results_logReg)\n    , evaluator.evaluate(\n        results_logReg\n        , {evaluator.metricName: 'weightedPrecision'}\n    ) \n    , evaluator.evaluate(\n        results_logReg\n        , {evaluator.metricName: 'accuracy'}\n    )\n)\n```", "```py\nrf_obj = cl.RandomForestClassifier(\n    labelCol='CoverType'\n    , featuresCol=selector.getOutputCol()\n    , minInstancesPerNode=10\n    , numTrees=10\n)\n\npipeline = Pipeline(\n    stages=[vectorAssembler, selector, rf_obj]\n)\n\npModel = pipeline.fit(forest_train)\n```", "```py\nresults_rf = (\n    pModel\n    .transform(forest_test)\n    .select('CoverType', 'probability', 'prediction')\n)\n\n(\n    evaluator.evaluate(results_rf)\n    , evaluator.evaluate(\n        results_rf\n        , {evaluator.metricName: 'weightedPrecision'}\n    )\n    , evaluator.evaluate(\n        results_rf\n        , {evaluator.metricName: 'accuracy'}\n    )\n)\n```", "```py\nvectorAssembler = feat.VectorAssembler(\n    inputCols=forest.columns[1:]\n    , outputCol='features')\n\nrf_obj = rg.RandomForestRegressor(\n    labelCol='Elevation'\n    , maxDepth=10\n    , minInstancesPerNode=10\n    , minInfoGain=0.1\n    , numTrees=10\n)\n\npip = Pipeline(stages=[vectorAssembler, rf_obj])\n```", "```py\nresults = (\n    pip\n    .fit(forest)\n    .transform(forest)\n    .select('Elevation', 'prediction')\n)\n\nevaluator = ev.RegressionEvaluator(labelCol='Elevation')\nevaluator.evaluate(results, {evaluator.metricName: 'r2'})\n```", "```py\ngbt_obj = rg.GBTRegressor(\n    labelCol='Elevation'\n    , minInstancesPerNode=10\n    , minInfoGain=0.1\n)\n\npip = Pipeline(stages=[vectorAssembler, gbt_obj])\n```", "```py\nresults = (\n    pip\n    .fit(forest)\n    .transform(forest)\n    .select('Elevation', 'prediction')\n)\n\nevaluator = ev.RegressionEvaluator(labelCol='Elevation')\nevaluator.evaluate(results, {evaluator.metricName: 'r2'})\n```", "```py\nimport pyspark.ml.clustering as clust\n\nvectorAssembler = feat.VectorAssembler(\n    inputCols=forest.columns[:-1]\n    , outputCol='features')\n\nkmeans_obj = clust.KMeans(k=7, seed=666)\n\npip = Pipeline(stages=[vectorAssembler, kmeans_obj])\n```", "```py\nresults = (\n    pip\n    .fit(forest)\n    .transform(forest)\n    .select('features', 'CoverType', 'prediction')\n)\n\nresults.show(5)\n```", "```py\nclustering_ev = ev.ClusteringEvaluator()\nclustering_ev.evaluate(results)\n```", "```py\nimport pyspark.ml.tuning as tune\n\nvectorAssembler = feat.VectorAssembler(\n    inputCols=forest.columns[0:-1]\n    , outputCol='features')\n\nselector = feat.ChiSqSelector(\n    labelCol='CoverType'\n    , numTopFeatures=5\n    , outputCol='selected')\n\nlogReg_obj = cl.LogisticRegression(\n    labelCol='CoverType'\n    , featuresCol=selector.getOutputCol()\n    , family='multinomial'\n)\n\nlogReg_grid = (\n    tune.ParamGridBuilder()\n    .addGrid(logReg_obj.regParam\n            , [0.01, 0.1]\n        )\n    .addGrid(logReg_obj.elasticNetParam\n            , [1.0, 0.5]\n        )\n    .build()\n)\n\nlogReg_ev = ev.MulticlassClassificationEvaluator(\n    predictionCol='prediction'\n    , labelCol='CoverType')\n\ncross_v = tune.CrossValidator(\n    estimator=logReg_obj\n    , estimatorParamMaps=logReg_grid\n    , evaluator=logReg_ev\n)\n\npipeline = Pipeline(stages=[vectorAssembler, selector])\ndata_trans = pipeline.fit(forest_train)\n\nlogReg_modelTest = cross_v.fit(\n    data_trans.transform(forest_train)\n)\n```", "```py\ndata_trans_test = data_trans.transform(forest_test)\nresults = logReg_modelTest.transform(data_trans_test)\n\nprint(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedPrecision'}))\nprint(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedRecall'}))\nprint(logReg_ev.evaluate(results, {logReg_ev.metricName: 'accuracy'}))\n```", "```py\ntrain_v = tune.TrainValidationSplit(\n    estimator=logReg_obj\n    , estimatorParamMaps=logReg_grid\n    , evaluator=logReg_ev\n    , parallelism=4\n)\n\nlogReg_modelTrainV = (\n    train_v\n    .fit(data_trans.transform(forest_train))\n\nresults = logReg_modelTrainV.transform(data_trans_test)\n\nprint(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedPrecision'}))\nprint(logReg_ev.evaluate(results, {logReg_ev.metricName: 'weightedRecall'}))\nprint(logReg_ev.evaluate(results, {logReg_ev.metricName: 'accuracy'}))\n```", "```py\nsome_text = spark.createDataFrame([\n    ['''\n    Apache Spark achieves high performance for both batch\n    and streaming data, using a state-of-the-art DAG scheduler, \n    a query optimizer, and a physical execution engine.\n    ''']\n    , ['''\n    Apache Spark is a fast and general-purpose cluster computing \n    system. It provides high-level APIs in Java, Scala, Python \n    and R, and an optimized engine that supports general execution \n    graphs. It also supports a rich set of higher-level tools including \n    Spark SQL for SQL and structured data processing, MLlib for machine \n    learning, GraphX for graph processing, and Spark Streaming.\n    ''']\n    , ['''\n    Machine learning is a field of computer science that often uses \n    statistical techniques to give computers the ability to \"learn\" \n    (i.e., progressively improve performance on a specific task) \n    with data, without being explicitly programmed.\n    ''']\n], ['text'])\n\nsplitter = feat.RegexTokenizer(\n    inputCol='text'\n    , outputCol='text_split'\n    , pattern='\\s+|[,.\\\"]'\n)\n\nsw_remover = feat.StopWordsRemover(\n    inputCol=splitter.getOutputCol()\n    , outputCol='no_stopWords'\n)\n\nhasher = feat.HashingTF(\n    inputCol=sw_remover.getOutputCol()\n    , outputCol='hashed'\n    , numFeatures=20\n)\n\nidf = feat.IDF(\n    inputCol=hasher.getOutputCol()\n    , outputCol='features'\n)\n\npipeline = Pipeline(stages=[splitter, sw_remover, hasher, idf])\n\npipelineModel = pipeline.fit(some_text)\n```", "```py\nw2v = feat.Word2Vec(\n    vectorSize=5\n    , minCount=2\n    , inputCol=sw_remover.getOutputCol()\n    , outputCol='vector'\n)\n```", "```py\nsignal_df = spark.read.csv(\n    '../data/fourier_signal.csv'\n    , header=True\n    , inferSchema=True\n)\n\nsteps = feat.QuantileDiscretizer(\n       numBuckets=10,\n       inputCol='signal',\n       outputCol='discretized')\n\ntransformed = (\n    steps\n    .fit(signal_df)\n    .transform(signal_df)\n)\n```", "```py\nvec = feat.VectorAssembler(\n    inputCols=['signal']\n    , outputCol='signal_vec'\n)\n\nnorm = feat.StandardScaler(\n    inputCol=vec.getOutputCol()\n    , outputCol='signal_norm'\n    , withMean=True\n    , withStd=True\n)\n\nnorm_pipeline = Pipeline(stages=[vec, norm])\nsignal_norm = (\n    norm_pipeline\n    .fit(signal_df)\n    .transform(signal_df)\n)\n```", "```py\narticles = spark.createDataFrame([\n    ('''\n        The Andromeda Galaxy, named after the mythological \n        Princess Andromeda, also known as Messier 31, M31, \n        or NGC 224, is a spiral galaxy approximately 780 \n        kiloparsecs (2.5 million light-years) from Earth, \n        and the nearest major galaxy to the Milky Way. \n        Its name stems from the area of the sky in which it \n        appears, the constellation of Andromeda. The 2006 \n        observations by the Spitzer Space Telescope revealed \n        that the Andromeda Galaxy contains approximately one \n        trillion stars, more than twice the number of the \n        Milky Way\u2019s estimated 200-400 billion stars. The \n        Andromeda Galaxy, spanning approximately 220,000 light \n        years, is the largest galaxy in our Local Group, \n        which is also home to the Triangulum Galaxy and \n        other minor galaxies. The Andromeda Galaxy's mass is \n        estimated to be around 1.76 times that of the Milky \n        Way Galaxy (~0.8-1.5\u00d71012 solar masses vs the Milky \n        Way's 8.5\u00d71011 solar masses).\n    ''','Galaxy', 'Andromeda')\n    (...) \n    , ('''\n        Washington, officially the State of Washington, is a state in the Pacific \n        Northwest region of the United States. Named after George Washington, \n        the first president of the United States, the state was made out of the \n        western part of the Washington Territory, which was ceded by Britain in \n        1846 in accordance with the Oregon Treaty in the settlement of the \n        Oregon boundary dispute. It was admitted to the Union as the 42nd state \n        in 1889\\. Olympia is the state capital. Washington is sometimes referred \n        to as Washington State, to distinguish it from Washington, D.C., the \n        capital of the United States, which is often shortened to Washington.\n    ''','Geography', 'Washington State') \n], ['articles', 'Topic', 'Object'])\n\nsplitter = feat.RegexTokenizer(\n    inputCol='articles'\n    , outputCol='articles_split'\n    , pattern='\\s+|[,.\\\"]'\n)\n\nsw_remover = feat.StopWordsRemover(\n    inputCol=splitter.getOutputCol()\n    , outputCol='no_stopWords'\n)\n\ncount_vec = feat.CountVectorizer(\n    inputCol=sw_remover.getOutputCol()\n    , outputCol='vector'\n)\n\nlda_clusters = clust.LDA(\n    k=3\n    , optimizer='online'\n    , featuresCol=count_vec.getOutputCol()\n)\n\ntopic_pipeline = Pipeline(\n    stages=[\n        splitter\n        , sw_remover\n        , count_vec\n        , lda_clusters\n    ]\n)\n```", "```py\nfor topic in ( \n        topic_pipeline\n        .fit(articles)\n        .transform(articles)\n        .select('Topic','Object','topicDistribution')\n        .take(10)\n):\n    print(\n        topic.Topic\n        , topic.Object\n        , np.argmax(topic.topicDistribution)\n        , topic.topicDistribution\n    )\n```"]