["```py\ngit clone git@github.com:drabastomek/PySparkCookbook.git\n```", "```py\n#!/bin/bash\n\n# Shell script for checking the dependencies \n#\n# PySpark Cookbook\n# Author: Tomasz Drabas, Denny Lee\n# Version: 0.1\n# Date: 12/2/2017\n\n_java_required=1.8\n_python_required=3.4\n_r_required=3.1\n_scala_required=2.11\n_mvn_required=3.3.9\n\n# parse command line arguments\n_args_len=\"$#\"\n...\n\nprintHeader\ncheckJava\ncheckPython\n\nif [ \"${_check_R_req}\" = \"true\" ]; then\n checkR\nfi\n\nif [ \"${_check_Scala_req}\" = \"true\" ]; then\n checkScala\nfi\n\nif [ \"${_check_Maven_req}\" = \"true\" ]; then\n checkMaven\nfi\n```", "```py\nif [ \"$_args_len\" -ge 0 ]; then\n  while [[ \"$#\" -gt 0 ]]\n  do\n   key=\"$1\"\n   case $key in\n    -m|--Maven)\n    _check_Maven_req=\"true\"\n    shift # past argument\n    ;;\n    -r|--R)\n    _check_R_req=\"true\"\n    shift # past argument\n    ;;\n    -s|--Scala)\n    _check_Scala_req=\"true\"\n    shift # past argument\n    ;;\n    *)\n    shift # past argument\n   esac\n  done\nfi\n```", "```py\n./checkRequirements.sh -s -m -r\n```", "```py\n./checkRequirements.sh --Scala --Maven --R\n```", "```py\nfunction checkJava() {\n echo\n echo \"##########################\"\n echo\n echo \"Checking Java\"\n echo\n```", "```py\nif type -p java; then\n echo \"Java executable found in PATH\"\n _java=java\nelif [[ -n \"$JAVA_HOME\" ]] && [[ -x \"$JAVA_HOME/bin/java\" ]]; then\n echo \"Found Java executable in JAVA_HOME\"\n _java=\"$JAVA_HOME/bin/java\"\nelse\n echo \"No Java found. Install Java version $_java_required or higher first or specify JAVA_HOME variable that will point to your Java binaries.\"\n exit\nfi\n```", "```py\n_java_version=$(\"$_java\" -version 2>&1 | awk -F '\"' '/version/ {print $2}')\necho \"Java version: $_java_version (min.: $_java_required)\"\n\nif [[ \"$_java_version\" < \"$_java_required\" ]]; then\n echo \"Java version required is $_java_required. Install the required version first.\"\n exit\nfi\n echo\n```", "```py\ntar -xvf sbt-1.0.4.tgz\nsudo mv sbt-1.0.4/ /opt/scala/\n```", "```py\ntar -xvf apache-maven-3.5.2-bin.tar.gz\n```", "```py\nsudo mv apache-maven-3.5.2-bin/ /opt/apache-maven/\n```", "```py\ncp ~/.bash_profile ~/.bash_profile_old   # make a copy just in case\n```", "```py\necho export SCALA_HOME=/opt/scala >> ~/.bash_profile\n```", "```py\necho export MAVEN_HOME=/opt/apache-maven >> ~/.bash_profile\n```", "```py\necho PATH=$SCALA_HOME/bin:$MAVEN_HOME/bin:$PATH >> ~/.bash_profile\n```", "```py\ncp ~/.bashrc ~/.bashrc_old   # make a copy just in case\n```", "```py\necho export SCALA_HOME=/opt/scala >> ~/.bashrc\n```", "```py\necho export MAVEN_HOME=/opt/apache-maven >> ~/.bashrc\n```", "```py\necho PATH=$SCALA_HOME/bin:$MAVEN_HOME/bin:$PATH >> ~/.bashrc\n```", "```py\necho $PATH\n```", "```py\n#!/bin/bash\n```", "```py\n# Shell script for installing Spark from sources\n#\n# PySpark Cookbook\n# Author: Tomasz Drabas, Denny Lee\n# Version: 0.1\n# Date: 12/2/2017\n```", "```py\n_spark_source=\"http://mirrors.ocf.berkeley.edu/apache/spark/spark-2.3.1/spark-2.3.1.tgz\"\n_spark_archive=$( echo \"$_spark_source\" | awk -F '/' '{print $NF}' )\n_spark_dir=$( echo \"${_spark_archive%.*}\" )\n_spark_destination=\"/opt/spark\"\n```", "```py\n...\n```", "```py\ncheckOS\nprintHeader\ndownloadThePackage\nunpack\nbuild\nmoveTheBinaries\nsetSparkEnvironmentVariables\ncleanUp\n```", "```py\nfunction checkOS(){\n _uname_out=\"$(uname -s)\"\n case \"$_uname_out\" in\n   Linux*) _machine=\"Linux\";;\n   Darwin*) _machine=\"Mac\";;\n   *) _machine=\"UNKNOWN:${_uname_out}\"\n esac\n```", "```py\n if [ \"$_machine\" = \"UNKNOWN:${_uname_out}\" ]; then\n   echo \"Machine $_machine. Stopping.\"\n   exit\n fi\n}\n```", "```py\nfunction downloadThePackage() {\n ...\n if [ -d _temp ]; then\n    sudo rm -rf _temp\n fi\n```", "```py\n mkdir _temp \n cd _temp\n```", "```py\n if [ \"$_machine\" = \"Mac\" ]; then\n    curl -O $_spark_source\n elif [ \"$_machine\" = \"Linux\"]; then\n    wget $_spark_source\n else\n    echo \"System: $_machine not supported.\"\n    exit\n fi\n```", "```py\n}\n```", "```py\nfunction build(){\n ...\n```", "```py\n cd \"$_spark_dir\"\n ./dev/make-distribution.sh --name pyspark-cookbook -Phadoop-2.7 -Phive -Phive-thriftserver -Pyarn\n```", "```py\n}\n```", "```py\nfunction moveTheBinaries() {\n```", "```py\n ...\n if [ -d \"$_spark_destination\" ]; then \n    sudo rm -rf \"$_spark_destination\"\n fi\n```", "```py\n cd ..\n sudo mv $_spark_dir/ $_spark_destination/\n```", "```py\n}\n```", "```py\nfunction setSparkEnvironmentVariables() {\n ...\n```", "```py\n if [ \"$_machine\" = \"Mac\" ]; then\n    _bash=~/.bash_profile\n else\n    _bash=~/.bashrc\n fi\n _today=$( date +%Y-%m-%d )\n```", "```py\n # make a copy just in case \n if ! [ -f \"$_bash.spark_copy\" ]; then\n        cp \"$_bash\" \"$_bash.spark_copy\"\n fi\n```", "```py\n echo >> $_bash \n echo \"###################################################\" >> $_bash\n echo \"# SPARK environment variables\" >> $_bash\n echo \"#\" >> $_bash\n echo \"# Script: installFromSource.sh\" >> $_bash\n echo \"# Added on: $_today\" >>$_bash\n echo >> $_bash\n```", "```py\n echo \"export SPARK_HOME=$_spark_destination\" >> $_bash\n echo \"export PYSPARK_SUBMIT_ARGS=\\\"--master local[4]\\\"\" >> $_bash\n echo \"export PYSPARK_PYTHON=$(type -p python)\" >> $_bash\n echo \"export PYSPARK_DRIVER_PYTHON=jupyter\" >> $_bash\n```", "```py\n echo \"export PYSPARK_DRIVER_PYTHON_OPTS=\\\"notebook --NotebookApp.open_browser=False --NotebookApp.port=6661\\\"\" >> $_bash\n\n echo \"export PATH=$SPARK_HOME/bin:\\$PATH\" >> $_bash\n}\n```", "```py\nsource ~/.bash_profile\n```", "```py\nsource ~/.bashrc\n```", "```py\npyspark --version\n```", "```py\n./build/mvn clean package\n```", "```py\n./build/mvn -Pyarn -Phadoop-2.7 -Dhadoop.version=2.7.2 -DskipTests clean package\n```", "```py\n./build/sbt package\n```", "```py\n#!/bin/bash\n```", "```py\n# Shell script for installing Spark from binaries\n```", "```py\n\n#\n# PySpark Cookbook\n# Author: Tomasz Drabas, Denny Lee\n# Version: 0.1\n# Date: 12/2/2017\n```", "```py\n_spark_binary=\"http://mirrors.ocf.berkeley.edu/apache/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\"\n_spark_archive=$( echo \"$_spark_binary\" | awk -F '/' '{print $NF}' )\n_spark_dir=$( echo \"${_spark_archive%.*}\" )\n_spark_destination=\"/opt/spark\"\n```", "```py\n...\n```", "```py\ncheckOS\nprintHeader\ndownloadThePackage\nunpack\nmoveTheBinaries\nsetSparkEnvironmentVariables\ncleanUp\n```", "```py\npip install pyspark\n```", "```py\npip3 install pyspark\n```", "```py\nspark = SparkSession.builder \\\n    .master(\"local[2]\") \\\n    .appName(\"Your-app-name\") \\\n    .config(\"spark.some.config.option\", \"some-value\") \\\n    .getOrCreate() \n```", "```py\ndriver:\n192.168.17.160  pathfinder\n```", "```py\nexecutors:\n192.168.17.161  discovery1\n192.168.17.162  discovery2\n```", "```py\n#!/bin/bash\n```", "```py\n# Shell script for installing Spark from binaries\n# on remote servers\n#\n# PySpark Cookbook\n# Author: Tomasz Drabas, Denny Lee\n# Version: 0.1\n# Date: 12/9/2017\n```", "```py\n_spark_binary=\"http://mirrors.ocf.berkeley.edu/apache/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\"\n_spark_archive=$( echo \"$_spark_binary\" | awk -F '/' '{print $NF}' )\n_spark_dir=$( echo \"${_spark_archive%.*}\" )\n_spark_destination=\"/opt/spark\"\n_java_destination=\"/usr/lib/jvm/java-8-oracle\"\n\n_python_binary=\"https://repo.continuum.io/archive/Anaconda3-5.0.1-Linux-x86_64.sh\"\n```", "```py\n\n_python_archive=$( echo \"$_python_binary\" | awk -F '/' '{print $NF}' )\n_python_destination=\"/opt/python\"\n```", "```py\n_machine=$(cat /etc/hostname)\n_today=$( date +%Y-%m-%d )\n```", "```py\n_current_dir=$(pwd) # store current working directory\n```", "```py\n...\n```", "```py\nprintHeader\nreadIPs\ncheckJava\ninstallScala\ninstallPython\nupdateHosts\nconfigureSSH\ndownloadThePackage\nunpack\nmoveTheBinaries\nsetSparkEnvironmentVariables\nupdateSparkConfig\ncleanUp\n```", "```py\nfunction readIPs() {\n input=\"./hosts.txt\"\n```", "```py\n driver=0\n executors=0\n _executors=\"\"\n\n IFS=''\n while read line\n do\n```", "```py\n if [[ \"$master\" = \"1\" ]]; then\n    _driverNode=\"$line\"\n    driver=0\n fi\n```", "```py\n if [[ \"$slaves\" = \"1\" ]]; then\n   _executors=$_executors\"$line\\n\"\n fi\n```", "```py\n if [[ \"$line\" = \"driver:\" ]]; then\n    driver=1\n    executors=0\n fi\n```", "```py\n if [[ \"$line\" = \"executors:\" ]]; then\n    executors=1\n    driver=0\n fi\n```", "```py\n if [[ -z \"${line}\" ]]; then\n     continue\n fi\n done < \"$input\"\n}\n```", "```py\nfunction checkJava() {\n if type -p java; then\n    echo \"Java executable found in PATH\"\n    _java=java\n elif [[ -n \"$JAVA_HOME\" ]] && [[ -x \"$JAVA_HOME/bin/java\" ]]; then\n    echo \"Found Java executable in JAVA_HOME\"\n    _java=\"$JAVA_HOME/bin/java\"\n else\n    echo \"No Java found. Install Java version $_java_required or higher first or specify JAVA_HOME     variable that will point to your Java binaries.\"\n    installJava\n fi\n}\n```", "```py\nfunction installJava() {\n sudo apt-get install python-software-properties\n sudo add-apt-repository ppa:webupd8team/java\n sudo apt-get update\n sudo apt-get install oracle-java8-installer\n}\n```", "```py\nfunction installScala() {\n sudo apt-get install scala\n}\n\nfunction installPython() {\n curl -O \"$_python_binary\"\n chmod 0755 ./\"$_python_archive\"\n sudo bash ./\"$_python_archive\" -b -u -p \"$_python_destination\"\n}\n```", "```py\nfunction updateHosts() {\n```", "```py\n _hostsFile=\"/etc/hosts\"\n```", "```py\n # make a copy (if one already doesn't exist)\n if ! [ -f \"/etc/hosts.old\" ]; then\n    sudo cp \"$_hostsFile\" /etc/hosts.old\n fi\n```", "```py\n t=\"###################################################\\n\"\n t=$t\"#\\n\"\n t=$t\"# IPs of the Spark cluster machines\\n\"\n t=$t\"#\\n\"\n t=$t\"# Script: installOnRemote.sh\\n\"\n t=$t\"# Added on: $_today\\n\"\n t=$t\"#\\n\"\n t=$t\"$_driverNode\\n\"\n t=$t\"$_executors\\n\"\n```", "```py\n sudo printf \"$t\" >> $_hostsFile\n```", "```py\n}\n```", "```py\nfunction configureSSH() {\n    # check if driver node\n    IFS=\" \"\n    read -ra temp <<< \"$_driverNode\"\n    _driver_machine=( ${temp[1]} )\n    _all_machines=\"$_driver_machine\\n\"\n\n    if [ \"$_driver_machine\" = \"$_machine\" ]; then\n        # generate key pairs (passwordless)\n        sudo -u hduser rm -f ~/.ssh/id_rsa\n        sudo -u hduser ssh-keygen -t rsa -P \"\" -f ~/.ssh/id_rsa\n\n        IFS=\"\\n\"\n        read -ra temp <<< \"$_executors\"\n        for executor in ${temp[@]}; do \n            # skip if empty line\n            if [[ -z \"${executor}\" ]]; then\n                continue\n            fi\n\n            # split on space\n            IFS=\" \"\n            read -ra temp_inner <<< \"$executor\"\n            echo\n            echo \"Trying to connect to ${temp_inner[1]}\"\n\n            cat ~/.ssh/id_rsa.pub | ssh \"hduser\"@\"${temp_inner[1]}\" 'mkdir -p .ssh && cat >> .ssh/authorized_keys'\n\n            _all_machines=$_all_machines\"${temp_inner[1]}\\n\"\n        done\n    fi\n\n    echo \"Finishing up the SSH configuration\"\n}\n```", "```py\nfunction updateSparkConfig() {\n    cd $_spark_destination/conf\n\n    sudo -u hduser cp spark-env.sh.template spark-env.sh\n    echo \"export JAVA_HOME=$_java_destination\" >> spark-env.sh\n    echo \"export SPARK_WORKER_CORES=12\" >> spark-env.sh\n\n    sudo -u hduser cp slaves.template slaves\n    printf \"$_all_machines\" >> slaves\n}\n```", "```py\nssh -tq hduser@pathfinder \"echo $(base64 -i installOnRemote.sh) | base64 -d | sudo bash\"\n```", "```py\nssh -tq hduser@pathfinder \"echo $(base64 -w0 installOnRemote.sh) | base64 -d | sudo bash\"\n```", "```py\nsource ~/.bashrc\n```", "```py\nstart-all.sh\n```", "```py\njps\n```", "```py\n40334 Master\n41297 Worker\n41058 Worker\n```", "```py\ncurl -O https://bootstrap.pypa.io/get-pip.py\n```", "```py\nwget https://bootstrap.pypa.io/get-pip.py\n```", "```py\npython get-pip.py\n```", "```py\npip install jupyter\n```", "```py\nconda install jupyter\n```", "```py\npyspark\n```", "```py\n#!/bin/bash\n\n# Shell script for installing Spark from binaries \n#\n# PySpark Cookbook\n# Author: Tomasz Drabas, Denny Lee\n# Version: 0.1\n# Date: 12/2/2017\n\n_livy_binary=\"http://mirrors.ocf.berkeley.edu/apache/incubator/livy/0.4.0-incubating/livy-0.4.0-incubating-bin.zip\"\n_livy_archive=$( echo \"$_livy_binary\" | awk -F '/' '{print $NF}' )\n_livy_dir=$( echo \"${_livy_archive%.*}\" )\n_livy_destination=\"/opt/livy\"\n_hadoop_destination=\"/opt/hadoop\"\n...\ncheckOS\nprintHeader\ncreateTempDir\ndownloadThePackage $( echo \"${_livy_binary}\" )\nunpack $( echo \"${_livy_archive}\" )\nmoveTheBinaries $( echo \"${_livy_dir}\" ) $( echo \"${_livy_destination}\" ) \n# create log directory inside the folder\nmkdir -p \"$_livy_destination/logs\"\n\ncheckHadoop\ninstallJupyterKernels\nsetSparkEnvironmentVariables\ncleanUp\n```", "```py\nfunction checkHadoop() {\n    if type -p hadoop; then\n        echo \"Hadoop executable found in PATH\"\n        _hadoop=hadoop\n    elif [[ -n \"$HADOOP_HOME\" ]] && [[ -x \"$HADOOP_HOME/bin/hadoop\" ]]; then\n        echo \"Found Hadoop executable in HADOOP_HOME\"\n        _hadoop=\"$HADOOP_HOME/bin/hadoop\"\n    else\n        echo \"No Hadoop found. You should install Hadoop first. You can still continue but some functionality might not be available. \"\n        echo \n        echo -n \"Do you want to install the latest version of Hadoop? [y/n]: \"\n        read _install_hadoop\n\n        case \"$_install_hadoop\" in\n            y*) installHadoop ;;\n            n*) echo \"Will not install Hadoop\" ;;\n            *)  echo \"Will not install Hadoop\" ;;\n        esac\n    fi\n}\n\nfunction installHadoop() {\n    _hadoop_binary=\"http://mirrors.ocf.berkeley.edu/apache/hadoop/common/hadoop-2.9.0/hadoop-2.9.0.tar.gz\"\n    _hadoop_archive=$( echo \"$_hadoop_binary\" | awk -F '/' '{print $NF}' )\n    _hadoop_dir=$( echo \"${_hadoop_archive%.*}\" )\n    _hadoop_dir=$( echo \"${_hadoop_dir%.*}\" )\n\n    downloadThePackage $( echo \"${_hadoop_binary}\" )\n```", "```py\n\n    unpack $( echo \"${_hadoop_archive}\" )\n    moveTheBinaries $( echo \"${_hadoop_dir}\" ) $( echo \"${_hadoop_destination}\" )\n}\n```", "```py\nfunction installJupyterKernels() {\n    # install the library \n    pip install sparkmagic\n    echo\n\n    # ipywidgets should work properly\n    jupyter nbextension enable --py --sys-prefix widgetsnbextension \n    echo\n\n    # install kernels\n    # get the location of sparkmagic\n    _sparkmagic_location=$(pip show sparkmagic | awk -F ':' '/Location/ {print $2}') \n\n    _temp_dir=$(pwd) # store current working directory\n\n    cd $_sparkmagic_location # move to the sparkmagic folder\n    jupyter-kernelspec install sparkmagic/kernels/sparkkernel\n    jupyter-kernelspec install sparkmagic/kernels/pysparkkernel\n    jupyter-kernelspec install sparkmagic/kernels/pyspark3kernel\n\n    echo\n\n    # enable the ability to change clusters programmatically\n    jupyter serverextension enable --py sparkmagic\n    echo\n\n    # install autowizwidget\n    pip install autovizwidget\n\n    cd $_temp_dir\n}\n```", "```py\njupyter-kernelspec install sparkmagic/kernels/sparkkernel \n```", "```py\njupyter notebook\n```", "```py\nfrom pyspark.sql.types import *\n\n# Generate our data \nListRDD = sc.parallelize([\n    (123, 'Skye', 19, 'brown'), \n    (223, 'Rachel', 22, 'green'), \n    (333, 'Albert', 23, 'blue')\n])\n\n# The schema is encoded using StructType \nschema = StructType([\n    StructField(\"id\", LongType(), True), \n    StructField(\"name\", StringType(), True),\n    StructField(\"age\", LongType(), True),\n    StructField(\"eyeColor\", StringType(), True)\n])\n\n# Apply the schema to the RDD and create DataFrame\ndrivers = spark.createDataFrame(ListRDD, schema)\n\n# Creates a temporary view using the data frame\ndrivers.createOrReplaceTempView(\"drivers\")\n```"]