["```py\ncensus_path = '../data/census_income.csv'\n\ncensus = spark.read.csv(\n    census_path\n    , header=True\n    , inferSchema=True\n)\n```", "```py\ncensus.printSchema()\n```", "```py\nimport pyspark.sql.functions as func\n\nfor col, typ in census.dtypes:\n    if typ == 'string':\n        census = census.withColumn(\n            col\n            , func.ltrim(func.rtrim(census[col]))\n        )\n```", "```py\ncols_to_keep = census.dtypes\n\ncols_to_keep = (\n    ['label','age'\n     ,'capital-gain'\n     ,'capital-loss'\n     ,'hours-per-week'\n    ] + [\n        e[0] for e in cols_to_keep[:-1] \n        if e[1] == 'string'\n    ]\n)\n```", "```py\ncensus_subset = census.select(cols_to_keep)\n\ncols_num = [\n    e[0] for e in census_subset.dtypes \n    if e[1] == 'int'\n]\ncols_cat = [\n    e[0] for e in census_subset.dtypes[1:] \n    if e[1] == 'string'\n]\n```", "```py\nimport pyspark.mllib.stat as st\nimport numpy as np\n\nrdd_num = (\n    census_subset\n    .select(cols_num)\n    .rdd\n    .map(lambda row: [e for e in row])\n)\n\nstats_num = st.Statistics.colStats(rdd_num)\n\nfor col, min_, mean_, max_, var_ in zip(\n      cols_num\n    , stats_num.min()\n    , stats_num.mean()\n    , stats_num.max()\n    , stats_num.variance()\n):\n    print('{0}: min->{1:.1f}, mean->{2:.1f}, max->{3:.1f}, stdev->{4:.1f}'\n          .format(col, min_, mean_, max_, np.sqrt(var_)))\n```", "```py\nrdd_cat = (\n    census_subset\n    .select(cols_cat + ['label'])\n    .rdd\n    .map(lambda row: [e for e in row])\n)\n\nresults_cat = {}\n\nfor i, col in enumerate(cols_cat + ['label']):\n    results_cat[col] = (\n        rdd_cat\n        .groupBy(lambda row: row[i])\n        .map(lambda el: (el[0], len(el[1])))\n        .collect()\n    )\n```", "```py\ncorrelations = st.Statistics.corr(rdd_num)\n```", "```py\nfor i, el_i in enumerate(abs(correlations) > 0.05):\n    print(cols_num[i])\n\n    for j, el_j in enumerate(el_i):\n        if el_j and j != i:\n            print(\n                '    '\n```", "```py\n                , cols_num[j]\n                , correlations[i][j]\n            )\n\n    print()\n```", "```py\nimport pyspark.mllib.linalg as ln\n\ncensus_occupation = (\n    census\n    .groupby('label')\n    .pivot('occupation')\n    .count()\n)\n\ncensus_occupation_coll = (\n    census_occupation\n    .rdd\n    .map(lambda row: (row[1:]))\n    .flatMap(lambda row: row)\n    .collect()\n)\n\nlen_row = census_occupation.count()\ndense_mat = ln.DenseMatrix(\n    len_row\n    , 2\n    , census_occupation_coll\n    , True)\nchi_sq = st.Statistics.chiSqTest(dense_mat)\n\nprint(chi_sq.pValue)\n```", "```py\nlen_ftrs = []\n\nfor col in cols_cat:\n    (\n        len_ftrs\n        .append(\n            (col\n             , census\n                 .select(col)\n                 .distinct()\n                 .count()\n            )\n        )\n    )\n\nlen_ftrs = dict(len_ftrs)\n```", "```py\nimport pyspark.mllib.feature as feat\n```", "```py\nfinal_data = (    census\n    .select(cols_to_keep)\n    .rdd\n    .map(lambda row: [\n        list(\n            feat.HashingTF(int(len_ftrs[col] / 2.0))\n            .transform(row[i])\n            .toArray()\n        ) if i >= 5\n        else [row[i]] \n        for i, col in enumerate(cols_to_keep)]\n    )\n)\n\nfinal_data.take(3)\n```", "```py\ndef labelEncode(label):\n    return [int(label[0] == '>50K')]\n\nfinal_data = (\n    final_data\n    .map(lambda row: labelEncode(row[0]) \n         + [item \n            for sublist in row[1:] \n            for item in sublist]\n        )\n)\n```", "```py\nstandardizer = feat.StandardScaler(True, True)\nsModel = standardizer.fit(final_data.map(lambda row: row[1:]))\nfinal_data_scaled = sModel.transform(final_data.map(lambda row: row[1:]))\n\nfinal_data = (\n    final_data\n    .map(lambda row: row[0])\n    .zipWithIndex()\n    .map(lambda row: (row[1], row[0]))\n    .join(\n        final_data_scaled\n        .zipWithIndex()\n        .map(lambda row: (row[1], row[0]))\n    )\n    .map(lambda row: row[1])\n)\n\nfinal_data.take(1)\n```", "```py\nfinal_data_income = (\n    final_data\n    .map(lambda row: reg.LabeledPoint(\n        row[0]\n        , row[1:]\n        )\n)\n```", "```py\nmu, std = sModel.mean[3], sModel.std[3]\n\nfinal_data_hours = (\n    final_data\n    .map(lambda row: reg.LabeledPoint(\n        row[1][3] * std + mu\n        , ln.Vectors.dense([row[0]] + list(row[1][0:3]) + list(row[1][4:]))\n        )\n)\n```", "```py\nimport pyspark.mllib.regression as reg\n```", "```py\n(\n    final_data_income_train\n    , final_data_income_test\n) = (\n    final_data_income.randomSplit([0.7, 0.3])\n)\n```", "```py\n(\n    final_data_hours_train\n    , final_data_hours_test\n) = (\n    final_data_hours.randomSplit([0.7, 0.3])\n)\n```", "```py\nworkhours_model_lm = reg.LinearRegressionWithSGD.train(final_data_hours_train)\n```", "```py\nsmall_sample_hours = sc.parallelize(final_data_hours_test.take(10))\n\nfor t,p in zip(\n    small_sample_hours\n        .map(lambda row: row.label)\n        .collect()\n    , workhours_model_lm.predict(\n        small_sample_hours\n            .map(lambda row: row.features)\n    ).collect()):\n    print(t,p)\n```", "```py\nimport pyspark.mllib.classification as cl\n\nincome_model_lr = cl.LogisticRegressionWithSGD.train(final_data_income_train)\n```", "```py\nsmall_sample_income = sc.parallelize(final_data_income_test.take(10))\n\nfor t,p in zip(\n    small_sample_income\n        .map(lambda row: row.label)\n        .collect()\n    , income_model_lr.predict(\n        small_sample_income\n            .map(lambda row: row.features)\n    ).collect()):\n    print(t,p)\n```", "```py\nincome_model_svm = cl.SVMWithSGD.train(\n    final_data_income\n    , miniBatchFraction=1/2.0\n)\n```", "```py\nfor t,p in zip(\n    small_sample_income\n        .map(lambda row: row.label)\n        .collect()\n    , income_model_svm.predict(\n        small_sample_income\n            .map(lambda row: row.features)\n    ).collect()):\n    print(t,p)\n```", "```py\nimport pyspark.mllib.clustering as clu\n\nmodel = clu.KMeans.train(\n```", "```py\n    final_data.map(lambda row: row[1])\n    , 2\n    , initializationMode='random'\n    , seed=666\n)\n```", "```py\nimport sklearn.metrics as m\n\npredicted = (\n    model\n        .predict(\n            final_data.map(lambda row: row[1])\n        )\n)\npredicted = predicted.collect()\n```", "```py\ntrue = final_data.map(lambda row: row[0]).collect()\n\nprint(m.homogeneity_score(true, predicted))\nprint(m.completeness_score(true, predicted))\n```", "```py\nimport pyspark.mllib.evaluation as ev\n\n(...)\n\nmetrics_lm = ev.RegressionMetrics(true_pred_reg)\n\n(...)\n\nmetrics_lr = ev.BinaryClassificationMetrics(true_pred_class_lr)\n```", "```py\ntrue_pred_reg = (\n    final_data_hours_test\n    .map(lambda row: (\n         float(workhours_model_lm.predict(row.features))\n         , row.label))\n)\n```", "```py\nprint('R^2: ', metrics_lm.r2)\nprint('Explained Variance: ', metrics_lm.explainedVariance)\nprint('meanAbsoluteError: ', metrics_lm.meanAbsoluteError)\n```", "```py\ntrue_pred_class_lr = (\n    final_data_income_test\n    .map(lambda row: (\n        float(income_model_lr.predict(row.features))\n        , row.label))\n)\n\nmetrics_lr = ev.BinaryClassificationMetrics(true_pred_class_lr)\n\nprint('areaUnderPR: ', metrics_lr.areaUnderPR)\nprint('areaUnderROC: ', metrics_lr.areaUnderROC)\n```", "```py\ntrue_pred_class_svm = (\n    final_data_income_test\n    .map(lambda row: (\n        float(income_model_svm.predict(row.features))\n        , row.label))\n)\n\nmetrics_svm = ev.BinaryClassificationMetrics(true_pred_class_svm)\n\nprint('areaUnderPR: ', metrics_svm.areaUnderPR)\nprint('areaUnderROC: ', metrics_svm.areaUnderROC)\n```", "```py\n(\n    true_pred_class_lr\n    .map(lambda el: ((el), 1))\n    .reduceByKey(lambda x,y: x+y)\n    .take(4)\n)\n\n```", "```py\n(\n    true_pred_class_svm\n    .map(lambda el: ((el), 1))\n    .reduceByKey(lambda x,y: x+y)\n    .take(4)\n)\n```", "```py\ntrainErr = (\n    true_pred_class_lr\n    .filter(lambda lp: lp[0] != lp[1]).count() \n    / float(true_pred_class_lr.count())\n)\nprint(\"Training Error = \" + str(trainErr))\n```"]