["```py\n./bin/pyspark --master local[n]\n```", "```py\nmyRDD) using the\u00a0sc.parallelize() method:\n```", "```py\nmyRDD = sc.parallelize([('Mike', 19), ('June', 18), ('Rachel',16), ('Rob', 18), ('Scott', 17)])\n```", "```py\nmyRDD.take(5)\n```", "```py\nOut[10]: [('Mike', 19), ('June', 18), ('Rachel',16), ('Rob', 18), ('Scott', 17)]\n```", "```py\nsc.parallelize() and take().\n```", "```py\nmyRDD = sc.parallelize( \n [('Mike', 19), ('June', 18), ('Rachel',16), ('Rob', 18), ('Scott', 17)]\n)\n```", "```py\nmyRDD = (\n    sc\n    .textFile(\n        '~/data/flights/airport-codes-na.txt'\n        , minPartitions=4\n        , use_unicode=True\n    ).map(lambda element: element.split(\"\\t\"))\n)\n```", "```py\nmyRDD.take(5)\n```", "```py\nOut[22]:  [[u'City', u'State', u'Country', u'IATA'], [u'Abbotsford', u'BC', u'Canada', u'YXX'], [u'Aberdeen', u'SD', u'USA', u'ABR'], [u'Abilene', u'TX', u'USA', u'ABI'], [u'Akron', u'OH', u'USA', u'CAK']]\n```", "```py\nmyRDD.count()\n\n# Output\n# Out[37]: 527\n```", "```py\nmyRDD.getNumPartitions()\n\n# Output\n# Out[33]: 4\n```", "```py\ntake can be broken down into its two components: sc.textFile()\u00a0and map().\n```", "```py\n(\n    sc\n    .textFile(\n        '~/data/flights/airport-codes-na.txt'\n        , minPartitions=4\n        , use_unicode=True\n    )\n)\n```", "```py\nmyRDD = sc.textFile('~/data/flights/airport-codes-na.txt')\nmyRDD.take(5)\n\n# Out[35]:  [u'City\\tState\\tCountry\\tIATA', u'Abbotsford\\tBC\\tCanada\\tYXX', u'Aberdeen\\tSD\\tUSA\\tABR', u'Abilene\\tTX\\tUSA\\tABI', u'Akron\\tOH\\tUSA\\tCAK']\n```", "```py\nmyRDD = (\n    sc\n    .textFile('~/data/flights/airport-codes-na.txt')\n    .map(lambda element: element.split(\"\\t\")) )\n```", "```py\nOut[22]:  [[u'City', u'State', u'Country', u'IATA'], [u'Abbotsford', u'BC', u'Canada', u'YXX'], [u'Aberdeen', u'SD', u'USA', u'ABR'], [u'Abilene', u'TX', u'USA', u'ABI'], [u'Akron', u'OH', u'USA', u'CAK']]\n```", "```py\nmyRDD = (\n    sc\n    .textFile('/databricks-datasets/flights/airport-codes-na.txt')\n    .map(lambda element: element.split(\"\\t\"))\n)\n\nmyRDD.getNumPartitions()\n\n# Output\nOut[2]: 2\n```", "```py\nmyRDD = (\n    sc\n    .textFile(\n        '/databricks-datasets/flights/airport-codes-na.txt'\n        , minPartitions=4\n    ).map(lambda element: element.split(\"\\t\"))\n)\n\nmyRDD.getNumPartitions()\n\n# Output\nOut[6]: 4\n```", "```py\n# Read the `departuredelays.csv` file and count number of rows\nmyRDD = (\n    sc\n    .textFile('/data/flights/departuredelays.csv')\n    .map(lambda element: element.split(\",\"))\n)\n\nmyRDD.count()\n\n# Output Duration: 3.33s\nOut[17]: 1391579\n\n# Get the number of partitions\nmyRDD.getNumPartitions()\n\n# Output:\nOut[20]: 2\n\n```", "```py\n# Read the `departuredelays.csv` file and count number of rows\nmyRDD = (\n    sc\n    .textFile('/data/flights/departuredelays.csv', minPartitions=8)\n    .map(lambda element: element.split(\",\"))\n)\n\nmyRDD.count()\n\n# Output Duration: 2.96s\nOut[17]: 1391579\n\n# Get the number of partitions\nmyRDD.getNumPartitions()\n\n# Output:\nOut[20]: 8\n```", "```py\n# Setup the RDD: airports\nairports = (\n    sc\n    .textFile('~/data/flights/airport-codes-na.txt')\n    .map(lambda element: element.split(\"\\t\"))\n)\n\nairports.take(5)\n\n# Output\nOut[11]:  \n[[u'City', u'State', u'Country', u'IATA'], \n [u'Abbotsford', u'BC', u'Canada', u'YXX'], \n [u'Aberdeen', u'SD', u'USA', u'ABR'], \n [u'Abilene', u'TX', u'USA', u'ABI'], \n [u'Akron', u'OH', u'USA', u'CAK']]\n\n# Setup the RDD: flights\nflights = (\n    sc\n    .textFile('/databricks-datasets/flights/departuredelays.csv')\n    .map(lambda element: element.split(\",\"))\n)\n\nflights.take(5)\n\n# Output\n[[u'date', u'delay', u'distance', u'origin', u'destination'],  \n [u'01011245', u'6', u'602', u'ABE', u'ATL'],  \n [u'01020600', u'-8', u'369', u'ABE', u'DTW'],  \n [u'01021245', u'-2', u'602', u'ABE', u'ATL'],  \n [u'01020605', u'-4', u'602', u'ABE', u'ATL']]\n\n```", "```py\n# Use map() to extract out the first two columns\nairports.map(lambda c: (c[0], c[1])).take(5)\n```", "```py\n# Output\n[(u'City', u'State'),  \n (u'Abbotsford', u'BC'),  \n (u'Aberdeen', u'SD'),\n```", "```py\n\n (u'Abilene', u'TX'),  \n (u'Akron', u'OH')]\n```", "```py\n# User filter() to filter where second column == \"WA\"\n(\n    airports\n    .map(lambda c: (c[0], c[1]))\n    .filter(lambda c: c[1] == \"WA\")\n    .take(5)\n)\n```", "```py\n# Output\n[(u'Bellingham', u'WA'),\n (u'Moses Lake', u'WA'),  \n (u'Pasco', u'WA'),  \n (u'Pullman', u'WA'),  \n (u'Seattle', u'WA')]\n```", "```py\n# Filter only second column == \"WA\", \n# select first two columns within the RDD,\n# and flatten out all values\n(\n    airports\n    .filter(lambda c: c[1] == \"WA\")\n    .map(lambda c: (c[0], c[1]))\n    .flatMap(lambda x: x)\n    .take(10)\n)\n```", "```py\n# Output\n[u'Bellingham',  \n u'WA',  \n u'Moses Lake',  \n u'WA',  \n u'Pasco',  \n u'WA',  \n u'Pullman',  \n u'WA',  \n u'Seattle',  \n u'WA']\n```", "```py\n# Provide the distinct elements for the \n# third column of airports representing\n# countries\n(\n    airports\n    .map(lambda c: c[2])\n    .distinct()\n    .take(5)\n)\n```", "```py\n# Output\n[u'Canada', u'USA', u'Country']    \n```", "```py\n# Provide a sample based on 0.001% the\n# flights RDD data specific to the fourth\n# column (origin city of flight)\n# without replacement (False) using random\n# seed of 123 \n(\n    flights\n    .map(lambda c: c[3])\n    .sample(False, 0.001, 123)\n    .take(5)\n)\n```", "```py\n# Output\n[u'ABQ', u'AEX', u'AGS', u'ANC', u'ATL'] \n```", "```py\n# Flights data\n#  e.g. (u'JFK', u'01010900')\nflt = flights.map(lambda c: (c[3], c[0]))\n\n# Airports data\n# e.g. (u'JFK', u'NY')\nair = airports.map(lambda c: (c[3], c[1]))\n\n# Execute inner join between RDDs\nflt.join(air).take(5)\n```", "```py\n# Output\n[(u'JFK', (u'01010900', u'NY')),  \n (u'JFK', (u'01011200', u'NY')),  \n (u'JFK', (u'01011900', u'NY')),  \n (u'JFK', (u'01011700', u'NY')),  \n (u'JFK', (u'01010800', u'NY'))]\n```", "```py\n# The flights RDD originally generated has 2 partitions \nflights.getNumPartitions()\n\n# Output\n2 \n\n# Let's re-partition this to 8 so we can have 8 \n# partitions\nflights2 = flights.repartition(8)\n\n# Checking the number of partitions for the flights2 RDD\nflights2.getNumPartitions()\n\n# Output\n8\n```", "```py\n# View each row within RDD + the index \n# i.e. output is in form ([row], idx)\nac = airports.map(lambda c: (c[0], c[3]))\nac.zipWithIndex().take(5)\n```", "```py\n# Output\n[((u'City', u'IATA'), 0),  \n ((u'Abbotsford', u'YXX'), 1),  \n ((u'Aberdeen', u'ABR'), 2),  \n ((u'Abilene', u'ABI'), 3),  \n ((u'Akron', u'CAK'), 4)]\n```", "```py\n# Using zipWithIndex to skip header row\n# - filter out row 0\n# - extract only row info\n(\n    ac\n    .zipWithIndex()\n    .filter(lambda (row, idx): idx > 0)\n    .map(lambda (row, idx): row)\n    .take(5)\n)\n```", "```py\n# Output\n[(u'Abbotsford', u'YXX'),  \n (u'Aberdeen', u'ABR'),  \n (u'Abilene', u'ABI'),  \n (u'Akron', u'CAK'),  \n (u'Alamosa', u'ALS')]\n```", "```py\n# Determine delays by originating city\n# - remove header row via zipWithIndex() \n#   and map() \n(\n    flights\n    .zipWithIndex()\n    .filter(lambda (row, idx): idx > 0)\n    .map(lambda (row, idx): row)\n    .map(lambda c: (c[3], int(c[1])))\n    .reduceByKey(lambda x, y: x + y)\n    .take(5)\n)\n```", "```py\n# Output\n[(u'JFK', 387929),  \n (u'MIA', 169373),  \n (u'LIH', -646),  \n (u'LIT', 34489),  \n (u'RDM', 3445)]\n```", "```py\n# Takes the origin code and delays, remove header\n# runs a group by origin code via reduceByKey()\n# sorting by the key (origin code)\n(\n    flights\n    .zipWithIndex()\n    .filter(lambda (row, idx): idx > 0)\n    .map(lambda (row, idx): row)\n    .map(lambda c: (c[3], int(c[1])))\n    .reduceByKey(lambda x, y: x + y)\n    .sortByKey()\n    .take(50)\n)\n```", "```py\n# Output\n[(u'ABE', 5113),  \n (u'ABI', 5128),  \n (u'ABQ', 64422),  \n (u'ABY', 1554),  \n (u'ACT', 392),\n ...]\n```", "```py\n# Create `a` RDD of Washington airports\na = (\n    airports\n    .zipWithIndex()\n    .filter(lambda (row, idx): idx > 0)\n    .map(lambda (row, idx): row)\n    .filter(lambda c: c[1] == \"WA\")\n)\n\n# Create `b` RDD of British Columbia airports\nb = (\n    airports\n    .zipWithIndex()\n    .filter(lambda (row, idx): idx > 0)\n    .map(lambda (row, idx): row)\n    .filter(lambda c: c[1] == \"BC\")\n)\n\n# Union WA and BC airports\na.union(b).collect()\n```", "```py\n# Output\n[[u'Bellingham', u'WA', u'USA', u'BLI'],\n [u'Moses Lake', u'WA', u'USA', u'MWH'],\n [u'Pasco', u'WA', u'USA', u'PSC'],\n [u'Pullman', u'WA', u'USA', u'PUW'],\n [u'Seattle', u'WA', u'USA', u'SEA'],\n...\n [u'Vancouver', u'BC', u'Canada', u'YVR'],\n [u'Victoria', u'BC', u'Canada', u'YYJ'], \n [u'Williams Lake', u'BC', u'Canada', u'YWL']]\n```", "```py\n# Source: https://stackoverflow.com/a/38957067/1100699\ndef partitionElementCount(idx, iterator):\n  count = 0\n  for _ in iterator:\n    count += 1\n  return idx, count\n\n# Use mapPartitionsWithIndex to determine \nflights.mapPartitionsWithIndex(partitionElementCount).collect()\n```", "```py\n# Output\n[0,  \n 174293,  \n 1,  \n 174020,  \n 2,  \n 173849,  \n 3,  \n 174006,  \n 4,  \n 173864,  \n 5,  \n 174308,  \n 6,  \n 173620,  \n 7,  \n 173618]\n```", "```py\n# Flights data\n#  e.g. (u'JFK', u'01010900')\nflt = flights.map(lambda c: (c[3], c[0]))\n\n# Airports data\n# e.g. (u'JFK', u'NY')\nair = airports.map(lambda c: (c[3], c[1]))\n\n# Execute inner join between RDDs\nflt.join(air).take(5)\n\n# Output\n[(u'JFK', (u'01010900', u'NY')),  \n (u'JFK', (u'01011200', u'NY')),  \n (u'JFK', (u'01011900', u'NY')),  \n (u'JFK', (u'01011700', u'NY')),  \n (u'JFK', (u'01010800', u'NY'))]\n```", "```py\n# Same join statement as above but no action operation such as take()\nflt = flights.map(lambda c: (c[3], c[0]))\nair = airports.map(lambda c: (c[3], c[1]))\nflt.join(air)\n\n# Output\nOut[32]: PythonRDD[101] at RDD at PythonRDD.scala:50\n```", "```py\n# Setup the RDD: airports\nairports = (\n    sc\n    .textFile('~/data/flights/airport-codes-na.txt')\n    .map(lambda element: element.split(\"\\t\"))\n)\n\nairports.take(5)\n\n# Output\nOut[11]:  \n[[u'City', u'State', u'Country', u'IATA'], \n [u'Abbotsford', u'BC', u'Canada', u'YXX'], \n [u'Aberdeen', u'SD', u'USA', u'ABR'], \n [u'Abilene', u'TX', u'USA', u'ABI'], \n [u'Akron', u'OH', u'USA', u'CAK']]\n\n# Setup the RDD: flights\nflights = (\n    sc\n    .textFile('~/data/flights/departuredelays.csv', minPartitions=8)\n    .map(lambda line: line.split(\",\"))\n)\n\nflights.take(5)\n\n# Output\n[[u'date', u'delay', u'distance', u'origin', u'destination'],  \n [u'01011245', u'6', u'602', u'ABE', u'ATL'],  \n [u'01020600', u'-8', u'369', u'ABE', u'DTW'],  \n [u'01021245', u'-2', u'602', u'ABE', u'ATL'],  \n [u'01020605', u'-4', u'602', u'ABE', u'ATL']]\n\n```", "```py\n# Print to console the first 3 elements of\n# the airports RDD\nairports.take(3)\n```", "```py\n# Output\n[[u'City', u'State', u'Country', u'IATA'], \n [u'Abbotsford', u'BC', u'Canada', u'YXX'], \n [u'Aberdeen', u'SD', u'USA', u'ABR']]\n```", "```py\n# Return all airports elements\n# filtered by WA state\nairports.filter(lambda c: c[1] == \"WA\").collect()\n```", "```py\n# Output\n[[u'Bellingham', u'WA', u'USA', u'BLI'],  [u'Moses Lake', u'WA', u'USA', u'MWH'],  [u'Pasco', u'WA', u'USA', u'PSC'],  [u'Pullman', u'WA', u'USA', u'PUW'],  [u'Seattle', u'WA', u'USA', u'SEA'],  [u'Spokane', u'WA', u'USA', u'GEG'],  [u'Walla Walla', u'WA', u'USA', u'ALW'],  [u'Wenatchee', u'WA', u'USA', u'EAT'],  [u'Yakima', u'WA', u'USA', u'YKM']]\n```", "```py\n# Calculate the total delays of flights\n# between SEA (origin) and SFO (dest),\n# convert delays column to int \n# and summarize\nflights\\\n .filter(lambda c: c[3] == 'SEA' and c[4] == 'SFO')\\\n .map(lambda c: int(c[1]))\\\n .reduce(lambda x, y: x + y)\n```", "```py\n# Output\n22293\n```", "```py\n(\n    flights\n    .zipWithIndex()\n    .filter(lambda (row, idx): idx > 0)\n    .map(lambda (row, idx): row)\n    .count()\n)\n```", "```py\n# Output\n1391578\n```", "```py\n# Saves airports as a text file\n#   Note, each partition has their own file\n```", "```py\n\n# saveAsTextFile\nairports.saveAsTextFile(\"/tmp/denny/airports\")\n```", "```py\n# Review file structure\n# Note that `airports` is a folder with two\n# files (part-zzzzz) as the airports RDD is \n# comprised of two partitions.\n/tmp/denny/airports/_SUCCESS\n/tmp/denny/airports/part-00000\n/tmp/denny/airports/part-00001\n```", "```py\n# Determine delays by originating city\n# - remove header row via zipWithIndex() \n#   and map() \nflights.zipWithIndex()\\\n  .filter(lambda (row, idx): idx > 0)\\\n  .map(lambda (row, idx): row)\\\n  .map(lambda c: (c[3], int(c[1])))\\\n  .reduceByKey(lambda x, y: x + y)\\\n  .take(5)\n\n# Output\n[(u'JFK', 387929),  \n (u'MIA', 169373),  \n (u'LIH', -646),  \n (u'LIT', 34489),  \n (u'RDM', 3445)]\n```", "```py\nreduceByKey()\u00a0action\u00a0is called; note that Job 14 represents only the reduceByKey()\u00a0of\u00a0part the DAG. A previous job had executed and returned the results based on the zipWithIndex() transformation, which is not included in Job 14:\n```", "```py\n## Create flights RDD\nflights = sc.textFile('/databricks-datasets/flights/departuredelays.csv')\\\n  .map(lambda line: line.split(\",\"))\\\n  .zipWithIndex()\\\n  .filter(lambda (row, idx): idx > 0)\\\n  .map(lambda (row, idx): row)\n\n# Create flightsDF DataFrame\nflightsDF = spark.read\\\n  .options(header='true', inferSchema='true')\n  .csv('~/data/flights/departuredelays.csv')\nflightsDF.createOrReplaceTempView(\"flightsDF\")\n```", "```py\n# RDD: Sum delays, group by and order by originating city\nflights.map(lambda c: (c[3], int(c[1]))).reduceByKey(lambda x, y: x + y).sortByKey().take(50)\n\n# Output (truncated)\n# Duration: 11.08 seconds\n[(u'ABE', 5113),  \n (u'ABI', 5128),  \n (u'ABQ', 64422),  \n (u'ABY', 1554),  \n (u'ACT', 392),\n ... ]\n```", "```py\n# RDD: Sum delays, group by and order by originating city\nspark.sql(\"select origin, sum(delay) as TotalDelay from flightsDF group by origin order by origin\").show(50)\n\n# Output (truncated)\n# Duration: 4.76s\n+------+----------+ \n|origin|TotalDelay| \n+------+----------+ \n| ABE  |      5113| \n| ABI  |      5128|\n| ABQ  |     64422| \n| ABY  |      1554| \n| ACT  |       392|\n...\n+------+----------+ \n\n```", "```py\nflights = sc.textFile('/databricks-datasets/flights/departuredelays.csv', minPartitions=8), ...\n```"]