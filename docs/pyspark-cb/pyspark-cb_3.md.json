["```py\nsample_data = sc.parallelize([\n      (1, 'MacBook Pro', 2015, '15\"', '16GB', '512GB SSD'\n        , 13.75, 9.48, 0.61, 4.02)\n    , (2, 'MacBook', 2016, '12\"', '8GB', '256GB SSD'\n        , 11.04, 7.74, 0.52, 2.03)\n    , (3, 'MacBook Air', 2016, '13.3\"', '8GB', '128GB SSD'\n        , 12.8, 8.94, 0.68, 2.96)\n    , (4, 'iMac', 2017, '27\"', '64GB', '1TB SSD'\n        , 25.6, 8.0, 20.3, 20.8)\n])\n\nsample_data_df = spark.createDataFrame(\n    sample_data\n    , [\n        'Id'\n        , 'Model'\n        , 'Year'\n        , 'ScreenSize'\n        , 'RAM'\n        , 'HDD'\n        , 'W'\n        , 'D'\n        , 'H'\n        , 'Weight'\n    ]\n)\n```", "```py\nsample_data_json_df = (\n    spark\n    .read\n    .json('../Data/DataFrames_sample.json')\n)\n```", "```py\nsample_data_csv = (\n    spark\n    .read\n    .csv(\n        '../Data/DataFrames_sample.csv'\n        , header=True\n        , inferSchema=True)\n)\n```", "```py\nimport pyspark.sql as sql\nimport pyspark.sql.functions as f\n\nsample_data_transformed = (\n    sample_data_df\n    .rdd\n    .map(lambda row: sql.Row(\n        **row.asDict()\n        , HDD_size=row.HDD.split(' ')[0]\n        )\n    )\n    .map(lambda row: sql.Row(\n        **row.asDict()\n        , HDD_type=row.HDD.split(' ')[1]\n        )\n    )\n    .map(lambda row: sql.Row(\n        **row.asDict()\n        , Volume=row.H * row.D * row.W\n        )\n    )\n    .toDF()\n    .select(\n        sample_data_df.columns + \n        [\n              'HDD_size'\n            , 'HDD_type'\n            , f.round(\n                f.col('Volume')\n            ).alias('Volume_cuIn')\n        ]\n    )\n)\n```", "```py\nsample_data_df.rdd.take(1)\n```", "```py\nsample_data.take(1)\n```", "```py\nimport pyspark.sql.functions as f\nimport pandas as pd\nfrom scipy import stats\n\nbig_df = (\n    spark\n    .range(0, 1000000)\n    .withColumn('val', f.rand())\n)\n\nbig_df.cache()\nbig_df.show(3)\n\n@f.pandas_udf('double', f.PandasUDFType.SCALAR)\ndef pandas_pdf(v):\n    return pd.Series(stats.norm.pdf(v))\n\n(\n    big_df\n    .withColumn('probability', pandas_pdf(big_df.val))\n    .show(5)\n)\n```", "```py\ndef test_pandas_pdf():\n    return (big_df\n            .withColumn('probability', pandas_pdf(big_df.val))\n            .agg(f.count(f.col('probability')))\n            .show()\n        )\n\n%timeit -n 1 test_pandas_pdf()\n\n# row-by-row version with Python-JVM conversion\n@f.udf('double')\ndef pdf(v):\n    return float(stats.norm.pdf(v))\n\ndef test_pdf():\n    return (big_df\n            .withColumn('probability', pdf(big_df.val))\n            .agg(f.count(f.col('probability')))\n            .show()\n        )\n\n%timeit -n 1 test_pdf()\n```", "```py\nimport pyspark.sql as sql\n\nsample_data_rdd = sc.textFile('../Data/DataFrames_sample.csv')\n\nheader = sample_data_rdd.first()\n\nsample_data_rdd_row = (\n    sample_data_rdd\n    .filter(lambda row: row != header)\n    .map(lambda row: row.split(','))\n    .map(lambda row:\n        sql.Row(\n            Id=int(row[0])\n            , Model=row[1]\n            , Year=int(row[2])\n            , ScreenSize=row[3]\n            , RAM=row[4]\n            , HDD=row[5]\n            , W=float(row[6])\n            , D=float(row[7])\n            , H=float(row[8])\n            , Weight=float(row[9])\n        )\n    )\n)\n```", "```py\nimport pyspark.sql.types as typ\n\nsch = typ.StructType([\n      typ.StructField('Id', typ.LongType(), False)\n    , typ.StructField('Model', typ.StringType(), True)\n    , typ.StructField('Year', typ.IntegerType(), True)\n    , typ.StructField('ScreenSize', typ.StringType(), True)\n    , typ.StructField('RAM', typ.StringType(), True)\n    , typ.StructField('HDD', typ.StringType(), True)\n    , typ.StructField('W', typ.DoubleType(), True)\n    , typ.StructField('D', typ.DoubleType(), True)\n    , typ.StructField('H', typ.DoubleType(), True)\n    , typ.StructField('Weight', typ.DoubleType(), True)\n])\n\nsample_data_rdd = sc.textFile('../Data/DataFrames_sample.csv')\n\nheader = sample_data_rdd.first()\n\nsample_data_rdd = (\n    sample_data_rdd\n    .filter(lambda row: row != header)\n    .map(lambda row: row.split(','))\n    .map(lambda row: (\n                int(row[0])\n                , row[1]\n                , int(row[2])\n                , row[3]\n                , row[4]\n                , row[5]\n                , float(row[6])\n                , float(row[7])\n                , float(row[8])\n                , float(row[9])\n        )\n    )\n)\n\nsample_data_schema = spark.createDataFrame(sample_data_rdd, schema=sch)\nsample_data_schema.show()\n```", "```py\nsample_data_schema.createTempView('sample_data_view')\n```", "```py\nspark.sql('''\n    SELECT Model\n        , Year\n        , RAM\n        , HDD\n    FROM sample_data_view\n''').show()\n```", "```py\nsample_data_schema.createOrReplaceTempView('sample_data_view')\n```", "```py\nspark.sql('''\n    SELECT Model\n        , Year\n        , RAM\n        , HDD\n        , ScreenSize\n    FROM sample_data_view\n''').show()\n```", "```py\nmodels_df = sc.parallelize([\n      ('MacBook Pro', 'Laptop')\n    , ('MacBook', 'Laptop')\n    , ('MacBook Air', 'Laptop')\n    , ('iMac', 'Desktop')\n]).toDF(['Model', 'FormFactor'])\n\nmodels_df.createOrReplaceTempView('models')\n\nsample_data_schema.createOrReplaceTempView('sample_data_view')\n\nspark.sql('''\n    SELECT a.*\n        , b.FormFactor\n    FROM sample_data_view AS a\n    LEFT JOIN models AS b\n        ON a.Model == b.Model\n    ORDER BY Weight DESC\n''').show()\n```", "```py\nspark.sql('''\n    SELECT b.FormFactor\n        , COUNT(*) AS ComputerCnt\n    FROM sample_data_view AS a\n    LEFT JOIN models AS b\n        ON a.Model == b.Model\n    GROUP BY FormFactor\n''').show()\n```", "```py\n# select Model and ScreenSize from the DataFrame\n\nsample_data_schema.select('Model', 'ScreenSize').show()\n```", "```py\nSELECT Model\n    , ScreenSize\nFROM sample_data_schema;\n```", "```py\n# extract only machines from 2015 onwards\n\n(\n    sample_data_schema\n    .filter(sample_data_schema.Year > 2015)\n    .show()\n)\n```", "```py\nSELECT *\nFROM sample_data_schema\nWHERE Year > 2015\n```", "```py\n(\n    sample_data_schema\n    .groupBy('RAM')\n    .count()\n    .show()\n)\n```", "```py\nSELECT RAM\n    , COUNT(*) AS count\nFROM sample_data_schema\nGROUP BY RAM\n```", "```py\n# sort by width (W)\n\nsample_data_schema.orderBy('W').show()\n```", "```py\nSELECT *\nFROM sample_data_schema\nORDER BY W\n```", "```py\n# sort by height (H) in descending order\n\nsample_data_schema.orderBy(f.col('H').desc()).show()\n```", "```py\nSELECT *\nFROM sample_data_schema\nORDER BY H DESC\n```", "```py\n# split the HDD into size and type\n\n(\n    sample_data_schema\n    .withColumn('HDDSplit', f.split(f.col('HDD'), ' '))\n    .show()\n)\n```", "```py\n# do the same as withColumn\n\n(\n    sample_data_schema\n    .select(\n        f.col('*')\n        , f.split(f.col('HDD'), ' ').alias('HDD_Array')\n    ).show()\n)\n```", "```py\nSELECT *\n    , STRING_SPLIT(HDD, ' ') AS HDD_Array\nFROM sample_data_schema\n```", "```py\nmodels_df = sc.parallelize([\n      ('MacBook Pro', 'Laptop')\n    , ('MacBook', 'Laptop')\n    , ('MacBook Air', 'Laptop')\n    , ('iMac', 'Desktop')\n]).toDF(['Model', 'FormFactor'])\n\n(\n    sample_data_schema\n    .join(\n        models_df\n        , sample_data_schema.Model == models_df.Model\n        , 'left'\n    ).show()\n)\n```", "```py\nSELECT a.*\n    , b,FormFactor\nFROM sample_data_schema AS a\nLEFT JOIN models_df AS b\n    ON a.Model == b.Model\n```", "```py\nmodels_df = sc.parallelize([\n      ('MacBook Pro', 'Laptop')\n    , ('MacBook Air', 'Laptop')\n    , ('iMac', 'Desktop')\n]).toDF(['Model', 'FormFactor'])\n\n(\n    sample_data_schema\n    .join(\n        models_df\n        , sample_data_schema.Model == models_df.Model\n        , 'left'\n    ).show()\n)\n```", "```py\n(\n    sample_data_schema\n    .join(\n        models_df\n        , sample_data_schema.Model == models_df.Model\n        , 'right'\n    ).show()\n)\n```", "```py\n(\n    sample_data_schema\n    .join(\n        models_df\n        , sample_data_schema.Model == models_df.Model\n        , 'left_semi'\n    ).show()\n)\n```", "```py\n(\n    sample_data_schema\n    .join(\n        models_df\n        , sample_data_schema.Model == models_df.Model\n        , 'left_anti'\n    ).show()\n)\n```", "```py\nanother_macBookPro = sc.parallelize([\n      (5, 'MacBook Pro', 2018, '15\"', '16GB', '256GB SSD', 13.75, 9.48, 0.61, 4.02)\n]).toDF(sample_data_schema.columns)\n\nsample_data_schema.unionAll(another_macBookPro).show()\n```", "```py\nSELECT *\nFROM sample_data_schema\n\nUNION ALL\nSELECT *\nFROM another_macBookPro\n```", "```py\n# select the distinct values from the RAM column\n\nsample_data_schema.select('RAM').distinct().show()\n```", "```py\nSELECT DISTINCT RAM\nFROM sample_data_schema\n```", "```py\nsample_data_schema_rep = (\n    sample_data_schema\n    .repartition(2, 'Year')\n)\n\nsample_data_schema_rep.rdd.getNumPartitions()\n```", "```py\n2\n```", "```py\nmissing_df = sc.parallelize([\n    (None, 36.3, 24.2)\n    , (1.6, 32.1, 27.9)\n    , (3.2, 38.7, 24.7)\n    , (2.8, None, 23.9)\n    , (3.9, 34.1, 27.9)\n    , (9.2, None, None)\n]).toDF(['A', 'B', 'C'])\n\nmissing_df.fillna(21.4).show()\n```", "```py\nmiss_dict = (\n    missing_df\n    .agg(\n        f.mean('A').alias('A')\n        , f.mean('B').alias('B')\n        , f.mean('C').alias('C')\n    )\n).toPandas().to_dict('records')[0]\n\nmissing_df.fillna(miss_dict).show()\n```", "```py\nmissing_df.dropna().show()\n```", "```py\nmissing_df.dropna(thresh=2).show()\n```", "```py\ndupes_df = sc.parallelize([\n      (1.6, 32.1, 27.9)\n    , (3.2, 38.7, 24.7)\n    , (3.9, 34.1, 27.9)\n    , (3.2, 38.7, 24.7)\n]).toDF(['A', 'B', 'C'])\n\ndupes_df.dropDuplicates().show()\n```", "```py\nsample_data_schema.select('W').summary().show()\nsample_data_schema.select('W').describe().show()\n```", "```py\nsample_data_schema.freqItems(['RAM']).show()\n```", "```py\nsample_data_schema.select('W').describe().show()\n```", "```py\nsample_data_schema.groupBy('Year').count().collect()\n```", "```py\nLook at the following code:sample_data_schema.take(2)\n```", "```py\nsample_data_schema.toPandas()\n```"]