["```java\n<!-- https://mvnrepository.com/artifact/org.apache.flink/flink-ml_2.11 --> \n<dependency> \n    <groupId>org.apache.flink</groupId> \n    <artifactId>flink-ml_2.11</artifactId> \n    <version>1.1.4</version> \n</dependency> \n\n```", "```java\n    csv2libsvm.py <input file> <output file> [<label index = 0>] [<skip \n    headers = 0>]\n\n```", "```java\npackage com.demo.chapter06 \n\nimport org.apache.flink.api.scala._ \nimport org.apache.flink.ml.math.Vector \nimport org.apache.flink.ml.common.LabeledVector \nimport org.apache.flink.ml.classification.SVM \nimport org.apache.flink.ml.RichExecutionEnvironment \n\nobject MySVMApp { \n  def main(args: Array[String]) { \n    // set up the execution environment \n    val pathToTrainingFile: String = \"iris-train.txt\" \n    val pathToTestingFile: String = \"iris-train.txt\" \n    val env = ExecutionEnvironment.getExecutionEnvironment \n\n    // Read the training dataset, from a LibSVM formatted file \n    val trainingDS: DataSet[LabeledVector] = \n    env.readLibSVM(pathToTrainingFile) \n\n    // Create the SVM learner \n    val svm = SVM() \n      .setBlocks(10) \n\n    // Learn the SVM model \n    svm.fit(trainingDS) \n\n    // Read the testing dataset \n    val testingDS: DataSet[Vector] = \n    env.readLibSVM(pathToTestingFile).map(_.vector) \n\n    // Calculate the predictions for the testing dataset \n    val predictionDS: DataSet[(Vector, Double)] = \n    svm.predict(testingDS) \n    predictionDS.writeAsText(\"out\") \n\n    env.execute(\"Flink SVM App\") \n  } \n} \n\n```", "```java\n(SparseVector((0,5.1), (1,3.5), (2,1.4), (3,0.2)),1.0) \n(SparseVector((0,4.9), (1,3.0), (2,1.4), (3,0.2)),1.0) \n(SparseVector((0,4.7), (1,3.2), (2,1.3), (3,0.2)),1.0) \n(SparseVector((0,4.6), (1,3.1), (2,1.5), (3,0.2)),1.0) \n(SparseVector((0,5.0), (1,3.6), (2,1.4), (3,0.2)),1.0) \n(SparseVector((0,5.4), (1,3.9), (2,1.7), (3,0.4)),1.0) \n(SparseVector((0,4.6), (1,3.4), (2,1.4), (3,0.3)),1.0) \n(SparseVector((0,5.0), (1,3.4), (2,1.5), (3,0.2)),1.0) \n(SparseVector((0,4.4), (1,2.9), (2,1.4), (3,0.2)),1.0) \n(SparseVector((0,4.9), (1,3.1), (2,1.5), (3,0.1)),1.0) \n(SparseVector((0,5.4), (1,3.7), (2,1.5), (3,0.2)),1.0) \n(SparseVector((0,4.8), (1,3.4), (2,1.6), (3,0.2)),1.0) \n(SparseVector((0,4.8), (1,3.0), (2,1.4), (3,0.1)),1.0) \n\n```", "```java\npackage com.demo.flink.ml \n\nimport org.apache.flink.api.scala._ \nimport org.apache.flink.ml._ \nimport org.apache.flink.ml.common.LabeledVector \nimport org.apache.flink.ml.math.DenseVector \nimport org.apache.flink.ml.math.Vector \nimport org.apache.flink.ml.preprocessing.Splitter \nimport org.apache.flink.ml.regression.MultipleLinearRegression \n\nobject MLRJob { \n  def main(args: Array[String]) { \n    // set up the execution environment \n    val env = ExecutionEnvironment.getExecutionEnvironment \n    val trainingDataset = MLUtils.readLibSVM(env, \"iris-train.txt\") \n    val testingDataset = MLUtils.readLibSVM(env, \"iris-test.txt\").map { \n    lv => lv.vector } \n    val mlr = MultipleLinearRegression() \n      .setStepsize(1.0) \n      .setIterations(5) \n      .setConvergenceThreshold(0.001) \n\n    mlr.fit(trainingDataset) \n\n    // The fitted model can now be used to make predictions \n    val predictions = mlr.predict(testingDataset) \n\n    predictions.print() \n\n  } \n} \n\n```", "```java\n// Create SGD solver \nval sgd = GradientDescentL1() \n  .setLossFunction(SquaredLoss()) \n  .setRegularizationConstant(0.2) \n  .setIterations(100) \n  .setLearningRate(0.01) \n  .setLearningRateMethod(LearningRateMethod.Xu(-0.75)) \n\n// Obtain data \nval trainingDS: DataSet[LabeledVector] = ... \n\n// Optimize the weights, according to the provided data \nval weightDS = sgd.optimize(trainingDS) \n\n```", "```java\n1  10 1 \n1  11 2 \n1  12 5 \n1  13 5 \n1  14 5 \n1  15 4 \n1  16 5 \n1  17 1 \n1  18 5 \n2  10 1 \n2  11 2 \n2  15 5 \n2  16 4.5 \n2  17 1 \n2  18 5 \n3  11 2.5 \n3  12 4.5 \n3  13 4 \n3  14 3 \n3  15 3.5 \n3  16 4.5 \n3  17 4 \n3  18 5 \n4  10 5 \n4  11 5 \n4  12 5 \n4  13 0 \n4  14 2 \n4  15 3 \n4  16 1 \n4  17 4 \n4  18 1 \n\n```", "```java\npackage com.demo.chapter06 \n\nimport org.apache.flink.api.scala._ \nimport org.apache.flink.ml.recommendation._ \nimport org.apache.flink.ml.common.ParameterMap \n\nobject MyALSApp { \n  def main(args: Array[String]): Unit = { \n\n    val env = ExecutionEnvironment.getExecutionEnvironment \n    val inputDS: DataSet[(Int, Int, Double)] = env.readCsvFile[(Int,  \n    Int, Double)](\"input.csv\") \n\n    // Setup the ALS learner \n    val als = ALS() \n      .setIterations(10) \n      .setNumFactors(10) \n      .setBlocks(100) \n      .setTemporaryPath(\"tmp\") \n\n    // Set the other parameters via a parameter map \n    val parameters = ParameterMap() \n      .add(ALS.Lambda, 0.9) \n      .add(ALS.Seed, 42L) \n\n    // Calculate the factorization \n    als.fit(inputDS, parameters) \n\n    // Read the testing dataset from a csv file \n    val testingDS: DataSet[(Int, Int)] = env.readCsvFile[(Int, Int)]   \n    (\"test-data.csv\") \n\n    // Calculate the ratings according to the matrix factorization \n    val predictedRatings = als.predict(testingDS) \n\n    predictedRatings.writeAsCsv(\"output\") \n\n    env.execute(\"Flink Recommendation App\") \n  } \n} \n\n```", "```java\nimport org.apache.flink.api.common.operators.base.CrossOperatorBase.CrossHint \nimport org.apache.flink.api.scala._ \nimport org.apache.flink.ml.nn.KNN \nimport org.apache.flink.ml.math.Vector \nimport org.apache.flink.ml.metrics.distances.SquaredEuclideanDistanceMetric \n\nval env = ExecutionEnvironment.getExecutionEnvironment \n\n// prepare data \nval trainingSet: DataSet[Vector] = ... \nval testingSet: DataSet[Vector] = ... \n\nval knn = KNN() \n  .setK(3) \n  .setBlocks(10) \n  .setDistanceMetric(SquaredEuclideanDistanceMetric()) \n  .setUseQuadTree(false) \n  .setSizeHint(CrossHint.SECOND_IS_SMALL) \n\n// run knn join \nknn.fit(trainingSet) \nval result = knn.predict(testingSet).collect() \n\n```", "```java\nclass MyDistance extends DistanceMetric { \n  override def distance(a: Vector, b: Vector) = ... // your implementation  \n} \n\nobject MyDistance { \n  def apply() = new MyDistance() \n} \n\nval myMetric = MyDistance() \n\n```", "```java\n// A Simple Train-Test-Split \nval dataTrainTest: TrainTestDataSet = Splitter.trainTestSplit(data, 0.6, true) \n\n```", "```java\n// Create a train test holdout DataSet \nval dataTrainTestHO: trainTestHoldoutDataSet = Splitter.trainTestHoldoutSplit(data, Array(6.0, 3.0, 1.0)) \n\n```", "```java\n// Create an Array of K TrainTestDataSets \nval dataKFolded: Array[TrainTestDataSet] =  Splitter.kFoldSplit(data, 10) \n\n```", "```java\n// create an array of 5 datasets of 1 of 50%, and 5 of 10% each  \nval dataMultiRandom: Array[DataSet[T]] = Splitter.multiRandomSplit(data, Array(0.5, 0.1, 0.1, 0.1, 0.1)) \n\n```", "```java\nval polyFeatures = PolynomialFeatures() \n      .setDegree(3) \n\n```", "```java\n  val scaler = StandardScaler() \n      .setMean(10.0) \n      .setStd(2.0) \n\n```", "```java\nscaler.fit(trainingDataset)\n```", "```java\nval scaledDS = scaler.transform(trainingDataset)\n```", "```java\nval minMaxscaler = MinMaxScaler()\n.setMin(1.0)\n.setMax(3.0)\nminMaxscaler.fit(trainingDataset)\nval scaledDS = minMaxscaler.transform(trainingDataset)\n```", "```java\n// Create pipeline PolynomialFeatures -> MultipleLinearRegression\nval pipeline = polyFeatures.chainPredictor(mlr)\n// train the model\npipeline.fit(scaledDS)\n// The fitted model can now be used to make predictions\nval predictions = pipeline.predict(testingDataset)\npredictions.print()\n```"]