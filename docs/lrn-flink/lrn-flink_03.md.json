["```java\ninputSet.map(new MapFunction<Integer, Integer>() { \n  @Override \n  public Integer map(Integer value) throws Exception { \n        return 5 * value; \n      } \n    }); \n\n```", "```java\ninputSet.map { x => x * 5 } \n\n```", "```java\ninputSet.map { lambda x : x * 5 } \n\n```", "```java\ninputSet.flatMap(new FlatMapFunction<String, String>() { \n    @Override \n    public void flatMap(String value, Collector<String> out) \n        throws Exception { \n        for(String word: value.split(\" \")){ \n            out.collect(word); \n        } \n    } \n}); \n\n```", "```java\ninputSet.flatMap { str => str.split(\" \") } \n\n```", "```java\ninputSet.flat_map {lambda str, c:[str.split() for line in str } \n\n```", "```java\ninputSet.filter(new FilterFunction<Integer>() { \n    @Override \n    public boolean filter(Integer value) throws Exception { \n        return value != 1; \n    } \n}); \nIn Scala: \ninputSet.filter { _ != 1 } \n\n```", "```java\ninputSet.filter {lambda x: x != 1 } \n\n```", "```java\nDataSet<Tuple3<Integer, String, Double>> in = // [...] \nDataSet<Tuple2<String, Integer>> out = in.project(1,0); \n\n```", "```java\ninputSet.project(1,0) \n\n```", "```java\npublic class WC { \n  public String word; \n  public int count; \n} \n\n//Reduce function \npublic class WordCounter implements ReduceFunction<WC> { \n  @Override \n  public WC reduce(WC in1, WC in2) { \n    return new WC(in1.word, in1.count + in2.count); \n  } \n} \n\n// [...] \nDataSet<WC> words = // [...] \nDataSet<WC> wordCounts = words \n                         // grouping on field \"word\" \n                         .groupBy(\"word\") \n                         // apply ReduceFunction on grouped DataSet \n                         .reduce(new WordCounter()); \n\n```", "```java\nclass WC(val word: String, val count: Int) { \n  def this() { \n    this(null, -1) \n  } \n} \n\nval words: DataSet[WC] = // [...] \nval wordCounts = words.groupBy(\"word\").reduce { \n  (w1, w2) => new WC(w1.word, w1.count + w2.count) \n} \n\n```", "```java\nDataSet<Tuple3<String, Integer, Double>> reducedTuples = tuples \n                           // group by on second and third field  \n                            .groupBy(1, 2) \n                            // apply ReduceFunction \n                            .reduce(new MyTupleReducer()); \n\n```", "```java\nval reducedTuples = tuples.groupBy(1, 2).reduce { ... } \n\n```", "```java\nreducedTuples = tuples.group_by(1, 2).reduce( ... ) \n\n```", "```java\nDataSet<String> input = [..]  \n\n  DataSet<Tuple2<String, Integer>> combinedWords = input \n  .groupBy(0); // group similar words \n  .combineGroup(new GroupCombineFunction<String, Tuple2<String,  \n   Integer>() { \n\n    public void combine(Iterable<String> words,   \n    Collector<Tuple2<String, Integer>>) { // combine \n        String key = null; \n        int count = 0; \n\n        for (String word : words) { \n            key = word; \n            count++; \n        } \n        // emit tuple with word and count \n        out.collect(new Tuple2(key, count)); \n    } \n}); \n\n```", "```java\nval input: DataSet[String] = [..]  \n\nval combinedWords: DataSet[(String, Int)] = input \n  .groupBy(0) \n  .combineGroup { \n    (words, out: Collector[(String, Int)]) => \n        var key: String = null \n        var count = 0 \n\n        for (word <- words) { \n            key = word \n            count += 1 \n        } \n        out.collect((key, count)) \n} \n\n```", "```java\nDataSet<Tuple3<Integer, String, Double>> input = // [...] \nDataSet<Tuple3<Integer, String, Double>> output = input \n             .groupBy(1)        // group DataSet on second field \n             .aggregate(SUM, 0) // compute sum of the first field \n             .and(MIN, 2);      // compute minimum of the third field \n\n```", "```java\nval input: DataSet[(Int, String, Double)] = // [...] \nval output = input.groupBy(1).aggregate(SUM, 0).and(MIN, 2) \n\n```", "```java\ninput = # [...] \noutput = input.group_by(1).aggregate(Sum, 0).and_agg(Min, 2) \n\n```", "```java\nDataSet<Tuple3<Integer, String, Double>> input = // [...] \nDataSet<Tuple3<Integer, String, Double>> output = input \n                  .groupBy(1)   // group by on second field \n                  .minBy(0, 2); // select tuple with minimum values for first and third field. \n\n```", "```java\nval input: DataSet[(Int, String, Double)] = // [...] \nval output: DataSet[(Int, String, Double)] = input \n           .groupBy(1)                                     \n           .minBy(0, 2)\n```", "```java\nDataSet<Tuple3<Integer, String, Double>> input = // [...] \nDataSet<Tuple3<Integer, String, Double>> output = input \n                  .groupBy(1)   // group by on second field \n                  .maxBy(0, 2); // select tuple with maximum values for         \n                                /*first and third field. */\n\n```", "```java\nval input: DataSet[(Int, String, Double)] = // [...] \nval output: DataSet[(Int, String, Double)] = input \n.groupBy(1)                                    \n.maxBy(0, 2)  \n\n```", "```java\npublic class IntSumReducer implements ReduceFunction<Integer> { \n  @Override \n  public Integer reduce(Integer num1, Integer num2) { \n    return num1 + num2; \n  } \n} \n\nDataSet<Integer> intNumbers = // [...] \nDataSet<Integer> sum = intNumbers.reduce(new IntSumReducer()); \n\n```", "```java\nval sum = intNumbers.reduce (_ + _) \n\n```", "```java\nsum = intNumbers.reduce(lambda x,y: x + y) \n\n```", "```java\nDataSet<Integer> input = // [...] \nDataSet<Integer> output = input.reduceGroup(new MyGroupReducer()); \n\n```", "```java\nval input: DataSet[Int] = // [...] \nval output = input.reduceGroup(new MyGroupReducer())  \n\n```", "```java\noutput = data.reduce_group(MyGroupReducer()) \n\n```", "```java\nDataSet<Tuple2<Integer, Double>> output = input \n.aggregate(SUM, 0) // SUM of first field                   \n.and(MIN, 1); // Minimum of second  \n\n```", "```java\nval input: DataSet[(Int, String, Double)] = // [...] \nval output = input.aggregate(SUM, 0).and(MIN, 2)  \n\n```", "```java\noutput = input.aggregate(Sum, 0).and_agg(Min, 2) \n\n```", "```java\nDataSet<Tuple3<Integer, String, Double>> input = // [...] \nDataSet<Tuple3<Integer, String, Double>> output = input \n                  .minBy(0, 2); // select tuple with minimum values for \n                                first and third field. \n\n```", "```java\nval input: DataSet[(Int, String, Double)] = // [...] \nval output: DataSet[(Int, String, Double)] = input \n.minBy(0, 2)  \n\n```", "```java\nDataSet<Tuple3<Integer, String, Double>> input = // [...] \nDataSet<Tuple3<Integer, String, Double>> output = input \n                 .maxBy(0, 2); // select tuple with maximum values for first and third field. \n\n```", "```java\nval input: DataSet[(Int, String, Double)] = // [...] \nval output: DataSet[(Int, String, Double)] = input \n                                  .maxBy(0, 2)  \n\n```", "```java\nDataSet<Tuple2<Integer, Double>> output = input.distinct(); \n\n```", "```java\nval output = input.distinct() \n\n```", "```java\npublic static class Student { public String name; public int deptId; } \npublic static class Dept { public String name; public int id; } \nDataSet<Student> input1 = // [...] \nDataSet<Dept> input2 = // [...] \nDataSet<Tuple2<Student, Dept>> \n            result = input1.join(input2) \n.where(\"deptId\")                                  \n.equalTo(\"id\"); \n\n```", "```java\nval input1: DataSet[(String, Int)] = // [...] \nval input2: DataSet[(String, Int)] = // [...] \nval result = input1.join(input2).where(1).equalTo(1) \n\n```", "```java\nresult = input1.join(input2).where(1).equal_to(1)  \n\n```", "```java\nDataSet<Class> input1 = // [...] \nDataSet<class> input2 = // [...] \nDataSet<Tuple3<Integer, Integer, Double>> \n            result = \n            input1.cross(input2) \n                   // applying CrossFunction \n                   .with(new MyCrossFunction()); \n\n```", "```java\nval result = input1.cross(input2) { \n//custom function \n} \n\n```", "```java\nresult = input1.cross(input2).using(MyCrossFunction()) \n\n```", "```java\nDataSet<Tuple2<String, Integer>> input1 = // [...] \nDataSet<Tuple2<String, Integer>> input2 = // [...] \nDataSet<Tuple2<String, Integer>> input3 = // [...] \nDataSet<Tuple2<String, Integer>> unioned = input1.union(input2).union(input3); \n\n```", "```java\nval input1: DataSet[(String, Int)] = // [...] \nval input2: DataSet[(String, Int)] = // [...] \nval input3: DataSet[(String, Int)] = // [...] \nval unioned = input1.union(input2).union(input3)  \n\n```", "```java\nunioned = input1.union(input2).union(input3) \n\n```", "```java\nDataSet<String> in = // [...] \nDataSet<Tuple2<String, String>> out = in.rebalance(); \n\n```", "```java\nval in: DataSet[String] = // [...] \nval out = in.rebalance() \n\n```", "```java\nDataSet<Tuple2<String, Integer>> in = // [...] \nDataSet<Tuple2<String, String>> out = in.partitionByHash(1); \n\n```", "```java\nval in: DataSet[(String, Int)] = // [...] \nval out = in.partitionByHash(1) \n\n```", "```java\nDataSet<Tuple2<String, Integer>> in = // [...] \nDataSet<Tuple2<String, String>> out = in.partitionByRange(1); \n\n```", "```java\nval in: DataSet[(String, Int)] = // [...] \nval out = in.partitionByRange(1) \n\n```", "```java\nDataSet<Tuple2<String, Integer>> in = // [...] \nDataSet<Tuple2<String, String>> out = in.sortPartition(1,Order.ASCENDING); \n\n```", "```java\nval in: DataSet[(String, Int)] = // [...] \nval out = in.sortPartition(1, Order.ASCENDING) \n\n```", "```java\nDataSet<Tuple2<String, Integer>> in = // [...] \n// Returns first 10 elements of the data set.  \nDataSet<Tuple2<String, String>> out = in.first(10); \n\n```", "```java\nval in: DataSet[(String, Int)] = // [...] \nval out = in.first(10) \n\n```", "```java\n// Get a data set to be broadcasted \nDataSet<Integer> toBroadcast = env.fromElements(1, 2, 3); \nDataSet<String> data = env.fromElements(\"India\", \"USA\", \"UK\").map(new RichMapFunction<String, String>() { \n    private List<Integer> toBroadcast; \n    // We have to use open method to get broadcast set from the context \n    @Override \n    public void open(Configuration parameters) throws Exception { \n    // Get the broadcast set, available as collection \n    this.toBroadcast = \n    getRuntimeContext().getBroadcastVariable(\"country\"); \n    } \n\n    @Override \n    public String map(String input) throws Exception { \n          int sum = 0; \n          for (int a : toBroadcast) { \n                sum = a + sum; \n          } \n          return input.toUpperCase() + sum; \n    } \n}).withBroadcastSet(toBroadcast, \"country\"); // Broadcast the set with name \ndata.print(); \n\n```", "```java\n<dependency> \n  <groupId>org.apache.flink</groupId> \n  <artifactId>flink-hadoop-compatibility_2.11</artifactId> \n  <version>1.1.4</version> \n</dependency> \n\n```", "```java\nExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \nDataSet<Tuple2<LongWritable, Text>> input = \n    env.readHadoopFile(new TextInputFormat(), LongWritable.class, Text.class, textPath);  \n\n```", "```java\nval env = ExecutionEnvironment.getExecutionEnvironment \nval input: DataSet[(LongWritable, Text)] = \n  env.readHadoopFile(new TextInputFormat, classOf[LongWritable], classOf[Text], textPath) \n\n```", "```java\n// Get the processed data set \nDataSet<Tuple2<Text, IntWritable>> results = [...] \n\n// Set up the Hadoop Output Format. \nHadoopOutputFormat<Text, IntWritable> hadoopOF = \n  // create the Flink wrapper. \n  new HadoopOutputFormat<Text, IntWritable>( \n    // set the Hadoop OutputFormat and specify the job. \n    new TextOutputFormat<Text, IntWritable>(), job \n  ); \nhadoopOF.getConfiguration().set(\"mapreduce.output.textoutputformat.separator\", \" \"); \nTextOutputFormat.setOutputPath(job, new Path(outputPath)); \n\n// Emit data  \nresult.output(hadoopOF); \n\n```", "```java\n// Get the processed data set \nval result: DataSet[(Text, IntWritable)] = [...] \n\nval hadoopOF = new HadoopOutputFormat[Text,IntWritable]( \n  new TextOutputFormat[Text, IntWritable], \n  new JobConf) \n\nhadoopOF.getJobConf.set(\"mapred.textoutputformat.separator\", \" \") \nFileOutputFormat.setOutputPath(hadoopOF.getJobConf, new Path(resultPath)) \n\nresult.output(hadoopOF) \n\n```", "```java\n<!-- configure the file system implementation --> \n<property> \n  <name>fs.s3.impl</name> \n  <value>org.apache.hadoop.fs.s3native.NativeS3FileSystem</value> \n</property> \n<!-- set your AWS ID --> \n<property> \n  <name>fs.s3.awsAccessKeyId</name> \n  <value>putKeyHere</value> \n</property> \n<!-- set your AWS access key --> \n<property> \n  <name>fs.s3.awsSecretAccessKey</name> \n  <value>putSecretHere</value> \n</property> \n\n```", "```java\n// Read from S3 bucket \nenv.readTextFile(\"s3://<bucket>/<endpoint>\"); \n// Write to S3 bucket \nstream.writeAsText(\"s3://<bucket>/<endpoint>\"); \n\n```", "```java\n<property> \n  <name>fs.alluxio.impl</name> \n  <value>alluxio.hadoop.FileSystem</value> \n</property> \n\n```", "```java\n// Read from Alluxio path \nenv.readTextFile(\"alluxio://<path>\"); \n\n// Write to Alluxio path \nstream.writeAsText(\"alluxio://<path>\"); \n\n```", "```java\nAvroInputFormat<User> users = new AvroInputFormat<User>(in, User.class); \nDataSet<User> userSet = env.createInput(users); \n\n```", "```java\nuserSet.groupBy(\"city\") \n\n```", "```java\ngit clone https://github.com/mooso/azure-tables-hadoop.git \ncd azure-tables-hadoop \nmvn clean install \n\n```", "```java\n<dependency> \n    <groupId>org.apache.flink</groupId> \n    <artifactId>flink-hadoop-compatibility_2.11</artifactId> \n    <version>1.1.4</version> \n</dependency> \n<dependency> \n  <groupId>com.microsoft.hadoop</groupId> \n  <artifactId>microsoft-hadoop-azure</artifactId> \n  <version>0.0.4</version> \n</dependency> \n\n```", "```java\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n\n    // create a  AzureTableInputFormat, using a Hadoop input format wrapper \n    HadoopInputFormat<Text, WritableEntity> hdIf = new HadoopInputFormat<Text, WritableEntity>(new AzureTableInputFormat(), Text.class, WritableEntity.class, new Job()); \n\n// set account URI     \nhdIf.getConfiguration().set(AzureTableConfiguration.Keys.ACCOUNT_URI.getKey(), \"XXXX\"); \n    // set the secret storage key \n    hdIf.getConfiguration().set(AzureTableConfiguration.Keys.STORAGE_KEY.getKey(), \"XXXX\"); \n    // set the table name  \n    hdIf.getConfiguration().set(AzureTableConfiguration.Keys.TABLE_NAME.getKey(), \"XXXX\"); \n\n DataSet<Tuple2<Text, WritableEntity>> input = env.createInput(hdIf); \n\n```", "```java\ngit clone https://github.com/okkam-it/flink-mongodb-test.git \ncd flink-mongodb-test \nmvn clean install \n\n```", "```java\n// set up the execution environment \n    final ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \n\n// create a MongodbInputFormat, using a Hadoop input format wrapper \nHadoopInputFormat<BSONWritable, BSONWritable> hdIf =  \n        new HadoopInputFormat<BSONWritable, BSONWritable>(new MongoInputFormat(), \n       BSONWritable.class, BSONWritable.class, new JobConf()); \n\n// specify connection parameters \nhdIf.getJobConf().set(\"mongo.input.uri\",  \n                \"mongodb://localhost:27017/dbname.collectioname\"); \n\nDataSet<Tuple2<BSONWritable, BSONWritable>> input = env.createInput(hdIf); \n\n```", "```java\nMongoConfigUtil.setOutputURI( hdIf.getJobConf(),  \n                \"mongodb://localhost:27017/dbname.collectionname \"); \n // emit result (this works only locally) \n result.output(new HadoopOutputFormat<Text,BSONWritable>( \n                new MongoOutputFormat<Text,BSONWritable>(), hdIf.getJobConf())); \n\n```", "```java\nfinal ExecutionEnvironment env = ExecutionEnvironment.getExecutionEnvironment(); \nDataSet<Record> csvInput = env.readCsvFile(\"olympic-athletes.csv\") \n                     .pojoType(Record.class, \"playerName\", \"country\", \"year\", \"game\", \"gold\", \"silver\", \"bronze\", \"total\"); \n\n```", "```java\nDataSet<Tuple2<String, Integer>> groupedByCountry = csvInput\n.flatMap(new FlatMapFunction<Record, Tuple2<String, Integer>>() {\nprivate static final long serialVersionUID = 1L;\n@Override\npublic void flatMap(Record record, Collector<Tuple2<String, Integer>> out) throws Exception {\nout.collect(new Tuple2<String, Integer>(record.getCountry(), 1));\n}\n}).groupBy(0).sum(1);\ngroupedByCountry.print();\n```", "```java\n(Australia,11)\n(Belarus,7)\n(China,25)\n(France,3)\n(Germany,2)\n(Italy,4)\n(Turkey,1)\n(United States,22)\n(Cameroon,2)\n(Hungary,1)\n(Kenya,1)\n(Lithuania,1)\n(Russia,23)\n(Spain,2)\n(Ukraine,1)\n(Chinese Taipei,2)\n(Great Britain,1)\n(Romania,14)\n(Switzerland,1)\n(Bulgaria,3)\n(Finland,1)\n(Greece,7)\n(Japan,1)\n(Mexico,1)\n(Netherlands,2)\n(Poland,1)\n(South Korea,6)\n(Sweden,6)\n(Thailand,1)\n```", "```java\nDataSet<Tuple2<String, Integer>> groupedByGame = csvInput\n.flatMap(new FlatMapFunction<Record, Tuple2<String, Integer>>() { private static final long serialVersionUID = 1L;\n@Override\npublic void flatMap(Record record, Collector<Tuple2<String, Integer>> out) throws Exception {\nout.collect(new Tuple2<String, Integer>(record.getGame(), 1));\n}\n}).groupBy(0).sum(1);\ngroupedByGame.print();\n```", "```java\n(Basketball,1)\n(Gymnastics,42)\n(Ice Hockey,7)\n(Judo,1)\n(Swimming,33)\n(Athletics,2)\n(Fencing,2)\n(Nordic Combined,1)\n(Rhythmic Gymnastics,27)\n(Short-Track Speed Skating,5)\n(Table Tennis,1)\n(Weightlifting,4)\n(Boxing,3)\n(Taekwondo,3)\n(Archery,3)\n(Diving,14)\n(Figure Skating,1)\n(Football,2)\n(Shooting,1)\n```"]