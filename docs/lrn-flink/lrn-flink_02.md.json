["```java\nsocketTextStream(hostName, port); \n\n```", "```java\nsocketTextStream(hostName,port,delimiter) \n\n```", "```java\nsocketTextStream(hostName,port,delimiter, maxRetry) \n\n```", "```java\nreadFile(FileInputFormat<Out> inputFormat, String path) \n\n```", "```java\nreadFileStream(String filePath, long intervalMillis, FileMonitoringFunction.WatchType watchType) \n\n```", "```java\nreadFile(fileInputFormat, path, watchType, interval, pathFilter, typeInfo) \n\n```", "```java\ninputStream.map(new MapFunction<Integer, Integer>() { \n  @Override \n  public Integer map(Integer value) throws Exception { \n        return 5 * value; \n      } \n    }); \n\n```", "```java\ninputStream.map { x => x * 5 } \n\n```", "```java\ninputStream.flatMap(new FlatMapFunction<String, String>() { \n    @Override \n    public void flatMap(String value, Collector<String> out) \n        throws Exception { \n        for(String word: value.split(\" \")){ \n            out.collect(word); \n        } \n    } \n}); \n\n```", "```java\ninputStream.flatMap { str => str.split(\" \") } \n\n```", "```java\ninputStream.filter(new FilterFunction<Integer>() { \n    @Override \n    public boolean filter(Integer value) throws Exception { \n        return value != 1; \n    } \n}); \n\n```", "```java\ninputStream.filter { _ != 1 } \n\n```", "```java\ninputStream.keyBy(\"someKey\"); \n\n```", "```java\ninputStream.keyBy(\"someKey\") \n\n```", "```java\nkeyedInputStream. reduce(new ReduceFunction<Integer>() { \n    @Override \n    public Integer reduce(Integer value1, Integer value2) \n    throws Exception { \n        return value1 + value2; \n    } \n}); \n\n```", "```java\nkeyedInputStream. reduce { _ + _ } \n\n```", "```java\nkeyedInputStream keyedStream.fold(\"Start\", new FoldFunction<Integer, String>() { \n    @Override \n    public String fold(String current, Integer value) { \n        return current + \"=\" + value; \n    } \n  }); \n\n```", "```java\nkeyedInputStream.fold(\"Start\")((str, i) => { str + \"=\" + i }) \n\n```", "```java\nkeyedInputStream.sum(0) \nkeyedInputStream.sum(\"key\") \nkeyedInputStream.min(0) \nkeyedInputStream.min(\"key\") \nkeyedInputStream.max(0) \nkeyedInputStream.max(\"key\") \nkeyedInputStream.minBy(0) \nkeyedInputStream.minBy(\"key\") \nkeyedInputStream.maxBy(0) \nkeyedInputStream.maxBy(\"key\") \n\n```", "```java\nkeyedInputStream.sum(0) \nkeyedInputStream.sum(\"key\") \nkeyedInputStream.min(0) \nkeyedInputStream.min(\"key\") \nkeyedInputStream.max(0) \nkeyedInputStream.max(\"key\") \nkeyedInputStream.minBy(0) \nkeyedInputStream.minBy(\"key\") \nkeyedInputStream.maxBy(0) \nkeyedInputStream.maxBy(\"key\") \n\n```", "```java\ninputStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(10))); \n\n```", "```java\ninputStream.keyBy(0).window(TumblingEventTimeWindows.of(Time.seconds(10))) \n\n```", "```java\ninputStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(10))); \n\n```", "```java\ninputStream.windowAll(TumblingEventTimeWindows.of(Time.seconds(10))) \n\n```", "```java\ninputStream. union(inputStream1, inputStream2, ...); \n\n```", "```java\ninputStream. union(inputStream1, inputStream2, ...) \n\n```", "```java\ninputStream. join(inputStream1) \n   .where(0).equalTo(1) \n    .window(TumblingEventTimeWindows.of(Time.seconds(5))) \n    .apply (new JoinFunction () {...}); \n\n```", "```java\ninputStream. join(inputStream1) \n    .where(0).equalTo(1) \n    .window(TumblingEventTimeWindows.of(Time.seconds(5))) \n    .apply { ... }\n```", "```java\nSplitStream<Integer> split = inputStream.split(new OutputSelector<Integer>() { \n    @Override \n    public Iterable<String> select(Integer value) { \n        List<String> output = new ArrayList<String>(); \n        if (value % 2 == 0) { \n            output.add(\"even\"); \n        } \n        else { \n            output.add(\"odd\"); \n        } \n        return output; \n    } \n}); \n\n```", "```java\nval split = inputStream.split( \n  (num: Int) => \n    (num % 2) match { \n      case 0 => List(\"even\") \n      case 1 => List(\"odd\") \n    } \n) \n\n```", "```java\nSplitStream<Integer> split; \nDataStream<Integer> even = split.select(\"even\"); \nDataStream<Integer> odd = split.select(\"odd\"); \nDataStream<Integer> all = split.select(\"even\",\"odd\"); \n\n```", "```java\nval even = split select \"even\" \nval odd = split select \"odd\" \nval all = split.select(\"even\",\"odd\") \n\n```", "```java\nDataStream<Tuple4<Integer, Double, String, String>> in = // [...] \nDataStream<Tuple2<String, String>> out = in.project(3,2); \n\n```", "```java\nval in : DataStream[(Int,Double,String)] = // [...] \nval out = in.project(3,2) \n\n```", "```java\n(1,10.0, A, B )=> (B,A) \n(2,20.0, C, D )=> (D,C) \n\n```", "```java\ninputStream.partitionCustom(partitioner, \"someKey\"); \ninputStream.partitionCustom(partitioner, 0); \n\n```", "```java\ninputStream.partitionCustom(partitioner, \"someKey\") \ninputStream.partitionCustom(partitioner, 0) \n\n```", "```java\ninputStream.shuffle(); \n\n```", "```java\ninputStream.shuffle() \n\n```", "```java\ninputStream.rebalance(); \n\n```", "```java\ninputStream.rebalance() \n\n```", "```java\ninputStream.rescale(); \n\n```", "```java\ninputStream.rescale() \n\n```", "```java\ninputStream.broadcast(); \n\n```", "```java\ninputStream.broadcast() \n\n```", "```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); \nenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime); \n//or \nenv.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime); \n\n```", "```java\nval env = StreamExecutionEnvironment.getExecutionEnvironment \nenv.setStreamTimeCharacteristic(TimeCharacteristic.ProcessingTime) \n//or  \nenv.setStreamTimeCharacteristic(TimeCharacteristic.IngestionTime) \n\n```", "```java\nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); \nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime; \n\n```", "```java\nval env = StreamExecutionEnvironment.getExecutionEnvironment \nenv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime) \n\n```", "```java\n<dependency> \n  <groupId>org.apache.flink</groupId> \n  <artifactId>flink-connector-kafka-0.9_2.11/artifactId> \n  <version>1.1.4</version> \n</dependency> \n\n```", "```java\nProperties properties = new Properties(); \n  properties.setProperty(\"bootstrap.servers\", \"localhost:9092\"); \n  properties.setProperty(\"group.id\", \"test\"); \nDataStream<String> input  = env.addSource(new FlinkKafkaConsumer09<String>(\"mytopic\", new SimpleStringSchema(), properties)); \n\n```", "```java\nval properties = new Properties(); \nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\"); \n// only required for Kafka 0.8 \nproperties.setProperty(\"zookeeper.connect\", \"localhost:2181\"); \nproperties.setProperty(\"group.id\", \"test\"); \nstream = env \n    .addSource(new FlinkKafkaConsumer09[String](\"mytopic\", new SimpleStringSchema(), properties)) \n    .print \n\n```", "```java\nstream.addSink(new FlinkKafkaProducer09<String>(\"localhost:9092\", \"mytopic\", new SimpleStringSchema())); \n\n```", "```java\nstream.addSink(new FlinkKafkaProducer09[String](\"localhost:9092\", \"mytopic\", new SimpleStringSchema())) \n\n```", "```java\n<dependency> \n  <groupId>org.apache.flink</groupId> \n  <artifactId>flink-connector-twitter_2.11/artifactId> \n  <version>1.1.4</version> \n</dependency> \n\n```", "```java\nProperties props = new Properties(); \nprops.setProperty(TwitterSource.CONSUMER_KEY, \"\"); \nprops.setProperty(TwitterSource.CONSUMER_SECRET, \"\"); \nprops.setProperty(TwitterSource.TOKEN, \"\"); \nprops.setProperty(TwitterSource.TOKEN_SECRET, \"\"); \nDataStream<String> streamSource = env.addSource(new TwitterSource(props)); \n\n```", "```java\nval props = new Properties(); \nprops.setProperty(TwitterSource.CONSUMER_KEY, \"\"); \nprops.setProperty(TwitterSource.CONSUMER_SECRET, \"\"); \nprops.setProperty(TwitterSource.TOKEN, \"\"); \nprops.setProperty(TwitterSource.TOKEN_SECRET, \"\"); \nDataStream<String> streamSource = env.addSource(new TwitterSource(props)); \n\n```", "```java\n{ \n... \n\"text\": \"\"Loyalty 3.0: How to Revolutionize Customer &amp; Employee Engagement with Big Data &amp; #Gamification\" can be ordered here: http://t.co/1XhqyaNjuR\", \n  \"geo\": null, \n  \"retweeted\": false, \n  \"in_reply_to_screen_name\": null, \n  \"possibly_sensitive\": false, \n  \"truncated\": false, \n  \"lang\": \"en\", \n    \"hashtags\": [{ \n      \"text\": \"Gamification\", \n      \"indices\": [90, \n      103] \n    }], \n  }, \n  \"in_reply_to_status_id_str\": null, \n  \"id\": 330094515484508160 \n... \n} \n\n```", "```java\n<dependency> \n  <groupId>org.apache.flink</groupId> \n  <artifactId>flink-connector-rabbitmq_2.11/artifactId> \n  <version>1.1.4</version> \n</dependency> \n\n```", "```java\n//Configurations \nRMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() \n.setHost(<host>).setPort(<port>).setUserName(..) \n.setPassword(..).setVirtualHost(\"/\").build(); \n\n//Get Data Stream without correlation ids \nDataStream<String> streamWO = env.addSource(new RMQSource<String>(connectionConfig, \"my-queue\", new SimpleStringSchema())) \n  .print \n//Get Data Stream with correlation ids \nDataStream<String> streamW = env.addSource(new RMQSource<String>(connectionConfig, \"my-queue\", true, new SimpleStringSchema())) \n  .print \n\n```", "```java\nval connectionConfig = new RMQConnectionConfig.Builder() \n.setHost(<host>).setPort(<port>).setUserName(..) \n.setPassword(..).setVirtualHost(\"/\").build() \nstreamsWOIds = env \n    .addSource(new RMQSource[String](connectionConfig, \" my-queue\", new SimpleStringSchema)) \n    .print \n\nstreamsWIds = env \n    .addSource(new RMQSource[String](connectionConfig, \"my-queue\", true, new SimpleStringSchema)) \n    .print \n\n```", "```java\nRMQConnectionConfig connectionConfig = new RMQConnectionConfig.Builder() \n.setHost(<host>).setPort(<port>).setUserName(..) \n.setPassword(..).setVirtualHost(\"/\").build(); \nstream.addSink(new RMQSink<String>(connectionConfig, \"target-queue\", new StringToByteSerializer())); \n\n```", "```java\nval connectionConfig = new RMQConnectionConfig.Builder() \n.setHost(<host>).setPort(<port>).setUserName(..) \n.setPassword(..).setVirtualHost(\"/\").build() \nstream.addSink(new RMQSink[String](connectionConfig, \"target-queue\", new StringToByteSerializer \n\n```", "```java\n<dependency> \n  <groupId>org.apache.flink</groupId> \n  <artifactId>flink-connector-elasticsearch_2.11</artifactId> \n  <version>1.1.4</version> \n</dependency> \n\n```", "```java\nDataStream<String> input = ...; \n\nMap<String, String> config = Maps.newHashMap(); \nconfig.put(\"bulk.flush.max.actions\", \"1\"); \nconfig.put(\"cluster.name\", \"cluster-name\"); \n\ninput.addSink(new ElasticsearchSink<>(config, new IndexRequestBuilder<String>() { \n    @Override \n    public IndexRequest createIndexRequest(String element, RuntimeContext ctx) { \n        Map<String, Object> json = new HashMap<>(); \n        json.put(\"data\", element); \n\n        return Requests.indexRequest() \n                .index(\"my-index\") \n                .type(\"my-type\") \n                .source(json); \n    } \n})); \n\n```", "```java\nval input: DataStream[String] = ... \n\nval config = new util.HashMap[String, String] \nconfig.put(\"bulk.flush.max.actions\", \"1\") \nconfig.put(\"cluster.name\", \"cluster-name\") \n\ntext.addSink(new ElasticsearchSink(config, new IndexRequestBuilder[String] { \n  override def createIndexRequest(element: String, ctx: RuntimeContext): IndexRequest = { \n    val json = new util.HashMap[String, AnyRef] \n    json.put(\"data\", element) \n    Requests.indexRequest.index(\"my-index\").`type`(\"my-type\").source(json) \n  } \n})) \n\n```", "```java\nDataStream<String> input = ...; \n\nMap<String, String> config = Maps.newHashMap(); \nconfig.put(\"bulk.flush.max.actions\", \"1\"); \nconfig.put(\"cluster.name\", \"cluster-name\"); \n\nList<TransportAddress> transports = new ArrayList<String>(); \ntransports.add(new InetSocketTransportAddress(\"es-node-1\", 9300)); \ntransports.add(new InetSocketTransportAddress(\"es-node-2\", 9300)); \ntransports.add(new InetSocketTransportAddress(\"es-node-3\", 9300)); \n\ninput.addSink(new ElasticsearchSink<>(config, transports, new IndexRequestBuilder<String>() { \n    @Override \n    public IndexRequest createIndexRequest(String element, RuntimeContext ctx) { \n        Map<String, Object> json = new HashMap<>(); \n        json.put(\"data\", element); \n\n        return Requests.indexRequest() \n                .index(\"my-index\") \n                .type(\"my-type\") \n                .source(json); \n    } \n})); \n\n```", "```java\nval input: DataStream[String] = ... \n\nval config = new util.HashMap[String, String] \nconfig.put(\"bulk.flush.max.actions\", \"1\") \nconfig.put(\"cluster.name\", \"cluster-name\") \n\nval transports = new ArrayList[String] \ntransports.add(new InetSocketTransportAddress(\"es-node-1\", 9300)) \ntransports.add(new InetSocketTransportAddress(\"es-node-2\", 9300)) \ntransports.add(new InetSocketTransportAddress(\"es-node-3\", 9300)) \n\ntext.addSink(new ElasticsearchSink(config, transports, new IndexRequestBuilder[String] { \n  override def createIndexRequest(element: String, ctx: RuntimeContext): IndexRequest = { \n    val json = new util.HashMap[String, AnyRef] \n    json.put(\"data\", element) \n    Requests.indexRequest.index(\"my-index\").`type`(\"my-type\").source(json) \n  } \n})) \n\n```", "```java\n<dependency> \n  <groupId>org.apache.flink</groupId> \n  <artifactId>flink-connector-cassandra_2.11</artifactId> \n  <version>1.1.4</version> \n</dependency>\n```", "```java\nCassandraSink.addSink(input) \n  .setQuery(\"INSERT INTO cep.events (id, message) values (?, ?);\") \n  .setClusterBuilder(new ClusterBuilder() { \n    @Override \n    public Cluster buildCluster(Cluster.Builder builder) { \n      return builder.addContactPoint(\"127.0.0.1\").build(); \n    } \n  }) \n  .build() \n\n```", "```java\nCassandraSink.addSink(input) \n  .setQuery(\"INSERT INTO cep.events (id, message) values (?, ?);\") \n  .setClusterBuilder(new ClusterBuilder() { \n    @Override \n    public Cluster buildCluster(Cluster.Builder builder) { \n      return builder.addContactPoint(\"127.0.0.1\").build(); \n    } \n  }) \n  .build(); \n\n```", "```java\n// set up the streaming execution environment \nfinal StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(); \n// env.enableCheckpointing(5000); \nnv.setStreamTimeCharacteristic(TimeCharacteristic.EventTime); \nProperties properties = new Properties(); \nproperties.setProperty(\"bootstrap.servers\", \"localhost:9092\"); \n\nproperties.setProperty(\"zookeeper.connect\", \"localhost:2181\"); \nproperties.setProperty(\"group.id\", \"test\"); \n\nFlinkKafkaConsumer09<String> myConsumer = new FlinkKafkaConsumer09<>(\"temp\", new SimpleStringSchema(), \n                      properties); \nmyConsumer.assignTimestampsAndWatermarks(new CustomWatermarkEmitter()); \n\n```", "```java\nTimestamp,Temperature,Sensor-Id\n```", "```java\npublic class CustomWatermarkEmitter implements AssignerWithPunctuatedWatermarks<String> {\n    private static final long serialVersionUID = 1L;\n\n    @Override\n    public long extractTimestamp(String arg0, long arg1) {\n        if (null != arg0 && arg0.contains(\",\")) {\n           String parts[] = arg0.split(\",\");\n           return Long.parseLong(parts[0]);\n           }\n\n          return 0;\n    }\n    @Override\n    public Watermark checkAndGetNextWatermark(String arg0, long arg1) {\n        if (null != arg0 && arg0.contains(\",\")) {\n            String parts[] = arg0.split(\",\");\n            return new Watermark(Long.parseLong(parts[0]));\n        }\n        return null;\n    }\n}\n```", "```java\nDataStream<Tuple2<String, Double>> keyedStream = env.addSource(myConsumer).flatMap(new Splitter()).keyBy(0)\n.timeWindow(Time.seconds(300))\n.apply(new WindowFunction<Tuple2<String, Double>, Tuple2<String, Double>, Tuple, TimeWindow>() {\n    @Override\n    public void apply(Tuple key, TimeWindow window, \n    Iterable<Tuple2<String, Double>> input,\n    Collector<Tuple2<String, Double>> out) throws Exception {\n        double sum = 0L;\n            int count = 0;\n            for (Tuple2<String, Double> record : input) {\n                sum += record.f1;\n                count++;\n            }\n     Tuple2<String, Double> result = input.iterator().next();\n     result.f1 = (sum/count);\n     out.collect(result);\n   }\n});\n```"]