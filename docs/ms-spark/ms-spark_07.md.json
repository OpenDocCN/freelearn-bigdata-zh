["```scala\n15/04/25 17:43:06 ERROR netty.NettyTransport: failed to bind to /192.168.1.103:0, shutting down Netty transport\n15/04/25 17:43:06 WARN util.Utils: Service 'sparkDriver' could not bind on port 0\\. Attempting port 1.\n\n```", "```scala\n[hadoop@hc2r1m2 h2o]$ pwd ; ls -l\n/home/hadoop/h2o\ntotal 15892\n-rw-r--r-- 1 hadoop hadoop 16272364 Apr 11 12:37 sparkling-water-0.2.12-92.zip\n\n```", "```scala\n[hadoop@hc2r1m2 h2o]$ unzip sparkling-water-0.2.12-92.zip\n\n[hadoop@hc2r1m2 h2o]$ ls -d sparkling-water*\nsparkling-water-0.2.12-92  sparkling-water-0.2.12-92.zip\n\n```", "```scala\n[hadoop@hc2r1m2 h2o]$ su -\n[root@hc2r1m2 ~]# cd /home/hadoop/h2o\n[root@hc2r1m2 h2o]# mv sparkling-water-0.2.12-92 /usr/local\n[root@hc2r1m2 h2o]# cd /usr/local\n\n[root@hc2r1m2 local]# chown -R hadoop:hadoop sparkling-water-0.2.12-92\n[root@hc2r1m2 local]#  ln \u2013s sparkling-water-0.2.12-92 h2o\n\n[root@hc2r1m2 local]# ls \u2013lrt  | grep sparkling\ntotal 52\ndrwxr-xr-x   6 hadoop hadoop 4096 Mar 28 02:27 sparkling-water-0.2.12-92\nlrwxrwxrwx   1 root   root     25 Apr 11 12:43 h2o -> sparkling-water-0.2.12-92\n\n```", "```scala\n[hadoop@hc2r1m2 h2o_spark_1_2]$ pwd\n/home/hadoop/spark/h2o_spark_1_2\n\n```", "```scala\n[hadoop@hc2r1m2 h2o_spark_1_2]$ more h2o.sbt\n\nname := \"H 2 O\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.10.4\"\n\nlibraryDependencies += \"org.apache.hadoop\" % \"hadoop-client\" % \"2.3.0\"\n\nlibraryDependencies += \"org.apache.spark\" % \"spark-core\"  % \"1.2.0\" from \"file:///opt/cloudera/parcels/CDH-5.3.3-1.cdh5.3.3.p0.5/jars/spark-assembly-1.2.0-cdh5.3.3-hadoop2.5.0-cdh5.3.3.jar\"\n\nlibraryDependencies += \"org.apache.spark\" % \"mllib\"  % \"1.2.0\" from \"file:///opt/cloudera/parcels/CDH-5.3-1.cdh5.3.3.p0.5/jars/spark-assembly-1.2.0-cdh5.3.3-hadoop2.5.0-cdh5.3.3.jar\"\n\nlibraryDependencies += \"org.apache.spark\" % \"sql\"  % \"1.2.0\" from \"file:///opt/cloudera/parcels/CDH-5.3.3-1.cdh5.3.3.p0.5/jars/spark-assembly-1.2.0-cdh5.3.3-hadoop2.5.0-cdh5.3.3.jar\"\n\nlibraryDependencies += \"org.apache.spark\" % \"h2o\"  % \"0.2.12-95\" from \"file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar\"\n\nlibraryDependencies += \"hex.deeplearning\" % \"DeepLearningModel\"  % \"0.2.12-95\" from \"file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar\"\n\nlibraryDependencies += \"hex\" % \"ModelMetricsBinomial\"  % \"0.2.12-95\" from \"file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar\"\n\nlibraryDependencies += \"water\" % \"Key\"  % \"0.2.12-95\" from \"file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar\"\n\nlibraryDependencies += \"water\" % \"fvec\"  % \"0.2.12-95\" from \"file:///usr/local/h2o/assembly/build/libs/sparkling-water-assembly-0.2.12-95-all.jar\"\n\n```", "```scala\n[hadoop@hc2r1m2 h2o_spark_1_2]$ more run_h2o.bash\n\n#!/bin/bash\n\nSPARK_HOME=/opt/cloudera/parcels/CDH\nSPARK_LIB=$SPARK_HOME/lib\nSPARK_BIN=$SPARK_HOME/bin\nSPARK_SBIN=$SPARK_HOME/sbin\nSPARK_JAR=$SPARK_LIB/spark-assembly-1.2.0-cdh5.3.3-hadoop2.5.0-cdh5.3.3.jar\n\nH2O_PATH=/usr/local/h2o/assembly/build/libs\nH2O_JAR=$H2O_PATH/sparkling-water-assembly-0.2.12-95-all.jar\n\nPATH=$SPARK_BIN:$PATH\nPATH=$SPARK_SBIN:$PATH\nexport PATH\n\ncd $SPARK_BIN\n\n./spark-submit \\\n --class $1 \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 85m \\\n --total-executor-cores 50 \\\n --jars $H2O_JAR \\\n /home/hadoop/spark/h2o_spark_1_2/target/scala-2.10/h-2-o_2.10-1.0.jar\n\n```", "```scala\n  val testFrame:DataFrame = schemaRddTest\n  testFrame.replace( testFrame.find(\"income\"), testFrame.vec(\"income\").toEnum)\n```", "```scala\n15/05/14 14:05:27 WARN TaskSetManager: Lost task 0.0 in stage 9.0 (TID 256, hc2r1m4.semtech-solutions.co.nz): java.lang.ArrayIndexOutOfBoundsException: -128\n```", "```scala\nNumber of attributes: 16\nNumber of cases: 45,225\n\nage workclass fnlwgt education educational-num marital-status occupation relationship race gender capital-gain capital-loss hours-per-week native-country income\n\n39, State-gov, 77516, Bachelors, 13, Never-married, Adm-clerical, Not-in-family, White, Male, 2174, 0, 40, United-States, <=50K\n\n```", "```scala\nval testRDD  = rawTestData\n  .filter(!_.isEmpty)\n  .map(_.split(\",\"))\n  .filter( rawRow => ! rawRow(14).trim.isEmpty )\n```", "```scala\n05-15 13:55:38.176 192.168.1.105:54321   6375   Thread-10 ERRR: Out of Memory and no swap space left from hc2r1m1.semtech-solutions.co.nz/192.168.1.105:54321\n\n```", "```scala\n05-19 13:46:57.300 192.168.1.105:54321   10044  Thread-11 WARN: Unblock allocations; cache emptied but memory is low:  OOM but cache is emptied:  MEM_MAX = 89.5 MB, DESIRED_CACHE = 96.4 MB, CACHE = N/A, POJO = N/A, this request bytes = 36.4 MB\n\n```", "```scala\n --executor-memory 85m\n\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\nimport hex.deeplearning.{DeepLearningModel, DeepLearning}\nimport hex.deeplearning.DeepLearningModel.DeepLearningParameters\nimport org.apache.spark.h2o._\nimport org.apache.spark.mllib\nimport org.apache.spark.mllib.feature.{IDFModel, IDF, HashingTF}\nimport org.apache.spark.rdd.RDD\nimport water.Key\n```", "```scala\nobject h2o_spark_dl2  extends App\n{\n  val sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:7077\"\n  val appName = \"Spark h2o ex1\"\n  val conf = new SparkConf()\n\n  conf.setMaster(sparkMaster)\n  conf.setAppName(appName)\n\n  val sparkCxt = new SparkContext(conf)\n```", "```scala\n  import org.apache.spark.h2o._\n  implicit val h2oContext = new org.apache.spark.h2o.H2OContext(sparkCxt).start()\n\n  import h2oContext._\n  import org.apache.spark.sql._\n\n  implicit val sqlContext = new SQLContext(sparkCxt)\n```", "```scala\n  import sqlContext._\n  openFlow\n```", "```scala\n  val server    = \"hdfs://hc2nn.semtech-solutions.co.nz:8020\"\n  val path      = \"/data/spark/h2o/\"\n\n  val train_csv =  server + path + \"adult.train.data\" // 32,562 rows\n  val test_csv  =  server + path + \"adult.test.data\"  // 16,283 rows\n```", "```scala\n  val rawTrainData = sparkCxt.textFile(train_csv)\n  val rawTestData  = sparkCxt.textFile(test_csv)\n```", "```scala\n  val schemaString = \"age workclass fnlwgt education \u201c + \n\u201ceducationalnum maritalstatus \" + \"occupation relationship race \ngender \u201c + \u201ccapitalgain capitalloss \" + hoursperweek nativecountry income\"\n\n  val schema = StructType( schemaString.split(\" \")\n      .map(fieldName => StructField(fieldName, StringType, true)))\n```", "```scala\n  val trainRDD  = rawTrainData\n         .filter(!_.isEmpty)\n         .map(_.split(\",\"))\n         .filter( rawRow => ! rawRow(14).trim.isEmpty )\n         .map(rawRow => Row(\n               rawRow(0).toString.trim,  rawRow(1).toString.trim,\n               rawRow(2).toString.trim,  rawRow(3).toString.trim,\n               rawRow(4).toString.trim,  rawRow(5).toString.trim,\n               rawRow(6).toString.trim,  rawRow(7).toString.trim,\n               rawRow(8).toString.trim,  rawRow(9).toString.trim,\n               rawRow(10).toString.trim, rawRow(11).toString.trim,\n               rawRow(12).toString.trim, rawRow(13).toString.trim,\n               rawRow(14).toString.trim\n                           )\n             )\n\n  val testRDD  = rawTestData\n         .filter(!_.isEmpty)\n         .map(_.split(\",\"))\n         .filter( rawRow => ! rawRow(14).trim.isEmpty )\n         .map(rawRow => Row(\n               rawRow(0).toString.trim,  rawRow(1).toString.trim,\n               rawRow(2).toString.trim,  rawRow(3).toString.trim,\n               rawRow(4).toString.trim,  rawRow(5).toString.trim,\n               rawRow(6).toString.trim,  rawRow(7).toString.trim,\n               rawRow(8).toString.trim,  rawRow(9).toString.trim,\n               rawRow(10).toString.trim, rawRow(11).toString.trim,\n               rawRow(12).toString.trim, rawRow(13).toString.trim,\n               rawRow(14).toString.trim\n                           )\n             )\n```", "```scala\n  val trainSchemaRDD = sqlContext.applySchema(trainRDD, schema)\n  val testSchemaRDD  = sqlContext.applySchema(testRDD,  schema)\n```", "```scala\n  trainSchemaRDD.registerTempTable(\"trainingTable\")\n  testSchemaRDD.registerTempTable(\"testingTable\")\n```", "```scala\n  val schemaRddTrain = sqlContext.sql(\n    \"\"\"SELECT\n         |age,workclass,education,maritalstatus,\n         |occupation,relationship,race,\n         |gender,hoursperweek,nativecountry,income\n         |FROM trainingTable \"\"\".stripMargin)\n\n  val schemaRddTest = sqlContext.sql(\n    \"\"\"SELECT\n         |age,workclass,education,maritalstatus,\n         |occupation,relationship,race,\n         |gender,hoursperweek,nativecountry,income\n         |FROM testingTable \"\"\".stripMargin)\n```", "```scala\n  val trainFrame:DataFrame = schemaRddTrain\n  trainFrame.replace( trainFrame.find(\"income\"),        trainFrame.vec(\"income\").toEnum)\n  trainFrame.update(null)\n\n  val testFrame:DataFrame = schemaRddTest\n  testFrame.replace( testFrame.find(\"income\"),        testFrame.vec(\"income\").toEnum)\n  testFrame.update(null)\n```", "```scala\n  val testResArray = schemaRddTest.collect()\n  val sizeResults  = testResArray.length\n  var resArray     = new Array[Double](sizeResults)\n\n  for ( i <- 0 to ( resArray.length - 1)) {\n     resArray(i) = testFrame.vec(\"income\").at(i)\n  }\n```", "```scala\n  val dlParams = new DeepLearningParameters()\n\n  dlParams._epochs               = 100\n  dlParams._train                = trainFrame\n  dlParams._valid                = testFrame\n  dlParams._response_column      = 'income\n  dlParams._variable_importances = true\n  val dl = new DeepLearning(dlParams)\n  val dlModel = dl.trainModel.get\n```", "```scala\n  val testH2oPredict  = dlModel.score(schemaRddTest )('predict)\n  val testPredictions  = toRDD[DoubleHolder](testH2oPredict)\n          .collect.map(_.result.getOrElse(Double.NaN))\n  var resAccuracy = 0\n  for ( i <- 0 to ( resArray.length - 1)) {\n    if (  resArray(i) == testPredictions(i) )\n      resAccuracy = resAccuracy + 1\n  }\n\n  println()\n  println( \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\" )\n  println( \">>>>>> Model Test Accuracy = \"\n       + 100*resAccuracy / resArray.length  + \" % \" )\n  println( \">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\" )\n  println()\n```", "```scala\n  water.H2O.shutdown()\n  sparkCxt.stop()\n\n  println( \" >>>>> Script Finished <<<<< \" )\n\n} // end application\n```", "```scala\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n>>>>>> Model Test Accuracy = 83 %\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n\n```", "```scala\n  def getSchema(): String = {\n\n    var schema = \"\"\n    val limit = 28*28\n\n    for (i <- 1 to limit){\n      schema += \"P\" + i.toString + \" \"\n    }\n    schema += \"Label\"\n\n    schema // return value\n  }\n\n  val schemaString = getSchema()\n  val schema = StructType( schemaString.split(\" \")\n      .map(fieldName => StructField(fieldName, IntegerType, false)))\n```", "```scala\nval trainRDD  = rawTrainData.map( rawRow => Row( rawRow.split(\",\").map(_.toInt): _* ))\n```", "```scala\n  val trainRDD  = rawTrainData.map(rawRow => Row.fromSeq(rawRow.split(\",\") .map(_.toInt)))\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.h2o._\n\nobject h2o_spark_ex2  extends App\n{\n  val sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:7077\"\n  val appName = \"Spark h2o ex2\"\n  val conf = new SparkConf()\n\n  conf.setMaster(sparkMaster)\n  conf.setAppName(appName)\n\n  val sparkCxt = new SparkContext(conf)\n```", "```scala\n  implicit val h2oContext = new org.apache.spark.h2o.H2OContext(sparkCxt).start()\n\n  import h2oContext._\n\n  // Open H2O UI\n\n  openFlow\n```", "```scala\n  // shutdown h20\n\n//  water.H2O.shutdown()\n//  sparkCxt.stop()\n\n  println( \" >>>>> Script Finished <<<<< \" )\n\n} // end application\n```", "```scala\n[hadoop@hc2r1m2 h2o_spark_1_2]$ ./run_h2o.bash h2o_spark_ex2\n\n```", "```scala\n15/05/20 13:00:21 INFO H2OContext: Sparkling Water started, status of context:\nSparkling Water Context:\n * number of executors: 4\n * list of used executors:\n (executorId, host, port)\n ------------------------\n (1,hc2r1m4.semtech-solutions.co.nz,54321)\n (3,hc2r1m2.semtech-solutions.co.nz,54321)\n (0,hc2r1m3.semtech-solutions.co.nz,54321)\n (2,hc2r1m1.semtech-solutions.co.nz,54321)\n ------------------------\n\n Open H2O Flow in browser: http://192.168.1.108:54323 (CMD + click in Mac OSX)\n\n```", "```scala\n[hadoop@hc2nn ~]$ cat /etc/hosts | grep hc2\n192.168.1.103 hc2nn.semtech-solutions.co.nz   hc2nn\n192.168.1.105 hc2r1m1.semtech-solutions.co.nz   hc2r1m1\n192.168.1.108 hc2r1m2.semtech-solutions.co.nz   hc2r1m2\n192.168.1.109 hc2r1m3.semtech-solutions.co.nz   hc2r1m3\n192.168.1.110 hc2r1m4.semtech-solutions.co.nz   hc2r1m4\n\n```"]