["```scala\n[hadoop@hc2nn graphx]$ pwd\n/home/hadoop/spark/graphx\n\n[hadoop@hc2nn graphx]$ ls\n src   graph.sbt          project     target\n\n```", "```scala\n[hadoop@hc2nn scala]$ pwd\n/home/hadoop/spark/graphx/src/main/scala\n\n[hadoop@hc2nn scala]$ ls\ngraph1.scala  graph2.scala  graph3.scala  graph4.scala  graph5.scala\n\n```", "```scala\n[hadoop@hc2nn graphx]$ more graph.sbt\n\nname := \"Graph X\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.10.4\"\n\nlibraryDependencies += \"org.apache.hadoop\" % \"hadoop-client\" % \"2.3.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\"  % \"1.0.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-graphx\" % \"1.0.0\"\n\n// If using CDH, also add Cloudera repo\nresolvers += \"Cloudera Repository\" at https://repository.cloudera.com/artifactory/cloudera-repos/\n\n```", "```scala\n[hadoop@hc2nn scala]$ hdfs dfs -ls /data/spark/graphx\nFound 2 items\n-rw-r--r--   3 hadoop supergroup        129 2015-03-01 13:52 /data/spark/graphx/graph1_edges.csv\n-rw-r--r--   3 hadoop supergroup         59 2015-03-01 13:52 /data/spark/graphx/graph1_vertex.csv\n\n```", "```scala\n[hadoop@hc2nn scala]$ hdfs dfs -cat /data/spark/graphx/graph1_vertex.csv\n1,Mike,48\n2,Sarah,45\n3,John,25\n4,Jim,53\n5,Kate,22\n6,Flo,52\n\n```", "```scala\n[hadoop@hc2nn scala]$  hdfs dfs -cat /data/spark/graphx/graph1_edges.csv\n6,1,Sister\n1,2,Husband\n2,1,Wife\n5,1,Daughter\n5,2,Daughter\n3,1,Son\n3,2,Son\n4,1,Friend\n1,5,Father\n1,3,Father\n2,5,Mother\n2,3,Mother\n\n```", "```scala\n[hadoop@hc2nn graphx]$ pwd\n/home/hadoop/spark/graphx\n\n[hadoop@hc2nn graphx]$  sbt package\n\nLoading /usr/share/sbt/bin/sbt-launch-lib.bash\n[info] Set current project to Graph X (in build file:/home/hadoop/spark/graphx/)\n[info] Compiling 5 Scala sources to /home/hadoop/spark/graphx/target/scala-2.10/classes...\n[info] Packaging /home/hadoop/spark/graphx/target/scala-2.10/graph-x_2.10-1.0.jar ...\n[info] Done packaging.\n[success] Total time: 30 s, completed Mar 3, 2015 5:27:10 PM\n\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n```", "```scala\nobject graph1 extends App\n{\n```", "```scala\n  val hdfsServer = \"hdfs://hc2nn.semtech-solutions.co.nz:8020\"\n  val hdfsPath   = \"/data/spark/graphx/\"\n  val vertexFile = hdfsServer + hdfsPath + \"graph1_vertex.csv\"\n  val edgeFile   = hdfsServer + hdfsPath + \"graph1_edges.csv\"\n```", "```scala\n  val sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:7077\"\n  val appName = \"Graph 1\"\n  val conf = new SparkConf()\n  conf.setMaster(sparkMaster)\n  conf.setAppName(appName)\n```", "```scala\n  val sparkCxt = new SparkContext(conf)\n```", "```scala\n  val vertices: RDD[(VertexId, (String, String))] =\n      sparkCxt.textFile(vertexFile).map { line =>\n        val fields = line.split(\",\")\n        ( fields(0).toLong, ( fields(1), fields(2) ) )\n  }\n```", "```scala\n  val edges: RDD[Edge[String]] =\n      sparkCxt.textFile(edgeFile).map { line =>\n        val fields = line.split(\",\")\n        Edge(fields(0).toLong, fields(1).toLong, fields(2))\n  }\n```", "```scala\n  val default = (\"Unknown\", \"Missing\")\n  val graph = Graph(vertices, edges, default)\n```", "```scala\n  println( \"vertices : \" + graph.vertices.count )\n  println( \"edges    : \" + graph.edges.count )\n```", "```scala\nspark-submit \\\n  --class graph1 \\\n  --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n  --executor-memory 700M \\\n  --total-executor-cores 100 \\\n /home/hadoop/spark/graphx/target/scala-2.10/graph-x_2.10-1.0.jar\n```", "```scala\nvertices : 6\nedges    : 12\n\n```", "```scala\n  val c1 = graph.vertices.filter { case (id, (name, age)) => age.toLong > 40 }.count\n\n  val c2 = graph.edges.filter { case Edge(from, to, property)\n    => property == \"Father\" | property == \"Mother\" }.count\n\n  println( \"Vertices count : \" + c1 )\n  println( \"Edges    count : \" + c2 )\n```", "```scala\nVertices count : 4\nEdges    count : 4\n\n```", "```scala\n  val tolerance = 0.0001\n  val ranking = graph.pageRank(tolerance).vertices\n  val rankByPerson = vertices.join(ranking).map {\n    case (id, ( (person,age) , rank )) => (rank, id, person)\n  }\n```", "```scala\n  rankByPerson.collect().foreach {\n    case (rank, id, person) =>\n      println ( f\"Rank $rank%1.2f id $id person $person\")\n  }\n```", "```scala\nRank 0.15 id 4 person Jim\nRank 0.15 id 6 person Flo\nRank 1.62 id 2 person Sarah\nRank 1.82 id 1 person Mike\nRank 1.13 id 3 person John\nRank 1.13 id 5 person Kate\n\n```", "```scala\n  val tCount = graph.triangleCount().vertices\n  println( tCount.collect().mkString(\"\\n\") )\n```", "```scala\n(4,0)\n(6,0)\n(2,4)\n(1,4)\n(3,2)\n(5,2)\n\n```", "```scala\n  val iterations = 1000\n  val connected  = graph.connectedComponents().vertices\n  val connectedS = graph.stronglyConnectedComponents(iterations).vertices\n```", "```scala\n  val connByPerson = vertices.join(connected).map {\n    case (id, ( (person,age) , conn )) => (conn, id, person)\n  }\n\n  val connByPersonS = vertices.join(connectedS).map {\n    case (id, ( (person,age) , conn )) => (conn, id, person)\n  }\nThe results are then output using a case statement, and formatted printing:\n  connByPerson.collect().foreach {\n    case (conn, id, person) =>\n      println ( f\"Weak $conn  $id $person\" )\n  }\n```", "```scala\nWeak 1  4 Jim\nWeak 1  6 Flo\nWeak 1  2 Sarah\nWeak 1  1 Mike\nWeak 1  3 John\nWeak 1  5 Kate\n\n```", "```scala\n  connByPersonS.collect().foreach {\n    case (conn, id, person) =>\n      println ( f\"Strong $conn  $id $person\" )\n  }\n```", "```scala\n6,1,Sister\n4,1,Friend\n\n1,Mike,48\n4,Jim,53\n6,Flo,52\n\n```", "```scala\nStrong 4  4 Jim\nStrong 6  6 Flo\nStrong 1  2 Sarah\nStrong 1  1 Mike\nStrong 1  3 John\nStrong 1  5 Kate\n\n```", "```scala\n[root@hc1r1m1 bin]# yum -y install docker-io\n\n```", "```scala\n[root@hc1r1m1 ~]# yum-config-manager --enable public_ol6_latest\n[root@hc1r1m1 ~]# yum install device-mapper-event-libs\n\n```", "```scala\n/usr/bin/docker: relocation error: /usr/bin/docker: symbol dm_task_get_info_with_deferred_remove, version Base not defined in file libdevmapper.so.1.02 with link time reference\n\n```", "```scala\n[root@hc1r1m1 ~]# docker version\nClient version: 1.4.1\nClient API version: 1.16\nGo version (client): go1.3.3\nGit commit (client): 5bc2ff8/1.4.1\nOS/Arch (client): linux/amd64\nServer version: 1.4.1\nServer API version: 1.16\nGo version (server): go1.3.3\nGit commit (server): 5bc2ff8/1.4.1\n\n```", "```scala\n[root@hc1r1m1 bin]# service docker start\n[root@hc1r1m1 bin]# chkconfig docker on\n\n```", "```scala\n[root@hc1r1m1 ~]# docker pull sequenceiq/hadoop-docker:2.4.1\nStatus: Downloaded newer image for sequenceiq/hadoop-docker:2.4.1\n\n[root@hc1r1m1 ~]# docker pull kbastani/docker-neo4j:latest\nStatus: Downloaded newer image for kbastani/docker-neo4j:latest\n\n[root@hc1r1m1 ~]# docker pull kbastani/neo4j-graph-analytics:latest\nStatus: Downloaded newer image for kbastani/neo4j-graph-analytics:latest\n\n```", "```scala\n[root@hc1r1m1 ~]# docker run -i -t --name hdfs sequenceiq/hadoop-docker:2.4.1 /etc/bootstrap.sh \u2013bash\n\nStarting sshd:                                [  OK  ]\nStarting namenodes on [26d939395e84]\n26d939395e84: starting namenode, logging to /usr/local/hadoop/logs/hadoop-root-namenode-26d939395e84.out\nlocalhost: starting datanode, logging to /usr/local/hadoop/logs/hadoop-root-datanode-26d939395e84.out\nStarting secondary namenodes [0.0.0.0]\n0.0.0.0: starting secondarynamenode, logging to /usr/local/hadoop/logs/hadoop-root-secondarynamenode-26d939395e84.out\nstarting yarn daemons\nstarting resourcemanager, logging to /usr/local/hadoop/logs/yarn--resourcemanager-26d939395e84.out\nlocalhost: starting nodemanager, logging to /usr/local/hadoop/logs/yarn-root-nodemanager-26d939395e84.out\n\n```", "```scala\n[root@hc1r1m1 ~]# docker run -i -t --name mazerunner --link hdfs:hdfs kbastani/neo4j-graph-analytics\n\n```", "```scala\n[*] Waiting for messages. To exit press CTRL+C\n\n```", "```scala\n[root@hc1r1m1 ~]# docker run -d -P -v /home/hadoop/neo4j/data:/opt/data --name graphdb --link mazerunner:mazerunner --link hdfs:hdfs kbastani/docker-neo4j\n\n```", "```scala\n[root@hc1r1m1 data]# pwd\n/home/hadoop/neo4j/data\n\n[root@hc1r1m1 data]# ls\ngraph.db\n\n```", "```scala\n[root@hc1r1m1 data]# docker inspect --format=\"{{.NetworkSettings.IPAddress}}\" graphdb\n172.17.0.5\n\n[root@hc1r1m1 data]# curl  172.17.0.5:7474\n{\n \"management\" : \"http://172.17.0.5:7474/db/manage/\",\n \"data\" : \"http://172.17.0.5:7474/db/data/\"\n}\n\n```"]