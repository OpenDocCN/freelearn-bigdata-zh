["```scala\n[hadoop@hc2nn ~]# su -\n[root@hc2nn ~]# cd /tmp\n[root@hc2nn ~]#wget http://repo.scala-sbt.org/scalasbt/sbt-native-packages/org/scala-sbt/sbt/0.13.1/sbt.rpm\n[root@hc2nn ~]# rpm -ivh sbt.rpm\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ pwd\n/home/hadoop/spark/nbayes\n[hadoop@hc2nn nbayes]$ cat bayes.sbt\n\nname := \"Naive Bayes\"\n\nversion := \"1.0\"\n\nscalaVersion := \"2.10.4\"\n\nlibraryDependencies += \"org.apache.hadoop\" % \"hadoop-client\" % \"2.3.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\"  % \"1.0.0\"\n\nlibraryDependencies += \"org.apache.spark\" %% \"spark-mllib\" % \"1.0.0\"\n\n// If using CDH, also add Cloudera repo\nresolvers += \"Cloudera Repository\" at https://repository.cloudera.com/artifactory/cloudera-repos/\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ sbt compile\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ sbt package\n\n```", "```scala\n[root@hc2nn ~]# cd /etc/yum.repos.d\n[root@hc2nn yum.repos.d]# cat  cloudera-cdh5.repo\n\n[cloudera-cdh5]\n# Packages for Cloudera's Distribution for Hadoop, Version 5, on RedHat or CentOS 6 x86_64\nname=Cloudera's Distribution for Hadoop, Version 5\nbaseurl=http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/5/\ngpgkey = http://archive.cloudera.com/cdh5/redhat/6/x86_64/cdh/RPM-GPG-KEY-cloudera\ngpgcheck = 1\n\n```", "```scala\n[root@hc2nn ~]# yum install spark-core spark-master spark-worker spark-history-server spark-python\n\n```", "```scala\n[root@hc2nn ~]# cd /etc/spark/conf\n\n[root@hc2nn conf]# cat slaves\n# A Spark Worker will be started on each of the machines listed below.\nhc2r1m1\nhc2r1m2\nhc2r1m3\nhc2r1m4\n\n```", "```scala\nexport STANDALONE_SPARK_MASTER_HOST=hc2nn.semtech-solutions.co.nz\nexport SPARK_MASTER_IP=$STANDALONE_SPARK_MASTER_HOST\n\nexport SPARK_MASTER_WEBUI_PORT=18080\nexport SPARK_MASTER_PORT=7077\nexport SPARK_WORKER_PORT=7078\nexport SPARK_WORKER_WEBUI_PORT=18081\n\n```", "```scala\necho \"hc2r1m1 - start worker\"\nssh   hc2r1m1 'service spark-worker start'\n\necho \"hc2r1m2 - start worker\"\nssh   hc2r1m2 'service spark-worker start'\n\necho \"hc2r1m3 - start worker\"\nssh   hc2r1m3 'service spark-worker start'\n\necho \"hc2r1m4 - start worker\"\nssh   hc2r1m4 'service spark-worker start'\n\necho \"hc2nn - start master server\"\nservice spark-master         start\nservice spark-history-server start\n\n```", "```scala\n60% of emails are spam\n 80% of spam emails contain the word buy\n 20% of spam emails don't contain the word buy\n40% of emails are not spam\n 10% of non spam emails contain the word buy\n 90% of non spam emails don't contain the word buy\n\n```", "```scala\nP(Spam) = the probability that an email is spam = 0.6\nP(Not Spam) = the probability that an email is not spam = 0.4\nP(Buy|Spam) = the probability that an email that is spam has the word buy = 0.8\nP(Buy|Not Spam) = the probability that an email that is not spam has the word buy = 0.1\n\n```", "```scala\nP(Spam|Buy) = ( 0.8 * 0.6 ) / (( 0.8 * 0.6 )  + ( 0.1 * 0.4 )  )  = ( .48 ) / ( .48 + .04 )\n= .48 / .52 = .923\n\n```", "```scala\nReason,Month,Year,WeekType,TimeBand,BreathAlcohol,AgeBand,Gender\nSuspicion of Alcohol,Jan,2013,Weekday,12am-4am,75,30-39,Male\nMoving Traffic Violation,Jan,2013,Weekday,12am-4am,0,20-24,Male\nRoad Traffic Collision,Jan,2013,Weekend,12pm-4pm,0,20-24,Female\n\n```", "```scala\n[hadoop@hc2nn ~]$ hdfs dfs -cat /data/spark/nbayes/DigitalBreathTestData2013-MALE2.csv | wc -l\n467054\n\n[hadoop@hc2nn ~]$ hdfs dfs -cat /data/spark/nbayes/DigitalBreathTestData2013-MALE2.csv | head -5\nMale,Suspicion of Alcohol,Weekday,12am-4am,75,30-39\nMale,Moving Traffic Violation,Weekday,12am-4am,0,20-24\nMale,Suspicion of Alcohol,Weekend,4am-8am,12,40-49\nMale,Suspicion of Alcohol,Weekday,12am-4am,0,50-59\nFemale,Road Traffic Collision,Weekend,12pm-4pm,0,20-24\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ pwd\n/home/hadoop/spark/nbayes\n[hadoop@hc2nn nbayes]$ ls\nbayes.sbt     target   project   src\n\n```", "```scala\n[hadoop@hc2nn scala]$ pwd\n/home/hadoop/spark/nbayes/src/main/scala\n[hadoop@hc2nn scala]$ ls\nbayes1.scala  convert.scala\n\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\n```", "```scala\nobject convert1 extends App\n{\n\n```", "```scala\ndef enumerateCsvRecord( colData:Array[String]): String =\n{\n\n```", "```scala\n val colVal1 =\n colData(0) match\n {\n case \"Male\"                          => 0\n case \"Female\"                        => 1\n case \"Unknown\"                       => 2\n case _                               => 99\n }\n\n val colVal2 =\n colData(1) match\n {\n case \"Moving Traffic Violation\"      => 0\n case \"Other\"                         => 1\n case \"Road Traffic Collision\"        => 2\n case \"Suspicion of Alcohol\"          => 3\n case _                               => 99\n }\n\n val colVal3 =\n colData(2) match\n {\n case \"Weekday\"                       => 0\n case \"Weekend\"                       => 0\n case _                               => 99\n }\n\n val colVal4 =\n colData(3) match\n {\n case \"12am-4am\"                      => 0\n case \"4am-8am\"                       => 1\n case \"8am-12pm\"                      => 2\n case \"12pm-4pm\"                      => 3\n case \"4pm-8pm\"                       => 4\n case \"8pm-12pm\"                      => 5\n case _                               => 99\n }\n\n val colVal5 = colData(4)\n\n val colVal6 =\n colData(5) match\n {\n case \"16-19\"                         => 0\n case \"20-24\"                         => 1\n case \"25-29\"                         => 2\n case \"30-39\"                         => 3\n case \"40-49\"                         => 4\n case \"50-59\"                         => 5\n case \"60-69\"                         => 6\n case \"70-98\"                         => 7\n case \"Other\"                         => 8\n case _                               => 99\n }\n\n```", "```scala\n val lineString = colVal1+\",\"+colVal2+\" \"+colVal3+\" \"+colVal4+\" \"+colVal5+\" \"+colVal6\n\n return lineString\n}\n\n```", "```scala\nval hdfsServer = \"hdfs://hc2nn.semtech-solutions.co.nz:8020\"\nval hdfsPath   = \"/data/spark/nbayes/\"\nval inDataFile  = hdfsServer + hdfsPath + \"DigitalBreathTestData2013-MALE2.csv\"\nval outDataFile = hdfsServer + hdfsPath + \"result\"\n\nval sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:7077\"\nval appName = \"Convert 1\"\nval sparkConf = new SparkConf()\n\nsparkConf.setMaster(sparkMaster)\nsparkConf.setAppName(appName)\n\nval sparkCxt = new SparkContext(sparkConf)\n\n```", "```scala\nval csvData = sparkCxt.textFile(inDataFile)\nprintln(\"Records in  : \"+ csvData.count() )\n\n```", "```scala\n val enumRddData = csvData.map\n {\n csvLine =>\n val colData = csvLine.split(',')\n\n enumerateCsvRecord(colData)\n\n }\n\n```", "```scala\n println(\"Records out : \"+ enumRddData.count() )\n\n enumRddData.saveAsTextFile(outDataFile)\n\n} // end object\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ sbt package\nLoading /usr/share/sbt/bin/sbt-launch-lib.bash\n....\n[info] Done packaging.\n[success] Total time: 37 s, completed Feb 19, 2015 1:23:55 PM\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ pwd\n/home/hadoop/spark/nbayes\n[hadoop@hc2nn nbayes]$ ls -l target/scala-2.10\ntotal 24\ndrwxrwxr-x 2 hadoop hadoop  4096 Feb 19 13:23 classes\n-rw-rw-r-- 1 hadoop hadoop 17609 Feb 19 13:23 naive-bayes_2.10-1.0.jar\n\n```", "```scala\nspark-submit \\\n --class convert1 \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 700M \\\n --total-executor-cores 100 \\\n /home/hadoop/spark/nbayes/target/scala-2.10/naive-bayes_2.10-1.0.jar\n\n```", "```scala\n[hadoop@hc2nn nbayes]$  hdfs dfs -ls /data/spark/nbayes\nFound 2 items\n-rw-r--r--   3 hadoop supergroup   24645166 2015-01-29 21:27 /data/spark/nbayes/DigitalBreathTestData2013-MALE2.csv\ndrwxr-xr-x   - hadoop supergroup          0 2015-02-19 13:36 /data/spark/nbayes/result\n\n[hadoop@hc2nn nbayes]$ hdfs dfs -ls /data/spark/nbayes/result\nFound 3 items\n-rw-r--r--   3 hadoop supergroup          0 2015-02-19 13:36 /data/spark/nbayes/result/_SUCCESS\n-rw-r--r--   3 hadoop supergroup    2828727 2015-02-19 13:36 /data/spark/nbayes/result/part-00000\n-rw-r--r--   3 hadoop supergroup    2865499 2015-02-19 13:36 /data/spark/nbayes/result/part-00001\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ hdfs dfs -cat /data/spark/nbayes/result/part* > ./DigitalBreathTestData2013-MALE2a.csv\n\n[hadoop@hc2nn nbayes]$ head -5 DigitalBreathTestData2013-MALE2a.csv\n0,3 0 0 75 3\n0,0 0 0 0 1\n0,3 0 1 12 4\n0,3 0 0 0 5\n1,2 0 3 0 1\n\n[hadoop@hc2nn nbayes]$ hdfs dfs -put ./DigitalBreathTestData2013-MALE2a.csv /data/spark/nbayes\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ hdfs dfs -ls /data/spark/nbayes\nFound 3 items\n-rw-r--r--   3 hadoop supergroup   24645166 2015-01-29 21:27 /data/spark/nbayes/DigitalBreathTestData2013-MALE2.csv\n-rw-r--r--   3 hadoop supergroup    5694226 2015-02-19 13:39 /data/spark/nbayes/DigitalBreathTestData2013-MALE2a.csv\ndrwxr-xr-x   - hadoop supergroup          0 2015-02-19 13:36 /data/spark/nbayes/result\n\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.mllib.classification.NaiveBayes\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.regression.LabeledPoint\n\nobject bayes1 extends App\n{\n\n```", "```scala\n val hdfsServer = \"hdfs://hc2nn.semtech-solutions.co.nz:8020\"\n val hdfsPath   = \"/data/spark/nbayes/\"\n\n val dataFile = hdfsServer+hdfsPath+\"DigitalBreathTestData2013-MALE2a.csv\"\n\n val sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:7077\"\n val appName = \"Naive Bayes 1\"\n val conf = new SparkConf()\n conf.setMaster(sparkMaster)\n conf.setAppName(appName)\n\n val sparkCxt = new SparkContext(conf)\n\n```", "```scala\n val csvData = sparkCxt.textFile(dataFile)\n\n val ArrayData = csvData.map\n {\n csvLine =>\n val colData = csvLine.split(',')\n LabeledPoint(colData(0).toDouble, Vectors.dense(colData(1).split(' ').map(_.toDouble)))\n }\n\n```", "```scala\n val divData = ArrayData.randomSplit(Array(0.7, 0.3), seed = 13L)\n\n val trainDataSet = divData(0)\n val testDataSet  = divData(1)\n\n```", "```scala\n val nbTrained = NaiveBayes.train(trainDataSet)\n val nbPredict = nbTrained.predict(testDataSet.map(_.features))\n\n```", "```scala\n val predictionAndLabel = nbPredict.zip(testDataSet.map(_.label))\n val accuracy = 100.0 * predictionAndLabel.filter(x => x._1 == x._2).count() / testDataSet.count()\n println( \"Accuracy : \" + accuracy );\n}\n\n```", "```scala\nspark-submit \\\n --class bayes1 \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 700M \\\n --total-executor-cores 100 \\\n /home/hadoop/spark/nbayes/target/scala-2.10/naive-bayes_2.10-1.0.jar\n\n```", "```scala\nAccuracy: 43.30\n\n```", "```scala\nname := \"K-Means\"\n\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.clustering.{KMeans,KMeansModel}\n\nobject kmeans1 extends App\n{\n\n```", "```scala\n val hdfsServer = \"hdfs://hc2nn.semtech-solutions.co.nz:8020\"\n val hdfsPath   = \"/data/spark/kmeans/\"\n\n val dataFile   = hdfsServer + hdfsPath + \"DigitalBreathTestData2013-MALE2a.csv\"\n\n val sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:7077\"\n val appName = \"K-Means 1\"\n val conf = new SparkConf()\n\n conf.setMaster(sparkMaster)\n conf.setAppName(appName)\n\n val sparkCxt = new SparkContext(conf)\n\n```", "```scala\n val csvData = sparkCxt.textFile(dataFile)\n val VectorData = csvData.map\n {\n csvLine =>\n Vectors.dense( csvLine.split(',').map(_.toDouble))\n }\n\n```", "```scala\n val kMeans = new KMeans\n val numClusters         = 3\n val maxIterations       = 50\n\n```", "```scala\n val initializationMode  = KMeans.K_MEANS_PARALLEL\n val numRuns             = 1\n val numEpsilon          = 1e-4\n\n kMeans.setK( numClusters )\n kMeans.setMaxIterations( maxIterations )\n kMeans.setInitializationMode( initializationMode )\n kMeans.setRuns( numRuns )\n kMeans.setEpsilon( numEpsilon )\n\n```", "```scala\n VectorData.cache\n val kMeansModel = kMeans.run( VectorData )\n\n```", "```scala\n val kMeansCost = kMeansModel.computeCost( VectorData )\n\n println( \"Input data rows : \" + VectorData.count() )\n println( \"K-Means Cost    : \" + kMeansCost )\n\n```", "```scala\n kMeansModel.clusterCenters.foreach{ println }\n\n```", "```scala\n val clusterRddInt = kMeansModel.predict( VectorData )\n\n val clusterCount = clusterRddInt.countByValue\n\n clusterCount.toList.foreach{ println }\n\n} // end object kmeans1\n\n```", "```scala\n[hadoop@hc2nn kmeans]$ pwd\n/home/hadoop/spark/kmeans\n[hadoop@hc2nn kmeans]$ sbt package\n\nLoading /usr/share/sbt/bin/sbt-launch-lib.bash\n[info] Set current project to K-Means (in build file:/home/hadoop/spark/kmeans/)\n[info] Compiling 2 Scala sources to /home/hadoop/spark/kmeans/target/scala-2.10/classes...\n[info] Packaging /home/hadoop/spark/kmeans/target/scala-2.10/k-means_2.10-1.0.jar ...\n[info] Done packaging.\n[success] Total time: 20 s, completed Feb 19, 2015 5:02:07 PM\n\n```", "```scala\n[hadoop@hc2nn nbayes]$ hdfs dfs -ls /data/spark/kmeans\nFound 3 items\n-rw-r--r--   3 hadoop supergroup   24645166 2015-02-05 21:11 /data/spark/kmeans/DigitalBreathTestData2013-MALE2.csv\n-rw-r--r--   3 hadoop supergroup    5694226 2015-02-05 21:48 /data/spark/kmeans/DigitalBreathTestData2013-MALE2a.csv\ndrwxr-xr-x   - hadoop supergroup          0 2015-02-05 21:46 /data/spark/kmeans/result\n\n```", "```scala\nspark-submit \\\n --class kmeans1 \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 700M \\\n --total-executor-cores 100 \\\n /home/hadoop/spark/kmeans/target/scala-2.10/k-means_2.10-1.0.jar\n\n```", "```scala\nInput data rows : 467054\nK-Means Cost    : 5.40312223450789E7\n\n```", "```scala\n[0.24698249738061878,1.3015883142472253,0.005830116872250263,2.9173747788555207,1.156645130895448,3.4400290524342454]\n\n[0.3321793984152627,1.784137241326256,0.007615970459266097,2.5831987075928917,119.58366028156011,3.8379106085083468]\n\n[0.25247226760684494,1.702510963969387,0.006384899819416975,2.231404248000688,52.202897927594805,3.551509158139135]\n\n```", "```scala\n(0,407539)\n(1,12999)\n(2,46516)\n\n```", "```scala\nhttps://github.com/apache/spark/pull/1290.\n\n```", "```scala\nhttps://github.com/bgreeven/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/ann/ArtificialNeuralNetwork.scala\nhttps://github.com/bgreeven/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/classification/ANNClassifier.scala\n\n```", "```scala\nexport STANDALONE_SPARK_MASTER_HOST=hc2nn.semtech-solutions.co.nz\nexport SPARK_MASTER_IP=$STANDALONE_SPARK_MASTER_HOST\nexport SPARK_HOME=/home/hadoop/spark/spark\nexport SPARK_LAUNCH_WITH_SCALA=0\nexport SPARK_MASTER_WEBUI_PORT=19080\nexport SPARK_MASTER_PORT=8077\nexport SPARK_WORKER_PORT=8078\nexport SPARK_WORKER_WEBUI_PORT=19081\nexport SPARK_WORKER_DIR=/var/run/spark/work\nexport SPARK_LOG_DIR=/var/log/spark\nexport SPARK_HISTORY_SERVER_LOG_DIR=/var/log/spark\nexport SPARK_PID_DIR=/var/run/spark/\nexport HADOOP_CONF_DIR=/etc/hadoop/conf\nexport SPARK_JAR_PATH=${SPARK_HOME}/assembly/target/scala-2.10/\nexport SPARK_JAR=${SPARK_JAR_PATH}/spark-assembly-1.3.0-SNAPSHOT-hadoop2.3.0-cdh5.1.2.jar\nexport JAVA_HOME=/usr/lib/jvm/java-1.7.0\nexport SPARK_LOCAL_IP=192.168.1.103\n\n```", "```scala\nhttp://spark.apache.org/docs/latest/building-spark.html\n\n```", "```scala\nwget http://repos.fedorapeople.org/repos/dchen/apache-maven/epel-apache-maven.repo -O /etc/yum.repos.d/epel-apache-maven.repo\n\n```", "```scala\n[root@hc2nn ~]# ls -l /etc/yum.repos.d/epel-apache-maven.repo\n-rw-r--r-- 1 root root 445 Mar  4  2014 /etc/yum.repos.d/epel-apache-maven.repo\n\n```", "```scala\n[root@hc2nn ~]# yum install apache-maven\n[root@hc2nn ~]# ls -l /usr/share/apache-maven/bin/mvn\n-rwxr-xr-x 1 root root 6185 Dec 15 06:30 /usr/share/apache-maven/bin/mvn\n\n```", "```scala\ncd /home/hadoop/spark/spark/conf ; . ./spark-env.sh ; cd ..\n\nmvn  -Pyarn -Phadoop-2.3  -Dhadoop.version=2.3.0-cdh5.1.2 -DskipTests clean package | tee build.log 2>&1\n\n[INFO] ----------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] ----------------------------------------------------------\n[INFO] Total time: 44:20 min\n[INFO] Finished at: 2015-02-16T12:20:28+13:00\n[INFO] Final Memory: 76M/925M\n[INFO] ----------------------------------------------------------\n\n```", "```scala\n15/02/15 12:41:41 ERROR executor.Executor: Exception in task 0.1 in stage 0.0 (TID 2)\njava.lang.VerifyError: class org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$GetBlockLocationsRequestProto overrides final method getUnknownFields.()Lcom/google/protobuf/UnknownFieldSet;\n at java.lang.ClassLoader.defineClass1(Native Method)\n\n```", "```scala\n[hadoop@hc2nn spark]$ cd /home/hadoop/spark\n[hadoop@hc2nn spark]$ tar cvf spark_bld.tar spark\n[hadoop@hc2nn spark]$ scp ./spark_bld.tar hadoop@hc2r1m1:/home/hadoop/spark/spark_bld.tar\n\n```", "```scala\n[hadoop@hc2r1m1 ~]$ mkdir spark ; mv spark_bld.tar spark\n[hadoop@hc2r1m1 ~]$ cd spark ; ls\nspark_bld.tar\n[hadoop@hc2r1m1 spark]$ tar xvf spark_bld.tar\n\n```", "```scala\ncd /home/hadoop/spark/spark/conf ;  . ./spark-env.sh ; cd ../sbin\necho \"hc2nn - start master server\"\n./start-master.sh\necho \"sleep 5000 ms\"\nsleep 5\necho \"hc2nn - start history server\"\n./start-history-server.sh\necho \"Start Spark slaves workers\"\n./start-slaves.sh\n\n```", "```scala\nhttp://hc2nn.semtech-solutions.co.nz:19080/\n\n```", "```scala\nSpark Master at spark://hc2nn.semtech-solutions.co.nz:8077\n\n```", "```scala\n [hadoop@hc2nn ann]$ pwd\n/home/hadoop/spark/ann\n\n [hadoop@hc2nn ann]$ ls\nann.sbt    project  src  target\n\n```", "```scala\nname := \"A N N\"\nversion := \"1.0\"\nscalaVersion := \"2.10.4\"\nlibraryDependencies += \"org.apache.hadoop\" % \"hadoop-client\" % \"2.3.0\"\nlibraryDependencies += \"org.apache.spark\" % \"spark-core\"  % \"1.3.0\" from \"file:///home/hadoop/spark/spark/core/target/spark-core_2.10-1.3.0-SNAPSHOT.jar\"\nlibraryDependencies += \"org.apache.spark\" % \"spark-mllib\" % \"1.3.0\" from \"file:///home/hadoop/spark/spark/mllib/target/spark-mllib_2.10-1.3.0-SNAPSHOT.jar\"\nlibraryDependencies += \"org.apache.spark\" % \"akka\" % \"1.3.0\" from \"file:///home/hadoop/spark/spark/assembly/target/scala-2.10/spark-assembly-1.3.0-SNAPSHOT-hadoop2.3.0-cdh5.1.2.jar\"\n\n```", "```scala\n[hadoop@hc2nn scala]$ pwd\n/home/hadoop/spark/ann/src/main/scala\n\n[hadoop@hc2nn scala]$ ls\ntest_ann1.scala  test_ann2.scala\n\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\nimport org.apache.spark.mllib.classification.ANNClassifier\nimport org.apache.spark.mllib.regression.LabeledPoint\nimport org.apache.spark.mllib.linalg.Vectors\nimport org.apache.spark.mllib.linalg._\nimport org.apache.spark.rdd.RDD\n\nobject testann1 extends App\n{\n\n```", "```scala\n val server = \"hdfs://hc2nn.semtech-solutions.co.nz:8020\"\n val path   = \"/data/spark/ann/\"\n\n val data1 = server + path + \"close_square.img\"\n val data2 = server + path + \"close_triangle.img\"\n val data3 = server + path + \"lines.img\"\n val data4 = server + path + \"open_square.img\"\n val data5 = server + path + \"open_triangle.img\"\n val data6 = server + path + \"plus.img\"\n\n```", "```scala\n val sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:8077\"\n val appName = \"ANN 1\"\n val conf = new SparkConf()\n\n conf.setMaster(sparkMaster)\n conf.setAppName(appName)\n\n val sparkCxt = new SparkContext(conf)\n\n```", "```scala\n val rData1 = sparkCxt.textFile(data1).map(_.split(\" \").map(_.toDouble)).collect\n val rData2 = sparkCxt.textFile(data2).map(_.split(\" \").map(_.toDouble)).collect\n val rData3 = sparkCxt.textFile(data3).map(_.split(\" \").map(_.toDouble)).collect\n val rData4 = sparkCxt.textFile(data4).map(_.split(\" \").map(_.toDouble)).collect\n val rData5 = sparkCxt.textFile(data5).map(_.split(\" \").map(_.toDouble)).collect\n val rData6 = sparkCxt.textFile(data6).map(_.split(\" \").map(_.toDouble)).collect\n\n val inputs = Array[Array[Double]] (\n rData1(0), rData2(0), rData3(0), rData4(0), rData5(0), rData6(0) )\n\n val outputs = Array[Double]( 0.1, 0.2, 0.3, 0.4, 0.5, 0.6 )\n\n```", "```scala\n val ioData = inputs.zip( outputs )\n val lpData = ioData.map{ case(features,label) =>\n\n LabeledPoint( label, Vectors.dense(features) )\n }\n val rddData = sparkCxt.parallelize( lpData )\n\n```", "```scala\n val hiddenTopology : Array[Int] = Array( 100, 100 )\n val maxNumIterations = 1000\n val convTolerance    = 1e-4\n val batchSize        = 6\n\n val annModel = ANNClassifier.train(rddData,\n batchSize,\n hiddenTopology,\n maxNumIterations,\n convTolerance)\n\n```", "```scala\n val rPredictData = inputs.map{ case(features) =>\n\n ( Vectors.dense(features) )\n }\n val rddPredictData = sparkCxt.parallelize( rPredictData )\n val predictions = annModel.predict( rddPredictData )\n\n```", "```scala\n predictions.toArray().foreach( value => println( \"prediction > \" + value ) )\n} // end ann1\n\n```", "```scala\n[hadoop@hc2nn ann]$ pwd\n/home/hadoop/spark/ann\n[hadoop@hc2nn ann]$ sbt package\n\n```", "```scala\n/home/hadoop/spark/spark/bin/spark-submit \\\n --class testann1 \\\n --master spark://hc2nn.semtech-solutions.co.nz:8077  \\\n --executor-memory 700M \\\n --total-executor-cores 100 \\\n /home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar\n\n```", "```scala\nprediction > 0.1\nprediction > 0.2\nprediction > 0.3\nprediction > 0.4\nprediction > 0.5\nprediction > 0.6\n\n```", "```scala\nobject testann2 extends App\n\n```", "```scala\n val tData1 = server + path + \"close_square_test.img\"\n val tData2 = server + path + \"close_triangle_test.img\"\n val tData3 = server + path + \"lines_test.img\"\n val tData4 = server + path + \"open_square_test.img\"\n val tData5 = server + path + \"open_triangle_test.img\"\n val tData6 = server + path + \"plus_test.img\"\n\n```", "```scala\n val rtData1 = sparkCxt.textFile(tData1).map(_.split(\" \").map(_.toDouble)).collect\n val rtData2 = sparkCxt.textFile(tData2).map(_.split(\" \").map(_.toDouble)).collect\n val rtData3 = sparkCxt.textFile(tData3).map(_.split(\" \").map(_.toDouble)).collect\n val rtData4 = sparkCxt.textFile(tData4).map(_.split(\" \").map(_.toDouble)).collect\n val rtData5 = sparkCxt.textFile(tData5).map(_.split(\" \").map(_.toDouble)).collect\n val rtData6 = sparkCxt.textFile(tData6).map(_.split(\" \").map(_.toDouble)).collect\n\n val tInputs = Array[Array[Double]] (\n rtData1(0), rtData2(0), rtData3(0), rtData4(0), rtData5(0), rtData6(0) )\n\n val rTestPredictData = tInputs.map{ case(features) => ( Vectors.dense(features) ) }\n val rddTestPredictData = sparkCxt.parallelize( rTestPredictData )\n\n```", "```scala\n val testPredictions = annModel.predict( rddTestPredictData )\n testPredictions.toArray().foreach( value => println( \"test prediction > \" + value ) )\n\n```", "```scala\n/home/hadoop/spark/spark/bin/spark-submit \\\n --class testann2 \\\n --master spark://hc2nn.semtech-solutions.co.nz:8077  \\\n --executor-memory 700M \\\n --total-executor-cores 100 \\\n /home/hadoop/spark/ann/target/scala-2.10/a-n-n_2.10-1.0.jar\n\n```", "```scala\ntest prediction > 0.1\ntest prediction > 0.2\ntest prediction > 0.3\ntest prediction > 0.4\ntest prediction > 0.5\ntest prediction > 0.6\n\n```"]