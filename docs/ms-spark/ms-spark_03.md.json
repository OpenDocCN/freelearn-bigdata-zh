["```scala\n val ssc    = new StreamingContext(sc, Seconds(5) )\n val stream = TwitterUtils.createStream(ssc,None).window( Seconds(60) )\n\n```", "```scala\n ssc.start()\n ssc.awaitTermination()\n\n```", "```scala\n[hadoop@hc2nn stream]$ hdfs dfs -ls /data/spark/checkpoint\nls: `/data/spark/checkpoint': No such file or directory\n\n```", "```scala\npackage nz.co.semtechsolutions\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.twitter._\nimport org.apache.spark.streaming.StreamingContext._\n\nobject stream1 {\n\n```", "```scala\n def createContext( cpDir : String ) : StreamingContext = {\n\n val appName = \"Stream example 1\"\n val conf    = new SparkConf()\n\n conf.setAppName(appName)\n\n val sc = new SparkContext(conf)\n\n val ssc    = new StreamingContext(sc, Seconds(5) )\n\n ssc.checkpoint( cpDir )\n\n ssc\n }\n\n```", "```scala\n def main(args: Array[String]) {\n\n val hdfsDir = \"/data/spark/checkpoint\"\n\n val consumerKey       = \"QQpxx\"\n val consumerSecret    = \"0HFzxx\"\n val accessToken       = \"323xx\"\n val accessTokenSecret = \"IlQxx\"\n\n System.setProperty(\"twitter4j.oauth.consumerKey\", consumerKey)\n System.setProperty(\"twitter4j.oauth.consumerSecret\", consumerSecret)\n System.setProperty(\"twitter4j.oauth.accessToken\", accessToken)\n System.setProperty(\"twitter4j.oauth.accessTokenSecret\", accessTokenSecret)\n\n val ssc = StreamingContext.getOrCreate(hdfsDir,\n () => { createContext( hdfsDir ) })\n\n val stream = TwitterUtils.createStream(ssc,None).window( Seconds(60) )\n\n // do some processing\n\n ssc.start()\n ssc.awaitTermination()\n\n } // end main\n\n```", "```scala\n[hadoop@hc2nn stream]$ hdfs dfs -ls /data/spark/checkpoint\nFound 1 items\ndrwxr-xr-x   - hadoop supergroup          0 2015-07-02 13:41 /data/spark/checkpoint/0fc3d94e-6f53-40fb-910d-1eef044b12e9\n\n```", "```scala\nDStream.checkpoint( newRequiredInterval )\n\n```", "```scala\npackage nz.co.semtechsolutions\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\n\nobject stream2 {\n\n def main(args: Array[String]) {\n\n```", "```scala\n if ( args.length < 2 )\n {\n System.err.println(\"Usage: stream2 <host> <port>\")\n System.exit(1)\n }\n\n val hostname = args(0).trim\n val portnum  = args(1).toInt\n\n val appName = \"Stream example 2\"\n val conf    = new SparkConf()\n\n conf.setAppName(appName)\n\n val sc  = new SparkContext(conf)\n val ssc = new StreamingContext(sc, Seconds(10) )\n\n```", "```scala\n val rawDstream = ssc.socketTextStream( hostname, portnum )\n\n```", "```scala\n val wordCount = rawDstream\n .flatMap(line => line.split(\" \"))\n .map(word => (word,1))\n .reduceByKey(_+_)\n .map(item => item.swap)\n .transform(rdd => rdd.sortByKey(false))\n .foreachRDD( rdd =>\n { rdd.take(10).foreach(x=>println(\"List : \" + x)) })\n\n```", "```scala\n ssc.start()\n ssc.awaitTermination()\n\n } // end main\n\n} // end stream2\n\n```", "```scala\n[root@hc2nn log]# pwd\n/var/log\n[root@hc2nn log]# cat ./anaconda.storage.log | nc -lk 10777\n\n```", "```scala\nList : (17104,)\nList : (2333,=)\nList : (1656,:)\nList : (1603,;)\nList : (1557,DEBUG)\nList : (564,True)\nList : (495,False)\nList : (411,None)\nList : (356,at)\nList : (335,object)\n\n```", "```scala\n val rawDstream = ssc.textFileStream( directory )\n\n```", "```scala\n[root@hc2nn log]# hdfs dfs -put ./anaconda.storage.log /data/spark/stream\n\n```", "```scala\nList : (17104,)\nList : (2333,=)\n\u2026\u2026..\nList : (564,True)\nList : (495,False)\nList : (411,None)\nList : (356,at)\nList : (335,object)\n\n```", "```scala\n[root@hc2nn ~]# flume-ng version\nFlume 1.5.0-cdh5.3.3\nSource code repository: https://git-wip-us.apache.org/repos/asf/flume.git\nRevision: b88ce1fd016bc873d817343779dfff6aeea07706\nCompiled by jenkins on Wed Apr  8 14:57:43 PDT 2015\nFrom source with checksum 389d91c718e03341a2367bf4ef12428e\n\n```", "```scala\n[hadoop@hc2nn ~]$ nc  hc2r1m1.semtech-solutions.co.nz  10777\n\n```", "```scala\nagent1.sources  = source1\nagent1.channels = channel1\nagent1.sinks    = sink1\n\n```", "```scala\nagent1.sources.source1.channels=channel1\nagent1.sources.source1.type=netcat\nagent1.sources.source1.bind=hc2r1m1.semtech-solutions.co.nz\nagent1.sources.source1.port=10777\n\n```", "```scala\nagent1.channels.channel1.type=memory\nagent1.channels.channel1.capacity=1000\n\n```", "```scala\nagent1.sinks.sink1.type=avro\nagent1.sinks.sink1.hostname=hc2r1m1.semtech-solutions.co.nz\nagent1.sinks.sink1.port=11777\nagent1.sinks.sink1.channel=channel1\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ more flume.bash\n\n#!/bin/bash\n\n# run the bash agent\n\nflume-ng agent \\\n --conf /etc/flume-ng/conf \\\n --conf-file ./agent1.flume.cfg \\\n -Dflume.root.logger=DEBUG,INFO,console  \\\n -name agent1\n\n```", "```scala\npackage nz.co.semtechsolutions\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.flume._\n\nobject stream4 {\n\n def main(args: Array[String]) {\n\n```", "```scala\n if ( args.length < 2 )\n {\n System.err.println(\"Usage: stream4 <host> <port>\")\n System.exit(1)\n }\n val hostname = args(0).trim\n val portnum  = args(1).toInt\n\n println(\"hostname : \" + hostname)\n println(\"portnum  : \" + portnum)\n\n```", "```scala\n val appName = \"Stream example 4\"\n val conf    = new SparkConf()\n\n conf.setAppName(appName)\n\n val sc  = new SparkContext(conf)\n val ssc = new StreamingContext(sc, Seconds(10) )\n\n val rawDstream = FlumeUtils.createStream(ssc,hostname,portnum)\n\n```", "```scala\n rawDstream.count()\n .map(cnt => \">>>> Received events : \" + cnt )\n .print()\n\n rawDstream.map(e => new String(e.event.getBody.array() ))\n .print\n\n ssc.start()\n ssc.awaitTermination()\n\n } // end main\n} // end stream4\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ more run_stream.bash\n\n#!/bin/bash\n\nSPARK_HOME=/usr/local/spark\nSPARK_BIN=$SPARK_HOME/bin\nSPARK_SBIN=$SPARK_HOME/sbin\n\nJAR_PATH=/home/hadoop/spark/stream/target/scala-2.10/streaming_2.10-1.0.jar\nCLASS_VAL=$1\nCLASS_PARAMS=\"${*:2}\"\n\nSTREAM_JAR=/usr/local/spark/lib/spark-examples-1.3.1-hadoop2.3.0.jar\n\ncd $SPARK_BIN\n\n./spark-submit \\\n --class $CLASS_VAL \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 100M \\\n --total-executor-cores 50 \\\n --jars $STREAM_JAR \\\n $JAR_PATH \\\n $CLASS_PARAMS\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ ./run_stream.bash  \\\n nz.co.semtechsolutions.stream4 \\\n hc2r1m1.semtech-solutions.co.nz  \\\n 11777\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ ./flume.bash\n\n```", "```scala\n[hadoop@hc2nn ~]$ nc  hc2r1m1.semtech-solutions.co.nz 10777\nI hope that Apache Spark will print this\nOK\nI hope that Apache Spark will print this\nOK\nI hope that Apache Spark will print this\nOK\n\n```", "```scala\n2015-07-06 18:13:18,699 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41\n2015-07-06 18:13:18,700 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1\n2015-07-06 18:13:18,990 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41\n2015-07-06 18:13:18,991 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1\n2015-07-06 18:13:19,270 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:318)] Chars read = 41\n2015-07-06 18:13:19,271 (netcat-handler-0) [DEBUG - org.apache.flume.source.NetcatSource$NetcatSocketHandler.run(NetcatSource.java:322)] Events processed = 1\n\n```", "```scala\n-------------------------------------------\nTime: 1436163210000 ms\n-------------------------------------------\n>>> Received events : 3\n-------------------------------------------\nTime: 1436163210000 ms\n-------------------------------------------\nI hope that Apache Spark will print this\nI hope that Apache Spark will print this\nI hope that Apache Spark will print this\n\n```", "```scala\nagent1.sources.source1.type=exec\nagent1.sources.source.command=./rss.perl\n\n```", "```scala\n#!/usr/bin/perl\n\nuse strict;\nuse LWP::UserAgent;\nuse XML::XPath;\n\nmy $urlsource=\"http://feeds.reuters.com/reuters/scienceNews\" ;\n\nmy  $agent = LWP::UserAgent->new;\n\n```", "```scala\nwhile()\n{\n my  $req = HTTP::Request->new(GET => ($urlsource));\n\n $req->header('content-type' => 'application/json');\n $req->header('Accept'       => 'application/json');\n\n my $resp = $agent->request($req);\n\n```", "```scala\n if ( $resp->is_success )\n {\n my $xmlpage = $resp -> decoded_content;\n\n my $xp = XML::XPath->new( xml => $xmlpage );\n my $nodeset = $xp->find( '/rss/channel/item/title' );\n\n my @titles = () ;\n my $index = 0 ;\n\n```", "```scala\n foreach my $node ($nodeset->get_nodelist)\n {\n my $xmlstring = XML::XPath::XMLParser::as_string($node) ;\n\n $xmlstring =~ s/<title>//g;\n $xmlstring =~ s/<\\/title>//g;\n $xmlstring =~ s/\"//g;\n $xmlstring =~ s/,//g;\n\n $titles[$index] = $xmlstring ;\n $index = $index + 1 ;\n\n } # foreach find node\n\n```", "```scala\n my $nodeset = $xp->find( '/rss/channel/item/description' );\n\n my @desc = () ;\n $index = 0 ;\n\n foreach my $node ($nodeset->get_nodelist)\n {\n my $xmlstring = XML::XPath::XMLParser::as_string($node) ;\n\n $xmlstring =~ s/<img.+\\/img>//g;\n $xmlstring =~ s/href=\".+\"//g;\n $xmlstring =~ s/src=\"img/.+\"//g;\n $xmlstring =~ s/src='.+'//g;\n $xmlstring =~ s/<br.+\\/>//g;\n $xmlstring =~ s/<\\/div>//g;\n $xmlstring =~ s/<\\/a>//g;\n $xmlstring =~ s/<a >\\n//g;\n $xmlstring =~ s/<img >//g;\n $xmlstring =~ s/<img \\/>//g;\n $xmlstring =~ s/<div.+>//g;\n $xmlstring =~ s/<title>//g;\n $xmlstring =~ s/<\\/title>//g;\n $xmlstring =~ s/<description>//g;\n $xmlstring =~ s/<\\/description>//g;\n $xmlstring =~ s/&lt;.+>//g;\n $xmlstring =~ s/\"//g;\n $xmlstring =~ s/,//g;\n $xmlstring =~ s/\\r|\\n//g;\n\n $desc[$index] = $xmlstring ;\n $index = $index + 1 ;\n\n } # foreach find node\n\n```", "```scala\n my $newsitems = $index ;\n $index = 0 ;\n\n for ($index=0; $index < $newsitems; $index++) {\n\n print \"{\\\"category\\\": \\\"science\\\",\"\n . \" \\\"title\\\": \\\"\" .  $titles[$index] . \"\\\",\"\n . \" \\\"summary\\\": \\\"\" .  $desc[$index] . \"\\\"\"\n . \"}\\n\";\n\n } # for rss items\n\n } # success ?\n\n sleep(30) ;\n\n} # while\n\n```", "```scala\n case class RSSItem(category : String, title : String, summary : String)\n\n val now: Long = System.currentTimeMillis\n\n val hdfsdir = \"hdfs://hc2nn:8020/data/spark/flume/rss/\"\n\n```", "```scala\n rawDstream.map(record => {\n implicit val formats = DefaultFormats\n read[RSSItem](new String(record.event.getBody().array()))\n })\n .foreachRDD(rdd => {\n if (rdd.count() > 0) {\n rdd.map(item => {\n implicit val formats = DefaultFormats\n write(item)\n }).saveAsTextFile(hdfsdir+\"file_\"+now.toString())\n }\n })\n\n```", "```scala\n2015-07-07 14:14:24,017 (agent-shutdown-hook) [DEBUG - org.apache.flume.source.ExecSource.stop(ExecSource.java:219)] Exec source with command:./news_rss_collector.py stopped. Metrics:SOURCE:source1{src.events.accepted=80, src.events.received=80, src.append.accepted=0, src.append-batch.accepted=0, src.open-connection.count=0, src.append-batch.received=0, src.append.received=0}\n\n```", "```scala\n>>>> Received events : 73\n>>>> Received events : 7\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ hdfs dfs -ls /data/spark/flume/rss/\nFound 2 items\ndrwxr-xr-x   - hadoop supergroup          0 2015-07-07 14:09 /data/spark/flume/rss/file_1436234439794\ndrwxr-xr-x   - hadoop supergroup          0 2015-07-07 14:14 /data/spark/flume/rss/file_1436235208370\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$  hdfs dfs -cat /data/spark/flume/rss/file_1436235208370/part-00000 | head -1\n\n{\"category\":\"healthcare\",\"title\":\"BRIEF-Aetna CEO says has not had specific conversations with DOJ on Humana - CNBC\",\"summary\":\"* Aetna CEO Says Has Not Had Specific Conversations With Doj About Humana Acquisition - CNBC\"}\n\n```", "```scala\n[root@hc2nn csd]# pwd\n/opt/cloudera/csd\n\n[root@hc2nn csd]# ls -l KAFKA-1.2.0.jar\n-rw-r--r-- 1 hadoop hadoop 5670 Jul 11 14:56 KAFKA-1.2.0.jar\n\n```", "```scala\n[root@hc2nn hadoop]# service cloudera-scm-server restart\nStopping cloudera-scm-server:                              [  OK  ]\nStarting cloudera-scm-server:                              [  OK  ]\n\n```", "```scala\n[hadoop@hc2nn ~]$ ls /usr/bin/kafka*\n\n/usr/bin/kafka-console-consumer         /usr/bin/kafka-run-class\n/usr/bin/kafka-console-producer         /usr/bin/kafka-topics\n/usr/bin/kafka-consumer-offset-checker\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ more kafka.bash\n\n#!/bin/bash\n\nBROKER_LIST=\"hc2r1m1:9092,hc2r1m2:9092,hc2r1m3:9092,hc2r1m4:9092\"\nTOPIC=\"rss\"\n\n./rss.perl | /usr/bin/kafka-console-producer --broker-list $BROKER_LIST --topic $TOPIC\n\n```", "```scala\n/usr/bin/kafka-topics \\\n --create  \\\n --zookeeper hc2r1m2:2181,hc2r1m3:2181,hc2r1m4:2181/kafka \\\n --replication-factor 3  \\\n --partitions 5  \\\n --topic rss\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ cat kafka_list.bash\n\n#!/bin/bash\n\nZOOKEEPER=\"hc2r1m2:2181,hc2r1m3:2181,hc2r1m4:2181/kafka\"\nTOPIC=\"rss\"\nGROUP=\"group1\"\n\necho \"\"\necho \"================================\"\necho \" Kafka Topics \"\necho \"================================\"\n\n/usr/bin/kafka-topics --list --zookeeper $ZOOKEEPER\n\necho \"\"\necho \"================================\"\necho \" Kafka Offsets \"\necho \"================================\"\n\n```", "```scala\n/usr/bin/kafka-consumer-offset-checker \\\n --group $GROUP \\\n --topic $TOPIC \\\n --zookeeper $ZOOKEEPER\n\n```", "```scala\npackage nz.co.semtechsolutions\n\nimport kafka.serializer.StringDecoder\n\nimport org.apache.spark._\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.streaming._\nimport org.apache.spark.streaming.StreamingContext._\nimport org.apache.spark.streaming.kafka._\n\nobject stream6 {\n\n def main(args: Array[String]) {\n\n```", "```scala\n if ( args.length < 3 )\n {\n System.err.println(\"Usage: stream6 <brokers> <groupid> <topics>\\n\")\n System.err.println(\"<brokers> = host1:port1,host2:port2\\n\")\n System.err.println(\"<groupid> = group1\\n\")\n System.err.println(\"<topics>  = topic1,topic2\\n\")\n System.exit(1)\n }\n\n val brokers = args(0).trim\n val groupid = args(1).trim\n val topics  = args(2).trim\n\n println(\"brokers : \" + brokers)\n println(\"groupid : \" + groupid)\n println(\"topics  : \" + topics)\n\n```", "```scala\n val appName = \"Stream example 6\"\n val conf    = new SparkConf()\n\n conf.setAppName(appName)\n\n val sc  = new SparkContext(conf)\n val ssc = new StreamingContext(sc, Seconds(10) )\n\n```", "```scala\n val topicsSet = topics.split(\",\").toSet\n val kafkaParams : Map[String, String] =\n Map(\"metadata.broker.list\" -> brokers,\n \"group.id\" -> groupid )\n\n val rawDstream = KafkaUtils.createDirectStream[String, String, StringDecoder, StringDecoder](ssc, kafkaParams, topicsSet)\n\n```", "```scala\n rawDstream.count().map(cnt => \">>>>>>>>>>>>>>> Received events : \" + cnt ).print()\n\n```", "```scala\n val now: Long = System.currentTimeMillis\n\n val hdfsdir = \"hdfs://hc2nn:8020/data/spark/kafka/rss/\"\n\n val lines = rawDstream.map(record => record._2)\n\n lines.foreachRDD(rdd => {\n if (rdd.count() > 0) {\n rdd.saveAsTextFile(hdfsdir+\"file_\"+now.toString())\n }\n })\n\n```", "```scala\n ssc.start()\n ssc.awaitTermination()\n\n } // end main\n\n} // end stream6\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ /usr/bin/zookeeper-client -server hc2r1m2:2181\n\n[zk: hc2r1m2:2181(CONNECTED) 0]\n\n```", "```scala\n[zk: hc2r1m2:2181(CONNECTED) 2] ls /kafka\n[consumers, config, controller, admin, brokers, controller_epoch]\n\n[zk: hc2r1m2:2181(CONNECTED) 3] ls /kafka/brokers\n[topics, ids]\n\n[zk: hc2r1m2:2181(CONNECTED) 4] ls /kafka/brokers/ids\n[3, 2, 1, 4]\n\n```", "```scala\n[hadoop@hc2nn ~]$ /usr/bin/kafka-topics \\\n>   --create  \\\n>   --zookeeper hc2r1m2:2181,hc2r1m3:2181,hc2r1m4:2181/kafka \\\n>   --replication-factor 3  \\\n>   --partitions 5  \\\n>   --topic rss\n\nCreated topic \"rss\".\n\n```", "```scala\n[zk: hc2r1m2:2181(CONNECTED) 5] ls /kafka/brokers/topics\n[rss]\n\n[zk: hc2r1m2:2181(CONNECTED) 6] ls /kafka/brokers/topics/rss\n[partitions]\n\n[zk: hc2r1m2:2181(CONNECTED) 7] ls /kafka/brokers/topics/rss/partitions\n[3, 2, 1, 0, 4]\n\n```", "```scala\n[zk: hc2r1m2:2181(CONNECTED) 9]  ls /kafka/consumers\n[]\n[zk: hc2r1m2:2181(CONNECTED) 10] quit\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ more run_stream.bash\n\n#!/bin/bash\n\nSPARK_HOME=/usr/local/spark\nSPARK_BIN=$SPARK_HOME/bin\nSPARK_SBIN=$SPARK_HOME/sbin\n\nJAR_PATH=/home/hadoop/spark/stream/target/scala-2.10/streaming_2.10-1.0.jar\nCLASS_VAL=$1\nCLASS_PARAMS=\"${*:2}\"\n\nSTREAM_JAR=/usr/local/spark/lib/spark-examples-1.3.1-hadoop2.3.0.jar\ncd $SPARK_BIN\n\n./spark-submit \\\n --class $CLASS_VAL \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 100M \\\n --total-executor-cores 50 \\\n --jars $STREAM_JAR \\\n $JAR_PATH \\\n $CLASS_PARAMS\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ more run_kafka_example.bash\n\n#!/bin/bash\n\nRUN_CLASS=nz.co.semtechsolutions.stream6\nBROKERS=\"hc2r1m1:9092,hc2r1m2:9092,hc2r1m3:9092,hc2r1m4:9092\"\nGROUPID=group1\nTOPICS=rss\n\n# run the Apache Spark Kafka example\n\n./run_stream.bash $RUN_CLASS \\\n $BROKERS \\\n $GROUPID \\\n $TOPICS\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ ./kafka_list.bash\n\n================================\n Kafka Topics\n================================\n__consumer_offsets\nrss\n\n================================\n Kafka Offsets\n================================\nExiting due to: org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /consumers/group1/offsets/rss/4.\n\n```", "```scala\n-------------------------------------------\nTime: 1436834440000 ms\n-------------------------------------------\n>>>>>>>>>>>>>>> Received events : 28\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ hdfs dfs -ls /data/spark/kafka/rss\nFound 1 items\ndrwxr-xr-x   - hadoop supergroup          0 2015-07-14 12:40 /data/spark/kafka/rss/file_1436833769907\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ hdfs dfs -ls /data/spark/kafka/rss/file_1436833769907\nFound 2 items\n-rw-r--r--   3 hadoop supergroup          0 2015-07-14 12:40 /data/spark/kafka/rss/file_1436833769907/_SUCCESS\n-rw-r--r--   3 hadoop supergroup       8205 2015-07-14 12:40 /data/spark/kafka/rss/file_1436833769907/part-00001\n\n```", "```scala\n[hadoop@hc2r1m1 stream]$ hdfs dfs -cat /data/spark/kafka/rss/file_1436833769907/part-00001 | head -2\n\n{\"category\": \"science\", \"title\": \"Bear necessities: low metabolism lets pandas survive on bamboo\", \"summary\": \"WASHINGTON (Reuters) - Giant pandas eat vegetables even though their bodies are better equipped to eat meat. So how do these black-and-white bears from the remote misty mountains of central China survive on a diet almost exclusively of a low-nutrient food like bamboo?\"}\n\n{\"category\": \"science\", \"title\": \"PlanetiQ tests sensor for commercial weather satellites\", \"summary\": \"CAPE CANAVERAL (Reuters) - PlanetiQ a privately owned company is beginning a key test intended to pave the way for the first commercial weather satellites.\"}\n\n```"]