["```scala\n[[hadoop@hc2nn tmp]$ ls -lh titan-0.9.0-M2-hadoop1.zip\n-rw-r--r-- 1 hadoop hadoop 153M Jul 22 15:13 titan-0.9.0-M2-hadoop1.zip\n\n```", "```scala\n[hadoop@hc2nn tmp]$ unzip titan-0.9.0-M2-hadoop1.zip\n\n[hadoop@hc2nn tmp]$ ls -l\ntotal 155752\ndrwxr-xr-x 10 hadoop hadoop      4096 Jun  9 00:56 titan-0.9.0-M2-hadoop1\n-rw-r--r--  1 hadoop hadoop 159482381 Jul 22 15:13 titan-0.9.0-M2-hadoop1.zip\n\n```", "```scala\n[hadoop@hc2nn ~]$ su \u2013\n[root@hc2nn ~]# cd /home/hadoop/tmp\n[root@hc2nn titan]# mv titan-0.9.0-M2-hadoop1 /usr/local\n[root@hc2nn titan]# cd /usr/local\n[root@hc2nn local]# chown -R hadoop:hadoop titan-0.9.0-M2-hadoop1\n[root@hc2nn local]# ln -s titan-0.9.0-M2-hadoop1 titan\n[root@hc2nn local]# ls -ld *titan*\nlrwxrwxrwx  1 root   root     19 Mar 13 14:10 titan -> titan-0.9.0-M2-hadoop1\ndrwxr-xr-x 10 hadoop hadoop 4096 Feb 14 13:30 titan-0.9.0-M2-hadoop1\n\n```", "```scala\n[hadoop@hc2r1m2 ~]$ hbase shell\nVersion 0.98.6-cdh5.3.2, rUnknown, Tue Feb 24 12:56:59 PST 2015\nhbase(main):001:0>\n\n```", "```scala\nhbase(main):001:0> create 'table2', 'cf1'\nhbase(main):002:0> put 'table2', 'row1', 'cf1:1', 'value1'\nhbase(main):003:0> put 'table2', 'row2', 'cf1:1', 'value2'\n\n```", "```scala\n[hadoop@hc2r1m4 ~]$ hbase shell\n\nhbase(main):001:0> scan 'table2'\n\nROW                     COLUMN+CELL\n row1                   column=cf1:1, timestamp=1437968514021, value=value1\n row2                   column=cf1:1, timestamp=1437968520664, value=value2\n2 row(s) in 0.3870 seconds\n\n```", "```scala\n[hadoop@hc2r1m2 ~]$ java -version\nopenjdk version \"1.8.0_51\"\n\n```", "```scala\nException in thread \"main\" java.lang.UnsupportedClassVersionError: org/apache/tinkerpop/gremlin/groovy/plugin/RemoteAcceptor :\nUnsupported major.minor version 52.0\n\n```", "```scala\n[hadoop@hc2r1m2 bin]$ pwd\n/usr/local/titan/\n\n[hadoop@hc2r1m2 titan]$ bin/gremlin.sh\ngremlin>\n\n```", "```scala\nhBaseConf = new BaseConfiguration();\nhBaseConf.setProperty(\"storage.backend\",\"hbase\");\nhBaseConf.setProperty(\"storage.hostname\",\"hc2r1m2,hc2r1m3,hc2r1m4\");\nhBaseConf.setProperty(\"storage.hbase.ext.hbase.zookeeper.property.clientPort\",\"2181\")\nhBaseConf.setProperty(\"storage.hbase.table\",\"titan\")\n\ntitanGraph = TitanFactory.open(hBaseConf);\n\n```", "```scala\nmanageSys = titanGraph.openManagement();\nnameProp = manageSys.makePropertyKey('name').dataType(String.class).make();\nageProp  = manageSys.makePropertyKey('age').dataType(String.class).make();\nmanageSys.buildIndex('nameIdx',Vertex.class).addKey(nameProp).buildCompositeIndex();\nmanageSys.buildIndex('ageIdx',Vertex.class).addKey(ageProp).buildCompositeIndex();\n\nmanageSys.commit();\n\n```", "```scala\nv1=titanGraph.addVertex(label, '1');\nv1.property('name', 'Mike');\nv1.property('age', '48');\n\nv2=titanGraph.addVertex(label, '2');\nv2.property('name', 'Sarah');\nv2.property('age', '45');\n\nv3=titanGraph.addVertex(label, '3');\nv3.property('name', 'John');\nv3.property('age', '25');\n\nv4=titanGraph.addVertex(label, '4');\nv4.property('name', 'Jim');\nv4.property('age', '53');\n\nv5=titanGraph.addVertex(label, '5');\nv5.property('name', 'Kate');\nv5.property('age', '22');\n\nv6=titanGraph.addVertex(label, '6');\nv6.property('name', 'Flo');\nv6.property('age', '52');\n\n```", "```scala\nv6.addEdge(\"Sister\", v1)\nv1.addEdge(\"Husband\", v2)\nv2.addEdge(\"Wife\", v1)\nv5.addEdge(\"Daughter\", v1)\nv5.addEdge(\"Daughter\", v2)\nv3.addEdge(\"Son\", v1)\nv3.addEdge(\"Son\", v2)\nv4.addEdge(\"Friend\", v1)\nv1.addEdge(\"Father\", v5)\nv1.addEdge(\"Father\", v3)\nv2.addEdge(\"Mother\", v5)\nv2.addEdge(\"Mother\", v3)\n\ntitanGraph.tx().commit();\n\n```", "```scala\nhBaseConf = new BaseConfiguration();\nhBaseConf.setProperty(\"storage.backend\",\"hbase\");\nhBaseConf.setProperty(\"storage.hostname\",\"hc2r1m2,hc2r1m3,hc2r1m4\");\nhBaseConf.setProperty(\"storage.hbase.ext.hbase.zookeeper.property.clientPort\",\"2181\")\nhBaseConf.setProperty(\"storage.hbase.table\",\"titan\")\n\ntitanGraph = TitanFactory.open(hBaseConf);\n\ngremlin> g = titanGraph.traversal()\n\n```", "```scala\ngremlin> g.V().has('name','Mike').valueMap();\n==>[name:[Mike], age:[48]]\n\ngremlin> g.V().has('name','Flo').valueMap();\n==>[name:[Flo], age:[52]]\n\n```", "```scala\n[hadoop@hc2r1m2 ~]$ hbase shell\nhbase(main):002:0> scan 'titan'\n72 row(s) in 0.8310 seconds\n\n```", "```scala\n[hadoop@hc2r1m2 tmp]$ ls -l SparkOnHBase-cdh5-0.0.2.zip\n-rw-r--r-- 1 hadoop hadoop 370439 Jul 27 13:39 SparkOnHBase-cdh5-0.0.2.zip\n\n[hadoop@hc2r1m2 tmp]$ unzip SparkOnHBase-cdh5-0.0.2.zip\n\n[hadoop@hc2r1m2 tmp]$ ls\nSparkOnHBase-cdh5-0.0.2  SparkOnHBase-cdh5-0.0.2.zip\n\n```", "```scala\n[hadoop@hc2r1m2 tmp]$ cd SparkOnHBase-cdh5-0.0.2\n[hadoop@hc2r1m2 SparkOnHBase-cdh5-0.0.2]$ mvn clean package\n\n[INFO] -----------------------------------------------------------\n[INFO] BUILD SUCCESS\n[INFO] -----------------------------------------------------------\n[INFO] Total time: 13:17 min\n[INFO] Finished at: 2015-07-27T14:05:55+12:00\n[INFO] Final Memory: 50M/191M\n[INFO] -----------------------------------------------------------\n\n```", "```scala\n[hadoop@hc2r1m2 SparkOnHBase-cdh5-0.0.2]$ cd ..\n[hadoop@hc2r1m2 tmp]$ mv SparkOnHBase-cdh5-0.0.2 /home/hadoop/spark\n\n```", "```scala\n[hadoop@hc2r1m2 titan_hbase]$ pwd\n/home/hadoop/spark/titan_hbase\n\n[hadoop@hc2r1m2 titan_hbase]$ more titan.sbt\nname := \"T i t a n\"\nversion := \"1.0\"\nscalaVersion := \"2.10.4\"\n\nlibraryDependencies += \"org.apache.hadoop\" % \"hadoop-client\" % \"2.3.0\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\"  % \"1.3.1\"\nlibraryDependencies += \"com.cloudera.spark\" % \"hbase\"   % \"5-0.0.2\" from \"file:///home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/SparkHBase.jar\"\nlibraryDependencies += \"org.apache.hadoop.hbase\" % \"client\"   % \"5-0.0.2\" from \"file:///home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/SparkHBase.jar\"\nresolvers += \"Cloudera Repository\" at \"https://repository.cloudera.com/artifactory/clouder\na-repos/\"\n\n```", "```scala\n[hadoop@hc2r1m2 titan_hbase]$ pwd ; more run_titan.bash.hbase\n/home/hadoop/spark/titan_hbase\n\n#!/bin/bash\n\nSPARK_HOME=/usr/local/spark\nSPARK_BIN=$SPARK_HOME/bin\nSPARK_SBIN=$SPARK_HOME/sbin\n\nJAR_PATH=/home/hadoop/spark/titan_hbase/target/scala-2.10/t-i-t-a-n_2.10-1.0.jar\nCLASS_VAL=$1\n\nCDH_JAR_HOME=/opt/cloudera/parcels/CDH/lib/hbase/\nCONN_HOME=/home/hadoop/spark/SparkOnHBase-cdh5-0.0.2/target/\n\nHBASE_JAR1=$CDH_JAR_HOME/hbase-common-0.98.6-cdh5.3.3.jar\nHBASE_JAR2=$CONN_HOME/SparkHBase.jar\n\ncd $SPARK_BIN\n\n./spark-submit \\\n --jars $HBASE_JAR1 \\\n --jars $HBASE_JAR2 \\\n --class $CLASS_VAL \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 100M \\\n --total-executor-cores 50 \\\n $JAR_PATH\n\n```", "```scala\n[hadoop@hc2r1m2 scala]$ pwd\n/home/hadoop/spark/titan_hbase/src/main/scala\n\n[hadoop@hc2r1m2 scala]$ ls\nspark3_hbase2.scala\n\n```", "```scala\npackage nz.co.semtechsolutions\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\nimport org.apache.hadoop.hbase._\nimport org.apache.hadoop.fs.Path\nimport com.cloudera.spark.hbase.HBaseContext\nimport org.apache.hadoop.hbase.client.Scan\n```", "```scala\nobject spark3_hbase2\n{\n\n  def main(args: Array[String]) {\n\n    val sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:7077\"\n    val appName = \"Spark HBase 2\"\n    val conf = new SparkConf()\n\n    conf.setMaster(sparkMaster)\n    conf.setAppName(appName)\n\n    val sparkCxt = new SparkContext(conf)\n```", "```scala\n    val jobConf = HBaseConfiguration.create()\n\n    val hbasePath=\"/opt/cloudera/parcels/CDH/etc/hbase/conf.dist/\"\n\n    jobConf.addResource(new Path(hbasePath+\"hbase-site.xml\"))\n```", "```scala\n    val hbaseContext = new HBaseContext(sparkCxt, jobConf)\n\n    var scan = new Scan()\n    scan.setCaching(100)\n```", "```scala\n    var hbaseRdd = hbaseContext.hbaseRDD(\"titan\", scan)\n\n    println( \"Rows in Titan hbase table : \" + hbaseRdd.count() )\n\n    println( \" >>>>> Script Finished <<<<< \" )\n\n  } // end main\n\n} // end spark3_hbase2\n```", "```scala\n[hadoop@hc2r1m2 titan_hbase]$ ./run_titan.bash.hbase nz.co.semtechsolutions.spark3_hbase2\n\nRows in Titan hbase table : 72\n >>>>> Script Finished <<<<<\n\n```", "```scala\n[hadoop@hc2nn lib]$ su -\n[root@hc2nn ~]# vi /etc/yum.repos.d/datastax.repo\n\n[datastax]\nname= DataStax Repo for Apache Cassandra\nbaseurl=http://rpm.datastax.com/community\nenabled=1\ngpgcheck=0\n\n```", "```scala\n[root@hc2nn ~]# yum -y install dsc20-2.0.13-1 cassandra20-2.0.13-1\n\n```", "```scala\n[root@hc2nn ~]# cd /etc/cassandra/conf   ; vi cassandra.yaml\n\n```", "```scala\ncluster_name: 'Cluster1'\nseeds: \"192.168.1.103,192.168.1.108\"\nlisten_address:\nrpc_address: 0.0.0.0\nendpoint_snitch: GossipingPropertyFileSnitch\n\n```", "```scala\n[root@hc2nn ~]# service cassandra start\n\n```", "```scala\n[root@hc2nn cassandra]# nodetool status\nDatacenter: DC1\n===============\nStatus=Up/Down\n|/ State=Normal/Leaving/Joining/Moving\n--  Address        Load       Tokens  Owns (effective)  Host ID Rack\nUN  192.168.1.105  63.96 KB   256     37.2%             f230c5d7-ff6f-43e7-821d-c7ae2b5141d3  RAC1\nUN  192.168.1.110  45.86 KB   256     39.9%             fc1d80fe-6c2d-467d-9034-96a1f203c20d  RAC1\nUN  192.168.1.109  45.9 KB    256     40.9%             daadf2ee-f8c2-4177-ae72-683e39fd1ea0  RAC1\nUN  192.168.1.108  50.44 KB   256     40.5%             b9d796c0-5893-46bc-8e3c-187a524b1f5a  RAC1\nUN  192.168.1.103  70.68 KB   256     41.5%             53c2eebd-\na66c-4a65-b026-96e232846243  RAC1\n\n```", "```scala\n[hadoop@hc2nn ~]$ cqlsh\nConnected to Cluster1 at localhost:9160.\n[cqlsh 4.1.1 | Cassandra 2.0.13 | CQL spec 3.1.1 | Thrift protocol 19.39.0]\nUse HELP for help.\ncqlsh>\n\n```", "```scala\ncqlsh> CREATE KEYSPACE keyspace1 WITH REPLICATION = { 'class' : 'SimpleStrategy', 'replication_factor' : 1 };\n\ncqlsh> USE keyspace1;\n\ncqlsh:keyspace1> SELECT * FROM system.schema_keyspaces;\n\n keyspace_name | durable_writes | strategy_class                              | strategy_options\n--------------+------+---------------------------------------------+----------------------------\n keyspace1  | True | org.apache.cassandra.locator.SimpleStrategy | {\"replication_factor\":\"1\"}\n system  | True |  org.apache.cassandra.locator.LocalStrategy |                         {}\nsystem_traces | True | org.apache.cassandra.locator.SimpleStrategy | {\"replication_factor\":\"2\"}\n\n```", "```scala\ncassConf = new BaseConfiguration();\ncassConf.setProperty(\"storage.backend\",\"cassandra\");\ncassConf.setProperty(\"storage.hostname\",\"hc2nn,hc2r1m2\");\ncassConf.setProperty(\"storage.port\",\"9160\")\ncassConf.setProperty(\"storage.keyspace\",\"titan\")\ntitanGraph = TitanFactory.open(cassConf);\n```", "```scala\ngremlin> g = titanGraph.traversal()\n\ngremlin> g.V().has('name','Mike').valueMap();\n==>[name:[Mike], age:[48]]\n\ngremlin> g.V().has('name','Flo').valueMap();\n==>[name:[Flo], age:[52]]\n\n```", "```scala\n[hadoop@hc2nn ~]$ cqlsh\ncqlsh> use titan;\ncqlsh:titan> describe tables;\nedgestore        graphindex        system_properties systemlog  txlog\nedgestore_lock_  graphindex_lock_  system_properties_lock_  titan_ids\n\n```", "```scala\ncqlsh:titan> select * from edgestore;\n key                | column1            | value\n--------------------+--------------------+------------------------------------------------\n 0x0000000000004815 |               0x02 |                                     0x00011ee0\n 0x0000000000004815 |             0x10c0 |                           0xa0727425536fee1ec0\n.......\n 0x0000000000001005 |             0x10c8 |                       0x00800512644c1b149004a0\n 0x0000000000001005 | 0x30c9801009800c20 |   0x000101143c01023b0101696e6465782d706ff30200\n\n```", "```scala\n[hadoop@hc2r1m2 titan_cass]$ pwd ; ls *.jar\n/home/hadoop/spark/titan_cass\n\nspark-cassandra-connector_2.10-1.3.0-M1.jar\ncassandra-driver-core-2.1.5.jar\ncassandra-thrift-2.1.3.jar\nlibthrift-0.9.2.jar\ncassandra-clientutil-2.1.3.jar\nguava-14.0.1.jar\njoda-time-2.3.jar\njoda-convert-1.2.jar\n\n```", "```scala\n[hadoop@hc2r1m2 titan_cass]$ pwd ; more titan.sbt\n/home/hadoop/spark/titan_cass\n\nname := \"Spark Cass\"\nversion := \"1.0\"\nscalaVersion := \"2.10.4\"\nlibraryDependencies += \"org.apache.hadoop\" % \"hadoop-client\" % \"2.3.0\"\nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\"  % \"1.3.1\"\nlibraryDependencies += \"com.datastax.spark\" % \"spark-cassandra-connector\"  % \"1.3.0-M1\" fr\nom \"file:///home/hadoop/spark/titan_cass/spark-cassandra-connector_2.10-1.3.0-M1.jar\"\nlibraryDependencies += \"com.datastax.cassandra\" % \"cassandra-driver-core\"  % \"2.1.5\" from\n\"file:///home/hadoop/spark/titan_cass/cassandra-driver-core-2.1.5.jar\"\nlibraryDependencies += \"org.joda\"  % \"time\" % \"2.3\" from \"file:///home/hadoop/spark/titan_\ncass/joda-time-2.3.jar\"\nlibraryDependencies += \"org.apache.cassandra\" % \"thrift\" % \"2.1.3\" from \"file:///home/hado\nop/spark/titan_cass/cassandra-thrift-2.1.3.jar\"\nlibraryDependencies += \"com.google.common\" % \"collect\" % \"14.0.1\" from \"file:///home/hadoo\np/spark/titan_cass/guava-14.0.1.jar\nresolvers += \"Cloudera Repository\" at \"https://repository.cloudera.com/artifactory/clouder\na-repos/\"\n\n```", "```scala\npackage nz.co.semtechsolutions\n\nimport org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\n\nimport com.datastax.spark.connector._\n\nobject spark3_cass\n{\n\n  def main(args: Array[String]) {\n```", "```scala\n    val sparkMaster = \"spark://hc2nn.semtech-solutions.co.nz:7077\"\n    val appName = \"Spark Cass 1\"\n    val conf = new SparkConf()\n\n    conf.setMaster(sparkMaster)\n    conf.setAppName(appName)\n\n    conf.set(\"spark.cassandra.connection.host\", \"hc2r1m2\")\n\n    val sparkCxt = new SparkContext(conf)\n```", "```scala\n    val keySpace =  \"titan\"\n    val tableName = \"edgestore\"\n\n    val cassRDD = sparkCxt.cassandraTable( keySpace, tableName )\n\n    println( \"Cassandra Table Rows : \" + cassRDD.count )\n\n    println( \" >>>>> Script Finished <<<<< \" )\n\n  } // end main\n\n} // end spark3_cass\n```", "```scala\n[hadoop@hc2r1m2 titan_cass]$ more run_titan.bash\n\n#!/bin/bash\n\nSPARK_HOME=/usr/local/spark\nSPARK_BIN=$SPARK_HOME/bin\nSPARK_SBIN=$SPARK_HOME/sbin\n\nJAR_PATH=/home/hadoop/spark/titan_cass/target/scala-2.10/spark-cass_2.10-1.0.jar\nCLASS_VAL=$1\n\nCASS_HOME=/home/hadoop/spark/titan_cass/\n\nCASS_JAR1=$CASS_HOME/spark-cassandra-connector_2.10-1.3.0-M1.jar\nCASS_JAR2=$CASS_HOME/cassandra-driver-core-2.1.5.jar\nCASS_JAR3=$CASS_HOME/cassandra-thrift-2.1.3.jar\nCASS_JAR4=$CASS_HOME/libthrift-0.9.2.jar\nCASS_JAR5=$CASS_HOME/cassandra-clientutil-2.1.3.jar\nCASS_JAR6=$CASS_HOME/guava-14.0.1.jar\nCASS_JAR7=$CASS_HOME/joda-time-2.3.jar\nCASS_JAR8=$CASS_HOME/joda-convert-1.2.jar\n\ncd $SPARK_BIN\n\n./spark-submit \\\n --jars $CASS_JAR8,$CASS_JAR7,$CASS_JAR5,$CASS_JAR4,$CASS_JAR3,$CASS_JAR6,$CASS_JAR2,$CASS_JAR1 \\\n --class $CLASS_VAL \\\n --master spark://hc2nn.semtech-solutions.co.nz:7077  \\\n --executor-memory 100M \\\n --total-executor-cores 50 \\\n $JAR_PATH\n\n```", "```scala\n[hadoop@hc2r1m2 titan_cass]$ ./run_titan.bash.cass nz.co.semtechsolutions.spark3_cass\n\nCassandra Table Rows : 218\n >>>>> Script Finished <<<<<\n\n```", "```scala\ncd $TITAN_HOME/bin ; ./ gremlin.sh\n\n```", "```scala\nplugin activated: tinkerpop.server\nplugin activated: tinkerpop.utilities\nplugin activated: tinkerpop.hadoop\nplugin activated: tinkerpop.tinkergraph\nplugin activated: aurelius.titan\n\n```", "```scala\ngremlin>\n\n```", "```scala\n#!/bin/bash\n\nTITAN_HOME=/usr/local/titan/\n\ncd $TITAN_HOME\n\nbin/titan.sh start\n\nbin/gremlin.sh   <<  EOF\n\n t = TitanFactory.open('cassandra.properties')\n GraphOfTheGodsFactory.load(t)\n t.close()\nEOF\n\nbin/titan.sh stop\n\n```", "```scala\n#!/bin/bash\n\nTITAN_HOME=/usr/local/titan/\nSCRIPTS_HOME=/home/hadoop/spark/gremlin\nGREMLIN_LOG_FILE=$TITAN_HOME/log/gremlin_console.log\n\nGROOVY_SCRIPT=$1\n\nexport GREMLIN_LOG_LEVEL=\"DEBUG\"\n\ncd $TITAN_HOME\n\nbin/titan.sh start\n\nbin/gremlin.sh -e  $SCRIPTS_HOME/$GROOVY_SCRIPT  > $GREMLIN_LOG_FILE 2>&1\n\nbin/titan.sh stop\n\n```", "```scala\nimport com.thinkaurelius.titan.core.*\nimport com.thinkaurelius.titan.core.titan.*\nimport org.apache.tinkerpop.gremlin.*\n```", "```scala\n####################################\n# Storage details\n####################################\nstorage.backend=cassandra\nstorage.hostname=hc2r1m2\nstorage.port=9160\nstorage.cassandra.keyspace=dead\ncassandra.input.partitioner.class=org.apache.cassandra.dht.Murmur3Partitioner\n\n```", "```scala\n####################################\n# Spark\n####################################\nspark.master=spark://hc2nn.semtech-solutions.co.nz:6077\nspark.executor.memory=400M\nspark.serializer=org.apache.spark.serializer.KryoSerializer\n\n```", "```scala\n####################################\n# Hadoop Gremlin\n####################################\ngremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph\ngremlin.hadoop.graphInputFormat=com.thinkaurelius.titan.hadoop.formats.cassandra.CassandraInputFormat\ngremlin.hadoop.graphOutputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat\ngremlin.hadoop.memoryOutputFormat=org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat\n\ngremlin.hadoop.deriveMemory=false\ngremlin.hadoop.jarsInDistributedCache=true\ngremlin.hadoop.inputLocation=none\ngremlin.hadoop.outputLocation=output\n\n```", "```scala\ncom.thinkaurelius.titan.core.TitanFactory\n\n```", "```scala\ncassConf = new BaseConfiguration();\n\ncassConf.setProperty(\"storage.backend\",\"cassandra\");\ncassConf.setProperty(\"storage.hostname\",\"hc2r1m2\");\ncassConf.setProperty(\"storage.port\",\"9160\")\ncassConf.setProperty(\"storage.cassandra.keyspace\",\"titan\")\n\ntitanGraph = TitanFactory.open(cassConf);\n```", "```scala\nt1 = TitanFactory.open('/home/hadoop/spark/gremlin/cassandra.properties')\nGraphOfTheGodsFactory.load(t1)\n\nt1.traversal().V().count()\n\nt1.traversal().V().valueMap()\n\nt1.close()\n\n```", "```scala\n==>12\n\n==>[name:[jupiter], age:[5000]]\n==>[name:[hydra]]\n==>[name:[nemean]]\n==>[name:[tartarus]]\n==>[name:[saturn], age:[10000]]\n==>[name:[sky]]\n==>[name:[pluto], age:[4000]]\n==>[name:[alcmene], age:[45]]\n==>[name:[hercules], age:[30]]\n==>[name:[sea]]\n==>[name:[cerberus]]\n==>[name:[neptune], age:[4500]]\n\n```", "```scala\nt1.traversal(computer(SparkGraphComputer)).V().count()\n\n```", "```scala\ngremlin.hadoop.graphInputFormat=com.thinkaurelius.titan.hadoop.formats.hbase.HBaseInputFormat\n\ninput.conf.storage.backend=hbase\ninput.conf.storage.hostname=hc2r1m2\ninput.conf.storage.port=2181\ninput.conf.storage.hbase.table=titan\ninput.conf.storage.hbase.ext.zookeeper.znode.parent=/hbase\n\n```", "```scala\ngremlin.graph=org.apache.tinkerpop.gremlin.hadoop.structure.HadoopGraph\ngremlin.hadoop.graphInputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoInputFormat\ngremlin.hadoop.graphOutputFormat=org.apache.tinkerpop.gremlin.hadoop.structure.io.gryo.GryoOutputFormat\ngremlin.hadoop.jarsInDistributedCache=true\ngremlin.hadoop.deriveMemory=true\n\ngremlin.hadoop.inputLocation=/usr/local/titan/data/grateful-dead.kryo\ngremlin.hadoop.outputLocation=output\n\n```", "```scala\ngraph = GraphFactory.open('/home/hadoop/spark/gremlin/hadoop-gryo.properties')\ng1 = graph.traversal()\n\n```", "```scala\ng1.V().count()\n==>808\ng1.V().valueMap()\n==>[name:[MIGHT AS WELL], songType:[original], performances:[111]]\n==>[name:[BROWN EYED WOMEN], songType:[original], performances:[347]]\n\n```", "```scala\nr = graph.compute(SparkGraphComputer.class).program(PageRankVertexProgram.build().create()).submit().get()\n\n```"]