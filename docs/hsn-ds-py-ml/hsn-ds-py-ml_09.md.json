["```py\nnums = parallelize([1, 2, 3, 4]) \n\n```", "```py\nsc.textFile(\"file:///c:/users/frank/gobs-o-text.txt\")  \n\n```", "```py\nhiveCtx = HiveContext(sc)  rows = hiveCtx.sql(\"SELECT name, age FROM users\")  \n\n```", "```py\nrdd = sc.parallelize([1, 2, 3, 4]) \nrdd.map(lambda x: x*x) \n\n```", "```py\nfrom pyspark.mllib.regression import LabeledPoint \nfrom pyspark.mllib.tree import DecisionTree \n\n```", "```py\nfrom pyspark import SparkConf, SparkContext \n\n```", "```py\nfrom numpy import array \n\n```", "```py\nconf = SparkConf().setMaster(\"local\").setAppName(\"SparkDecisionTree\") \n\n```", "```py\nsc = SparkContext(conf = conf) \n\n```", "```py\n# Some functions that convert our CSV input data into numerical \n# features for each job candidate \ndef binary(YN): \n    if (YN == 'Y'): \n        return 1 \n    else: \n        return 0 \n\ndef mapEducation(degree): \n    if (degree == 'BS'): \n        return 1 \n    elif (degree =='MS'): \n        return 2 \n    elif (degree == 'PhD'): \n        return 3 \n    else: \n        return 0 \n\n# Convert a list of raw fields from our CSV file to a \n# LabeledPoint that MLLib can use. All data must be numerical... \ndef createLabeledPoints(fields): \n    yearsExperience = int(fields[0]) \n    employed = binary(fields[1]) \n    previousEmployers = int(fields[2]) \n    educationLevel = mapEducation(fields[3]) \n    topTier = binary(fields[4]) \n    interned = binary(fields[5]) \n    hired = binary(fields[6]) \n\n    return LabeledPoint(hired, array([yearsExperience, employed, \n        previousEmployers, educationLevel, topTier, interned])) \n\n```", "```py\nrawData = sc.textFile(\"e:/sundog-consult/udemy/datascience/PastHires.csv\") \nheader = rawData.first() \nrawData = rawData.filter(lambda x:x != header) \n\n```", "```py\ncsvData = rawData.map(lambda x: x.split(\",\")) \n\n```", "```py\ntrainingData = csvData.map(createLabeledPoints) \n\n```", "```py\ndef createLabeledPoints(fields): \n    yearsExperience = int(fields[0]) \n    employed = binary(fields[1]) \n    previousEmployers = int(fields[2]) \n    educationLevel = mapEducation(fields[3]) \n    topTier = binary(fields[4]) \n    interned = binary(fields[5]) \n    hired = binary(fields[6]) \n\n    return LabeledPoint(hired, array([yearsExperience, employed, \n        previousEmployers, educationLevel, topTier, interned])) \n\n```", "```py\ndef createLabeledPoints(fields): \n    yearsExperience = int(fields[0]) \n    employed = binary(fields[1]) \n    previousEmployers = int(fields[2]) \n    educationLevel = mapEducation(fields[3]) \n    topTier = binary(fields[4]) \n    interned = binary(fields[5]) \n    hired = binary(fields[6]) \n\n    return LabeledPoint(hired, array([yearsExperience, employed, \n        previousEmployers, educationLevel, topTier, interned])) \n\n```", "```py\ndef binary(YN): \n    if (YN == 'Y'): \n        return 1 \n    else: \n        return 0 \n\n```", "```py\ndef mapEducation(degree): \n    if (degree == 'BS'): \n        return 1 \n    elif (degree =='MS'): \n        return 2 \n    elif (degree == 'PhD'): \n        return 3 \n    else: \n        return 0 \n\n```", "```py\ntrainingData = csvData.map(createLabeledPoints) \n\n```", "```py\ntestCandidates = [ array([10, 1, 3, 1, 0, 0])] \n\n```", "```py\ntestData = sc.parallelize(testCandidates) \n\n```", "```py\nmodel = DecisionTree.trainClassifier(trainingData, numClasses=2, \n                    categoricalFeaturesInfo={1:2, 3:4, 4:2, 5:2}, \n                    impurity='gini', maxDepth=5, maxBins=32) \n\n```", "```py\npredictions = model.predict(testData) \nprint ('Hire prediction:') \nresults = predictions.collect() \nfor result in results: \n     print (result) \n\n```", "```py\nprint('Learned classification tree model:') \nprint(model.toDebugString()) \n\n```", "```py\nspark-submit SparkDecisionTree.py \n\n```", "```py\nfrom pyspark.mllib.clustering import KMeans \nfrom numpy import array, random \nfrom math import sqrt \nfrom pyspark import SparkConf, SparkContext \nfrom sklearn.preprocessing import scale \n\n```", "```py\n K=5 \n\n```", "```py\nconf = SparkConf().setMaster(\"local\").setAppName(\"SparkKMeans\") \nsc = SparkContext(conf = conf) \n\n```", "```py\ndata = sc.parallelize(scale(createClusteredData(100, K)))  \n\n```", "```py\nclusters = KMeans.train(data, K, maxIterations=10, \n        initializationMode=\"random\") \n\n```", "```py\nresultRDD = data.map(lambda point: clusters.predict(point)).cache() \n\n```", "```py\nprint (\"Counts by value:\") \ncounts = resultRDD.countByValue() \nprint (counts) \n\nprint (\"Cluster assignments:\") \nresults = resultRDD.collect() \nprint (results) \n\n```", "```py\ndef error(point): \n    center = clusters.centers[clusters.predict(point)] \n    return sqrt(sum([x**2 for x in (point - center)])) \n\nWSSSE = data.map(lambda point: error(point)).reduce(lambda x, y: x + y) \nprint(\"Within Set Sum of Squared Error = \" + str(WSSSE)) \n\n```", "```py\nspark-submit SparkKMeans.py  \n\n```", "```py\nfrom pyspark import SparkConf, SparkContext \nfrom pyspark.mllib.feature import HashingTF \nfrom pyspark.mllib.feature import IDF \n\n```", "```py\nconf = SparkConf().setMaster(\"local\").setAppName(\"SparkTFIDF\") \nsc = SparkContext(conf = conf) \n\n```", "```py\nrawData = sc.textFile(\"e:/sundog-consult/Udemy/DataScience/subset-small.tsv\") \n\n```", "```py\nfields = rawData.map(lambda x: x.split(\"\\t\")) \n\n```", "```py\ndocuments = fields.map(lambda x: x[3].split(\" \")) \n\n```", "```py\ndocumentNames = fields.map(lambda x: x[1]) \n\n```", "```py\nhashingTF = HashingTF(100000)  \n\n```", "```py\ntf = hashingTF.transform(documents) \n\n```", "```py\ntf.cache() \n\n```", "```py\nidf = IDF(minDocFreq=2).fit(tf) \n\n```", "```py\ntfidf = idf.transform(tf) \n\n```", "```py\ngettysburgTF = hashingTF.transform([\"Gettysburg\"]) \ngettysburgHashValue = int(gettysburgTF.indices[0]) \n\n```", "```py\ngettysburgRelevance = tfidf.map(lambda x: x[gettysburgHashValue])  \n\n```", "```py\nzippedResults = gettysburgRelevance.zip(documentNames)  \n\n```", "```py\nprint (\"Best document for Gettysburg is:\") \nprint (zippedResults.max()) \n\n```", "```py\nspark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"file:///C:/temp\").appName(\"LinearRegression\").getOrCreate() \n\n```", "```py\ninputLines = spark.sparkContext.textFile(\"regression.txt\")  \n\n```", "```py\ndata = inputLines.map(lambda x: x.split(\",\")).map(lambda x: (float(x[0]), Vectors.dense(float(x[1]))))  \n\n```", "```py\ncolNames = [\"label\", \"features\"] \ndf = data.toDF(colNames) \n\n```", "```py\ntrainTest = df.randomSplit([0.5, 0.5]) \ntrainingDF = trainTest[0] \ntestDF = trainTest[1] \n\n```", "```py\nlir = LinearRegression(maxIter=10, regParam=0.3, elasticNetParam=0.8) \n\n```", "```py\nmodel = lir.fit(trainingDF) \n\n```", "```py\nfullPredictions = model.transform(testDF).cache() \n\n```", "```py\npredictions = fullPredictions.select(\"prediction\").rdd.map(lambda x: x[0]) \n\n```", "```py\nlabels = fullPredictions.select(\"label\").rdd.map(lambda x: x[0]) \n\n```", "```py\npredictionAndLabel = predictions.zip(labels).collect() \n\nfor prediction in predictionAndLabel: \n    print(prediction) \n\nspark.stop() \n\n```"]