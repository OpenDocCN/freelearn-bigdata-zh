["```scala\n    scala> import org.apache.spark.graphx._\n    scala> import org.apache.spark.rdd.RDD\n\n    ```", "```scala\n    scala> val vertices = Array((1L, (\"Santa Clara\",\"CA\")),(2L, (\"Fremont\",\"CA\")),(3L, (\"San Francisco\",\"CA\")))\n\n    ```", "```scala\n    scala> val vrdd = sc.parallelize(vertices)\n\n    ```", "```scala\n    scala> val edges = Array(Edge(1L,2L,20),Edge(2L,3L,44),Edge(3L,1L,53))\n\n    ```", "```scala\n    scala> val erdd = sc.parallelize(edges)\n\n    ```", "```scala\n    scala> val graph = Graph(vrdd,erdd)\n\n    ```", "```scala\n    scala> graph.vertices.collect.foreach(println)\n\n    ```", "```scala\n    scala> graph.edges.collect.foreach(println)\n\n    ```", "```scala\n    scala> graph.triplets.collect.foreach(println)\n\n    ```", "```scala\n    scala> graph.inDegrees\n\n    ```", "```scala\n$ hdfs dfs -mkdir wiki\n$ hdfs dfs -put links-simple-sorted.txt wiki/links.txt\n$ hdfs dfs -put titles-sorted.txt wiki/nodes.txt\n\n```", "```scala\n    scala> import org.apache.spark.graphx._\n\n    ```", "```scala\n    scala> val edgesFile = sc.textFile(\"wiki/links.txt\",20)\n\n    ```", "```scala\n    scala> val edgesFile = sc.textFile(\"s3n:// com.infoobjects.wiki/links\",20)\n\n    ```", "```scala\n    scala> val edges = edgesFile.flatMap { line =>\n     val links = line.split(\"\\\\W+\")\n     val from = links(0)\n     val to = links.tail\n     for ( link <- to) yield (from,link)\n     }.map( e => Edge(e._1.toLong,e._2.toLong,1))\n\n    ```", "```scala\n    scala> val verticesFile = sc.textFile(\"wiki/nodes.txt\",20)\n\n    ```", "```scala\n    scala> val verticesFile = sc.textFile(\"s3n:// com.infoobjects.wiki/nodes\",20)\n\n    ```", "```scala\n    scala> val vertices = verticesFile.zipWithIndex.map(_.swap)\n\n    ```", "```scala\n    scala> val graph = Graph(vertices,edges)\n\n    ```", "```scala\n    scala> val ranks = graph.pageRank(0.001).vertices\n\n    ```", "```scala\n    scala> val swappedRanks = ranks.map(_.swap)\n\n    ```", "```scala\n    scala> val sortedRanks = swappedRanks.sortByKey(false)\n\n    ```", "```scala\n    scala> val highest = sortedRanks.first\n\n    ```", "```scala\n    scala> val join = sortedRanks.join(vertices)\n\n    ```", "```scala\n    scala> val final = join.map ( v => (v._2._1, (v._1,v._2._2))).sortByKey(false)\n\n    ```", "```scala\n    scala> final.take(5).collect.foreach(println)\n\n    ```", "```scala\n(12406.054646736622,(5302153,United_States'_Country_Reports_on_Human_Rights_Practices))\n(7925.094429748747,(84707,2007,_Canada_budget)) (7635.6564216408515,(88822,2008,_Madrid_plane_crash)) (7041.479913258444,(1921890,Geographic_coordinates)) (5675.169862343964,(5300058,United_Kingdom's))\n\n```", "```scala\n1,John\n2,Pat\n3,Dave\n4,Gary\n5,Chris\n6,Bill\n```", "```scala\n1,2,follows\n2,3,follows\n4,5,follows\n5,6,follows\n```", "```scala\n$ hdfs dfs -mkdir data/cc\n$ hdfs dfs -put nodes.csv data/cc/nodes.csv\n$ hdfs dfs -put edges.csv data/cc/edges.csv\n\n```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.graphx._\n\n    ```", "```scala\n    scala> val edgesFile = sc.textFile(\"hdfs://localhost:9000/user/hduser/data/cc/edges.csv\")\n\n    ```", "```scala\n    scala> val edges = edgesFile.map(_.split(\",\")).map(e => Edge(e(0).toLong,e(1).toLong,e(2)))\n\n    ```", "```scala\n    scala> val verticesFile = sc.textFile(\"hdfs://localhost:9000/user/hduser/data/cc/nodes.csv\")\n\n    ```", "```scala\n    scala> val vertices = verticesFile.map(_.split(\",\")).map( e => (e(0).toLong,e(1)))\n\n    ```", "```scala\n    scala> val graph = Graph(vertices,edges)\n\n    ```", "```scala\n    scala> val cc = graph.connectedComponents\n\n    ```", "```scala\n    scala> val ccVertices = cc.vertices\n\n    ```", "```scala\n    scala> ccVertices.collect.foreach(println)\n\n    ```", "```scala\n1,Barack\n2,John\n3,Pat\n4,Gary\n5,Mitt\n6,Chris\n7,Rob\n```", "```scala\n2,1,follows\n3,1,follows\n4,1,follows\n6,5,follows\n7,5,follows\n```", "```scala\n$ hdfs dfs -mkdir data/na\n$ hdfs dfs -put nodes.csv data/na/nodes.csv\n$ hdfs dfs -put edges.csv data/na/edges.csv\n\n```", "```scala\n    $ spark-shell\n\n    ```", "```scala\n    scala> import org.apache.spark.graphx._\n\n    ```", "```scala\n    scala> val edgesFile = sc.textFile(\"hdfs://localhost:9000/user/hduser/data/na/edges.csv\")\n\n    ```", "```scala\n    scala> val edges = edgesFile.map(_.split(\",\")).map(e => Edge(e(0).toLong,e(1).toLong,e(2)))\n\n    ```", "```scala\n    scala> val verticesFile = sc.textFile(\"hdfs://localhost:9000/user/hduser/data/cc/nodes.csv\")\n\n    ```", "```scala\n    scala> val vertices = verticesFile.map(_.split(\",\")).map( e => (e(0).toLong,e(1)))\n\n    ```", "```scala\n    scala> val graph = Graph(vertices,edges)\n\n    ```", "```scala\n    scala> val followerCount = graph.aggregateMessages[(Int)]( t => t.sendToDst(1), (a, b) => (a+b))\n\n    ```", "```scala\n    scala> followerCount.collect.foreach(println)\n\n    ```", "```scala\n(1,3)\n(5,2)\n\n```"]