["```py\npackage com.tomekl007.chapter_6\nimport com.tomekl007.UserTransaction\nobject BonusVerifier {\n private val superUsers = List(\"A\", \"X\", \"100-million\")\ndef qualifyForBonus(userTransaction: UserTransaction): Boolean = {\n superUsers.contains(userTransaction.userId) && userTransaction.amount > 100\n }\n}\n```", "```py\npackage com.tomekl007\n\nimport java.util.UUID\n\ncase class UserData(userId: String , data: String)\n\ncase class UserTransaction(userId: String, amount: Int)\n\ncase class InputRecord(uuid: String = UUID.*randomUUID()*.toString(), userId: String)\n```", "```py\npackage com.tomekl007.chapter_6\nimport com.tomekl007.UserTransaction\nimport org.scalatest.FunSuite\nclass SeparatingLogic extends FunSuite {\ntest(\"test complex logic separately from spark engine\") {\n //given\n val userTransaction = UserTransaction(\"X\", 101)\n//when\n val res = BonusVerifier.qualifyForBonus(userTransaction)\n//then\n assert(res)\n }\n}\n```", "```py\ntest(testName = \"test complex logic separately from spark engine - non qualify\") {\n //given\n val userTransaction = UserTransaction(\"X\", 99)\n//when\n val res = BonusVerifier.*qualifyForBonus*(userTransaction)\n//then\n assert(!res)\n }\n```", "```py\ntest(testName = \"test complex logic separately from spark engine - non qualify2\") {\n //given\n val userTransaction = UserTransaction(\"some_new_user\", 100000)\n//when\n val res = BonusVerifier.*qualifyForBonus*(userTransaction)\n//then\n assert(!res)\n }\n```", "```py\n val spark: SparkContext = SparkSession.builder().master(\"local[2]\").getOrCreate().sparkContext\n```", "```py\n val keysWithValuesList =\n Array(\n UserTransaction(\"A\", 100),\n UserTransaction(\"B\", 4),\n UserTransaction(\"A\", 100001),\n UserTransaction(\"B\", 10),\n UserTransaction(\"C\", 10)\n )\n```", "```py\n val data = spark.parallelize(keysWithValuesList)\n```", "```py\n val aggregatedTransactionsForUserId = data.filter(BonusVerifier.qualifyForBonus)\n```", "```py\n UserTransaction(\"A\", 100001)\n```", "```py\n ignore(\"loading data on prod from hive\") {\n UserDataLogic.loadAndGetAmount(spark, HiveDataLoader.loadUserTransactions)\n }\n```", "```py\nobject UserDataLogic {\n  def loadAndGetAmount(sparkSession: SparkSession, provider: SparkSession => DataFrame): DataFrame = {\n    val df = provider(sparkSession)\n    df.select(df(\"amount\"))\n  }\n}\n```", "```py\nobject HiveDataLoader {\n def loadUserTransactions(sparkSession: SparkSession): DataFrame = {\n sparkSession.sql(\"select * from transactions\")\n }\n}\n```", "```py\nUserDataLogic.loadAndGetAmount(spark, HiveDataLoader.loadUserTransactions)\n```", "```py\n val df = spark.sparkContext\n .makeRDD(List(UserTransaction(\"a\", 100), UserTransaction(\"b\", 200)))\n .toDF()\n```", "```py\nval res = UserDataLogic.loadAndGetAmount(spark, _ => df)\n```", "```py\n val df = provider(sparkSession)\n df.select(df(\"amount\"))\n```", "```py\nobject PropertyBasedTesting extends Properties(\"StringType\")\n```", "```py\nproperty(\"length of strings\") = forAll { (a: String, b: String) =>\n a.length + b.length >= a.length\n }\n```", "```py\na.length + b.length >= a.length\n```", "```py\nproperty(\"creating list of strings\") = forAll { (a: String, b: String, c: String) =>\n List(a,b,c).map(_.length).sum == a.length + b.length + c.length\n }\n```", "```py\nList(a,b,c)\n```", "```py\nprintln(s\"a: $a, b: $b\")\n```", "```py\ntest(\"mock loading data from hive\"){\n //given\n import spark.sqlContext.implicits._\n val df = spark.sparkContext\n .makeRDD(List(UserTransaction(\"a\", 100), UserTransaction(\"b\", 200)))\n .toDF()\n .rdd\n//when\n val res = UserDataLogicPre2.loadAndGetAmount(spark, _ => df)\n//then\n println(res.collect().toList)\n }\n}\n```", "```py\n val res = UserDataLogicPre2.loadAndGetAmount(spark, _ => rdd)\n```", "```py\nobject UserDataLogicPre2 {\n def loadAndGetAmount(sparkSession: SparkSession, provider: SparkSession => RDD[Row]): RDD[Int] = {\n provider(sparkSession).map(_.getAs[Int](\"amount\"))\n }\n}\nobject HiveDataLoaderPre2 {\n def loadUserTransactions(sparkSession: SparkSession): RDD[Row] = {\n sparkSession.sql(\"select * from transactions\").rdd\n }\n}\n```"]