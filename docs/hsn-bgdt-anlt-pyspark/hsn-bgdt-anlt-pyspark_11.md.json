["```py\n val keysWithValuesList =\n Array(\n UserTransaction(\"A\", 100),\n UserTransaction(\"B\", 4),\n UserTransaction(\"A\", 100001),\n UserTransaction(\"B\", 10),\n UserTransaction(\"C\", 10)\n )\n```", "```py\nval keyed = data.keyBy(_.userId)\n```", "```py\nval data = spark.parallelize(keysWithValuesList)\n val keyed = data.keyBy(_.userId)\n//when\n val counted = keyed.countByKey()\n// keyed.combineByKey()\n// keyed.aggregateByKey()\n// keyed.foldByKey()\n// keyed.groupByKey()\n//then\n counted should contain theSameElementsAs Map(\"B\" -> 2, \"A\" -> 2, \"C\" -> 1)\n```", "```py\ncounted should contain theSameElementsAs Map(\"B\" -> 2, \"A\" -> 2, \"C\" -> 1)\n```", "```py\n val keysWithValuesList =\n Array(\n UserTransaction(\"A\", 100),\n UserTransaction(\"B\", 4),\n UserTransaction(\"A\", 100001),\n UserTransaction(\"B\", 10),\n UserTransaction(\"C\", 10)\n )\n```", "```py\n val data = spark.parallelize(keysWithValuesList)\n val keyed = data.keyBy(_.userId)\n```", "```py\n val aggregatedTransactionsForUserId = keyed\n .aggregateByKey(amountForUser)(addAmount, mergeAmounts)\n```", "```py\naggregateByKey(amountForUser)(addAmount, mergeAmounts)\n```", "```py\nmutable.ArrayBuffer.empty[Long]\n```", "```py\n val mergeAmounts = (p1: mutable.ArrayBuffer[Long], p2: mutable.ArrayBuffer[Long]) => p1 ++= p2\n```", "```py\n aggregatedTransactionsForUserId.collect().toList should contain theSameElementsAs List(\n (\"A\", ArrayBuffer(100, 100001)),\n (\"B\", ArrayBuffer(4,10)),\n (\"C\", ArrayBuffer(10)))\n```", "```py\n val keysWithValuesList =\n Array(\n UserTransaction(\"A\", 100),\n UserTransaction(\"B\", 4),\n UserTransaction(\"A\", 100001),\n UserTransaction(\"B\", 10),\n UserTransaction(\"C\", 10)\n )\n```", "```py\n res should contain theSameElementsAs List(\n (\"A\",UserTransaction(\"A\",100)),\n (\"B\",UserTransaction(\"B\",4)),\n (\"A\",UserTransaction(\"A\",100001)),\n (\"B\",UserTransaction(\"B\",10)),\n (\"C\",UserTransaction(\"C\",10))\n )//note duplicated key\n```", "```py\n val keysWithValuesList =\n Array(\n UserTransaction(\"A\", 100),\n UserTransaction(\"B\", 4),\n UserTransaction(\"A\", 100001),\n UserTransaction(\"B\", 10),\n UserTransaction(\"C\", 10)\n )\n```", "```py\n val keyed = data.keyBy(_.userId)\n```", "```py\n val partitioner = keyed.partitioner\n```", "```py\n assert(partitioner.isEmpty)\n```", "```py\nval hashPartitioner = keyed.partitionBy(new HashPartitioner(100))\n```", "```py\n    def numPartitions: Int = partitions\n    def getPartition(key: Any): int = key match {\n        case null => 0\n        case_ => Utils.nonNegativeMode(key.hashCode, numPartitions)\n    }\n```", "```py\n val keysWithValuesList =\n Array(\n UserTransaction(\"A\", 100),\n UserTransaction(\"B\", 4),\n UserTransaction(\"A\", 100001),\n UserTransaction(\"B\", 10),\n UserTransaction(\"C\", 10)\n )\n val data = spark.parallelize(keysWithValuesList)\n val keyed = data.keyBy(_.amount)\n```", "```py\n val partitioned = keyed.partitionBy(new CustomRangePartitioner(List((0,100), (100, 10000), (10000, 1000000))))\n```", "```py\nclass CustomRangePartitioner(ranges: List[(Int,Int)]) extends Partitioner{\n override def numPartitions: Int = ranges.size\noverride def getPartition(key: Any): Int = {\n if(!key.isInstanceOf[Int]){\n throw new IllegalArgumentException(\"partitioner works only for Int type\")\n }\n val keyInt = key.asInstanceOf[Int]\n val index = ranges.lastIndexWhere(v => keyInt >= v._1 && keyInt <= v._2)\n println(s\"for key: $key return $index\")\n index\n }\n}\n```", "```py\n(ranges: List[(Int,Int)])\n```", "```py\nif(!key.isInstanceOf[Int])\n```", "```py\n val index = ranges.lastIndexWhere(v => keyInt >= v._1 && keyInt <= v._2)\n```", "```py\nprintln(s\"for key: $key return $index\")\n```"]