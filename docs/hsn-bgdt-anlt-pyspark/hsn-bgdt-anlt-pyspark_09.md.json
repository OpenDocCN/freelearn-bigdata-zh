["```py\nclass DetectingShuffle extends FunSuite {\n  val spark: SparkSession = SparkSession.builder().master(\"local[2]\").getOrCreate()\n\n  test(\"should explain plan showing logical and physical with UDF and DF\") {\n    //given\n    import spark.sqlContext.implicits._\n    val df = spark.sparkContext.makeRDD(List(\n      InputRecord(\"1234-3456-1235-1234\", \"user_1\"),\n      InputRecord(\"1123-3456-1235-1234\", \"user_1\"),\n      InputRecord(\"1123-3456-1235-9999\", \"user_2\")\n    )).toDF()\n```", "```py\n//when\n    val q = df.repartition(df(\"userId\"))\n```", "```py\n q.explain(true)\n```", "```py\ntest(\"example of operation that is causing shuffle\") {\n    import spark.sqlContext.implicits._\n    val userData =\n    spark.sparkContext.makeRDD(List(\n        UserData(\"user_1\", \"1\"),\n        UserData(\"user_2\", \"2\"),\n        UserData(\"user_4\", \"200\")\n    )).toDS()\n```", "```py\nval transactionData =\n    spark.sparkContext.makeRDD(List(\n        UserTransaction(\"user_1\", 100),\n        UserTransaction(\"user_2\", 300),\n        UserTransaction(\"user_3\", 1300)\n    )).toDS()\n```", "```py\n//shuffle: userData can stay on the current executors, but data from\n//transactionData needs to be send to those executors according to joinColumn\n//causing shuffle\n//when\nval res: Dataset[(UserData, UserTransaction)]\n= userData.joinWith(transactionData, userData(\"userId\") === transactionData(\"userId\"), \"inner\")\n```", "```py\n//then\n res.show()\n assert(res.count() == 2)\n }\n}\n```", "```py\n+------------+-------------+\n|         _1 |           _2|\n+----------- +-------------+\n+ [user_1,1] | [user_1,100]|\n| [user_2,2] | [user_2,300]|\n+------------+-------------+\n```", "```py\ntest(\"example of operation that is causing shuffle\") {\n    import spark.sqlContext.implicits._\n    val userData =\n        spark.sparkContext.makeRDD(List(\n            UserData(\"user_1\", \"1\"),\n            UserData(\"user_2\", \"2\"),\n            UserData(\"user_4\", \"200\")\n        )).toDS()\n```", "```py\nval repartitionedUserData = userData.repartition(userData(\"userId\"))\n```", "```py\n val repartitionedTransactionData = transactionData.repartition(transactionData(\"userId\"))\n```", "```py\n//when\n//data is already partitioned using join-column. Don't need to shuffle\nval res: Dataset[(UserData, UserTransaction)]\n= repartitionedUserData.joinWith(repartitionedTransactionData, userData(\"userId\") === transactionData(\"userId\"), \"inner\")\n```", "```py\n //then\n res.show()\n assert(res.count() == 2)\n }\n}\n```", "```py\ntest(\"Should use keyBy to distribute traffic properly\"){\n    //given\n    val rdd = spark.sparkContext.makeRDD(List(\n        InputRecord(\"1234-3456-1235-1234\", \"user_1\"),\n        InputRecord(\"1123-3456-1235-1234\", \"user_1\"),\n        InputRecord(\"1123-3456-1235-9999\", \"user_2\")\n    ))\n```", "```py\nprintln(rdd.toDebugString)\n```", "```py\nval res = rdd.keyBy(_.userId)\n```", "```py\nprintln(res.toDebugString)\n```", "```py\nres.collect()\n```", "```py\nimport com.tomekl007.UserTransaction\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.{Partitioner, SparkContext}\nimport org.scalatest.FunSuite\nimport org.scalatest.Matchers._\n\nclass CustomPartitioner extends FunSuite {\nval spark: SparkContext = SparkSession.builder().master(\"local[2]\").getOrCreate().sparkContext\n\ntest(\"should use custom partitioner\") {\n//given\nval numberOfExecutors = 2\n```", "```py\nval data = spark\n    .parallelize(List(\n        UserTransaction(\"a\", 100),\n        UserTransaction(\"b\", 101),\n        UserTransaction(\"a\", 202),\n        UserTransaction(\"b\", 1),\n        UserTransaction(\"c\", 55)\n```", "```py\n).keyBy(_.userId)\n.partitionBy(new Partitioner {\n    override def numPartitions: Int = numberOfExecutors\n```", "```py\noverride def getPartition(key: Any): Int = {\n    key.hashCode % numberOfExecutors\n    }\n})\n```", "```py\nprintln(data.partitions.length)\n```", "```py\n//when\nval res = data.mapPartitions[Long](iter =>\niter.map(_._2).map(_.amount)\n).collect().toList\n```", "```py\n//then\nres should contain theSameElementsAs List(55, 100, 202, 101, 1)\n}\n}\n```"]