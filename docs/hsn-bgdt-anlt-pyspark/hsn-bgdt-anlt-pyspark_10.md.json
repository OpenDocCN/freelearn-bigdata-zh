["```py\npackage com.tomekl007.chapter_4\n\nimport java.io.File\n\nimport com.tomekl007.UserTransaction\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.{Partitioner, SparkContext}\nimport org.scalatest.{BeforeAndAfterEach, FunSuite}\nimport org.scalatest.Matchers._\n\nimport scala.reflect.io.Path\n\nclass SavePlainText extends FunSuite with BeforeAndAfterEach{\n    val spark: SparkContext = SparkSession.builder().master(\"local[2]\").getOrCreate().sparkContext\n\n    private val FileName = \"transactions.txt\"\n\n    override def afterEach() {\n        val path = Path (FileName)\n        path.deleteRecursively()\n    }\n\n    test(\"should save and load in plain text\") {\n        //given\n        val rdd = spark.makeRDD(List(UserTransaction(\"a\", 100), UserTransaction(\"b\", 200)))\n\n        //when\n        rdd.coalesce(1).saveAsTextFile(FileName)\n\n        val fromFile = spark.textFile(FileName)\n\n        fromFile.collect().toList should contain theSameElementsAs List(\n            \"UserTransaction(a,100)\", \"UserTransaction(b,200)\"\n            //note - this is string!\n        )\n    }\n}\n```", "```py\nimport java.io.File\nimport com.tomekl007.UserTransaction\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.{Partitioner, SparkContext}\nimport org.scalatest.{BeforeAndAfterEach, FunSuite}\nimport org.scalatest.Matchers._\nimport scala.reflect.io.Path\nclass SavePlainText extends FunSuite with BeforeAndAfterEach{\n    val spark: SparkContext = SparkSession.builder().master(\"local[2]\").getOrCreate().sparkContext\n    private val FileName = \"transactions.txt\"\n```", "```py\n//override def afterEach() {\n//         val path = Path (FileName)\n//         path.deleteRecursively()\n//     }\n\n//test(\"should save and load in plain text\") {\n```", "```py\nval rdd = spark.makeRDD(List(UserTransaction(\"a\", 100), UserTransaction(\"b\", 200)))\n```", "```py\nrdd.coalesce(1).saveAsTextFile(FileName)\n```", "```py\nrdd.coalesce (numPartitions = 1).saveAsTextFile(FileName)\n```", "```py\n    val fromFile = spark.textFile(FileName)\n```", "```py\n    fromFile.collect().toList should contain theSameElementsAs List(\n      \"UserTransaction(a,100)\", \"UserTransaction(b,200)\"\n      //note - this is string!\n    )\n  }\n}\n```", "```py\nval rdd = spark.sparkContext\n         .makeRDD(List(UserTransaction(\"a\", 100), UserTransaction(\"b\", 200)))\n         .toDF()\n```", "```py\nrdd.coalesce(2).write.format(\"json\").save(FileName)\n```", "```py\nrdd.coalesce(2).write.format(\"not\").save(FileName)\n```", "```py\nval fromFile = spark.read.json(FileName)\n```", "```py\n// override def afterEach() {\n// val path = Path(FileName)\n// path.deleteRecursively()\n// }\n```", "```py\n fromFile.show()\n assert(fromFile.count() == 2)\n }\n}\n```", "```py\n+------+------+\n|amount|userId|\n|   200|     b|\n|   100|     a|\n+------+------+\n```", "```py\n{\"userId\":\"a\",\"amount\":\"100\"}\n```", "```py\n{\"userId\":\"b\",\"amount\":\"200\"}\n```", "```py\ntest(\"should save and load CSV with header\") {\n //given\n import spark.sqlContext.implicits._\n val rdd = spark.sparkContext\n .makeRDD(List(UserTransaction(\"a\", 100), UserTransaction(\"b\", 200)))\n .toDF()\n```", "```py\n//when\nrdd.coalesce(1)\n    .write\n    .format(\"csv\")\n    .option(\"header\", \"false\")\n    .save(FileName)\n```", "```py\n    //when\n    rdd.coalesce(1)\n      .write\n      .format(\"csv\")\n      .option(\"header\", \"true\")\n      .save(FileName)  \n```", "```py\nval fromFile = spark.read.option(\"header\", \"false\").csv(FileName)\n```", "```py\n+---+---+\n|_c0|_c1|\n+---+---+\n|  a|100|\n|  b|200|\n+---+---+\n```", "```py\nval fromFile = spark.read.option(\"header\", \"true).csv(FileName)\n```", "```py\n+------+------+\n|userId|amount|\n+------+------+\n|     a|   100|\n|     b|   200|\n+------+------+\n```", "```py\n  //when\n rdd.coalesce(1)\n     .write\n     .format(\"csv\")\n     .option(\"header\", \"false\")\n     .save(FileName)\n\nval fromFile = spark.read.option(\"header\", \"false\").csv(FileName)\n```", "```py\noverride def afterEach() {\n    val path = Path(FileName)\n    path.deleteRecursively()\n}\n```", "```py\n test(\"should save and load avro\") {\n //given\n import spark.sqlContext.implicits._\n val rdd = spark.sparkContext\n     .makeRDD(List(UserTransaction(\"a\", 100), UserTransaction(\"b\", 200)))\n     .toDF()\n```", "```py\n //when\n rdd.coalesce(2)\n     .write\n     .avro(FileName)\n```", "```py\nimport com.databricks.spark.avro._\n```", "```py\n//when\n rdd.coalesce(2)\n     .write.format(com.databricks.spark.avro)\n     .avro(FileName)\n```", "```py\n// override def afterEach() {\n    // val path = Path(FileName)\n    // path.deleteRecursively()\n// }\n```", "```py\n+------+------+\n|userId|amount|\n+------+------+\n|     a|   100|\n|     b|   200|\n+------+------+\n```", "```py\npackage com.tomekl007.chapter_4\n\nimport com.databricks.spark.avro._\nimport com.tomekl007.UserTransaction\nimport org.apache.spark.sql.SparkSession\nimport org.scalatest.{BeforeAndAfterEach, FunSuite}\n\nimport scala.reflect.io.Path\n\nclass SaveParquet extends FunSuite with BeforeAndAfterEach {\n  val spark = SparkSession.builder().master(\"local[2]\").getOrCreate()\n\n  private val FileName = \"transactions.parquet\"\n\n  override def afterEach() {\n    val path = Path(FileName)\n    path.deleteRecursively()\n  }\n\n  test(\"should save and load parquet\") {\n    //given\n    import spark.sqlContext.implicits._\n    val rdd = spark.sparkContext\n      .makeRDD(List(UserTransaction(\"a\", 100), UserTransaction(\"b\", 200)))\n      .toDF()\n\n    //when\n    rdd.coalesce(2)\n      .write\n      .parquet(FileName)\n```", "```py\n    val fromFile = spark.read.parquet(FileName)\n\n    fromFile.show()\n    assert(fromFile.count() == 2)\n  }\n\n}\n```", "```py\n//    override def afterEach() {\n//    val path = Path(FileName)\n//    path.deleteRecursively()\n//  } \n```"]