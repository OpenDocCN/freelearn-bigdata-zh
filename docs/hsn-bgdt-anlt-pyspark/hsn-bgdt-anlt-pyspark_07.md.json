["```py\nclass DeferComputations extends FunSuite {\nval spark: SparkContext = SparkSession.builder().master(\"local[2]\").getOrCreate().sparkContext\n```", "```py\ntest(\"should defer computations\") {\n //given\n    val input = spark.makeRDD(\n        List(InputRecord(userId = \"A\"),\n            InputRecord(userId = \"B\")))\n```", "```py\n//when apply transformation\nval rdd = input\n    .filter(_.userId.contains(\"A\"))\n    .keyBy(_.userId)\n.map(_._2.userId.toLowerCase)\n//.... built processing graph lazy\n```", "```py\nif (shouldExecutePartOfCode()) {\n     //rdd.saveAsTextFile(\"\") ||\n     rdd.collect().toList\n  } else {\n    //condition changed - don't need to evaluate DAG\n }\n}\n```", "```py\nprivate def shouldExecutePartOfCode(): Boolean = {\n    //domain logic that decide if we still need to calculate\n    true\n    }\n}\n```", "```py\n\"C:\\Program Files\\Java\\jdk-12\\bin\\java.exe\" \"-javaagent:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\lib\\idea_rt.jar=50627:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\bin\" -Dfile.encoding=UTF-8 -classpath C:\\Users\\Sneha\\IdeaProjects\\Chapter07\\out\\production\\Chapter07 com.company.Main\n\nProcess finished with exit code 0\n```", "```py\ntest(\"should trigger computations using actions\") {\n //given\n val input = spark.makeRDD(\n     List(\n         UserTransaction(userId = \"A\", amount = 1001),\n         UserTransaction(userId = \"A\", amount = 100),\n         UserTransaction(userId = \"A\", amount = 102),\n         UserTransaction(userId = \"A\", amount = 1),\n         UserTransaction(userId = \"B\", amount = 13)))\n```", "```py\n//when apply transformation\nval rdd = input\n    .groupBy(_.userId)\n    .map(x => (x._1,x._2.toList))\n    .collect()\n    .toList\n```", "```py\n//then\nrdd should contain theSameElementsAs List(\n    (\"B\", List(UserTransaction(\"B\", 13))),\n    (\"A\", List(\n        UserTransaction(\"A\", 1001),\n        UserTransaction(\"A\", 100),\n        UserTransaction(\"A\", 102),\n        UserTransaction(\"A\", 1))\n    )\n  )\n }\n}\n```", "```py\n\"C:\\Program Files\\Java\\jdk-12\\bin\\java.exe\" \"-javaagent:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\lib\\idea_rt.jar=50822:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\bin\" -Dfile.encoding=UTF-8 -classpath C:\\Users\\Sneha\\IdeaProjects\\Chapter07\\out\\production\\Chapter07 com.company.Main\n\nProcess finished with exit code 0\n```", "```py\ntest(\"should use reduce API\") {\n    //given\n    val input = spark.makeRDD(List(\n    UserTransaction(\"A\", 10),\n    UserTransaction(\"B\", 1),\n    UserTransaction(\"A\", 101)\n    ))\n```", "```py\n//when\nval result = input\n    .map(_.amount)\n    .reduce((a, b) => if (a > b) a else b)\n\n//then\nassert(result == 101)\n}\n```", "```py\n\"C:\\Program Files\\Java\\jdk-12\\bin\\java.exe\" \"-javaagent:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\lib\\idea_rt.jar=50894:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\bin\" -Dfile.encoding=UTF-8 -classpath C:\\Users\\Sneha\\IdeaProjects\\Chapter07\\out\\production\\Chapter07 com.company.Main\n\nProcess finished with exit code 0\n```", "```py\ntest(\"should use reduceByKey API\") {\n    //given\n    val input = spark.makeRDD(\n    List(\n        UserTransaction(\"A\", 10),\n        UserTransaction(\"B\", 1),\n        UserTransaction(\"A\", 101)\n    )\n)\n```", "```py\n    //when\n    val result = input\n      .keyBy(_.userId)\n      .reduceByKey((firstTransaction, secondTransaction) =>\n        TransactionChecker.higherTransactionAmount(firstTransaction, secondTransaction))\n      .collect()\n      .toList\n```", "```py\nobject TransactionChecker {\n    def higherTransactionAmount(firstTransaction: UserTransaction, secondTransaction: UserTransaction): UserTransaction = {\n        if (firstTransaction.amount > secondTransaction.amount) firstTransaction else     secondTransaction\n    }\n}\n```", "```py\n    //then\n    result should contain theSameElementsAs\n      List((\"B\", UserTransaction(\"B\", 1)), (\"A\", UserTransaction(\"A\", 101)))\n  }\n\n}\n```", "```py\n\"C:\\Program Files\\Java\\jdk-12\\bin\\java.exe\" \"-javaagent:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\lib\\idea_rt.jar=50909:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\bin\" -Dfile.encoding=UTF-8 -classpath C:\\Users\\Sneha\\IdeaProjects\\Chapter07\\out\\production\\Chapter07 com.company.Main\n\nProcess finished with exit code 0\n```", "```py\ntest(\"should trigger computations using actions\") {\n     //given\n     val input = spark.makeRDD(\n     List(\n         UserTransaction(userId = \"A\", amount = 1001),\n         UserTransaction(userId = \"A\", amount = 100),\n         UserTransaction(userId = \"A\", amount = 102),\n         UserTransaction(userId = \"A\", amount = 1),\n         UserTransaction(userId = \"B\", amount = 13)))\n\n//when apply transformation\n val rdd = input\n     .filter(_.userId.contains(\"A\"))\n     .keyBy(_.userId)\n     .map(_._2.amount)\n```", "```py\n//then\n println(rdd.collect().toList)\n println(rdd.count()) //and all count*\n```", "```py\nprintln(rdd.first())\n```", "```py\n rdd.foreach(println(_))\n rdd.foreachPartition(t => t.foreach(println(_)))\n println(rdd.max())\n println(rdd.min())\n```", "```py\n println(rdd.takeOrdered(1).toList)\n println(rdd.takeSample(false, 2).toList)\n }\n}\n```", "```py\nList(1001, 100, 102 ,1)\n4\n1001\n1001\n100\n102\n1\n```", "```py\n102\n1\n1001\n1\nList(1)\nList(100, 1)\n```", "```py\n//then every call to action means that we are going up to the RDD chain\n//if we are loading data from external file-system (I.E.: HDFS), every action means\n//that we need to load it from FS.\n    val start = System.currentTimeMillis()\n    println(rdd.collect().toList)\n    println(rdd.count())\n    println(rdd.first())\n    rdd.foreach(println(_))\n    rdd.foreachPartition(t => t.foreach(println(_)))\n    println(rdd.max())\n    println(rdd.min())\n    println(rdd.takeOrdered(1).toList)\n    println(rdd.takeSample(false, 2).toList)\n    val result = System.currentTimeMillis() - start\n\n    println(s\"time taken (no-cache): $result\")\n\n}\n```", "```py\n//when apply transformation\nval rdd = input\n    .filter(_.userId.contains(\"A\"))\n    .keyBy(_.userId)\n    .map(_._2.amount)\n```", "```py\nList(1)\nList(100, 1)\ntime taken (no-cache): 632\nProcess finished with exit code 0\n```", "```py\n//when apply transformation\nval rdd = input\n    .filter(_.userId.contains(\"A\"))\n    .keyBy(_.userId)\n    .map(_._2.amount)\n    .cache()\n```", "```py\n//then every call to action means that we are going up to the RDD chain\n//if we are loading data from external file-system (I.E.: HDFS), every action means\n//that we need to load it from FS.\n    val start = System.currentTimeMillis()\n    println(rdd.collect().toList)\n    println(rdd.count())\n    println(rdd.first())\n    rdd.foreach(println(_))\n    rdd.foreachPartition(t => t.foreach(println(_)))\n    println(rdd.max())\n    println(rdd.min())\n    println(rdd.takeOrdered(1).toList)\n    println(rdd.takeSample(false, 2).toList)\n    val result = System.currentTimeMillis() - start\n\n    println(s\"time taken(cache): $result\")\n\n    }\n}\n```", "```py\nList(1)\nList(100, 102)\ntime taken (no-cache): 585\nList(1001, 100, 102, 1)\n4\n```", "```py\n1\nList(1)\nList(102, 1)\ntime taken(cache): 336\nProcess finished with exit code 0\n```"]