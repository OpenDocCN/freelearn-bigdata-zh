["```py\nclass InheritanceRdd extends FunSuite {\n  val spark: SparkContext = SparkSession\n    .builder().master(\"local[2]\").getOrCreate().sparkContext\n\n  test(\"use extended RDD\") {\n    //given\n    val rdd = spark.makeRDD(List(Record(1, \"d1\")))\n```", "```py\nval extendedRdd = new MultipliedRDD(rdd, 10)\n```", "```py\nextendedRdd.collect().toList should contain theSameElementsAs List(\n Record(10, \"d1\")\n )\n }\n}\n```", "```py\n\"C:\\Program Files\\Java\\jdk-12\\bin\\java.exe\" \"-javaagent:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\lib\\idea_rt.jar=51687:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\bin\" -Dfile.encoding=UTF-8 -classpath C:\\Users\\Sneha\\IdeaProjects\\Chapter07\\out\\production\\Chapter07 com.company.Main\n\nProcess finished with exit code 0\n```", "```py\nclass ImmutableRDD extends FunSuite {\n    val spark: SparkContext = SparkSession\n        .builder().master(\"local[2]\").getOrCreate().sparkContext\n\ntest(\"RDD should be immutable\") {\n    //given\n    val data = spark.makeRDD(0 to 5)\n```", "```py\n//when\nval res = data.map(_ * 2)\n\nval leaf2 = data.map(_ * 4)\n```", "```py\n//then\nres.collect().toList should contain theSameElementsAs List(\n    0, 2, 4, 6, 8, 10\n)\n```", "```py\ndata.collect().toList should contain theSameElementsAs List(\n    0, 1, 2, 3, 4, 5\n    )\n  }\n}\n```", "```py\nleaf2.collect().toList should contain theSameElementsAs List(\n 0, 4, 8, 12, 16, 20\n)\n```", "```py\n\"C:\\Program Files\\Java\\jdk-12\\bin\\java.exe\" \"-javaagent:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\lib\\idea_rt.jar=51704:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\bin\" -Dfile.encoding=UTF-8 -classpath C:\\Users\\Sneha\\IdeaProjects\\Chapter07\\out\\production\\Chapter07 com.company.Main\n\nProcess finished with exit code 0\n```", "```py\ntest(\"Should use immutable DF API\") {\n import spark.sqlContext.implicits._\n //given\n val userData =\n spark.sparkContext.makeRDD(List(\n UserData(\"a\", \"1\"),\n UserData(\"b\", \"2\"),\n UserData(\"d\", \"200\")\n )).toDF()\n```", "```py\n//when\n    val res = userData.filter(userData(\"userId\").isin(\"a\"))\n```", "```py\n    assert(res.count() == 1)\n    assert(userData.count() == 3)\n\n    }\n}\n```", "```py\n\"C:\\Program Files\\Java\\jdk-12\\bin\\java.exe\" \"-javaagent:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\lib\\idea_rt.jar=51713:C:\\Program Files\\JetBrains\\IntelliJ IDEA 2018.3.5\\bin\" -Dfile.encoding=UTF-8 -classpath C:\\Users\\Sneha\\IdeaProjects\\Chapter07\\out\\production\\Chapter07 com.company.Main\n\nProcess finished with exit code 0\n```", "```py\nres.map(a => a.getString(\"userId\") + \"can\")\n```", "```py\nimport java.util.concurrent.{CountDownLatch, Executors}\nimport org.scalatest.FunSuite\nimport scala.collection.mutable.ListBuffer\nclass MultithreadedImmutabilityTest extends FunSuite {\n\ntest(\"warning: race condition with mutability\") {\n//given\nvar listMutable = new ListBuffer[String]()\nval executors = Executors.newFixedThreadPool(2)\nval latch = new CountDownLatch(2)\n```", "```py\n //when\n executors.submit(new Runnable {\n     override def run(): Unit = {\n         latch.countDown()\n         listMutable += \"A\"\n     }\n })\n```", "```py\n executors.submit(new Runnable {\n     override def run(): Unit = {\n         latch.countDown()\n         if(!listMutable.contains(\"A\")) {\n             listMutable += \"A\"\n         }\n     }\n })\n```", "```py\n    latch.await()\n```", "```py\n    //then\n    //listMutable can have (\"A\") or (\"A\",\"A\")\n    }\n}\n```", "```py\nval lock = new WriteLock()\n```", "```py\nlock.lock()\n```", "```py\nlock.unlock()\n```", "```py\nimport com.tomekl007.UserData\nimport org.apache.spark.sql.SparkSession\nimport org.scalatest.FunSuite\n\nclass ImmutableDataSet extends FunSuite {\n val spark: SparkSession = SparkSession\n .builder().master(\"local[2]\").getOrCreate()\n\ntest(\"Should use immutable DF API\") {\n import spark.sqlContext.implicits._\n //given\n val userData =\n spark.sparkContext.makeRDD(List(\n UserData(\"a\", \"1\"),\n UserData(\"b\", \"2\"),\n UserData(\"d\", \"200\")\n )).toDF()\n```", "```py\n   //when\n    val res = userData.filter(userData(\"userId\").isin(\"a\"))\n```", "```py\n    assert(res.count() == 1)\n    assert(userData.count() == 3)\n\n }\n}\n```", "```py\nuserData.show()\n```", "```py\n+------+----+\n|userId|data|\n|----- |----|\n|     a|   1|\n|     b|   2|\n|     d| 200|\n+------|----+ \n```"]