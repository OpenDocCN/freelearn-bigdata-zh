["```py\nlines = sc.textFile(\"data.txt\")\nlineLengths = lines.map(lambda s: len(s))\ntotalLength = lineLengths.reduce(lambda a, b: a + b)\n```", "```py\n#Register the DataFrame as a SQL temporary view\ndf.CreateOrReplaceTempView(\"people\")\n\nsqlDF = spark.sql(\"SELECT * FROM people\")\nsqlDF.show()\n\n#+----+-------+\n#| age|   name|\n#+----+-------+\n#+null|Jackson|\n#|  30| Martin|\n#|  19| Melvin|\n#+----|-------|\n```", "```py\n# spark is an existing SparkSession\ndf = spark.read.json(\"examples/src/main/resources/people.json\")\n# Displays the content of the DataFrame to stdout\ndf.show()\n\n#+----+-------+\n#| age|   name|\n#+----+-------+\n#+null|Jackson|\n#|  30| Martin|\n#|  19| Melvin|\n#+----|-------|\n```", "```py\ngow --list \n```", "```py\ngzip -d spark-2.1.0-bin-hadoop2.7.tgz tar xvf spark-2.1.0-bin-hadoop2.7.tar\n```", "```py\ncurl -k -L -o winutils.exe https://github.com/steveloughran/winutils/blob/master/hadoop-2.6.0/bin/winutils.exe?raw=true\n```", "```py\njava --version\n```", "```py\npython --version \n```", "```py\nsetx SPARK_HOME C:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\nsetx HADOOP_HOME C:\\opt\\spark\\spark-2.1.0-bin-hadoop2.7\nsetx PYSPARK_DRIVER_PYTHON ipython\nsetx PYSPARK_DRIVER_PYTHON_OPTS notebook\n```", "```py\n--master local[2]\n```", "```py\n.\\bin\\pyspark\n```", "```py\nfrom pyspark import SparkContext\nsc = SparkContext('local', 'hands on PySpark')\n```", "```py\nvisitors = [10, 3, 35, 25, 41, 9, 29] df_visitors = sc.parallelize(visitors) df_visitors_yearly = df_visitors.map(lambda x: x*365).collect() print(df_visitors_yearly)\n```", "```py\ntext_file = spark.read.text(\"README.md\")\n```", "```py\ntext_file.count()\n```", "```py\n103\n```", "```py\ntext_file.first()\n```", "```py\nRow(value='# Apache Spark')\n```", "```py\nlines_with_spark = text_file.filter(text_file.value.contains(\"Spark\"))\n```", "```py\ntext_file.filter(text_file.value.contains(\"Spark\")).count()\n```", "```py\n20\n```"]