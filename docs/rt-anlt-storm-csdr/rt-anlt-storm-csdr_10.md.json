["```scala\nFixedBatchSpout myFixedspout = new FixedBatchSpout(new  Fields(\"sentence\"), 3,\nnew Values(\"the basic storm topology do a great job\"),\nnew Values(\"they get tremendous speed and guaranteed processing\"),\nnew Values(\"that too in a reliable manner \"),\nnew Values(\"the new trident api over storm gets user more features  \"),\nnew Values(\"it gets micro batching over storm \"));\nmyFixedspout.setCycle(true);\n```", "```scala\nmyFixedspout cycles over the set of sentences added as values. This snippet ensures that we have an endless flow of data streams into the topology and enough points to perform all micro-batching functions that we intend to.\n```", "```scala\n//creating a new trident topology\nTridentTopology myTridentTopology = new TridentTopology();\n//Adding a spout and configuring the fields and query \nTridentState myWordCounts = topology.newStream(\"myFixedspout\",  spout)\n  .each(new Fields(\"sentence\"), new Split(), new Fields(\"word\"))\n  .groupBy(new Fields(\"word\"))\n  .persistentAggregate(new MemoryMapState.Factory(), new Count(),  new Fields(\"count\"))\n  .parallelismHint(6);\n```", "```scala\nNow the micro-batching; who does it and how? Well the Trident framework stores the state for each source (it kind of remembers what input data it has consumed so far). This state saving is done in the Zookeeper cluster. The tagging *spout* in the preceding code is actually a znode, which is created in the Zookeeper cluster to save the state metadata information.\n```", "```scala\npublic class Split extends BaseFunction {\n  public void execute(TridentTuple tuple, TridentCollector  collector) {\n      String sentence = tuple.getString(0);\n      for(String word: sentence.split(\" \")) {\n          collector.emit(new Values(word));\n      }\n  }\n}\n```", "```scala\nTrident with Storm is so popular because it guarantees the processing of all tuples in a fail-safe manner in exactly one semantic. In situations where retry is necessary because of failures, it does that exactly once and once only, so as a developer I don't end up updating the table storage multiple times on occurrence of a failure.\n\n```", "```scala\npublic class MyLocalFunction extends BaseFunction {\n  public void execute(TridentTuple myTuple, TridentCollector  myCollector) {\n      for(int i=0; i < myTuple.getInteger(0); i++) {\n          myCollector.emit(new Values(i));\n      }\n  }\n}\n```", "```scala\n[10, 2, 30]\n[40, 1, 60]\n[30, 0, 80]\n```", "```scala\nmystream.each(new Fields(\"b\"), new MyLocalFunction(), new  Fields(\"d\")))\n```", "```scala\n//for input tuple [10, 2, 30] loop in the function executes twice  //value of b=2\n[10, 2, 30, 0]\n[10, 2, 30, 1]\n//for input tuple [4, 1, 6] loop in the function executes once  value //of b =1\n[4, 1, 6, 0]\n//for input tuple [3, 0, 8]\n//no output because the value of field b is zero and the for loop  //would exit in first iteration itself value of b=0\n```", "```scala\npublic class MyLocalFilterFunction extends BaseFunction {\n    public boolean isKeep(TridentTuple tuple) {\n      return tuple.getInteger(0) == 1 && tuple.getInteger(1) == 2;\n    }\n}\n```", "```scala\n[1,2,3]\n[2,1,1]\n[2,3,4]\n```", "```scala\nmystream.each(new Fields(\"b\", \"a\"), new MyLocalFilterFunction())\n```", "```scala\n//for tuple 1 [1,2,3]\n// no output because valueof(\"field b\") ==1 && valueof(\"field a\")  ==2 //is not satisfied \n//for tuple 1 [2,1,1]\n// no output because valueof(\"field b\") ==1 && valueof(\"field a\")  ==2 [2,1,1]\n//for tuple 1 [2,3,4]\n// no output because valueof(\"field b\") ==1 && valueof(\"field a\")  ==2 //is not satisfied\n```", "```scala\nmystream.partitionAggregate(new Fields(\"b\"), new Sum(), new Fields(\"sum\"))\n```", "```scala\nPartition 0:\n[\"a\", 1]\n[\"b\", 2]\nPartition 1:\n[\"a\", 3]\n[\"c\", 8]\nPartition 2:\n[\"e\", 1]\n[\"d\", 9]\n[\"d\", 10]\n```", "```scala\nPartition 0:\n[3]\nPartition 1:\n[11]\nPartition 2:\n[20]\n```", "```scala\npublic interface CombinerAggregator<T> extends Serializable {\n    T init(TridentTuple tuple);\n    T combine(T val1, T val2);\n    T zero();\n}\n```", "```scala\npublic class myCount implements CombinerAggregator<Long> {\n    public Long init(TridentTuple mytuple) {\n        return 1L;\n    }\npublic Long combine(Long val1, Long val2) {\n        return val1 + val2;\n    }\n\n    public Long zero() {\n        return 0L;\n    }\n}\n```", "```scala\npublic interface ReducerAggregator<T> extends Serializable {\n    T init();\n    T reduce(T curr, TridentTuple tuple);\n}\n```", "```scala\npublic class myReducerCount implements ReducerAggregator<Long> {\n    public Long init() {\n        return 0L;\n    }\n\n    public Long reduce(Long curr, TridentTuple tuple) {\n        return curr + 1;\n    }\n}\n```", "```scala\npublic interface Aggregator<T> extends Operation {\n    T init(Object batchId, TridentCollector collector);\n    void aggregate(T state, TridentTuple tuple, TridentCollector  collector);\n    void complete(T state, TridentCollector collector);\n}\n```", "```scala\npublic class CountAggregate extends BaseAggregator<CountState> {\n    static class CountState {\n        long count = 0;\n    }\n    public CountState init(Object batchId, TridentCollector  collector) {\n        return new CountState();\n    }\n    public void aggregate(CountState state, TridentTuple tuple,  TridentCollector collector) {\n        state.count+=1;\n    }\n    public void complete(CountState state, TridentCollector  collector) {\n        collector.emit(new Values(state.count));\n    }\n}\n```", "```scala\nmyInputstream.chainedAgg()\n        .partitionAggregate(new Count(), new Fields(\"count\"))\n        .partitionAggregate(new Fields(\"b\"), new Sum(), new  Fields(\"sum\"))\n        .chainEnd()\n```", "```scala\n    myTridentTopology.merge(stream1,stream2,stream3);\n\n    ```", "```scala\nmyTridentTopology.join(stream1, new Fields(\"key\"), stream2, new  Fields(\"x\"), new Fields(\"key\", \"a\", \"b\", \"c\"));\n```", "```scala\nTridentState urlToTweeterState =  topology.newStaticState(getUrlToTweetersState());\nTridentState tweetersToFollowerState =  topology.newStaticState(getTweeterToFollowersState());\n\ntopology.newDRPCStream(\"reach\")\n       .stateQuery(urlToTweeterState, new Fields(\"args\"), new  MapGet(), new Fields(\"tweeters\"))\n       .each(new Fields(\"tweeters\"), new ExpandList(), new  Fields(\"tweeter\"))\n       .shuffle()\n       .stateQuery(tweetersToFollowerState, new Fields(\"tweeter\"),  new MapGet(), new Fields(\"followers\"))\n       .parallelismHint(200)\n       .each(new Fields(\"followers\"), new ExpandList(), new  Fields(\"follower\"))\n       .groupBy(new Fields(\"follower\"))\n       .aggregate(new One(), new Fields(\"one\"))\n       .parallelismHint(20)\n       .aggregate(new Count(), new Fields(\"reach\"));\n```"]