["```scala\n>tar xfvz 20news-bydate.tar.gz\n\n```", "```scala\n>cd 20news-bydate-train/ >ls\n\n```", "```scala\nalt.atheism                comp.windows.x          rec.sport.hockey\n  soc.religion.christian\ncomp.graphics              misc.forsale            sci.crypt\n  talk.politics.guns comp.os.ms-windows.misc    rec.autos               sci.electronics\n  talk.politics.mideast\ncomp.sys.ibm.pc.hardware   rec.motorcycles         sci.med\n  talk.politics.misc\ncomp.sys.mac.hardware      rec.sport.baseball      sci.space\n  talk.religion.misc\n\n```", "```scala\n> ls rec.sport.hockey\n52550 52580 52610 52640 53468 53550 53580 53610 53640 53670 53700 \n53731 53761 53791\n...\n\n```", "```scala\n> head -20 rec.sport.hockey/52550\nFrom: dchhabra@stpl.ists.ca (Deepak Chhabra)\nSubject: Superstars and attendance (was Teemu Selanne, was +/-\n  leaders)\nNntp-Posting-Host: stpl.ists.ca\nOrganization: Solar Terresterial Physics Laboratory, ISTS\nDistribution: na\nLines: 115\n\nDean J. Falcione (posting from jrmst+8@pitt.edu) writes:\n[I wrote:]\n\n>>When the Pens got Mario, granted there was big publicity,etc, etc,\n>>and interest was immediately generated. Gretzky did the same thing for\n>>LA.\n>>However, imnsho, neither team would have seen a marked improvement in\n>>attendance if the team record did not improve. In the year before Lemieux\n>>came, Pittsburgh finished with 38 points. Following his arrival, the Pens\n>>finished with 53, 76, 72, 81, 87, 72, 88, and 87 points, with a couple of\n ^^\n>>Stanley Cups thrown in.\n...\n\n```", "```scala\nobject TFIDFExtraction { \n\n  def main(args: Array[String]) { \n\n } \n}\n\n```", "```scala\nval sc = new SparkContext(\"local[2]\", \"First Spark App\") \n\nval path = \"../data/20news-bydate-train/*\" \nval rdd = sc.wholeTextFiles(path) \n// count the number of records in the dataset \nprintln(rdd.count)\n\n```", "```scala\n...\nINFO FileInputFormat: Total input paths to process : 11314\n...\n\n```", "```scala\n11314\n\n```", "```scala\n16/12/30 20:42:02 INFO DAGScheduler: Job 1 finished: first at \nTFIDFExtraction.scala:27, took 0.067414 s\n(file:/home/ubuntu/work/ml-resources/spark- \nml/Chapter_10/data/20news- bydate-train/alt.atheism/53186,From:  \nednclark@kraken.itc.gu.edu.au (Jeffrey Clark)\nSubject: Re: some thoughts.\nKeywords: Dan Bissell\nNntp-Posting-Host: kraken.itc.gu.edu.au\nOrganization: ITC, Griffith University, Brisbane, Australia\nLines: 70\n....\n\n```", "```scala\nval newsgroups = rdd.map { case (file, text) => \n  file.split(\"/\").takeRight(2).head } \nprintln(newsgroups.first()) \nval countByGroup = newsgroups.map(n => (n, 1)).reduceByKey(_ +\n  _).collect.sortBy(-_._2).mkString(\"n\") \nprintln(countByGroup)\n\n```", "```scala\n(rec.sport.hockey,600)\n(soc.religion.christian,599)\n(rec.motorcycles,598)\n(rec.sport.baseball,597)\n(sci.crypt,595)\n(rec.autos,594)\n(sci.med,594)\n(comp.windows.x,593)\n(sci.space,593)\n(sci.electronics,591)\n(comp.os.ms-windows.misc,591)\n(comp.sys.ibm.pc.hardware,590)\n(misc.forsale,585)\n(comp.graphics,584)\n(comp.sys.mac.hardware,578)\n(talk.politics.mideast,564)\n(talk.politics.guns,546)\n(alt.atheism,480)\n(talk.politics.misc,465)\n(talk.religion.misc,377)\n\n```", "```scala\nval text = rdd.map { case (file, text) => text } \nval whiteSpaceSplit = text.flatMap(t => t.split(\" \n  \").map(_.toLowerCase)) \nprintln(whiteSpaceSplit.distinct.count)\n\n```", "```scala\n402978\n\n```", "```scala\ndef sample( \n      withReplacement: Boolean, \n      fraction: Double, \n      seed: Long = Utils.random.nextLong): RDD[T] \n\nReturn a sampled subset of this RDD. \n@param withReplacement can elements be sampled multiple times    \n  (replaced when sampled out) \n@param fraction expected size of the sample as a fraction of this   \n  RDD's size without replacement: probability that each element is    \n  chosen; fraction must be [0, 1] with replacement: expected number   \n  of times each element is chosen; fraction must be >= 0 \n@param seed seed for the random number generator \n\n      println(nonWordSplit.distinct.sample( \n      true, 0.3, 42).take(100).mkString(\",\"))\n\n```", "```scala\natheist,resources\nsummary:,addresses,,to,atheism\nkeywords:,music,,thu,,11:57:19,11:57:19,gmt\ndistribution:,cambridge.,290\n\narchive-name:,atheism/resources\nalt-atheism-archive-  \nname:,december,,,,,,,,,,,,,,,,,,,,,,addresses,addresses,,,,,,,\nreligion,to:,to:,,p.o.,53701.\ntelephone:,sell,the,,fish,on,their,cars,,with,and,written\n\ninside.,3d,plastic,plastic,,evolution,evolution,7119,,,,,san,san,\nsan,mailing,net,who,to,atheist,press\n\naap,various,bible,,and,on.,,,one,book,is:\n\n\"the,w.p.,american,pp.,,1986.,bible,contains,ball,,based,based,\njames,of\n\n```", "```scala\nval nonWordSplit = text.flatMap(t => \n  t.split(\"\"\"W+\"\"\").map(_.toLowerCase)) \nprintln(nonWordSplit.distinct.count)\n\n```", "```scala\n130126\n\n```", "```scala\nprintln( \nnonWordSplit.distinct.sample(true, 0.3, \n  50).take(100).mkString(\",\"))\n\n```", "```scala\njejones,ml5,w1w3s1,k29p,nothin,42b,beleive,robin,believiing,749,\nsteaminess,tohc4,fzbv1u,ao,\ninstantaneous,nonmeasurable,3465,tiems,tiems,tiems,eur,3050,pgva4,\nanimating,10011100b,413,randall_clark,\nmswin,cannibal,cannibal,congresswoman,congresswoman,theoreticians,\n34ij,logically,kxo,contoler,\ncontoler,13963,13963,ets,sask,sask,sask,uninjured,930420,pws,vfj,\njesuit,kocharian,6192,1tbs,octopi,\n012537,012537,yc0,dmitriev,icbz,cj1v,bowdoin,computational,\njkis_ltd,\ncaramate,cfsmo,springer,springer,\n005117,shutdown,makewindow,nowadays,mtearle,discernible,\ndiscernible,qnh1,hindenburg,hindenburg,umaxc,\nnjn2e5,njn2e5,njn2e5,x4_i,x4_i,monger,rjs002c,rjs002c,rjs002c,\nwarms,ndallen,g45,herod,6w8rg,mqh0,suspects,\nfloor,flq1r,io21087,phoniest,funded,ncmh,c4uzus\n\n```", "```scala\nval regex = \"\"\"[^0-9]*\"\"\".r \nval filterNumbers = nonWordSplit.filter(token => \n  regex.pattern.matcher(token).matches) \nprintln(filterNumbers.distinct.count)\n\n```", "```scala\n84912\n\nprintln(filterNumbers.distinct.sample(true, 0.3,      \n50).take(100).mkString(\",\"))\n\n```", "```scala\njejones,silikian,reunion,schwabam,nothin,singen,husky,tenex,\neventuality,beleive,goofed,robin,upsets,aces,nondiscriminatory,\nunderscored,bxl,believiing,believiing,believiing,historians,\nnauseam,kielbasa,collins,noport,wargame,isv,bellevue,seetex,seetex,\nnegotiable,negotiable,viewed,rolled,unforeseen,dlr,museum,museum,\nwakaluk,wakaluk,dcbq,beekeeper,beekeeper,beekeeper,wales,mop,win,\nja_jp,relatifs,dolphin,strut,worshippers,wertheimer,jaze,jaze,\nlogically,kxo,nonnemacher,sunprops,sask,bbzx,jesuit,logos,aichi,\nremailing,remailing,winsor,dtn,astonished,butterfield,miserable,\nicbz,icbz,poking,sml,sml,makeing,deterministic,deterministic,\ndeterministic,rockefeller,rockefeller,explorers,bombardments,\nbombardments,bombardments,ray_bourque,hour,cfsmo,mishandles,\nscramblers,alchoholic,shutdown,almanac_,bruncati,karmann,hfd,\nmakewindow,perptration,mtearle\n\n```", "```scala\nval tokenCounts = filterNumbers.map(t => (t, 1)).reduceByKey(_ + \n  _) \nval oreringDesc = Ordering.by[(String, Int), Int](_._2) \nprintln(tokenCounts.top(20)(oreringDesc).mkString(\"n\"))\n\n```", "```scala\n(the,146532)\n(to,75064)\n(of,69034)\n(a,64195)\n(ax,62406)\n(and,57957)\n(i,53036)\n(in,49402)\n(is,43480)\n(that,39264)\n(it,33638)\n(for,28600)\n(you,26682)\n(from,22670)\n(s,22337)\n(edu,21321)\n(on,20493)\n(this,20121)\n(be,19285)\n(t,18728)\n\n```", "```scala\nval stopwords = Set( \n  \"the\",\"a\",\"an\",\"of\",\"or\",\"in\",\"for\",\"by\",\"on\",\"but\", \"is\", \n  \"not\", \"with\", \"as\", \"was\", \"if\", \n  \"they\", \"are\", \"this\", \"and\", \"it\", \"have\", \"from\", \"at\", \"my\",  \n  \"be\", \"that\", \"to\" \nval tokenCountsFilteredStopwords = tokenCounts.filter {  \n  case (k, v) => !stopwords.contains(k)  \n  } \n\nprintln(tokenCountsFilteredStopwords.top(20)   \n  (oreringDesc).mkString(\"n\"))\n\n```", "```scala\n(ax,62406)\n(i,53036)\n(you,26682)\n(s,22337)\n(edu,21321)\n(t,18728)\n(m,12756)\n(subject,12264)\n(com,12133)\n(lines,11835)\n(can,11355)\n(organization,11233)\n(re,10534)\n(what,9861)\n(there,9689)\n(x,9332)\n(all,9310)\n(will,9279)\n(we,9227)\n(one,9008)\n\n```", "```scala\nval tokenCountsFilteredSize =  \n  tokenCountsFilteredStopwords.filter {  \n    case (k, v) => k.size >= 2  \n  } \nprintln(tokenCountsFilteredSize.top(20)  \n  (oreringDesc).mkString(\"n\"))\n\n```", "```scala\n(ax,62406)\n(you,26682)\n(edu,21321)\n(subject,12264)\n(com,12133)\n(lines,11835)\n(can,11355)\n(organization,11233)\n(re,10534)\n(what,9861)\n(there,9689)\n(all,9310)\n(will,9279)\n(we,9227)\n(one,9008)\n(would,8905)\n(do,8674)\n(he,8441)\n(about,8336)\n(writes,7844)\n\n```", "```scala\nval oreringAsc = Ordering.by[(String, Int), Int](-_._2) \nprintln(tokenCountsFilteredSize.top(20)(oreringAsc)\n  .mkString(\"n\"))\n\n```", "```scala\n(lennips,1)\n(bluffing,1)\n(preload,1)\n(altina,1)\n(dan_jacobson,1)\n(vno,1)\n(actu,1)\n(donnalyn,1)\n(ydag,1)\n(mirosoft,1)\n(xiconfiywindow,1)\n(harger,1)\n(feh,1)\n(bankruptcies,1)\n(uncompression,1)\n(d_nibby,1)\n(bunuel,1)\n(odf,1)\n(swith,1)\n(lantastic,1)\n\n```", "```scala\nval rareTokens = tokenCounts.filter{ case (k, v) => v < 2 }.map {  \n  case (k, v) => k }.collect.toSet \nval tokenCountsFilteredAll = tokenCountsFilteredSize.filter {    \n  case (k, v) => !rareTokens.contains(k) } \nprintln(tokenCountsFilteredAll.top(20)    \n  (oreringAsc).mkString(\"n\"))\n\n```", "```scala\n(sina,2)\n(akachhy,2)\n(mvd,2)\n(hizbolah,2)\n(wendel_clark,2)\n(sarkis,2)\n(purposeful,2)\n(feagans,2)\n(wout,2)\n(uneven,2)\n(senna,2)\n(multimeters,2)\n(bushy,2)\n(subdivided,2)\n(coretest,2)\n(oww,2)\n(historicity,2)\n(mmg,2)\n(margitan,2)\n(defiance,2)\n\n```", "```scala\nprintln(tokenCountsFilteredAll.count)\n\n```", "```scala\n51801\n\n```", "```scala\ndef tokenize(line: String): Seq[String] = { \n  line.split(\"\"\"W+\"\"\") \n    .map(_.toLowerCase) \n    .filter(token => regex.pattern.matcher(token).matches) \n    .filterNot(token => stopwords.contains(token)) \n    .filterNot(token => rareTokens.contains(token)) \n    .filter(token => token.size >= 2) \n    .toSeq \n}\n\n```", "```scala\nprintln(text.flatMap(doc => tokenize(doc)).distinct.count)\n\n```", "```scala\nval tokens = text.map(doc => tokenize(doc)) \nprintln(tokens.first.take(20))\n\n```", "```scala\nWrappedArray(mathew, mantis, co, uk, subject, alt, atheism, \nfaq, atheist, resources, summary, books, addresses, music,         \nanything, related, atheism, keywords, faq)\n\n```", "```scala\nprivate[spark] def murmur3Hash(term: Any): Int = {\n  term match {\n  case null => seed\n  case b: Boolean => hashInt(if (b) 1 else 0, seed)\n  case b: Byte => hashInt(b, seed)\n  case s: Short => hashInt(s, seed)\n  case i: Int => hashInt(i, seed)\n  case l: Long => hashLong(l, seed)\n  case f: Float => hashInt(java.lang.Float\n    .floatToIntBits(f), seed)\n  case d: Double => hashLong(java.lang.Double.\n    doubleToLongBits(d), seed)\n  case s: String => val utf8 = UTF8String.fromString(s)\n    hashUnsafeBytes(utf8.getBaseObject, utf8.getBaseOffset, \n    utf8.numBytes(), seed)\n  case _ => throw new SparkException( \n  \"HashingTF with murmur3 algorithm does not \" +\n    s\"support type ${term.getClass.getCanonicalName} of input  \n  data.\")\n  }\n}\n\n```", "```scala\nimport org.apache.spark.mllib.linalg.{ SparseVector => SV } \nimport org.apache.spark.mllib.feature.HashingTF \nimport org.apache.spark.mllib.feature.IDF \nval dim = math.pow(2, 18).toInt \nval hashingTF = new HashingTF(dim) \nval tf = hashingTF.transform(tokens) \ntf.cache\n\n```", "```scala\nval v = tf.first.asInstanceOf[SV] \nprintln(v.size) \nprintln(v.values.size) \nprintln(v.values.take(10).toSeq) \nprintln(v.indices.take(10).toSeq)\n\n```", "```scala\n262144\n706\nWrappedArray(1.0, 1.0, 1.0, 1.0, 2.0, 1.0, 1.0, 2.0, 1.0, 1.0)\nWrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115,     \n3166)\n\n```", "```scala\nval idf = new IDF().fit(tf) \nval tfidf = idf.transform(tf) \nval v2 = tfidf.first.asInstanceOf[SV] \nprintln(v2.values.size) \nprintln(v2.values.take(10).toSeq) \nprintln(v2.indices.take(10).toSeq)\n\n```", "```scala\n706\nWrappedArray(2.3869085659322193, 4.670445463955571, \n6.561295835827856, 4.597686109673142,  ...\nWrappedArray(313, 713, 871, 1202, 1203, 1209, 1795, 1862, 3115,     \n3166)\n\n```", "```scala\nval idf = new IDF().fit(tf) \nval tfidf = idf.transform(tf)\n\n```", "```scala\nval minMaxVals = tfidf.map { v => \n  val sv = v.asInstanceOf[SV] \n  (sv.values.min, sv.values.max) \n} \nval globalMinMax = minMaxVals.reduce { case ((min1, max1), \n  (min2, max2)) => \n  (math.min(min1, min2), math.max(max1, max2)) \n} \nprintln(globalMinMax)\n\n```", "```scala\n(0.0,66155.39470409753)\n\n```", "```scala\nval common = sc.parallelize(Seq(Seq(\"you\", \"do\", \"we\"))) \nval tfCommon = hashingTF.transform(common) \nval tfidfCommon = idf.transform(tfCommon) \nval commonVector = tfidfCommon.first.asInstanceOf[SV] \nprintln(commonVector.values.toSeq)\n\n```", "```scala\nWrappedArray(0.9965359935704624, 1.3348773448236835, \n0.5457486182039175)\n\n```", "```scala\nval uncommon = sc.parallelize(Seq(Seq(\"telescope\", \n  \"legislation\", \"investment\"))) \nval tfUncommon = hashingTF.transform(uncommon) \nval tfidfUncommon = idf.transform(tfUncommon) \nval uncommonVector = tfidfUncommon.first.asInstanceOf[SV] \nprintln(uncommonVector.values.toSeq)\n\n```", "```scala\nWrappedArray(5.3265513728351666, 5.308532867332488, \n5.483736956357579)\n\n```", "```scala\nval hockeyText = rdd.filter { case (file, text) => \n  file.contains(\"hockey\") } \nval hockeyTF = hockeyText.mapValues(doc => \n  hashingTF.transform(tokenize(doc))) \nval hockeyTfIdf = idf.transform(hockeyTF.map(_._2))\n\n```", "```scala\nimport breeze.linalg._ \nval hockey1 = hockeyTfIdf.sample( \n  true, 0.1, 42).first.asInstanceOf[SV] \nval breeze1 = new SparseVector(hockey1.indices,\n  hockey1.values, hockey1.size) \nval hockey2 = hockeyTfIdf.sample(true, 0.1, \n  43).first.asInstanceOf[SV] \nval breeze2 = new SparseVector(hockey2.indices,\n  hockey2.values, hockey2.size) \nval cosineSim = breeze1.dot(breeze2) / \n  (norm(breeze1) * norm(breeze2)) \nprintln(cosineSim)\n\n```", "```scala\n0.06700095047242809\n\n```", "```scala\nval graphicsText = rdd.filter { case (file, text) => \n  file.contains(\"comp.graphics\") } \nval graphicsTF = graphicsText.mapValues(doc => \n  hashingTF.transform(tokenize(doc))) \nval graphicsTfIdf = idf.transform(graphicsTF.map(_._2)) \nval graphics = graphicsTfIdf.sample(true, 0.1, \n  42).first.asInstanceOf[SV] \nval breezeGraphics = new SparseVector(graphics.indices, \n  graphics.values, graphics.size) \nval cosineSim2 = breeze1.dot(breezeGraphics) / (norm(breeze1) * \n  norm(breezeGraphics)) \nprintln(cosineSim2)\n\n```", "```scala\n0.001950124251275256\n\n```", "```scala\n// compare to sport.baseball topic \nval baseballText = rdd.filter { case (file, text) => \n  file.contains(\"baseball\") } \nval baseballTF = baseballText.mapValues(doc => \n  hashingTF.transform(tokenize(doc))) \nval baseballTfIdf = idf.transform(baseballTF.map(_._2)) \nval baseball = baseballTfIdf.sample(true, 0.1, \n  42).first.asInstanceOf[SV] \nval breezeBaseball = new SparseVector(baseball.indices, \n  baseball.values, baseball.size) \nval cosineSim3 = breeze1.dot(breezeBaseball) / (norm(breeze1) * \n   norm(breezeBaseball)) \nprintln(cosineSim3)\n\n```", "```scala\n0.05047395039466008\n\n```", "```scala\nimport org.apache.spark.mllib.regression.LabeledPoint \nimport org.apache.spark.mllib.classification.NaiveBayes \nimport org.apache.spark.mllib.evaluation.MulticlassMetrics.\n\n```", "```scala\nobject DocumentClassification { \n\n  def main(args: Array[String]) { \n    val sc = new SparkContext(\"local[2]\", \"\") \n    ... \n}\n\n```", "```scala\nval newsgroupsMap = \n  newsgroups.distinct.collect().zipWithIndex.toMap \nval zipped = newsgroups.zip(tfidf) \nval train = zipped.map { case (topic, vector) => \n  LabeledPoint(newsgroupsMap(topic), vector) } \ntrain.cache\n\n```", "```scala\nval model = NaiveBayes.train(train, lambda = 0.1)\n\n```", "```scala\nval testPath = \"/PATH/20news-bydate-test/*\" \nval testRDD = sc.wholeTextFiles(testPath) \nval testLabels = testRDD.map { case (file, text) => \n  val topic = file.split(\"/\").takeRight(2).head \n  newsgroupsMap(topic) \n}\n\n```", "```scala\nval testTf = testRDD.map { case (file, text) => \n  hashingTF.transform(tokenize(text)) } \nval testTfIdf = idf.transform(testTf) \nval zippedTest = testLabels.zip(testTfIdf) \nval test = zippedTest.map { case (topic, vector) => \n  LabeledPoint(topic, vector) }\n\n```", "```scala\nval predictionAndLabel = test.map(p =>       \n  (model.predict(p.features),   p.label)) \nval accuracy = 1.0 * predictionAndLabel.filter\n  (x => x._1 == x._2).count() / test.count() \nval metrics = new MulticlassMetrics(predictionAndLabel) \nprintln(accuracy) \nprintln(metrics.weightedFMeasure)\n\n```", "```scala\n0.7928836962294211\n0.7822644376431702\n\n```", "```scala\nval rawTokens = rdd.map { case (file, text) => text.split(\" \") } \nval rawTF = texrawTokenst.map(doc => hashingTF.transform(doc)) \nval rawTrain = newsgroups.zip(rawTF).map { case (topic, vector)  \n  => LabeledPoint(newsgroupsMap(topic), vector) } \nval rawModel = NaiveBayes.train(rawTrain, lambda = 0.1) \nval rawTestTF = testRDD.map { case (file, text) => \n  hashingTF.transform(text.split(\" \")) } \nval rawZippedTest = testLabels.zip(rawTestTF) \nval rawTest = rawZippedTest.map { case (topic, vector) => \n  LabeledPoint(topic, vector) } \nval rawPredictionAndLabel = rawTest.map(p => \n  (rawModel.predict(p.features), p.label)) \nval rawAccuracy = 1.0 * rawPredictionAndLabel.filter(x => x._1 \n  == x._2).count() / rawTest.count() \nprintln(rawAccuracy) \nval rawMetrics = new MulticlassMetrics(rawPredictionAndLabel) \nprintln(rawMetrics.weightedFMeasure)\n\n```", "```scala\n0.7661975570897503\n0.7653320418573546\n\n```", "```scala\npackage org.apache.spark.examples.ml \n\nimport org.apache.spark.SparkConf \nimport org.apache.spark.ml.classification.NaiveBayes \nimport        \n\norg.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \n\nimport org.apache.spark.sql.SparkSession \n\nobject DocumentClassificationLibSVM { \n  def main(args: Array[String]): Unit = { \n\n  } \n}\n\n```", "```scala\nval spConfig = (new SparkConf).setMaster(\"local\")\n  .setAppName(\"SparkApp\") \nval spark = SparkSession \n  .builder() \n  .appName(\"SparkRatingData\").config(spConfig) \n  .getOrCreate() \n\nval data = spark.read.format(\"libsvm\").load(\"./output/20news-by-\n  date-train-libsvm/part-combined\") \n\nval Array(trainingData, testData) = data.randomSplit(Array(0.7,\n  0.3), seed = 1L)\n\n```", "```scala\nval model = new NaiveBayes().fit(trainingData) \nval predictions = model.transform(testData) \npredictions.show()\n\n```", "```scala\n+----+-------------------+--------------------+-----------------+----------+\n|label|     features     |    rawPrediction   |   probability   |prediction|\n+-----+------------------+--------------------+-----------------+----------+\n|0.0|(262141,[14,63,64...|[-8972.9535882773...|[1.0,0.0,1.009147...| 0.0|\n|0.0|(262141,[14,329,6...|[-5078.5468878602...|[1.0,0.0,0.0,0.0,...| 0.0|\n|0.0|(262141,[14,448,5...|[-3376.8302696656...|[1.0,0.0,2.138643...| 0.0|\n|0.0|(262141,[14,448,5...|[-3574.2782864683...|[1.0,2.8958758424...| 0.0|\n|0.0|(262141,[14,535,3...|[-5001.8808481928...|[8.85311976855360...| 12.0|\n|0.0|(262141,[14,573,8...|[-5950.1635030844...|[1.0,0.0,1.757049...| 0.0|\n|0.0|(262141,[14,836,5...|[-8795.2012408412...|[1.0,0.0,0.0,0.0,...| 0.0|\n|0.0|(262141,[14,991,2...|[-1892.8829282793...|[0.99999999999999...| 0.0|\n|0.0|(262141,[14,1176,...|[-4746.2275710890...|[1.0,5.8201E-319,...| 0.0|\n|0.0|(262141,[14,1379,...|[-7104.8373572933...|[1.0,8.9577444139...| 0.0|\n|0.0|(262141,[14,1582,...|[-5473.6206675848...|[1.0,5.3185120345...| 0.0|\n|0.0|(262141,[14,1836,...|[-11289.582479676...|[1.0,0.0,0.0,0.0,...| 0.0|\n|0.0|(262141,[14,2325,...|[-3957.9187837274...|[1.0,2.1880375223...| 0.0|\n|0.0|(262141,[14,2325,...|[-7131.2028421844...|[1.0,2.6110663778...| 0.0|\n|0.0|(262141,[14,3033,...|[-3014.6430319605...|[1.0,2.6341580467...| 0.0|\n|0.0|(262141,[14,4335,...|[-8283.7207917560...|[1.0,8.9559011053...| 0.0|\n|0.0|(262141,[14,5173,...|[-6811.3466537480...|[1.0,7.2593916980...| 0.0|\n|0.0|(262141,[14,5232,...|[-2752.8846541292...|[1.0,1.8619374091...| 0.0|\n|0.0|(262141,[15,5173,...|[-8741.7756643949...|[1.0,0.0,2.606005...| 0.0|\n|0.0|(262141,[168,170,...|[-41636.025208445...|[1.0,0.0,0.0,0.0,...| 0.0|\n+----+--------------------+-------------------+-------------------+--------+\n\n```", "```scala\nval accuracy = evaluator.evaluate(predictions) \nprintln(\"Test set accuracy = \" + accuracy) \nspark.stop()\n\n```", "```scala\nTest set accuracy = 0.8768458357944477\nAccuracy is better as the Naive Bayes implementation has improved \nfrom Spark 1.6 to Spark 2.0\n\n```", "```scala\nobject Word2VecMllib {\n  def main(args: Array[String]) {\n  val sc = new SparkContext(\"local[2]\", \"Word2Vector App\")\n  val path = \"./data/20news-bydate-train/alt.atheism/*\"\n  val rdd = sc.wholeTextFiles(path)\n  val text = rdd.map { case (file, text) => text }\n  val newsgroups = rdd.map { case (file, text) =>             \n    file.split(\"/\").takeRight(2).head }\n  val newsgroupsMap =       \n    newsgroups.distinct.collect().zipWithIndex.toMap\n  val dim = math.pow(2, 18).toInt\n  var tokens = text.map(doc => TFIDFExtraction.tokenize(doc))\n  import org.apache.spark.mllib.feature.Word2Vec\n  val word2vec = new Word2Vec()\n  val word2vecModel = word2vec.fit(tokens)\n    word2vecModel.findSynonyms(\"philosophers\", 5).foreach(println)\n  sc.stop()\n  }\n}\n\n```", "```scala\nimport org.apache.spark.SparkContext\nimport org.apache.spark.mllib.linalg.{SparseVector => SV}\nobject Word2VecMllib {\n  def main(args: Array[String]) {\n  }\n}\n\n```", "```scala\nval sc = new SparkContext(\"local[2]\", \"Word2Vector App\")\nval path = \"./data/20news-bydate-train/alt.atheism/*\"\nval rdd = sc.wholeTextFiles(path)\nval text = rdd.map { case (file, text) => text }\nval newsgroups = rdd.map { case (file, text) =>\n  file.split(\"/\").takeRight(2).head }\nval newsgroupsMap =      \n  newsgroups.distinct.collect().zipWithIndex.toMap\nval dim = math.pow(2, 18).toInt\n  var tokens = text.map(doc => TFIDFExtraction.tokenize(doc))\n\n```", "```scala\nimport org.apache.spark.mllib.feature.Word2Vec\n val word2vec = new Word2Vec()\n\n```", "```scala\nval word2vecModel = word2vec.fit(tokens)\n\n```", "```scala\nword2vecModel.findSynonyms(philosophers\", 5).foreach(println)\nsc.stop()\n\n```", "```scala\n(year,0.8417112940969042) (motivations,0.833017707021745) (solution,0.8284719617235932) (whereas,0.8242997325042509) (formed,0.8042383351975712)\n\n```", "```scala\nval spConfig = (new SparkConf).setMaster(\"local\").setAppName(\"SparkApp\")\nval spark = SparkSession\n  .builder\n  .appName(\"Word2Vec Sample\").config(spConfig)\n  .getOrCreate()\nimport spark.implicits._\nval rawDF = spark.sparkContext\n  .wholeTextFiles(\"./data/20news-bydate-train/alt.atheism/*\")\n  val temp = rawDF.map( x => {\n    (x._2.filter(_ >= ' ').filter(! _.toString.startsWith(\"(\")) )\n    })\n  val textDF = temp.map(x => x.split(\" \")).map(Tuple1.apply)\n    .toDF(\"text\")\n\n```", "```scala\nval word2Vec = new Word2Vec()\n  .setInputCol(\"text\")\n  .setOutputCol(\"result\")\n  .setVectorSize(3)\n  .setMinCount(0)\nval model = word2Vec.fit(textDF)\nval result = model.transform(textDF)\n  result.select(\"result\").take(3).foreach(println)\n)\n\n```", "```scala\nval ds = model.findSynonyms(\"philosophers\", 5).select(\"word\")\n  ds.rdd.saveAsTextFile(\"./output/philiosphers-synonyms\" +             System.nanoTime())\n  ds.show(\n\n```", "```scala\n +--------------+ | word         | +--------------+ | Fess         | | guide        | |validinference| | problems.    | | paperback    | +--------------+\n\n```"]