["```scala\nScala  \ndef squaredError(actual:Double, pred : Double) : Double = { \n  return Math.pow( (pred - actual), 2.0) \n} \n\n```", "```scala\nScala \ndef absError(actual:Double, pred: Double) : Double = { \n  return Math.abs( (pred - actual)) \n} \n\n```", "```scala\nScala \ndef squaredLogError(actual:Double, pred : Double) : Double = { \n  return Math.pow( (Math.log(pred +1) - Math.log(actual +1)), 2.0) \n} \n\n```", "```scala\nval spark = SparkSession \n  .builder \n  .appName(\"BikeSharing\") \n  .master(\"local[1]\") \n  .getOrCreate() \n\n// read from csv \nval df = spark.read.format(\"csv\").option(\"header\", \n   \"true\").load(\"/dataset/BikeSharing/hour.csv\") \ndf.cache() \n\ndf.registerTempTable(\"BikeSharing\") \nprint(df.count()) \n\nspark.sql(\"SELECT * FROM BikeSharing\").show() \n\n```", "```scala\n root\n |-- instant: integer (nullable = true)\n |-- dteday: timestamp (nullable = true)\n |-- season: integer (nullable = true)\n |-- yr: integer (nullable = true)\n |-- mnth: integer (nullable = true)\n |-- hr: integer (nullable = true)\n |-- holiday: integer (nullable = true)\n |-- weekday: integer (nullable = true)\n |-- workingday: integer (nullable = true)\n |-- weathersit: integer (nullable = true)\n |-- temp: double (nullable = true)\n |-- atemp: double (nullable = true)\n |-- hum: double (nullable = true)\n |-- windspeed: double (nullable = true)\n |-- casual: integer (nullable = true)\n |-- registered: integer (nullable = true)\n |-- cnt: integer (nullable = true)\n\n```", "```scala\nprint(df.count()) \n\n```", "```scala\n    17,379\n\n```", "```scala\n// drop record id, date, casual and registered columns \nval df1 = \n   df.drop(\"instant\").drop(\"dteday\").drop(\"casual\")\n   .drop(\"registered\") \ndf1.printSchema() \n\n```", "```scala\n root\n |-- season: integer (nullable = true)\n |-- yr: integer (nullable = true)\n |-- mnth: integer (nullable = true)\n |-- hr: integer (nullable = true)\n |-- holiday: integer (nullable = true)\n |-- weekday: integer (nullable = true)\n |-- workingday: integer (nullable = true)\n |-- weathersit: integer (nullable = true)\n |-- temp: double (nullable = true)\n |-- atemp: double (nullable = true)\n |-- hum: double (nullable = true)\n |-- windspeed: double (nullable = true)\n |-- cnt: integer (nullable = true)\n\n```", "```scala\n// convert to double: season,yr,mnth,hr,holiday,weekday,workingday,weathersit,temp,atemp,hum,windspeed,casual,registered,cnt \nval df2 = df1.withColumn(\"season\", \n   df1(\"season\").cast(\"double\")).withColumn(\"yr\", \n   df1(\"yr\").cast(\"double\")) \n  .withColumn(\"mnth\", df1(\"mnth\").cast(\"double\")).withColumn(\"hr\", \n     df1(\"hr\").cast(\"double\")).withColumn(\"holiday\", \n     df1(\"holiday\").cast(\"double\")) \n  .withColumn(\"weekday\", \n     df1(\"weekday\").cast(\"double\")).withColumn(\"workingday\", \n     df1(\"workingday\").cast(\"double\")).withColumn(\"weathersit\", \n     df1(\"weathersit\").cast(\"double\")) \n  .withColumn(\"temp\", \n     df1(\"temp\").cast(\"double\")).withColumn(\"atemp\", \n     df1(\"atemp\").cast(\"double\")).withColumn(\"hum\", \n     df1(\"hum\").cast(\"double\")) \n  .withColumn(\"windspeed\", \n     df1(\"windspeed\").cast(\"double\")).withColumn(\"label\", \n     df1(\"label\").cast(\"double\")) \n\ndf2.printSchema() \n\n```", "```scala\n root\n |-- season: double (nullable = true)\n |-- yr: double (nullable = true)\n |-- mnth: double (nullable = true)\n |-- hr: double (nullable = true)\n |-- holiday: double (nullable = true)\n |-- weekday: double (nullable = true)\n |-- workingday: double (nullable = true)\n |-- weathersit: double (nullable = true)\n |-- temp: double (nullable = true)\n |-- atemp: double (nullable = true)\n |-- hum: double (nullable = true)\n |-- windspeed: double (nullable = true)\n |-- label: double (nullable = true)\n\n```", "```scala\n// drop label and create feature vector \nval df3 = df2.drop(\"label\") \nval featureCols = df3.columns \n\nval vectorAssembler = new \n   VectorAssembler().setInputCols(featureCols)\n   .setOutputCol(\"rawFeatures\") \nval vectorIndexer = new \n   VectorIndexer().setInputCol(\"rawFeatures\")\n   .setOutputCol(\"features\").setMaxCategories(4) \n\n```", "```scala\ndef executeCommand(arg: String, vectorAssembler: VectorAssembler, \n   vectorIndexer: VectorIndexer, dataFrame: DataFrame, spark: \n   SparkSession) = arg match { \n    case \"LR_Vectors\" => \n     LinearRegressionPipeline.linearRegressionWithVectorFormat\n     (vectorAssembler, vectorIndexer, dataFrame) \n    case \"LR_SVM\" => \n     LinearRegressionPipeline.linearRegressionWithSVMFormat(spark) \n\n    case \"GLR_Vectors\" => \n     GeneralizedLinearRegressionPipeline\n     .genLinearRegressionWithVectorFormat(vectorAssembler, \n      vectorIndexer, dataFrame) \n    case \"GLR_SVM\"=> \n     GeneralizedLinearRegressionPipeline\n     .genLinearRegressionWithSVMFormat(spark) \n\n    case \"DT_Vectors\" => DecisionTreeRegressionPipeline\n     .decTreeRegressionWithVectorFormat(vectorAssembler, \n     vectorIndexer, dataFrame) \n    case \"DT_SVM\"=> \n     GeneralizedLinearRegressionPipeline\n     .genLinearRegressionWithSVMFormat(spark) \n\n    case \"RF_Vectors\" => \n     RandomForestRegressionPipeline\n     .randForestRegressionWithVectorFormat(vectorAssembler, \n     vectorIndexer, dataFrame) \n    case \"RF_SVM\"=> \n     RandomForestRegressionPipeline\n     .randForestRegressionWithSVMFormat(spark) \n\n    case \"GBT_Vectors\" => \n     GradientBoostedTreeRegressorPipeline\n     .gbtRegressionWithVectorFormat(vectorAssembler, vectorIndexer, \n     dataFrame) \n    case \"GBT_SVM\"=> \n     GradientBoostedTreeRegressorPipeline\n     .gbtRegressionWithSVMFormat(spark) \n\n} \n\n```", "```scala\ndef linearRegressionWithVectorFormat(vectorAssembler: \n   VectorAssembler, vectorIndexer: VectorIndexer, dataFrame: \n   DataFrame) = { \n  val lr = new LinearRegression() \n    .setFeaturesCol(\"features\") \n    .setLabelCol(\"label\") \n    .setRegParam(0.1) \n    .setElasticNetParam(1.0) \n    .setMaxIter(10) \n\n  val pipeline = new Pipeline().setStages(Array(vectorAssembler, \n   vectorIndexer, lr)) \n\n  val Array(training, test) = dataFrame.randomSplit(Array(0.8, \n   0.2), seed = 12345) \n\n  val model = pipeline.fit(training) \n\n  val fullPredictions = model.transform(test).cache() \n  val predictions = \n   fullPredictions.select(\"prediction\").rdd.map(_.getDouble(0)) \n  val labels = \n   fullPredictions.select(\"label\").rdd.map(_.getDouble(0)) \n  val RMSE = new \n   RegressionMetrics(predictions.zip(labels)).rootMeanSquaredError \n  println(s\"  Root mean squared error (RMSE): $RMSE\") \n} \n\ndef linearRegressionWithSVMFormat(spark: SparkSession) = { \n  // Load training data \n  val training = spark.read.format(\"libsvm\") \n    .load(\"/dataset/BikeSharing/lsvmHours.txt\") \n\n  val lr = new LinearRegression() \n    .setMaxIter(10) \n    .setRegParam(0.3) \n    .setElasticNetParam(0.8) \n\n  // Fit the model \n  val lrModel = lr.fit(training) \n\n  // Print the coefficients and intercept for linear regression \n  println(s\"Coefficients: ${lrModel.coefficients} Intercept: \n   ${lrModel.intercept}\") \n\n  // Summarize the model over the training set and print out some \n   metrics \n  val trainingSummary = lrModel.summary \n  println(s\"numIterations: ${trainingSummary.totalIterations}\") \n  println(s\"objectiveHistory: \n   ${trainingSummary.objectiveHistory.toList}\") \n  trainingSummary.residuals.show() \n  println(s\"RMSE: ${trainingSummary.rootMeanSquaredError}\") \n  println(s\"r2: ${trainingSummary.r2}\") \n} \n\n```", "```scala\n+-------------------+\n|          residuals|\n+-------------------+\n|  32.92325797801143|\n|  59.97614044359903|\n|  35.80737062786482|\n|-12.509886468051075|\n|-25.979774633117792|\n|-29.352862474201224|\n|-5.9517346926691435|\n| 18.453701019500947|\n|-24.859327293384787|\n| -47.14282080103287|\n| -27.50652100848832|\n| 21.865309097336535|\n|  4.037722798853395|\n|-25.691348213368343|\n| -13.59830538387368|\n|  9.336691727080336|\n|  12.83461983259582|\n|  -20.5026155752185|\n| -34.83240621318937|\n| -34.30229437825615|\n+-------------------+\nonly showing top 20 rows\nRMSE: 149.54567868651284\nr2: 0.3202369690447968\n\n```", "```scala\n@transient lazy val logger = Logger.getLogger(getClass.getName) \n\ndef genLinearRegressionWithVectorFormat(vectorAssembler: \n   VectorAssembler, vectorIndexer: VectorIndexer, dataFrame: \n   DataFrame) = { \n   val lr = new GeneralizedLinearRegression() \n    .setFeaturesCol(\"features\") \n    .setLabelCol(\"label\") \n    .setFamily(\"gaussian\") \n    .setLink(\"identity\") \n    .setMaxIter(10) \n    .setRegParam(0.3) \n\n  val pipeline = new Pipeline().setStages(Array(vectorAssembler, \n   vectorIndexer, lr)) \n\n  val Array(training, test) = dataFrame.randomSplit(Array(0.8, \n   0.2), seed = 12345) \n\n  val model = pipeline.fit(training) \n\n  val fullPredictions = model.transform(test).cache() \n  val predictions = \n   fullPredictions.select(\"prediction\").rdd.map(_.getDouble(0)) \n  val labels = \n   fullPredictions.select(\"label\").rdd.map(_.getDouble(0)) \n  val RMSE = new \n   RegressionMetrics(predictions.zip(labels)).rootMeanSquaredError \n  println(s\"  Root mean squared error (RMSE): $RMSE\") \n} \n\ndef genLinearRegressionWithSVMFormat(spark: SparkSession) = { \n  // Load training data \n  val training = spark.read.format(\"libsvm\") \n    .load(\"/dataset/BikeSharing/lsvmHours.txt\") \n\n  val lr = new GeneralizedLinearRegression() \n    .setFamily(\"gaussian\") \n    .setLink(\"identity\") \n    .setMaxIter(10) \n    .setRegParam(0.3) \n\n  // Fit the model \n  val model = lr.fit(training) \n\n  // Print the coefficients and intercept for generalized linear \n   regression model \n  println(s\"Coefficients: ${model.coefficients}\") \n  println(s\"Intercept: ${model.intercept}\") \n\n  // Summarize the model over the training set and print out some \n   metrics \n  val summary = model.summary \n  println(s\"Coefficient Standard Errors: \n   ${summary.coefficientStandardErrors.mkString(\",\")}\") \n  println(s\"T Values: ${summary.tValues.mkString(\",\")}\") \n  println(s\"P Values: ${summary.pValues.mkString(\",\")}\") \n  println(s\"Dispersion: ${summary.dispersion}\") \n  println(s\"Null Deviance: ${summary.nullDeviance}\") \n  println(s\"Residual Degree Of Freedom Null: \n   ${summary.residualDegreeOfFreedomNull}\") \n  println(s\"Deviance: ${summary.deviance}\") \n  println(s\"Residual Degree Of Freedom: \n   ${summary.residualDegreeOfFreedom}\") \n  println(s\"AIC: ${summary.aic}\") \n  println(\"Deviance Residuals: \") \n  summary.residuals().show()   \n} \n\n```", "```scala\n1.1353970394903834,2.2827202289405677,0.5060828045490352,0.1735367945\n   7103457,7.062338310890969,0.5694233355369813,2.5250738792716176,\n2.0099641224706573,0.7596421898012983,0.6228803024758551,0.0735818071\n   8894239,0.30550603737503224,12.369537640641184\n\n```", "```scala\nT Values: 15.186791802016964,33.26578339676457,-\n   11.27632316133038,8.658129103690262,-\n   3.8034120518318013,2.6451862430890807,0.9799958329796699,\n3.731755243874297,4.957582264860384,6.02053185645345,-\n   39.290272209592864,5.5283417898112726,-0.7966500413552742\n\n```", "```scala\nP Values: 0.0,0.0,0.0,0.0,1.4320532622846827E-\n   4,0.008171946193283652,0.3271018275330657,1.907562616410008E-\n   4,7.204877614519489E-7,\n1.773422964035376E-9,0.0,3.2792739856901676E-8,0.42566519676340153\n\n```", "```scala\nDispersion: 22378.414478769333\n\n```", "```scala\nNull Deviance: 5.717615910707208E8\n\n```", "```scala\nResidual Degree Of Freedom Null: 17378\n\n```", "```scala\nResidual Degree Of Freedom: 17366\n\n```", "```scala\nAIC: 223399.95490762248\n+-------------------+\n|  devianceResiduals|\n+-------------------+\n| 32.385412453563546|\n|   59.5079185994115|\n|  34.98037491140896|\n|-13.503450469022432|\n|-27.005954440659032|\n|-30.197952952158246|\n| -7.039656861683778|\n| 17.320193923055445|\n|  -26.0159703272054|\n| -48.69166247116218|\n| -29.50984967584955|\n| 20.520222192742004|\n| 1.6551311183207815|\n|-28.524373674665213|\n|-16.337935852841838|\n|  6.441923904310045|\n|   9.91072545492193|\n|-23.418896074866524|\n|-37.870797650696346|\n|-37.373301622332946|\n+-------------------+\nonly showing top 20 rows\n\n```", "```scala\n@transient lazy val logger = Logger.getLogger(getClass.getName) \n\ndef decTreeRegressionWithVectorFormat(vectorAssembler: \n   VectorAssembler, vectorIndexer: VectorIndexer, dataFrame: \n   DataFrame) = { \n  val lr = new DecisionTreeRegressor() \n    .setFeaturesCol(\"features\") \n    .setLabelCol(\"label\") \n\n  val pipeline = new Pipeline().setStages(Array(vectorAssembler, \n   vectorIndexer, lr)) \n\n  val Array(training, test) = dataFrame.randomSplit(Array(0.8, \n   0.2), seed = 12345) \n\n  val model = pipeline.fit(training) \n\n  // Make predictions. \n  val predictions = model.transform(test) \n\n  // Select example rows to display. \n  predictions.select(\"prediction\", \"label\", \"features\").show(5) \n\n  // Select (prediction, true label) and compute test error. \n  val evaluator = new RegressionEvaluator() \n    .setLabelCol(\"label\") \n    .setPredictionCol(\"prediction\") \n    .setMetricName(\"rmse\") \n  val rmse = evaluator.evaluate(predictions) \n  println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse) \n\n  val treeModel = \n   model.stages(1).asInstanceOf[DecisionTreeRegressionModel] \n  println(\"Learned regression tree model:\\n\" + \n   treeModel.toDebugString)  } \n\ndef decTreeRegressionWithSVMFormat(spark: SparkSession) = { \n  // Load training data \n  val training = spark.read.format(\"libsvm\") \n    .load(\"/dataset/BikeSharing/lsvmHours.txt\") \n\n  // Automatically identify categorical features, and index them. \n  // Here, we treat features with > 4 distinct values as \n   continuous. \n  val featureIndexer = new VectorIndexer() \n    .setInputCol(\"features\") \n    .setOutputCol(\"indexedFeatures\") \n    .setMaxCategories(4) \n    .fit(training) \n\n  // Split the data into training and test sets (30% held out for \n   testing). \n  val Array(trainingData, testData) = \n   training.randomSplit(Array(0.7, 0.3)) \n\n  // Train a DecisionTree model. \n  val dt = new DecisionTreeRegressor() \n    .setLabelCol(\"label\") \n    .setFeaturesCol(\"indexedFeatures\") \n\n  // Chain indexer and tree in a Pipeline. \n  val pipeline = new Pipeline() \n    .setStages(Array(featureIndexer, dt)) \n\n  // Train model. This also runs the indexer. \n  val model = pipeline.fit(trainingData) \n\n  // Make predictions. \n  val predictions = model.transform(testData) \n\n  // Select example rows to display. \n  predictions.select(\"prediction\", \"label\", \"features\").show(5) \n\n  // Select (prediction, true label) and compute test error. \n  val evaluator = new RegressionEvaluator() \n    .setLabelCol(\"label\") \n    .setPredictionCol(\"prediction\") \n    .setMetricName(\"rmse\") \n  val rmse = evaluator.evaluate(predictions) \n  println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse) \n\n  val treeModel = \n   model.stages(1).asInstanceOf[DecisionTreeRegressionModel] \n  println(\"Learned regression tree model:\\n\" + \n   treeModel.toDebugString) \n} \n\n```", "```scala\nCoefficients: [17.243038451366886,75.93647669134975,-5.7067532504873215,1.5025039716365927,-26.86098264575616,1.5062307736563205,2.4745618796519953,7.500694154029075,3.7659886477986215,3.7500707038132464,-2.8910492341273235,1.6889417934600353]\nIntercept: -9.85419267296242\n\nCoefficient Standard Errors: 1.1353970394903834,2.2827202289405677,0.5060828045490352,0.17353679457103457,7.062338310890969,0.5694233355369813,2.5250738792716176,2.0099641224706573,0.7596421898012983,0.6228803024758551,0.07358180718894239,0.30550603737503224,12.369537640641184\nT Values: 15.186791802016964,33.26578339676457,-11.27632316133038,8.658129103690262,-3.8034120518318013,2.6451862430890807,0.9799958329796699,3.731755243874297,4.957582264860384,6.02053185645345,-39.290272209592864,5.5283417898112726,-0.7966500413552742\nP Values: 0.0,0.0,0.0,0.0,1.4320532622846827E-4,0.008171946193283652,0.3271018275330657,1.907562616410008E-4,7.204877614519489E-7,1.773422964035376E-9,0.0,3.2792739856901676E-8,0.42566519676340153\nDispersion: 22378.414478769333\n\nNull Deviance: 5.717615910707208E8\nResidual Degree Of Freedom Null: 17378\nDeviance: 3.886235458383082E8\nResidual Degree Of Freedom: 17366\n\nAIC: 223399.95490762248\nDeviance Residuals:\n+-------------------+\n|  devianceResiduals|\n+-------------------+\n| 32.385412453563546|\n|   59.5079185994115|\n|  34.98037491140896|\n|-13.503450469022432|\n|-27.005954440659032|\n|-30.197952952158246|\n| -7.039656861683778|\n| 17.320193923055445|\n|  -26.0159703272054|\n| -48.69166247116218|\n| -29.50984967584955|\n| 20.520222192742004|\n| 1.6551311183207815|\n|-28.524373674665213|\n|-16.337935852841838|\n|  6.441923904310045|\n|   9.91072545492193|\n|-23.418896074866524|\n|-37.870797650696346|\n|-37.373301622332946|\n+-------------------+\nonly showing top 20 rows\n\n```", "```scala\n@transient lazy val logger = Logger.getLogger(getClass.getName) \n\ndef randForestRegressionWithVectorFormat(vectorAssembler: \n  VectorAssembler, vectorIndexer: VectorIndexer, dataFrame: \n   DataFrame) = { \n   val lr = new RandomForestRegressor() \n    .setFeaturesCol(\"features\") \n    .setLabelCol(\"label\") \n\n  val pipeline = new Pipeline().setStages(Array(vectorAssembler, \n   vectorIndexer, lr)) \n\n  val Array(training, test) = dataFrame.randomSplit(Array(0.8, \n   0.2), seed = 12345) \n\n  val model = pipeline.fit(training) \n\n  // Make predictions. \n  val predictions = model.transform(test) \n\n  // Select example rows to display. \n  predictions.select(\"prediction\", \"label\", \"features\").show(5) \n\n  // Select (prediction, true label) and compute test error. \n  val evaluator = new RegressionEvaluator() \n    .setLabelCol(\"label\") \n    .setPredictionCol(\"prediction\") \n    .setMetricName(\"rmse\") \n  val rmse = evaluator.evaluate(predictions) \n  println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse) \n\n  val treeModel = \n   model.stages(1).asInstanceOf[RandomForestRegressionModel] \n  println(\"Learned regression tree model:\\n\" + treeModel.toDebugString)  } \n\ndef randForestRegressionWithSVMFormat(spark: SparkSession) = { \n  // Load training data \n  val training = spark.read.format(\"libsvm\") \n    .load(\"/dataset/BikeSharing/lsvmHours.txt\") \n\n  // Automatically identify categorical features, and index them. \n  // Set maxCategories so features with > 4 distinct values are \n   treated as continuous. \n  val featureIndexer = new VectorIndexer() \n    .setInputCol(\"features\") \n    .setOutputCol(\"indexedFeatures\") \n    .setMaxCategories(4) \n    .fit(training) \n\n  // Split the data into training and test sets (30% held out for \n   testing). \n  val Array(trainingData, testData) = \n   training.randomSplit(Array(0.7, 0.3)) \n\n  // Train a RandomForest model. \n  val rf = new RandomForestRegressor() \n    .setLabelCol(\"label\") \n    .setFeaturesCol(\"indexedFeatures\") \n\n  // Chain indexer and forest in a Pipeline. \n  val pipeline = new Pipeline() \n    .setStages(Array(featureIndexer, rf)) \n\n  // Train model. This also runs the indexer. \n  val model = pipeline.fit(trainingData) \n\n  // Make predictions. \n  val predictions = model.transform(testData) \n\n  // Select example rows to display. \n  predictions.select(\"prediction\", \"label\", \"features\").show(5) \n\n  // Select (prediction, true label) and compute test error. \n  val evaluator = new RegressionEvaluator() \n    .setLabelCol(\"label\") \n    .setPredictionCol(\"prediction\") \n    .setMetricName(\"rmse\") \n  val rmse = evaluator.evaluate(predictions) \n  println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse) \n\n  val rfModel = \n   model.stages(1).asInstanceOf[RandomForestRegressionModel] \n  println(\"Learned regression forest model:\\n\" + \n   rfModel.toDebugString) \n} \n\n```", "```scala\nRandomForest:   init: 2.114590873\ntotal: 3.343042855\nfindSplits: 1.387490192\nfindBestSplits: 1.191715923\nchooseSplits: 1.176991821\n\n+------------------+-----+--------------------+\n|        prediction|label|            features|\n+------------------+-----+--------------------+\n| 70.75171441904584|  1.0|(12,[0,1,2,3,4,5,...|\n| 53.43733657257549|  1.0|(12,[0,1,2,3,4,5,...|\n| 57.18242812368521|  1.0|(12,[0,1,2,3,4,5,...|\n| 49.73744636247659|  1.0|(12,[0,1,2,3,4,5,...|\n|56.433579398691144|  1.0|(12,[0,1,2,3,4,5,...|\n\nRoot Mean Squared Error (RMSE) on test data = 123.03866156451954\nLearned regression forest model:\nRandomForestRegressionModel (uid=rfr_bd974271ffe6) with 20 trees\n Tree 0 (weight 1.0):\n If (feature 9 <= 40.0)\n If (feature 9 <= 22.0)\n If (feature 8 <= 13.0)\n If (feature 6 in {0.0})\n If (feature 1 in {0.0})\n Predict: 35.0945945945946\n Else (feature 1 not in {0.0})\n Predict: 63.3921568627451\n Else (feature 6 not in {0.0})\n If (feature 0 in {0.0,1.0})\n Predict: 83.05714285714286\n Else (feature 0 not in {0.0,1.0})\n Predict: 120.76608187134502\n Else (feature 8 > 13.0)\n If (feature 3 <= 21.0)\n If (feature 3 <= 12.0)\n Predict: 149.56363636363636\n Else (feature 3 > 12.0)\n Predict: 54.73593073593074\n Else (feature 3 > 21.0)\n If (feature 6 in {0.0})\n Predict: 89.63333333333334\n Else (feature 6 not in {0.0})\n Predict: 305.6588235294118\n\n```", "```scala\n@transient lazy val logger = Logger.getLogger(getClass.getName) \n\ndef gbtRegressionWithVectorFormat(vectorAssembler: \n   VectorAssembler, vectorIndexer: VectorIndexer, dataFrame: \n   DataFrame) = { \n  val lr = new GBTRegressor() \n    .setFeaturesCol(\"features\") \n    .setLabelCol(\"label\") \n    .setMaxIter(10) \n\n  val pipeline = new Pipeline().setStages(Array(vectorAssembler, \n   vectorIndexer, lr)) \n\n  val Array(training, test) = dataFrame.randomSplit(Array(0.8, \n   0.2), seed = 12345) \n\n  val model = pipeline.fit(training) \n\n  // Make predictions. \n  val predictions = model.transform(test) \n\n  // Select example rows to display. \n  predictions.select(\"prediction\", \"label\", \"features\").show(5) \n\n  // Select (prediction, true label) and compute test error. \n  val evaluator = new RegressionEvaluator() \n    .setLabelCol(\"label\") \n    .setPredictionCol(\"prediction\") \n    .setMetricName(\"rmse\") \n  val rmse = evaluator.evaluate(predictions) \n  println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse) \n\n  val treeModel = model.stages(1).asInstanceOf[GBTRegressionModel] \n  println(\"Learned regression tree model:\\n\" + \n   treeModel.toDebugString)  } \n\ndef gbtRegressionWithSVMFormat(spark: SparkSession) = { \n  // Load training data \n  val training = spark.read.format(\"libsvm\") \n    .load(\"/dataset/BikeSharing/lsvmHours.txt\") \n\n  // Automatically identify categorical features, and index them. \n  // Set maxCategories so features with > 4 distinct values are \n   treated as continuous. \n  val featureIndexer = new VectorIndexer() \n    .setInputCol(\"features\") \n    .setOutputCol(\"indexedFeatures\") \n    .setMaxCategories(4) \n    .fit(training) \n\n  // Split the data into training and test sets (30% held out for \n   testing). \n  val Array(trainingData, testData) = \n   training.randomSplit(Array(0.7, 0.3)) \n\n  // Train a GBT model. \n  val gbt = new GBTRegressor() \n    .setLabelCol(\"label\") \n    .setFeaturesCol(\"indexedFeatures\") \n    .setMaxIter(10) \n\n  // Chain indexer and GBT in a Pipeline. \n  val pipeline = new Pipeline() \n    .setStages(Array(featureIndexer, gbt)) \n\n  // Train model. This also runs the indexer. \n  val model = pipeline.fit(trainingData) \n\n  // Make predictions \n  val predictions = model.transform(testData) \n\n  // Select example rows to display.\n   predictions.select(\"prediction\", \"label\", \"features\").show(5) \n\n  // Select (prediction, true label) and compute test error. \n  val evaluator = new RegressionEvaluator() \n    .setLabelCol(\"label\") \n    .setPredictionCol(\"prediction\") \n    .setMetricName(\"rmse\") \n  val rmse = evaluator.evaluate(predictions) \n  println(\"Root Mean Squared Error (RMSE) on test data = \" + rmse) \n\n  val gbtModel = model.stages(1).asInstanceOf[GBTRegressionModel] \n  println(\"Learned regression GBT model:\\n\" + \n   gbtModel.toDebugString) \n} \n\n```", "```scala\nRandomForest:   init: 1.366356823\ntotal: 1.883186039\nfindSplits: 1.0378687\nfindBestSplits: 0.501171071\nchooseSplits: 0.495084674\n\n+-------------------+-----+--------------------+\n|         prediction|label|            features|\n+-------------------+-----+--------------------+\n|-20.753742348814352|  1.0|(12,[0,1,2,3,4,5,...|\n|-20.760717579684087|  1.0|(12,[0,1,2,3,4,5,...|\n| -17.73182527714976|  1.0|(12,[0,1,2,3,4,5,...|\n| -17.73182527714976|  1.0|(12,[0,1,2,3,4,5,...|\n|   -21.397094071362|  1.0|(12,[0,1,2,3,4,5,...|\n+-------------------+-----+--------------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 73.62468541448783\nLearned regression GBT model:\nGBTRegressionModel (uid=gbtr_24c6ef8f52a7) with 10 trees\n Tree 0 (weight 1.0):\n If (feature 9 <= 41.0)\n If (feature 3 <= 12.0)\n If (feature 3 <= 3.0)\n If (feature 3 <= 2.0)\n If (feature 6 in {1.0})\n Predict: 24.50709219858156\n Else (feature 6 not in {1.0})\n Predict: 74.94945848375451\n Else (feature 3 > 2.0)\n If (feature 6 in {1.0})\n Predict: 122.1732283464567\n Else (feature 6 not in {1.0})\n Predict: 206.3304347826087\n Else (feature 3 > 3.0)\n If (feature 8 <= 18.0)\n If (feature 0 in {0.0,1.0})\n Predict: 137.29818181818183\n Else (feature 0 not in {0.0,1.0})\n Predict: 257.90157480314963\n\n```", "```scala\nobject PlotRawData { \n\n  def main(args: Array[String]) { \n    val records = Util.getRecords()._1 \n    val records_x = records.map(r => r(r.length -1)) \n    var records_int = new Array[Int](records_x.collect().length) \n    print(records_x.first()) \n    val records_collect = records_x.collect() \n\n    for (i <- 0 until records_collect.length){ \n      records_int(i) = records_collect(i).toInt \n    } \n    val min_1 = records_int.min \n    val max_1 = records_int.max \n\n    val min = min_1 \n    val max = max_1 \n    val bins = 40 \n    val step = (max/bins).toInt \n\n    var mx = Map(0 -> 0) \n    for (i <- step until (max + step) by step) { \n      mx += (i -> 0); \n    } \n\n    for(i <- 0 until records_collect.length){ \n      for (j <- 0 until (max + step) by step) { \n        if(records_int(i) >= (j) && records_int(i) < (j + step)){ \n          mx = mx + (j -> (mx(j) + 1)) \n        } \n      } \n    } \n    val mx_sorted = ListMap(mx.toSeq.sortBy(_._1):_*) \n    val ds = new org.jfree.data.category.DefaultCategoryDataset \n    var i = 0 \n    mx_sorted.foreach{ case (k,v) => ds.addValue(v,\"\", k)} \n\n    val chart = ChartFactories.BarChart(ds) \n    val font = new Font(\"Dialog\", Font.PLAIN,4); \n\n    chart.peer.getCategoryPlot.getDomainAxis(). \n      setCategoryLabelPositions(CategoryLabelPositions.UP_90); \n    chart.peer.getCategoryPlot.getDomainAxis.setLabelFont(font) \n    chart.show() \n    Util.sc.stop() \n  } \n} \n\n```", "```scala\nobject PlotLogData { \n\n  def main(args: Array[String]) { \n    val records = Util.getRecords()._1 \n    val records_x = records.map( \n      r => Math.log(r(r.length -1).toDouble)) \n    var records_int = new Array[Int](records_x.collect().length) \n    print(records_x.first()) \n    val records_collect = records_x.collect() \n\n    for (i <- 0 until records_collect.length){ \n      records_int(i) = records_collect(i).toInt \n    } \n    val min_1 = records_int.min \n    val max_1 = records_int.max \n\n    val min = min_1.toFloat \n    val max = max_1.toFloat \n    val bins = 10 \n    val step = (max/bins).toFloat \n\n    var mx = Map(0.0.toString -> 0) \n    for (i <- step until (max + step) by step) { \n      mx += (i.toString -> 0); \n    } \n\n    for(i <- 0 until records_collect.length){ \n      for (j <- 0.0 until (max + step) by step) { \n        if(records_int(i) >= (j) && records_int(i) < (j + step)){ \n          mx = mx + (j.toString -> (mx(j.toString) + 1)) \n        } \n      } \n    } \n    val mx_sorted = ListMap(mx.toSeq.sortBy(_._1.toFloat):_*) \n    val ds = new org.jfree.data.category.DefaultCategoryDataset \n    var i = 0 \n    mx_sorted.foreach{ case (k,v) => ds.addValue(v,\"\", k)} \n\n    val chart = ChartFactories.BarChart(ds) \n    val font = new Font(\"Dialog\", Font.PLAIN,4); \n\n    chart.peer.getCategoryPlot.getDomainAxis(). \n      setCategoryLabelPositions(CategoryLabelPositions.UP_90); \n    chart.peer.getCategoryPlot.getDomainAxis.setLabelFont(font) \n    chart.show() \n    Util.sc.stop() \n  } \n} \n\n```", "```scala\nobject LinearRegressionWithLog{ \n\n  def main(args: Array[String]) { \n\n    val recordsArray = Util.getRecords() \n    val records = recordsArray._1 \n    val first = records.first() \n    val numData = recordsArray._2 \n\n    println(numData.toString()) \n    records.cache()\n     print(\"Mapping of first categorical feature column: \" + \n       Util.get_mapping(records, 2)) \n    var list = new ListBuffer[Map[String, Long]]() \n    for( i <- 2 to 9){ \n      val m =  Util.get_mapping(records, i) \n      list += m \n    } \n    val mappings = list.toList \n    var catLen = 0 \n    mappings.foreach( m => (catLen +=m.size)) \n\n    val numLen = records.first().slice(11, 15).size \n    val totalLen = catLen + numLen\n    print(\"Feature vector length for categorical features:\"+ \n       catLen)\n     print(\"Feature vector length for numerical features:\" +\n       numLen)\n     print(\"Total feature vector length: \" + totalLen) \n\n    val data = { \n      records.map(r => LabeledPoint(Math.log(Util.extractLabel(r)),\n         Util.extractFeatures(r, catLen, mappings)))\n    } \n    val first_point = data.first() \n    println(\"Linear Model feature vector:\" + \n       first_point.features.toString) \n    println(\"Linear Model feature vector length: \" + \n       first_point.features.size) \n\n    val iterations = 10 \n    val step = 0.025 \n    val intercept =true \n    val linear_model = LinearRegressionWithSGD.train(data, \n       iterations, step) \n    val x = linear_model.predict(data.first().features) \n    val true_vs_predicted = data.map(p => (Math.exp(p.label), \n       Math.exp(linear_model.predict(p.features)))) \n    val true_vs_predicted_csv = data.map(p => p.label + \" ,\" + \n       linear_model.predict(p.features)) \n    val format = new java.text.SimpleDateFormat(\n       \"dd-MM-yyyy-hh-mm-ss\") \n    val date = format.format(new java.util.Date()) \n    val save = false \n    if (save){ \n         true_vs_predicted_csv.saveAsTextFile( \n           \"./output/linear_model_\" + date + \".csv\") \n    } \n    val true_vs_predicted_take5 = true_vs_predicted.take(5) \n    for(i <- 0 until 5) { \n      println(\"True vs Predicted: \" + \"i :\" + \n         true_vs_predicted_take5(i)) \n    } \n\n    Util.calculatePrintMetrics(true_vs_predicted, \n       \"LinearRegressioWithSGD Log\")\n  } \n} \n\n```", "```scala\nLinearRegressioWithSGD Log - Mean Squared Error: 5055.089410453301\nLinearRegressioWithSGD Log - Mean Absolute Error: 51.56719871511336\nLinearRegressioWithSGD Log - Root Mean Squared Log \n   Error:1.7785399629180894\n\n```", "```scala\nLinearRegressioWithSGD - Mean Squared Error: 35817.9777663029\nLinearRegressioWithSGD - Mean Absolute Error: 136.94887209426008\nLinearRegressioWithSGD - Root Mean Squared Log Error: \n    1.4482391780194306\nLinearRegressioWithSGD Log - Mean Squared Error: 60192.54096079104\nLinearRegressioWithSGD Log - Mean Absolute Error: \n    170.82191606911752\nLinearRegressioWithSGD Log - Root Mean Squared Log Error: \n    1.9587586971094555\n\n```", "```scala\nval splits = data.randomSplit(Array(0.8, 0.2), seed = 11L) \nval training = splits(0).cache() \nval test = splits(1) \n\n```", "```scala\nval splits = data_dt.randomSplit(Array(0.8, 0.2), seed = 11L) \nval training = splits(0).cache() \nval test = splits(1) \n\n```", "```scala\ndef evaluate(train: RDD[LabeledPoint],test: RDD[LabeledPoint], \n  iterations:Int,step:Double, \n  intercept:Boolean): Double ={ \n  val linReg =  \n    new LinearRegressionWithSGD().setIntercept(intercept) \n\n  linReg.optimizer.setNumIterations(iterations).setStepSize(step) \n  val linear_model = linReg.run(train) \n\n  val true_vs_predicted = test.map(p => (p.label,  \n    linear_model.predict(p.features))) \n  val rmsle = Math.sqrt(true_vs_predicted.map{  \n    case(t, p) => Util.squaredLogError(t, p)}.mean()) \n  return rmsle \n} \n\n```", "```scala\nval data = LinearRegressionUtil.getTrainTestData() \nval train_data = data._1 \nval test_data = data._2 \nval iterations = 10 \n//LinearRegressionCrossValidationStep$ \n//params = [1, 5, 10, 20, 50, 100, 200] \nval iterations_param = Array(1, 5, 10, 20, 50, 100, 200) \nval step =0.01 \n//val steps_param = Array(0.01, 0.025, 0.05, 0.1, 1.0) \nval intercept =false \n\nval i = 0 \nval results = new Array[String](5) \nval resultsMap = new scala.collection.mutable.HashMap[String, \n   String] \nval dataset = new DefaultCategoryDataset() \nfor(i <- 0 until iterations_param.length) { \n  val iteration = iterations_param(i) \n  val rmsle = LinearRegressionUtil.evaluate(train_data, \n   test_data,iteration,step,intercept) \n  //results(i) = step + \":\" + rmsle \n  resultsMap.put(iteration.toString,rmsle.toString) \n  dataset.addValue(rmsle, \"RMSLE\", iteration) \n} \n\n```", "```scala\n  Map(5 -> 0.8403179051522236, 200 -> 0.35682322830872604, 50 -> \n   0.07224447567763903, 1 -> 1.6381266770967882, 20 -> \n   0.23992956602621263, 100 -> 0.2525579338412989, 10 -> \n   0.5236271681647611) \n\n```", "```scala\nval steps_param = Array(0.01, 0.025, 0.05, 0.1, 1.0) \nval intercept =false \n\nval i = 0 \nval results = new Array[String](5) \nval resultsMap = new scala.collection.mutable.HashMap[String, String] \nval dataset = new DefaultCategoryDataset() \nfor(i <- 0 until steps_param.length) { \n  val step = steps_param(i) \n  val rmsle = LinearRegressionUtil.evaluate(train_data, \n         test_data,iterations,step,intercept) \n  resultsMap.put(step.toString,rmsle.toString) \n  dataset.addValue(rmsle, \"RMSLE\", step) \n} \n\n```", "```scala\n    [1.7904244862988534, 1.4241062778987466, 1.3840130355866163, \n   1.4560061007109475, nan]\n\n```", "```scala\nparams = [0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0] \nmetrics = [evaluate(train_data, test_data, 10, 0.1, param, 'l1', \n   False) for param in params] \nprint params \nprint metrics \nplot(params, metrics) \nfig = matplotlib.pyplot.gcf() \npyplot.xscale('log') \n\n```", "```scala\n[0.0, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n[1.5384660954019971, 1.5384518080419873, 1.5383237472930684, \n    1.5372017600929164, 1.5303809928601677, 1.4352494587433793, \n    4.7551250073268614]\n\n```", "```scala\nmodel_l1 = LinearRegressionWithSGD.train(train_data, 10, 0.1, \n   regParam=1.0, regType='l1', intercept=False) \nmodel_l1_10 = LinearRegressionWithSGD.train(train_data, 10, 0.1, \n   regParam=10.0, regType='l1', intercept=False) \nmodel_l1_100 = LinearRegressionWithSGD.train(train_data, 10, 0.1, \n   regParam=100.0, regType='l1', intercept=False) \nprint \"L1 (1.0) number of zero weights: \" + \n   str(sum(model_l1.weights.array == 0)) \nprint \"L1 (10.0) number of zeros weights: \" + \n   str(sum(model_l1_10.weights.array == 0)) \nprint \"L1 (100.0) number of zeros weights: \" + \n   str(sum(model_l1_100.weights.array == 0)) \n\n```", "```scala\nL1 (1.0) number of zero weights: 4\nL1 (10.0) number of zeros weights: 20\nL1 (100.0) number of zeros weights: 55\n\n```", "```scala\nobject LinearRegressionCrossValidationIntercept{ \n  def main(args: Array[String]) { \n    val data = LinearRegressionUtil.getTrainTestData() \n    val train_data = data._1 \n    val test_data = data._2 \n\n    val iterations = 10 \n    val step = 0.1 \n    val paramsArray = new Array[Boolean](2) \n    paramsArray(0) = true \n    paramsArray(1) = false \n    val i = 0 \n    val results = new Array[String](2) \n    val resultsMap = new scala.collection.mutable.HashMap[ \n    String, String] \n    val dataset = new DefaultCategoryDataset() \n    for(i <- 0 until 2) { \n      val intercept = paramsArray(i) \n      val rmsle = LinearRegressionUtil.evaluate(train_data,  \n        test_data,iterations,step,intercept) \n      results(i) = intercept + \":\" + rmsle \n      resultsMap.put(intercept.toString,rmsle.toString) \n      dataset.addValue(rmsle, \"RMSLE\", intercept.toString) \n    } \n    val chart = new LineChart( \n      \"Steps\" , \n      \"LinearRegressionWithSGD : RMSLE vs Intercept\") \n    chart.exec(\"Steps\",\"RMSLE\",dataset) \n    chart.lineChart.getCategoryPlot().getRangeAxis().setRange( \n    1.56, 1.57) \n    chart.pack( ) \n    RefineryUtilities.centerFrameOnScreen( chart ) \n    chart.setVisible( true ) \n    println(results) \n  } \n} \n\n```", "```scala\ndef evaluate(train: RDD[LabeledPoint],test: RDD[LabeledPoint], \n  categoricalFeaturesInfo: scala.Predef.Map[Int, Int], \n  maxDepth :Int, maxBins: Int): Double = { \n    val impurity = \"variance\" \n    val decisionTreeModel = DecisionTree.trainRegressor(train, \n      categoricalFeaturesInfo, \n      impurity, maxDepth, maxBins) \n    val true_vs_predicted = test.map(p => (p.label,  \n      decisionTreeModel.predict(p.features))) \n    val rmsle = Math.sqrt(true_vs_predicted.map{  \n      case(t, p) => Util.squaredLogError(t, p)}.mean()) \n      return rmsle \n  } \n\n```", "```scala\nval data = DecisionTreeUtil.getTrainTestData() \n  val train_data = data._1 \n  val test_data = data._2 \n  val iterations = 10 \n  val bins_param = Array(2, 4, 8, 16, 32, 64, 100) \n  val depth_param = Array(1, 2, 3, 4, 5, 10, 20) \n  val bin = 32 \n  val categoricalFeaturesInfo = scala.Predef.Map[Int, Int]() \n  val i = 0 \n  val results = new Array[String](7) \n  val resultsMap = new scala.collection.mutable.HashMap[ \n    String, String] \n  val dataset = new DefaultCategoryDataset() \n  for(i <- 0 until depth_param.length) { \n    val depth = depth_param(i) \n    val rmsle = DecisionTreeUtil.evaluate( \n    train_data, test_data, categoricalFeaturesInfo, depth, bin) \n\n    resultsMap.put(depth.toString,rmsle.toString) \n    dataset.addValue(rmsle, \"RMSLE\", depth) \n  } \n  val chart = new LineChart( \n    \"MaxDepth\" , \n    \"DecisionTree : RMSLE vs MaxDepth\") \n  chart.exec(\"MaxDepth\",\"RMSLE\",dataset) \n  chart.pack() \n  RefineryUtilities.centerFrameOnScreen( chart ) \n  chart.setVisible( true ) \n  print(resultsMap) \n} \n\n```", "```scala\nobject DecisionTreeMaxBins{ \n  def main(args: Array[String]) { \n    val data = DecisionTreeUtil.getTrainTestData() \n    val train_data = data._1 \n    val test_data = data._2 \n    val iterations = 10 \n    val bins_param = Array(2, 4, 8, 16, 32, 64, 100) \n    val maxDepth = 5 \n    val categoricalFeaturesInfo = scala.Predef.Map[Int, Int]() \n    val i = 0 \n    val results = new Array[String](5) \n    val resultsMap = new scala.collection.mutable.HashMap[ \n        String, String] \n    val dataset = new DefaultCategoryDataset() \n    for(i <- 0 until bins_param.length) { \n      val bin = bins_param(i) \n      val rmsle = { \n        DecisionTreeUtil.evaluate(train_data, test_data, \n         categoricalFeaturesInfo, 5, bin) \n      } \n      resultsMap.put(bin.toString,rmsle.toString) \n      dataset.addValue(rmsle, \"RMSLE\", bin) \n    } \n    val chart = new LineChart( \n      \"MaxBins\" , \n      \"DecisionTree : RMSLE vs MaxBins\") \n    chart.exec(\"MaxBins\",\"RMSLE\",dataset) \n    chart.pack( ) \n    RefineryUtilities.centerFrameOnScreen( chart ) \n    chart.setVisible( true ) \n    print(resultsMap) \n  } \n\n```", "```scala\nobject GradientBoostedTreesIterations{ \n\n  def main(args: Array[String]) { \n    val data = GradientBoostedTreesUtil.getTrainTestData() \n    val train_data = data._1 \n    val test_data = data._2 \n\n    val iterations_param = Array(1, 5, 10, 15, 18) \n\n    val i = 0 \n    val resultsMap = new scala.collection.mutable.HashMap[ \n        String, String] \n    val dataset = new DefaultCategoryDataset() \n    for(i <- 0 until iterations_param.length) { \n      val iteration = iterations_param(i) \n      val rmsle = GradientBoostedTreesUtil.evaluate(train_data,  \n        test_data,iteration,maxDepth) \n      resultsMap.put(iteration.toString,rmsle.toString) \n      dataset.addValue(rmsle, \"RMSLE\", iteration) \n    } \n    val chart = new LineChart( \n      \"Iterations\" , \n      \"GradientBoostedTrees : RMSLE vs Iterations\") \n    chart.exec(\"Iterations\",\"RMSLE\",dataset) \n    chart.pack( ) \n    chart.lineChart.getCategoryPlot().\n       getRangeAxis().setRange(1.32, 1.37) \n    RefineryUtilities.centerFrameOnScreen( chart ) \n    chart.setVisible( true ) \n    print(resultsMap) \n  } \n} \n\n```", "```scala\nobject GradientBoostedTreesMaxBins{ \n\n  def main(args: Array[String]) { \n    val data = GradientBoostedTreesUtil.getTrainTestData() \n    val train_data = data._1 \n    val test_data = data._2 \n\n    val maxBins_param = Array(10,16,32,64) \n    val iteration = 10 \n    val maxDepth = 3 \n\n    val i = 0 \n    val resultsMap =  \n    new scala.collection.mutable.HashMap[String, String] \n    val dataset = new DefaultCategoryDataset() \n    for(i <- 0 until maxBins_param.length) { \n      val maxBin = maxBins_param(i) \n      val rmsle = GradientBoostedTreesUtil.evaluate(train_data, \n         test_data,iteration,maxDepth, maxBin) \n\n      resultsMap.put(maxBin.toString,rmsle.toString) \n      dataset.addValue(rmsle, \"RMSLE\", maxBin) \n    } \n    val chart = new LineChart( \n      \"Max Bin\" , \n      \"GradientBoostedTrees : RMSLE vs MaxBin\") \n    chart.exec(\"MaxBins\",\"RMSLE\",dataset) \n    chart.pack( ) \n    chart.lineChart.getCategoryPlot(). \n        getRangeAxis().setRange(1.35, 1.37) \n    RefineryUtilities.centerFrameOnScreen( chart ) \n    chart.setVisible(true) \n    print(resultsMap) \n  } \n\n```"]