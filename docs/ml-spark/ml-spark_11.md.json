["```scala\nname := \"scala-spark-streaming-app\" \nversion := \"1.0\" \nscalaVersion := \"2.11.7\"\nval sparkVersion = \"2.0.0\" \n\nlibraryDependencies ++= Seq(\n  \"org.apache.spark\" %% \"spark-core\" % sparkVersion, \n  \"org.apache.spark\" %% \"spark-mllib\" % sparkVersion, \n  \"org.jfree\" % \"jfreechart\" % \"1.0.14\", \n  \"com.github.wookietreiber\" % \"scala-chart_2.11\" % \"0.5.0\", \n  \"org.apache.spark\" %% \"spark-streaming\" % sparkVersion \n)\n\n```", "```scala\nMiguel,Eric,James,Juan,Shawn,James,Doug,Gary,Frank,Janet,Michael,\nJames,Malinda,Mike,Elaine,Kevin,Janet,Richard,Saul,Manuela\n\n```", "```scala\n/** \n  * A producer application that generates random \"product \n  * events\", up to 5 per second, and sends them over a network  \n  * connection \n*/ \nobject StreamingProducer { \n\n  def main(args: Array[String]) { \n\n    val random = new Random() \n\n    // Maximum number of events per second \n    val MaxEvents = 6 \n\n    // Read the list of possible names \n    val namesResource = \n      this.getClass.getResourceAsStream(\"/names.csv\") \n    val names = scala.io.Source.fromInputStream(namesResource) \n      .getLines() \n      .toList \n      .head \n      .split(\",\") \n      .toSeq \n\n    // Generate a sequence of possible products \n    val products = Seq( \n      \"iPhone Cover\" -> 9.99, \n      \"Headphones\" -> 5.49, \n      \"Samsung Galaxy Cover\" -> 8.95, \n      \"iPad Cover\" -> 7.49 \n    )\n\n```", "```scala\n/** Generate a number of random product events */ \ndef generateProductEvents(n: Int) = { \n  (1 to n).map { i => \n    val (product, price) = \n      products(random.nextInt(products.size)) \n    val user = random.shuffle(names).head \n      (user, product, price) \n  } \n}\n\n```", "```scala\n// create a network producer \nval listener = new ServerSocket(9999) \nprintln(\"Listening on port: 9999\") \n\nwhile (true) { \n  val socket = listener.accept() \n  new Thread() { \n    override def run = { \n      println(\"Got client connected from: \" + \n        socket.getInetAddress) \n      val out = new PrintWriter(socket.getOutputStream(), true) \n\n      while (true) { \n        Thread.sleep(1000) \n        val num = random.nextInt(MaxEvents) \n        val productEvents = generateProductEvents(num) \n        productEvents.foreach{ event => \n          out.write(event.productIterator.mkString(\",\")) \n          out.write(\"n\") \n        } \n        out.flush() \n        println(s\"Created $num events...\") \n      } \n      socket.close() \n    } \n  }.start() \n}\n\n```", "```scala\n>cd scala-spark-streaming-app\n>sbt\n[info] ...\n>\n\n```", "```scala\n>run\n\n```", "```scala\n...\nMultiple main classes detected, select one to run:\n\n[1] StreamingProducer\n[2] SimpleStreamingApp\n[3] StreamingAnalyticsApp\n[4] StreamingStateApp\n[5] StreamingModelProducer\n[6] SimpleStreamingModel\n[7] MonitoringStreamingModel\n\nEnter number:\n\n```", "```scala\n[info] Running StreamingProducer\nListening on port: 9999\n\n```", "```scala\n/** \n  * A simple Spark Streaming app in Scala \n**/ \nobject SimpleStreamingApp { \n  def main(args: Array[String]) { \n    val ssc = new StreamingContext(\"local[2]\", \"First Streaming \n      App\", Seconds(10)) \n    val stream = ssc.socketTextStream(\"localhost\", 9999) \n\n    // here we simply print out the first few elements of each batch \n    stream.print() \n    ssc.start() \n    ssc.awaitTermination() \n  } \n}\n\n```", "```scala\n**>**sbt\n[info] ...\n>run\n\n```", "```scala\nMultiple main classes detected, select one to run:\n\n[1] StreamingProducer\n[2] SimpleStreamingApp\n[3] StreamingAnalyticsApp\n[4] StreamingStateApp\n[5] StreamingModelProducer\n[6] SimpleStreamingModel\n[7] MonitoringStreamingModel\n\n```", "```scala\n...\n14/11/15 21:02:23 INFO scheduler.ReceiverTracker: ReceiverTracker \nstarted\n14/11/15 21:02:23 INFO dstream.ForEachDStream:  \nmetadataCleanupDelay  \n=  -1\n14/11/15 21:02:23 INFO dstream.SocketInputDStream: \nmetadataCleanupDelay = -1\n14/11/15 21:02:23 INFO dstream.SocketInputDStream: Slide time =  \n10000 ms\n14/11/15 21:02:23 INFO dstream.SocketInputDStream: Storage level = \nStorageLevel(false, false, false, false, 1)\n14/11/15 21:02:23 INFO dstream.SocketInputDStream: Checkpoint \ninterval = null\n14/11/15 21:02:23 INFO dstream.SocketInputDStream: Remember      \nduration = 10000 ms\n14/11/15 21:02:23 INFO dstream.SocketInputDStream: Initialized and \nvalidated  \norg.apache.spark.streaming.dstream.SocketInputDStream@ff3436d\n14/11/15 21:02:23 INFO dstream.ForEachDStream: Slide time = \n10000   \nms\n14/11/15 21:02:23 INFO dstream.ForEachDStream: Storage level = \nStorageLevel(false, false, false, false, 1)\n14/11/15 21:02:23 INFO dstream.ForEachDStream: Checkpoint \ninterval  \n=  null\n14/11/15 21:02:23 INFO dstream.ForEachDStream: Remember duration = \n10000 ms\n14/11/15 21:02:23 INFO dstream.ForEachDStream: Initialized and \nvalidated   \norg.apache.spark.streaming.dstream.ForEachDStream@5a10b6e8\n14/11/15 21:02:23 INFO scheduler.ReceiverTracker: Starting 1 \nreceivers\n14/11/15 21:02:23 INFO spark.SparkContext: Starting job: runJob at \nReceiverTracker.scala:275\n...\n\n```", "```scala\n...\nGot client connected from: /127.0.0.1\nCreated 2 events...\nCreated 2 events...\nCreated 3 events...\nCreated 1 events...\nCreated 5 events...\n...\n\n```", "```scala\n...\n14/11/15 21:02:30 INFO spark.SparkContext: Job finished: take at \nDStream.scala:608, took 0.05596 s\n-------------------------------------------\nTime: 1416078150000 ms\n-------------------------------------------\nMichael,Headphones,5.49\nFrank,Samsung Galaxy Cover,8.95\nEric,Headphones,5.49\nMalinda,iPad Cover,7.49\nJames,iPhone Cover,9.99\nJames,Headphones,5.49\nDoug,iPhone Cover,9.99\nJuan,Headphones,5.49\nJames,iPhone Cover,9.99\nRichard,iPad Cover,7.49\n...\n\n```", "```scala\n/** \n * A more complex Streaming app, which computes statistics and \n   prints the results for each batch in a DStream \n*/ \nobject StreamingAnalyticsApp { \n\n  def main(args: Array[String]) { \n\n    val ssc = new StreamingContext(\"local[2]\", \"First Streaming \n      App\", Seconds(10)) \n    val stream = ssc.socketTextStream(\"localhost\", 9999) \n\n    // create stream of events from raw text elements \n    val events = stream.map { record => \n      val event = record.split(\",\") \n      (event(0), event(1), event(2)) \n    }\n\n```", "```scala\n/* \n  We compute and print out stats for each batch. \n  Since each batch is an RDD, we call forEeachRDD on the \n  DStream, and apply the usual RDD functions \n  we used in Chapter 1\\. \n*/ \nevents.foreachRDD { (rdd, time) => \n  val numPurchases = rdd.count() \n  val uniqueUsers = rdd.map { case (user, _, _) => user \n    }.distinct().count() \n  val totalRevenue = rdd.map { case (_, _, price) => \n    price.toDouble }.sum() \n  val productsByPopularity = rdd \n    .map { case (user, product, price) => (product, 1) } \n    .reduceByKey(_ + _) \n    .collect() \n    .sortBy(-_._2) \n  val mostPopular = productsByPopularity(0) \n\n  val formatter = new SimpleDateFormat \n  val dateStr = formatter.format(new \n    Date(time.milliseconds)) \n  println(s\"== Batch start time: $dateStr ==\") \n  println(\"Total purchases: \" + numPurchases) \n  println(\"Unique users: \" + uniqueUsers) \n  println(\"Total revenue: \" + totalRevenue) \n  println(\"Most popular product: %s with %d \n    purchases\".format(mostPopular._1, mostPopular._2)) \n} \n\n// start the context \nssc.start() \nssc.awaitTermination() \n\n} \n\n}\n\n```", "```scala\n...\n14/11/15 21:27:30 INFO spark.SparkContext: Job finished: collect \nat \nStreaming.scala:125, took 0.071145 s\n== Batch start time: 2014/11/15 9:27 PM ==\nTotal purchases: 16\nUnique users: 10\nTotal revenue: 123.72\nMost popular product: iPad Cover with 6 purchases\n...\n\n```", "```scala\nobject StreamingStateApp { \n  import org.apache.spark.streaming.StreamingContext\n\n```", "```scala\ndef updateState(prices: Seq[(String, Double)], currentTotal: \n  Option[(Int, Double)]) = { \n  val currentRevenue = prices.map(_._2).sum \n  val currentNumberPurchases = prices.size \n  val state = currentTotal.getOrElse((0, 0.0)) \n  Some((currentNumberPurchases + state._1, currentRevenue + \n   state._2)) \n} \n\ndef main(args: Array[String]) { \n\n  val ssc = new StreamingContext(\"local[2]\", \"First Streaming \n    App\", Seconds(10)) \n  // for stateful operations, we need to set a checkpoint location \n  ssc.checkpoint(\"/tmp/sparkstreaming/\") \n  val stream = ssc.socketTextStream(\"localhost\", 9999) \n\n  // create stream of events from raw text elements \n  val events = stream.map { record => \n    val event = record.split(\",\") \n    (event(0), event(1), event(2).toDouble) \n  } \n\n  val users = events.map{ case (user, product, price) => \n    (user, (product, price)) } \n  val revenuePerUser = users.updateStateByKey(updateState) \n  revenuePerUser.print() \n\n  // start the context \n  ssc.start() \n  ssc.awaitTermination() \n\n  } \n}\n\n```", "```scala\n...\n-------------------------------------------\nTime: 1416080440000 ms\n-------------------------------------------\n(Janet,(2,10.98))\n(Frank,(1,5.49))\n(James,(2,12.98))\n(Malinda,(1,9.99))\n(Elaine,(3,29.97))\n(Gary,(2,12.98))\n(Miguel,(3,20.47))\n(Saul,(1,5.49))\n(Manuela,(2,18.939999999999998))\n(Eric,(2,18.939999999999998))\n...\n-------------------------------------------\nTime: 1416080441000 ms\n-------------------------------------------\n(Janet,(6,34.94))\n(Juan,(4,33.92))\n(Frank,(2,14.44))\n(James,(7,48.93000000000001))\n(Malinda,(1,9.99))\n(Elaine,(7,61.89))\n(Gary,(4,28.46))\n(Michael,(1,8.95))\n(Richard,(2,16.439999999999998))\n(Miguel,(5,35.95))\n...\n\n```", "```scala\n/** \n * A producer application that generates random linear \n regression data. \n*/ \nobject StreamingModelProducer { \n  import breeze.linalg._ \n\n  def main(args: Array[String]) { \n\n    // Maximum number of events per second \n    val MaxEvents = 100 \n    val NumFeatures = 100 \n\n    val random = new Random()\n\n```", "```scala\n/** Function to generate a normally distributed dense vector */ \ndef generateRandomArray(n: Int) = Array.tabulate(n)(_ => \n  random.nextGaussian()) \n\n// Generate a fixed random model weight vector \nval w = new DenseVector(generateRandomArray(NumFeatures)) \nval intercept = random.nextGaussian() * 10\n\n```", "```scala\n/** Generate a number of random product events */ \ndef generateNoisyData(n: Int) = { \n  (1 to n).map { i => \n    val x = new DenseVector(generateRandomArray(NumFeatures)) \n    val y: Double = w.dot(x) \n    val noisy = y + intercept //+ 0.1 * random.nextGaussian() \n    (noisy, x) \n  } \n}\n\n```", "```scala\n// create a network producer \n  val listener = new ServerSocket(9999) \n  println(\"Listening on port: 9999\") \n\n  while (true) { \n    val socket = listener.accept() \n    new Thread() { \n      override def run = { \n        println(\"Got client connected from: \" + \n          socket.getInetAddress) \n        val out = new PrintWriter(socket.getOutputStream(), \n          true) \n\n        while (true) { \n          Thread.sleep(1000) \n          val num = random.nextInt(MaxEvents) \n          val data = generateNoisyData(num) \n          data.foreach { case (y, x) => \n            val xStr = x.data.mkString(\",\") \n            val eventStr = s\"$yt$xStr\" \n            out.write(eventStr) \n            out.write(\"n\") \n            } \n            out.flush() \n            println(s\"Created $num events...\") \n          } \n          socket.close() \n        } \n      }.start() \n    } \n  } \n}\n\n```", "```scala\n[info] Running StreamingModelProducer\nListening on port: 9999\n\n```", "```scala\n/** \n  * A simple streaming linear regression that prints out predicted   \n   value for each batch \n */ \nobject SimpleStreamingModel { \n\n  def main(args: Array[String]) { \n\n  val ssc = new StreamingContext(\"local[2]\", \"First Streaming       \n    App\", Seconds(10)) \n  val stream = ssc.socketTextStream(\"localhost\", 9999)\n\n```", "```scala\nval NumFeatures = 100 \nval zeroVector = DenseVector.zeros[Double](NumFeatures) \nval model = new StreamingLinearRegressionWithSGD() \n  .setInitialWeights(Vectors.dense(zeroVector.data)) \n  .setNumIterations(1) \n  .setStepSize(0.01)\n\n```", "```scala\n// create a stream of labeled points \nval labeledStream = stream.map { event => \n  val split = event.split(\"t\") \n  val y = split(0).toDouble \n  val features = split(1).split(\",\").map(_.toDouble) \n  LabeledPoint(label = y, features = Vectors.dense(features)) \n}\n\n```", "```scala\n// train and test model on the stream, and print predictions\n// for illustrative purposes \n    model.trainOn(labeledStream) \n    //model.predictOn(labeledStream).print() \n    model.predictOnValues(labeledStream.map(lp => (lp.label,       \n    lp.features))).print() \n\n    ssc.start() \n    ssc.awaitTermination() \n\n  } \n}\n\n```", "```scala\nGot client connected from: /127.0.0.1\n...\nCreated 10 events...\nCreated 83 events...\nCreated 75 events...\n...\n\n```", "```scala\n14/11/16 14:54:00 INFO StreamingLinearRegressionWithSGD: Model \nupdated at time 1416142440000 ms\n14/11/16 14:54:00 INFO StreamingLinearRegressionWithSGD: Current \nmodel: weights, [0.05160959387864821,0.05122747155689144,-\n0.17224086785756998,0.05822993392274008,0.07848094246845688,-\n0.1298315806501979,0.006059323642394124, ...\n...\n14/11/16 14:54:00 INFO JobScheduler: Finished job streaming job \n1416142440000 ms.0 from job set of time 1416142440000 ms\n14/11/16 14:54:00 INFO JobScheduler: Starting job streaming job \n1416142440000 ms.1 from job set of time 1416142440000 ms\n14/11/16 14:54:00 INFO SparkContext: Starting job: take at \nDStream.scala:608\n14/11/16 14:54:00 INFO DAGScheduler: Got job 3 (take at \nDStream.scala:608) with 1 output partitions (allowLocal=true)\n14/11/16 14:54:00 INFO DAGScheduler: Final stage: Stage 3(take at \nDStream.scala:608)\n14/11/16 14:54:00 INFO DAGScheduler: Parents of final stage: List()\n14/11/16 14:54:00 INFO DAGScheduler: Missing parents: List()\n14/11/16 14:54:00 INFO DAGScheduler: Computing the requested \npartition locally\n14/11/16 14:54:00 INFO SparkContext: Job finished: take at \nDStream.scala:608, took 0.014064 s\n-------------------------------------------\nTime: 1416142440000 ms\n-------------------------------------------\n-2.0851430248312526\n4.609405228401022\n2.817934589675725\n3.3526557917118813\n4.624236379848475\n-2.3509098272485156\n-0.7228551577759544\n2.914231548990703\n0.896926579927631\n1.1968162940541283\n...\n\n```", "```scala\n/** \n * A streaming regression model that compares the model             \n * performance of two models, printing out metrics for \n * each batch \n*/ \nobject MonitoringStreamingModel { \n  import org.apache.spark.SparkContext._ \n\n  def main(args: Array[String]) { \n\n    val ssc = new StreamingContext(\"local[2]\", \"First Streaming \n      App\", Seconds(10)) \n    val stream = ssc.socketTextStream(\"localhost\", 9999) \n\n    val NumFeatures = 100 \n    val zeroVector = DenseVector.zeros[Double](NumFeatures) \n    val model1 = new StreamingLinearRegressionWithSGD() \n      .setInitialWeights(Vectors.dense(zeroVector.data)) \n      .setNumIterations(1) \n      .setStepSize(0.01) \n\n    val model2 = new StreamingLinearRegressionWithSGD() \n      .setInitialWeights(Vectors.dense(zeroVector.data)) \n      .setNumIterations(1) \n      .setStepSize(1.0) \n\n    // create a stream of labeled points \n    val labeledStream = stream.map { event => \n    val split = event.split(\"t\") \n    val y = split(0).toDouble \n    val features = split(1).split(\",\").map(_.toDouble) \n    LabeledPoint(label = y, features =   \n      Vectors.dense(features)) \n    }\n\n```", "```scala\n// train both models on the same stream \nmodel1.trainOn(labeledStream) \nmodel2.trainOn(labeledStream) \n\n// use transform to create a stream with model error rates \nval predsAndTrue = labeledStream.transform { rdd => \n  val latest1 = model1.latestModel() \n  val latest2 = model2.latestModel() \n  rdd.map { point => \n    val pred1 = latest1.predict(point.features) \n    val pred2 = latest2.predict(point.features) \n    (pred1 - point.label, pred2 - point.label) \n  } \n}\n\n```", "```scala\n// print out the MSE and RMSE metrics for each model per batch \npredsAndTrue.foreachRDD { (rdd, time) => \n  val mse1 = rdd.map { case (err1, err2) => err1 * err1 \n  }.mean() \n  val rmse1 = math.sqrt(mse1) \n  val mse2 = rdd.map { case (err1, err2) => err2 * err2 \n  }.mean() \n  val rmse2 = math.sqrt(mse2) \n  println( \n    s\"\"\" \n       |------------------------------------------- \n       |Time: $time \n       |------------------------------------------- \n     \"\"\".stripMargin) \n  println(s\"MSE current batch: Model 1: $mse1; Model 2: \n    $mse2\") \n  println(s\"RMSE current batch: Model 1: $rmse1; Model 2:\n    $rmse2\") \n  println(\"...n\") \n} \n\nssc.start() \nssc.awaitTermination() \n\n  } \n}\n\n```", "```scala\n...\n14/11/16 14:56:11 INFO SparkContext: Job finished: mean at \nStreamingModel.scala:159, took 0.09122 s\n\n-------------------------------------------\nTime: 1416142570000 ms\n-------------------------------------------\n\nMSE current batch: Model 1: 97.9475827857361; Model 2: \n97.9475827857361\nRMSE current batch: Model 1: 9.896847113385965; Model 2: \n9.896847113385965\n...\n\n```", "```scala\n...\n14/11/16 14:57:30 INFO SparkContext: Job finished: mean at \nStreamingModel.scala:159, took 0.069175 s\n\n-------------------------------------------\nTime: 1416142650000 ms\n -------------------------------------------\n\nMSE current batch: Model 1: 75.54543031658632; Model 2: \n10318.213926882852\nRMSE current batch: Model 1: 8.691687426304878; Model 2: \n101.57860959317593\n...\n\n```", "```scala\n...\n14/11/16 17:27:00 INFO SparkContext: Job finished: mean at \nStreamingModel.scala:159, took 0.037856 s\n\n-------------------------------------------\nTime: 1416151620000 ms\n-------------------------------------------\n\nMSE current batch: Model 1: 6.551475362521364; Model 2: \n1.057088005456417E26\nRMSE current batch: Model 1: 2.559584998104451; Model 2: \n1.0281478519436867E13\n...\n\n```"]