["```scala\n $ cd /PATH/spark-ml/Chapter_02/breeze\n\n```", "```scala\n $ cd /PATH/spark-ml/Chapter_02/spark\n\n```", "```scala\n      $ sbt compile\n\n```", "```scala\n $ sbt run\n\n Multiple main classes detected, select one to run:\n ....\n Enter number:\n\n```", "```scala\nimport breeze.linalg.DenseVector \nimport breeze.math.Complex \nval i = Complex.i \n\n// add \nprintln((1 + 2 * i) + (2 + 3 * i)) \n\n// sub \nprintln((1 + 2 * i) - (2 + 3 * i)) \n\n// divide \nprintln((5 + 10 * i) / (3 - 4 * i)) \n\n// mul \nprintln((1 + 2 * i) * (-3 + 6 * i)) \nprintln((1 + 5 * i) * (-3 + 2 * i)) \n\n// neg \nprintln(-(1 + 2 * i)) \n\n// sum of complex numbers \nval x = List((5 + 7 * i), (1 + 3 * i), (13 + 17 * i)) \nprintln(x.sum) \n// product of complex numbers \nval x1 = List((5 + 7 * i), (1 + 3 * i), (13 + 17 * i)) \nprintln(x1.product) \n// sort list of complex numbers \nval x2 = List((5 + 7 * i), (1 + 3 * i), (13 + 17 * i)) \nprintln(x2.sorted) \n\n```", "```scala\n3.0 + 5.0i\n-1.0 + -1.0i -1.0 + 2.0i\n-15.0 + 0.0i\n-13.0 + -13.0i\n-1.0 + -2.0i\n19.0 + 27.0i\n-582.0 + 14.0i\nList(1.0 + 3.0i, 5.0 + 7.0i, 13.0 + 17.0i)\n\n```", "```scala\n[2, 4, 5, 9, 10]\n[3.14159, 2.718281828, \u22121.0, 2.0]\n[1.0, 1.1, 2.0]\n\n```", "```scala\nimport breeze.linalg.DenseVector \nval v = DenseVector(2f, 0f, 3f, 2f, -1f) \nv.update(3, 6f) \nprintln(v) \n\n```", "```scala\nimport breeze.linalg.SparseVectorval sv:SparseVector[Double] = \nSparseVector(5)() \nsv(0) = 1 \nsv(2) = 3 \nsv(4) = 5 \nval m:SparseVector[Double] = sv.mapActivePairs((i,x) => x+1) \nprintln(m) \n\n```", "```scala\nval dVectorOne: Vector = Vectors.dense(1.0, 0.0, 2.0) \nprintln(\"dVectorOne:\" + dVectorOne) \n//  Sparse vector (1.0, 0.0, 2.0, 3.0) \n// corresponding to nonzero entries. \nval sVectorOne: Vector = Vectors.sparse(4,  Array(0, 2,3), \n   Array(1.0, 2.0, 3.0)) \n// Create a sparse vector (1.0, 0.0, 2.0, 2.0) by specifying its \n// nonzero entries. \nval sVectorTwo: Vector = Vectors.sparse(4, Seq((0, 1.0), (2, 2.0), \n  (3, 3.0))) \n\n```", "```scala\ndVectorOne:[1.0,0.0,2.0]\nsVectorOne:(4,[0,2,3],[1.0,2.0,3.0])\nsVectorTwo:(4,[0,2,3],[1.0,2.0,3.0])\n\n```", "```scala\nval sVectorOneMax = sVectorOne.argmax\nval sVectorOneNumNonZeros = sVectorOne.numNonzeros\nval sVectorOneSize = sVectorOne.size\nval sVectorOneArray = sVectorOne.toArray\nval sVectorOneJson = sVectorOne.toJson\n\nprintln(\"sVectorOneMax:\" + sVectorOneMax)\nprintln(\"sVectorOneNumNonZeros:\" + sVectorOneNumNonZeros)\nprintln(\"sVectorOneSize:\" + sVectorOneSize)\nprintln(\"sVectorOneArray:\" + sVectorOneArray)\nprintln(\"sVectorOneJson:\" + sVectorOneJson)\nval dVectorOneToSparse = dVectorOne.toSparse\n\n```", "```scala\nsVectorOneMax:3\nsVectorOneNumNonZeros:3\nsVectorOneSize:4\nsVectorOneArray:[D@38684d54\nsVectorOneJson:{\"type\":0,\"size\":4,\"indices\":[0,2,3],\"values\":\n  [1.0,2.0,3.0]}\ndVectorOneToSparse:(3,[0,2],[1.0,2.0])\n\n```", "```scala\n        // vector's \n        val v1 = DenseVector(3, 7, 8.1, 4, 5) \n        val v2 = DenseVector(1, 9, 3, 2.3, 8) \n        // elementwise add operation \n        def add(): Unit = { \n          println(v1 + v2) \n        } \n\n```", "```scala\n        a   b = |a| \u00d7 |b| \u00d7 cos(\u03b8) OR a   b = ax \u00d7 bx + ay \u00d7 by \n\n        import breeze.linalg.{DenseVector, SparseVector} \n        val a = DenseVector(0.56390, 0.36231, 0.14601, 0.60294, \n           0.14535) \n        val b = DenseVector(0.15951, 0.83671, 0.56002, 0.57797, \n           0.54450) \n       println(a.t * b) \n       println(a dot b) \n\n```", "```scala\n 0.9024889161, 0.9024889161\n\n        import breeze.linalg.{DenseVector, SparseVector} \n        val sva = \n           SparseVector(0.56390,0.36231,0.14601,0.60294,0.14535) \n        val svb = \n           SparseVector(0.15951,0.83671,0.56002,0.57797,0.54450) \n        println(sva.t * svb) \n        println(sva dot svb) \n\n```", "```scala\n        import breeze.linalg.{DenseVector, SparseVector} \n        import breeze.stats.mean \n        val mean = mean(DenseVector(0.0,1.0,2.0)) \n        println(mean) \n\n```", "```scala\n        import breeze.linalg.{DenseVector, SparseVector} \n        import breeze.stats.mean \n        val svm = mean(SparseVector(0.0,1.0,2.0)) \n        val svm1 = mean(SparseVector(0.0,3.0)) \n        println(svm, svm1) \n\n```", "```scala\n        import breeze.linalg.{norm, DenseVector, SparseVector} \n        import breeze.stats.mean \n        val v = DenseVector(-0.4326, -1.6656, 0.1253, 0.2877, -\n          1.1465) \n        val nm = norm(v, 1) \n\n        //Normalizes the argument such that its norm is 1.0 \n        val nmlize = normalize(v) \n\n        // finally check if the norm of normalized vector is 1 or not \n        println(norm(nmlize)) \n\n```", "```scala\n Norm(of dense vector) = 3.6577\n\n Normalized vector is = DenseVector(-0.2068389122442966,  \n      -0.7963728438143791, 0.05990965257561341, 0.1375579173663526,     \n      -0.5481757117154094)\n\n Norm(of normalized vector) = 0.9999999999999999\n\n```", "```scala\n        import breeze.linalg._ \n        val v1 = DenseVector(2, 0, 3, 2, -1) \n        println(argmin(v1)) \n        println(argmax(v1)) \n        println(min(v1)) \n        println(max(v1)) \n\n```", "```scala\n        import breeze.linalg._ \n        val a1 = DenseVector(1, 2, 3) \n        val b1 = DenseVector(1, 4, 1) \n        println((a1 :== b1)) \n        println((a1 :<= b1)) \n        println((a1 :>= b1)) \n        println((a1 :< b1)) \n        println((a1 :> b1)) \n\n```", "```scala\n BitVector(0), BitVector(0, 1), BitVector(0, 2),   \n      BitVector(1),    \n      BitVector(2)\n\n```", "```scala\n        val a = DenseMatrix((1,2),(3,4)) \n          println(\"a : n\" + a) \n         val m = DenseMatrix.zeros[Int](5,5) \n\n        The columns of a matrix can be accessed as Dense Vectors, and    \n        the rows as  Dense Matrices. \n\n          println( \"m.rows :\" + m.rows + \" m.cols : \"  + m.cols) \n          m(::,1) \n         println(\"m : n\" + m) \n\n```", "```scala\n        m(4,::) := DenseVector(5,5,5,5,5).t \n        println(m) \n\n```", "```scala\n\n a : \n 1  2 \n 3  4 \n Created a 5x5 matrix\n 0  0  0  0  0 \n 0  0  0  0  0 \n 0  0  0  0  0 \n 0  0  0  0  0 \n 0  0  0  0  0 \n m.rows :5 m.cols : 5\n First Column of m : \n            DenseVector(0, 0, 0, 0, 0)\n            Assigned 5,5,5,5,5 to last row of m.\n\n 0  0  0  0  0 \n 0  0  0  0  0 \n 0  0  0  0  0 \n      0  0  0  0  0 \n      5  5  5  5  5 \n\n```", "```scala\n        val builder = new CSCMatrix.Builder[Double](rows=10, \n           cols=10) \n        builder.add(3,4, 1.0) \n        // etc. \n        val myMatrix = builder.result() \n\n```", "```scala\n       val dMatrix: Matrix = Matrices.dense(2, 2, Array(1.0, 2.0, 3.0, \n          4.0)) \n        println(\"dMatrix: n\" + dMatrix) \n\n        val sMatrixOne: Matrix = Matrices.sparse(3, 2, Array(0, 1, 3), \n           Array(0, 2, 1), Array(5, 6, 7)) \n        println(\"sMatrixOne: n\" + sMatrixOne) \n\n        val sMatrixTwo: Matrix = Matrices.sparse(3, 2, Array(0, 1, 3), \n           Array(0, 1, 2), Array(5, 6, 7)) \n        println(\"sMatrixTwo: n\" + sMatrixTwo) \n\n```", "```scala\n [info] Running linalg.matrix.SparkMatrix \n dMatrix: \n 1.0  3.0 \n 2.0  4.0 \n sMatrixOne: \n 3 x 2 CSCMatrix\n (0,0) 5.0\n (2,1) 6.0\n (1,1) 7.0\n sMatrixTwo: \n 3 x 2 CSCMatrix\n (0,0) 5.0\n (1,1) 6.0\n (2,1) 7.0\n\n```", "```scala\nval spConfig = (new \n    SparkConf).setMaster(\"local\").setAppName(\"SparkApp\") \n     val sc = new SparkContext(spConfig) \n     val denseData = Seq( \n       Vectors.dense(0.0, 1.0, 2.1), \n       Vectors.dense(3.0, 2.0, 4.0), \n       Vectors.dense(5.0, 7.0, 8.0), \n       Vectors.dense(9.0, 0.0, 1.1) \n     ) \n     val sparseData = Seq( \n       Vectors.sparse(3, Seq((1, 1.0), (2, 2.1))), \n       Vectors.sparse(3, Seq((0, 3.0), (1, 2.0), (2, 4.0))), \n       Vectors.sparse(3, Seq((0, 5.0), (1, 7.0), (2, 8.0))), \n       Vectors.sparse(3, Seq((0, 9.0), (2, 1.0))) \n     ) \n\nval denseMat = new RowMatrix(sc.parallelize(denseData, 2)) \nval sparseMat = new RowMatrix(sc.parallelize(sparseData, 2)) \n\nprintln(\"Dense Matrix - Num of Rows :\" + denseMat.numRows()) \nprintln(\"Dense Matrix - Num of Cols:\" + denseMat.numCols()) \nprintln(\"Sparse Matrix - Num of Rows :\" + sparseMat.numRows()) \nprintln(\"Sparse Matrix - Num of Cols:\" + sparseMat.numCols()) \n\nsc.stop() \n\n```", "```scala\nUsing Spark's default log4j profile: \norg/apache/spark/log4j-  \ndefaults.properties\n16/01/27 04:51:59 INFO SparkContext: Running Spark version \n1.6.0\nDense Matrix - Num of Rows :4\nDense Matrix - Num of Cols:3\n...\nSparse Matrix - Num of Rows :4\nSparse Matrix - Num of Cols :3\n\n```", "```scala\nval data = Seq(\n(0L, Vectors.dense(0.0, 1.0, 2.0)),\n(1L, Vectors.dense(3.0, 4.0, 5.0)),\n(3L, Vectors.dense(9.0, 0.0, 1.0))\n).map(x => IndexedRow(x._1, x._2))\nval indexedRows: RDD[IndexedRow] = sc.parallelize(data, 2)\nval indexedRowsMat = new IndexedRowMatrix(indexedRows)\n println(\"Indexed Row Matrix - No of Rows: \" + \nindexedRowsMat.numRows())\n println(\"Indexed Row Matrix - No of Cols: \" + \nindexedRowsMat.numCols())\n\n```", "```scala\nIndexed Row Matrix - No of Rows: 4\nIndexed Row Matrix - No of Cols: 3\n\n```", "```scala\nval entries = sc.parallelize(Seq( \n      (0, 0, 1.0), \n      (0, 1, 2.0), \n      (1, 1, 3.0), \n      (1, 2, 4.0), \n      (2, 2, 5.0), \n      (2, 3, 6.0), \n      (3, 0, 7.0), \n      (3, 3, 8.0), \n      (4, 1, 9.0)), 3).map { case (i, j, value) => \n      MatrixEntry(i, j, value) \n    } \nval coordinateMat = new CoordinateMatrix(entries) \nprintln(\"Coordinate Matrix - No of Rows: \" + \n  coordinateMat.numRows()) \nprintln(\"Coordinate Matrix - No of Cols: \" + \n  coordinateMat.numCols()) \n\n```", "```scala\nCoordinate Matrix - No of Rows: 5\nCoordinate - No of Cols: 4\n\n```", "```scala\n        val a = DenseMatrix((1,2),(3,4)) \n        val b = DenseMatrix((2,2),(2,2)) \n        val c = a + b \n        println(\"a: n\" + a) \n        println(\"b: n\" + b) \n        println(\"a + b : n\" + c) \n\n```", "```scala\n a:1  2 \n 3  4 \n b: 2  2 \n 2  2 \n a + b : \n 3  4 \n 5  6 \n\n```", "```scala\n        a :* b  \n        val d = a*b \n        println(\"Dot product a*b : n\" + d) \n\n```", "```scala\n Dot product a*b :\n 6   6 \n 14  14\n\n```", "```scala\n        a :< b \n\n```", "```scala\n a :< b \n false  false \n false  false\n\n```", "```scala\n Inplace Addition : a :+= 1\n 2  3 \n 4  5 \n      value = a :+= 1\n println(\"Inplace Addition : a :+= 1n\" + e)\n\n```", "```scala\n        val sumA = sum(a) \n        println(\"sum(a):n\" + sumA) \n\n```", "```scala\n sum(a):\n 14\n\n```", "```scala\n        a.max\n\n```", "```scala\n        println(\"a.max:n\" + a.max) \n\n```", "```scala\n        println(\"argmax(a):n\" + argmax(a)) \n\n```", "```scala\n argmax(a):\n (1,1)\n\n```", "```scala\n        val g = DenseMatrix((1.1, 1.2), (3.9, 3.5)) \n        println(\"g: n\" + g) \n        val gCeil =ceil(g) \n        println(\"ceil(g)n \" + gCeil) \n\n```", "```scala\n g: \n          1.1  1.2 \n          3.9  3.5 \n\n ceil(g)\n          2.0  2.0 \n          4.0  4.0 \n\n```", "```scala\n        val gFloor =floor(g) \n        println(\"floor(g)n\" + gFloor) \n\n```", "```scala\n floor(g)\n 1.0  1.0\n 3.0  3.0\n\n```", "```scala\nval detm: Matrix = Matrices.dense(3, 3, Array(1.0, 3.0, 5.0, 2.0, \n  4.0, 6.0, 2.0, 4.0, 5.0)) \nprint(det(detm)) \n\n```", "```scala\nval A = DenseMatrix((9.0,0.0,0.0),(0.0,82.0,0.0),(0.0,0.0,25.0)) \nval es = eigSym(A) \nval lambda = es.eigenvalues \nval evs = es.eigenvectors \nprintln(\"lambda is : \" + lambda) \nprintln(\"evs is : \" + evs) \n\n```", "```scala\nlambda is : DenseVector(9.0, 25.0, 82.0)\nevs is : 1.0  0.0  0.0 \n0.0  0.0  1.0 \n0.0  1.0  -0.0 \n\n```", "```scala\npackage linalg.svd \n\nimport org.apache.spark.{SparkConf, SparkContext} \nimport org.apache.spark.mllib.linalg.distributed.RowMatrix \nimport org.apache.spark.mllib.linalg.{Matrix,       \nSingularValueDecomposition, Vector, Vectors} \nobject SparkSVDExampleOne { \n\n  def main(args: Array[String]) { \n    val denseData = Seq( \n      Vectors.dense(0.0, 1.0, 2.0, 1.0, 5.0, 3.3, 2.1), \n      Vectors.dense(3.0, 4.0, 5.0, 3.1, 4.5, 5.1, 3.3), \n      Vectors.dense(6.0, 7.0, 8.0, 2.1, 6.0, 6.7, 6.8), \n      Vectors.dense(9.0, 0.0, 1.0, 3.4, 4.3, 1.0, 1.0) \n    ) \n    val spConfig = (new \n      SparkConf).setMaster(\"local\").setAppName(\"SparkSVDDemo\") \n    val sc = new SparkContext(spConfig) \n    val mat: RowMatrix = new RowMatrix(sc.parallelize(denseData, 2)) \n\n     // Compute the top 20 singular values and corresponding    \n       singular vectors. \n    val svd: SingularValueDecomposition[RowMatrix, Matrix] = \n    mat.computeSVD(7, computeU = true) \n    val U: RowMatrix = svd.U // The U factor is a RowMatrix. \n    val s: Vector = svd.s // The singular values are stored in a \n      local dense  vector. \n    val V: Matrix = svd.V // The V factor is a local dense matrix. \n    println(\"U:\" + U) \n    println(\"s:\" + s) \n    println(\"V:\" + V) \n    sc.stop() \n  } \n}\n\n```", "```scala\n// The data \nval msData = DenseMatrix( \n  (2.5,2.4), (0.5,0.7), (2.2,2.9), (1.9,2.2), (3.1,3.0), \n  (2.3,2.7), (2.0,1.6), (1.0,1.1), (1.5,1.6), (1.1,0.9)) \n\ndef main(args: Array[String]): Unit = { \n       val pca = breeze.linalg.princomp(msData) \n\n       print(\"Center\" , msData(*,::) - pca.center) \n\n       //the covariance matrix of the data \n\n       print(\"covariance matrix\", pca.covmat) \n\n       // the eigenvalues of the covariance matrix, IN SORTED ORDER \n       print(\"eigen values\",pca.eigenvalues) \n\n       // eigenvectors \n       print(\"eigen vectors\",pca.loadings) \n       print(pca.scores) \n} \n\n```", "```scala\neigen values = DenseVector(1.2840277121727839, 0.04908339893832732)\neigen vectors = -0.6778733985280118  -0.735178655544408 \n\n```", "```scala\nobject SparkSGD { \n def main(args: Array[String]): Unit = { \n    val m = 4 \n    val n = 200000 \n    val sc = new SparkContext(\"local[2]\", \"\") \n    val points = sc.parallelize(0 until m, \n      2).mapPartitionsWithIndex { (idx, iter) => \n      val random = new Random(idx) \n      iter.map(i => (1.0, \n       Vectors.dense(Array.fill(n)(random.nextDouble())))) \n    }.cache() \n    val (weights, loss) = GradientDescent.runMiniBatchSGD( \n      points, \n      new LogisticGradient, \n      new SquaredL2Updater, \n      0.1, \n      2, \n      1.0, \n      1.0, \n      Vectors.dense(new Array[Double](n))) \n    println(\"w:\"  + weights(0)) \n    println(\"loss:\" + loss(0)) \n    sc.stop() \n  } \n\n```", "```scala\npackage linalg.plot \nimport breeze.linalg._ \nimport breeze.plot._ \n\nobject BreezePlotSampleOne { \n  def main(args: Array[String]): Unit = { \n\n    val f = Figure() \n    val p = f.subplot(0) \n    val x = DenseVector(0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8) \n    val y = DenseVector(1.1, 2.1, 0.5, 1.0,3.0, 1.1, 0.0, 0.5,2.5) \n    p += plot(x,  y) \n    p.xlabel = \"x axis\" \n    p.ylabel = \"y axis\" \n    f.saveas(\"lines-graph.png\") \n  } \n } \n\n```", "```scala\npackage linalg.plot \nimport breeze.linalg._ \nimport breeze.plot._ \n\nobject BreezePlotGaussian { \n  def main(args: Array[String]): Unit = { \n    val f = Figure() \n    val p = f.subplot(2, 1, 1) \n    val g = breeze.stats.distributions.Gaussian(0, 1) \n    p += hist(g.sample(100000), 100) \n    p.title = \"A normal distribution\" \n    f.saveas(\"plot-gaussian-100000.png\") \n  } \n } \n\n```"]