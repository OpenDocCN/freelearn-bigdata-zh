["```scala\nArray2DRowRealMatrix \n{{0.5306513708,0.5144338501,0.5183049}, \n{0.0612665269,0.0595122885,0.0611548878}, \n{0.3215637836,0.2964382622,0.1439834964}}\n\n```", "```scala\nms = {RealVector[3]@3600} \n 0 = {ArrayRealVector@3605} \"{0.489603683; 0.5979051631}\" \n 1 = {ArrayRealVector@3606} \"{0.2069873135; 0.4887559609}\" \n 2 = {ArrayRealVector@3607} \"{0.5286582698; 0.6787608323}\"\n\n```", "```scala\nus = {RealVector[3]@3602} \n 0 = {ArrayRealVector@3611} \"{0.7964247309; 0.091570682}\" \n 1 = {ArrayRealVector@3612} \"{0.4509758768; 0.0684475614}\" \n 2 = {ArrayRealVector@3613} \"{0.7812240904; 0.4180722562}\"\n\n```", "```scala\nm: {0.489603683; 0.5979051631} \nus: [Lorg.apache.commons.math3.linear.RealVector;@75961f16 \n XtX: Array2DRowRealMatrix{{0.0,0.0},{0.0,0.0}} \n Xty: {0; 0}\n\n```", "```scala\nu: {0.7964247309; 0.091570682} \nu.outerProduct(u): \n   Array2DRowRealMatrix{{0.634292352,0.0729291558},\n   {0.0729291558,0.0083851898}} \nXtX = XtX.add(u.outerProduct(u)): \n   Array2DRowRealMatrix{{0.634292352,0.0729291558},\n   {0.0729291558,0.0083851898}} \nR.getEntry(i, j)):0.5306513708051035 \nu.mapMultiply(R.getEntry(i, j): {0.4226238752; 0.0485921079} \nXty = Xty.add(u.mapMultiply(R.getEntry(i, j))): {0.4226238752; \n   0.0485921079}\n\n```", "```scala\nu: {0.4509758768; 0.0684475614} \nu.outerProduct(u): Array2DRowRealMatrix{{0.2033792414,0.030868199},{0.030868199,0.0046850687}} \nXtX = XtX.add(u.outerProduct(u)): Array2DRowRealMatrix{{0.8376715935,0.1037973548},{0.1037973548,0.0130702585}} \nR.getEntry(i, j)):0.5144338501354986 \nu.mapMultiply(R.getEntry(i, j): {0.2319972566; 0.0352117425} \nXty = Xty.add(u.mapMultiply(R.getEntry(i, j))): {0.6546211318; 0.0838038505}\n\n```", "```scala\nu: {0.7812240904; 0.4180722562} \nu.outerProduct(u): \n   Array2DRowRealMatrix{{0.6103110794,0.326608118},\n   {0.326608118,0.1747844114}} \nXtX = XtX.add(u.outerProduct(u)): \n   Array2DRowRealMatrix{{1.4479826729,0.4304054728},\n   {0.4304054728,0.1878546698}} \nR.getEntry(i, j)):0.5183049000396933 \nu.mapMultiply(R.getEntry(i, j): {0.4049122741; 0.2166888989} \nXty = Xty.add(u.mapMultiply(R.getEntry(i, j))): {1.0595334059; \n   0.3004927494} \nAfter Regularization XtX: \n   Array2DRowRealMatrix{{1.4779826729,0.4304054728},\n   {0.4304054728,0.1878546698}} \nAfter Regularization XtX: Array2DRowRealMatrix{{1.4779826729,0.4304054728},{0.4304054728,0.2178546698}}\n\n```", "```scala\nCholeskyDecomposition{0.7422344051; -0.0870718111}\n\n```", "```scala\nms = {RealVector[3]@5078} \n 0 = {ArrayRealVector@5125} \"{0.7422344051; -0.0870718111}\" \n 1 = {ArrayRealVector@5126} \"{0.0856607011; -0.007426896}\" \n 2 = {ArrayRealVector@5127} \"{0.4542083563; -0.392747909}\"\n\n```", "```scala\nobject AlternatingLeastSquares { \n\n  var movies = 0 \n  var users = 0 \n  var features = 0 \n  var ITERATIONS = 0 \n  val LAMBDA = 0.01 // Regularization coefficient \n\n  private def vector(n: Int): RealVector = \n    new ArrayRealVector(Array.fill(n)(math.random)) \n\n  private def matrix(rows: Int, cols: Int): RealMatrix = \n    new Array2DRowRealMatrix(Array.fill(rows, cols)(math.random)) \n\n  def rSpace(): RealMatrix = { \n    val mh = matrix(movies, features) \n    val uh = matrix(users, features) \n    mh.multiply(uh.transpose()) \n  } \n\n  def rmse(targetR: RealMatrix, ms: Array[RealVector], us: \n   Array[RealVector]): Double = { \n    val r = new Array2DRowRealMatrix(movies, users) \n    for (i <- 0 until movies; j <- 0 until users) { \n      r.setEntry(i, j, ms(i).dotProduct(us(j))) \n    } \n    val diffs = r.subtract(targetR) \n    var sumSqs = 0.0 \n    for (i <- 0 until movies; j <- 0 until users) { \n      val diff = diffs.getEntry(i, j) \n      sumSqs += diff * diff \n    } \n    math.sqrt(sumSqs / (movies.toDouble * users.toDouble)) \n  } \n\n  def update(i: Int, m: RealVector, us: Array[RealVector], R: \n   RealMatrix) : RealVector = { \n    val U = us.length \n    val F = us(0).getDimension \n    var XtX: RealMatrix = new Array2DRowRealMatrix(F, F) \n    var Xty: RealVector = new ArrayRealVector(F) \n    // For each user that rated the movie \n    for (j <- 0 until U) { \n      val u = us(j) \n      // Add u * u^t to XtX \n      XtX = XtX.add(u.outerProduct(u)) \n      // Add u * rating to Xty \n      Xty = Xty.add(u.mapMultiply(R.getEntry(i, j))) \n    } \n    // Add regularization coefs to diagonal terms \n    for (d <- 0 until F) { \n      XtX.addToEntry(d, d, LAMBDA * U) \n    } \n    // Solve it with Cholesky \n    new CholeskyDecomposition(XtX).getSolver.solve(Xty) \n  } \n\n  def main(args: Array[String]) { \n\n    movies = 100 \n    users = 500 \n    features = 10 \n    ITERATIONS = 5 \n    var slices = 2 \n\n    val spark = \n     SparkSession.builder.master(\"local[2]\").\n     appName(\"AlternatingLeastS\n   quares\").getOrCreate() \n    val sc = spark.sparkContext \n\n    val r_space = rSpace() \n\n    // Initialize m and u randomly \n    var ms = Array.fill(movies)(vector(features)) \n    var us = Array.fill(users)(vector(features)) \n\n    // Iteratively update movies then users \n    val Rc = sc.broadcast(r_space) \n    var msb = sc.broadcast(ms) \n    var usb = sc.broadcast(us) \n    for (iter <- 1 to ITERATIONS) { \n      println(s\"Iteration $iter:\") \n      ms = sc.parallelize(0 until movies, slices) \n        .map(i => update(i, msb.value(i), usb.value, Rc.value)) \n        .collect() \n      msb = sc.broadcast(ms) // Re-broadcast ms because it was \n   updated \n      us = sc.parallelize(0 until users, slices) \n        .map(i => update(i, usb.value(i), msb.value, \n   Rc.value.transpose())) \n        .collect() \n      usb = sc.broadcast(us) // Re-broadcast us because it was \n   updated \n      println(\"RMSE = \" + rmse(r_space, ms, us)) \n      println() \n    } \n\n    spark.stop() \n  } \n}\n\n```", "```scala\nobject FeatureExtraction { \n\ndef getFeatures(): Dataset[FeatureExtraction.Rating] = { \n  val spark = SparkSession.builder.master(\"local[2]\").appName(\"FeatureExtraction\").getOrCreate() \n\n  import spark.implicits._ \n  val ratings = spark.read.textFile(\"/data/ml-100k 2/u.data\").map(parseRating) \n  println(ratings.first()) \n\n  return ratings \n} \n\ncase class Rating(userId: Int, movieId: Int, rating: Float) \ndef parseRating(str: String): Rating = { \n  val fields = str.split(\"t\") \n  Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat) \n}\n\n```", "```scala\n16/09/07 11:23:38 INFO CodeGenerator: Code generated in 7.029838 ms\n16/09/07 11:23:38 INFO Executor: Finished task 0.0 in stage 0.0 (TID \n   0). 1276 bytes result sent to driver\n16/09/07 11:23:38 INFO TaskSetManager: Finished task 0.0 in stage 0.0 \n   (TID 0) in 82 ms on localhost (1/1)\n16/09/07 11:23:38 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose \n   tasks have all completed, from pool\n16/09/07 11:23:38 INFO DAGScheduler: ResultStage 0 (first at \n   FeatureExtraction.scala:25) finished in 0.106 s\n16/09/07 11:23:38 INFO DAGScheduler: Job 0 finished: first at \n   FeatureExtraction.scala:25, took 0.175165 s\n16/09/07 11:23:38 INFO CodeGenerator: Code generated in 6.834794 ms\nRating(196,242,3.0)\n\n```", "```scala\ncase class Rating(userId: Int, movieId: Int, rating: Float) \ndef parseRating(str: String): Rating = { \n  val fields = str.split(\"t\") \n  Rating(fields(0).toInt, fields(1).toInt, fields(2).toFloat) \n}\n\n```", "```scala\ndef createALSModel() { \n  val ratings = FeatureExtraction.getFeatures(); \n\n  val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2)) \n  println(training.first()) \n}\n\n```", "```scala\n16/09/07 13:23:28 INFO Executor: Finished task 0.0 in stage 1.0 (TID \n   1). 1768 bytes result sent to driver\n16/09/07 13:23:28 INFO TaskSetManager: Finished task 0.0 in stage 1.0 \n   (TID 1) in 459 ms on localhost (1/1)\n16/09/07 13:23:28 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose \n   tasks have all completed, from pool\n16/09/07 13:23:28 INFO DAGScheduler: ResultStage 1 (first at \n   FeatureExtraction.scala:34) finished in 0.459 s\n16/09/07 13:23:28 INFO DAGScheduler: Job 1 finished: first at \n   FeatureExtraction.scala:34, took 0.465730 s\nRating(1,1,5.0)\n\n```", "```scala\n// Build the recommendation model using ALS on the training data \nval als = new ALS() \n  .setMaxIter(5) \n  .setRegParam(0.01) \n  .setUserCol(\"userId\") \n  .setItemCol(\"movieId\") \n  .setRatingCol(\"rating\") \n\nval model = als.fit(training)\n\n```", "```scala\n16/09/07 13:08:16 INFO MapPartitionsRDD: Removing RDD 16 from \n   persistence list\n16/09/07 13:08:16 INFO BlockManager: Removing RDD 16\n16/09/07 13:08:16 INFO Instrumentation: ALS-als_1ca69e2ffef7-\n   10603412-1: training finished\n16/09/07 13:08:16 INFO SparkContext: Invoking stop() from shutdown \n   hook\n[id: int, features: array<float>]\n\n```", "```scala\nmodel.userFactors.count()\n\n```", "```scala\n16/09/07 13:21:54 INFO Executor: Running task 0.0 in stage 53.0 (TID \n   166)\n16/09/07 13:21:54 INFO ShuffleBlockFetcherIterator: Getting 10 non-\n   empty blocks out of 10 blocks\n16/09/07 13:21:54 INFO ShuffleBlockFetcherIterator: Started 0 remote \n   fetches in 0 ms\n16/09/07 13:21:54 INFO Executor: Finished task 0.0 in stage 53.0 (TID \n   166). 1873 bytes result sent to driver\n16/09/07 13:21:54 INFO TaskSetManager: Finished task 0.0 in stage \n   53.0 (TID 166) in 12 ms on localhost (1/1)\n16/09/07 13:21:54 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose \n   tasks have all completed, from pool\n16/09/07 13:21:54 INFO DAGScheduler: ResultStage 53 (count at \n   ALSModeling.scala:25) finished in 0.012 s\n16/09/07 13:21:54 INFO DAGScheduler: Job 7 finished: count at \n   ALSModeling.scala:25, took 0.123073 s\n16/09/07 13:21:54 INFO CodeGenerator: Code generated in 11.162514 ms\n943\n\n```", "```scala\nmodel.itemFactors.count()\n\n```", "```scala\n16/09/07 13:23:32 INFO TaskSetManager: Starting task 0.0 in stage \n   68.0 (TID 177, localhost, partition 0, ANY, 5276 bytes)\n16/09/07 13:23:32 INFO Executor: Running task 0.0 in stage 68.0 (TID \n   177)\n16/09/07 13:23:32 INFO ShuffleBlockFetcherIterator: Getting 10 non-\n   empty blocks out of 10 blocks\n16/09/07 13:23:32 INFO ShuffleBlockFetcherIterator: Started 0 remote \n   fetches in 0 ms\n16/09/07 13:23:32 INFO Executor: Finished task 0.0 in stage 68.0 (TID \n   177). 1873 bytes result sent to driver\n16/09/07 13:23:32 INFO TaskSetManager: Finished task 0.0 in stage \n   68.0 (TID 177) in 3 ms on localhost (1/1)\n16/09/07 13:23:32 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose \n   tasks have all completed, from pool\n16/09/07 13:23:32 INFO DAGScheduler: ResultStage 68 (count at \n   ALSModeling.scala:26) finished in 0.003 s\n16/09/07 13:23:32 INFO DAGScheduler: Job 8 finished: count at \n   ALSModeling.scala:26, took 0.072450 s\n\n1651\n\n```", "```scala\nval als = new ALS() \n  .setMaxIter(5) \n  .setRegParam(0.01) \n  .setImplicitPrefs(true) \n  .setUserCol(\"userId\") \n  .setItemCol(\"movieId\") \n  .setRatingCol(\"rating\")\n\n```", "```scala\nobject ALSModeling { \n\n  def createALSModel() { \n    val ratings = FeatureExtraction.getFeatures(); \n\n    val Array(training, test) = ratings.randomSplit(Array(0.8, \n   0.2)) \n    println(training.first()) \n\n    // Build the recommendation model using ALS on the training \n   data \n    val als = new ALS() \n      .setMaxIter(5) \n      .setRegParam(0.01) \n      .setUserCol(\"userId\") \n      .setItemCol(\"movieId\") \n      .setRatingCol(\"rating\") \n\n    val model = als.fit(training) \n    println(model.userFactors.count()) \n    println(model.itemFactors.count()) \n\n    val predictions = model.transform(test) \n    println(predictions.printSchema()) \n\n}\n\n```", "```scala\n16/09/07 17:58:42 INFO SparkContext: Created broadcast 26 from \n   broadcast at DAGScheduler.scala:1012\n16/09/07 17:58:42 INFO DAGScheduler: Submitting 1 missing tasks from \n   ResultStage 67 (MapPartitionsRDD[138] at count at \n   ALSModeling.scala:31)\n16/09/07 17:58:42 INFO TaskSchedulerImpl: Adding task set 67.0 with 1 \n   tasks\n16/09/07 17:58:42 INFO TaskSetManager: Starting task 0.0 in stage \n   67.0 (TID 176, localhost, partition 0, ANY, 5276 bytes)\n16/09/07 17:58:42 INFO Executor: Running task 0.0 in stage 67.0 (TID \n   176)\n16/09/07 17:58:42 INFO ShuffleBlockFetcherIterator: Getting 10 non-\n   empty blocks out of 10 blocks\n16/09/07 17:58:42 INFO ShuffleBlockFetcherIterator: Started 0 remote \n   fetches in 0 ms\n16/09/07 17:58:42 INFO Executor: Finished task 0.0 in stage 67.0 (TID \n   176). 1960 bytes result sent to driver\n16/09/07 17:58:42 INFO TaskSetManager: Finished task 0.0 in stage \n   67.0 (TID 176) in 3 ms on localhost (1/1)\n16/09/07 17:58:42 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose \n   tasks have all completed, from pool\n16/09/07 17:58:42 INFO DAGScheduler: ResultStage 67 (count at \n   ALSModeling.scala:31) finished in 0.003 s\n16/09/07 17:58:42 INFO DAGScheduler: Job 7 finished: count at \n   ALSModeling.scala:31, took 0.060748 s\n100\nroot\n |-- userId: integer (nullable = true)\n |-- movieId: integer (nullable = true)\n |-- rating: float (nullable = true)\n |-- timestamp: long (nullable = true)\n |-- prediction: float (nullable = true)\n\n```", "```scala\nval predictedRating = model.predict(789, 123)\n\n```", "```scala\n14/03/30 16:10:10 INFO SparkContext: Starting job: lookup at \n   MatrixFactorizationModel.scala:45\n14/03/30 16:10:10 INFO DAGScheduler: Got job 30 (lookup at \n   MatrixFactorizationModel.scala:45) with 1 output partitions \n   (allowLocal=false)\n...\n14/03/30 16:10:10 INFO SparkContext: Job finished: lookup at \n   MatrixFactorizationModel.scala:46, took 0.023077 s\npredictedRating: Double = 3.128545693368485\n\n```", "```scala\nval userId = 789 \nval K = 10 \nval topKRecs = model.recommendProducts(userId, K)\n\n```", "```scala\nprintln(topKRecs.mkString(\"n\"))\n\n```", "```scala\nRating(789,715,5.931851273771102)\nRating(789,12,5.582301095666215)\nRating(789,959,5.516272981542168)\nRating(789,42,5.458065302395629)\nRating(789,584,5.449949837103569)\nRating(789,750,5.348768847643657)\nRating(789,663,5.30832117499004)\nRating(789,134,5.278933936827717)\nRating(789,156,5.250959077906759)\nRating(789,432,5.169863417126231)\n\n```", "```scala\nval movies = sc.textFile(\"/PATH/ml-100k/u.item\") \nval titles = movies.map(line => \n   line.split(\"|\").take(2)).map(array => (array(0).toInt,\n   array(1))).collectAsMap() \ntitles(123)\n\n```", "```scala\nres68: String = Frighteners, The (1996)\n\n```", "```scala\nval moviesForUser = ratings.keyBy(_.user).lookup(789)\n\n```", "```scala\nprintln(moviesForUser.size)\n\n```", "```scala\nmoviesForUser.sortBy(-_.rating).take(10).map(rating => \n   (titles(rating.product), rating.rating)).foreach(println)\n\n```", "```scala\n(Godfather, The (1972),5.0)\n(Trainspotting (1996),5.0)\n(Dead Man Walking (1995),5.0)\n(Star Wars (1977),5.0)\n(Swingers (1996),5.0)\n(Leaving Las Vegas (1995),5.0)\n(Bound (1996),5.0)\n(Fargo (1996),5.0)\n(Last Supper, The (1995),5.0)\n(Private Parts (1997),4.0)\n\n```", "```scala\ntopKRecs.map(rating => (titles(rating.product), \n   rating.rating)).foreach(println)\n\n```", "```scala\n(To Die For (1995),5.931851273771102)\n(Usual Suspects, The (1995),5.582301095666215)\n(Dazed and Confused (1993),5.516272981542168)\n(Clerks (1994),5.458065302395629)\n(Secret Garden, The (1993),5.449949837103569)\n(Amistad (1997),5.348768847643657)\n(Being There (1979),5.30832117499004)\n(Citizen Kane (1941),5.278933936827717)\n(Reservoir Dogs (1992),5.250959077906759)\n(Fantasia (1940),5.169863417126231)\n\n```", "```scala\nimport org.jblas.DoubleMatrix\n\n```", "```scala\npublic DoubleMatrix(double[] newData)\n\n```", "```scala\nval aMatrix = new DoubleMatrix(Array(1.0, 2.0, 3.0))\n\n```", "```scala\naMatrix: org.jblas.DoubleMatrix = [1.000000; 2.000000; 3.000000]\n\n```", "```scala\ndef cosineSimilarity(vec1: DoubleMatrix, vec2: DoubleMatrix): Double = { \n  vec1.dot(vec2) / (vec1.norm2() * vec2.norm2()) \n}\n\n```", "```scala\nval itemId = 567 \nval itemFactor = model.productFeatures.lookup(itemId).head \nval itemVector = new DoubleMatrix(itemFactor) \ncosineSimilarity(itemVector, itemVector)\n\n```", "```scala\nres113: Double = 1.0\n\n```", "```scala\nval sims = model.productFeatures.map{ case (id, factor) => \n  val factorVector = new DoubleMatrix(factor) \n  val sim = cosineSimilarity(factorVector, itemVector) \n  (id, sim) \n}\n\n```", "```scala\n// recall we defined K = 10 earlier \nval sortedSims = sims.top(K)(Ordering.by[(Int, Double), Double] { \n   case (id, similarity) => similarity })\n\n```", "```scala\nprintln(sortedSims.take(10).mkString(\"n\"))\n\n```", "```scala\n(567,1.0000000000000002)\n(1471,0.6932331537649621)\n(670,0.6898690594544726)\n(201,0.6897964975027041)\n(343,0.6891221044611473)\n(563,0.6864214133620066)\n(294,0.6812075443259535)\n(413,0.6754663844488256)\n(184,0.6702643811753909)\n(109,0.6594872765176396)\n\n```", "```scala\nprintln(titles(itemId))\n\n```", "```scala\n    Wes Craven's New Nightmare (1994)\n\n```", "```scala\nval sortedSims2 = sims.top(K + 1)(Ordering.by[(Int, Double), \n   Double] { case (id, similarity) => similarity }) \nsortedSims2.slice(1, 11).map{ case (id, sim) => (titles(id), sim) \n   }.mkString(\"n\")\n\n```", "```scala\n(Hideaway (1995),0.6932331537649621)\n(Body Snatchers (1993),0.6898690594544726)\n(Evil Dead II (1987),0.6897964975027041)\n(Alien: Resurrection (1997),0.6891221044611473)\n(Stephen King's The Langoliers (1995),0.6864214133620066)\n(Liar Liar (1997),0.6812075443259535)\n(Tales from the Crypt Presents: Bordello of Blood (1996),0.6754663844488256)\n(Army of Darkness (1993),0.6702643811753909)\n(Mystery Science Theater 3000: The Movie (1996),0.6594872765176396)\n(Scream (1996),0.6538249646863378)\n\n```", "```scala\nobject ALSModeling { \n\n  def createALSModel() { \n    val ratings = FeatureExtraction.getFeatures(); \n\n    val Array(training, test) = ratings.randomSplit(Array(0.8, 0.2)) \n    println(training.first()) \n\n    // Build the recommendation model using ALS on the training data \n    val als = new ALS() \n      .setMaxIter(5) \n      .setRegParam(0.01) \n      .setUserCol(\"userId\") \n      .setItemCol(\"movieId\") \n      .setRatingCol(\"rating\") \n\n    val model = als.fit(training) \n    println(model.userFactors.count()) \n    println(model.itemFactors.count()) \n\n    val predictions = model.transform(test) \n    println(predictions.printSchema()) \n\n    val evaluator = new RegressionEvaluator() \n      .setMetricName(\"rmse\") \n      .setLabelCol(\"rating\") \n      .setPredictionCol(\"prediction\") \n    val rmse = evaluator.evaluate(predictions) \n\n    println(s\"Root-mean-square error = $rmse\") \n  } \n\n  def main(args: Array[String]) { \n    createALSModel() \n  } \n\n}\n\n```", "```scala\n16/09/07 17:58:45 INFO ShuffleBlockFetcherIterator: Getting 4 non-\n   empty blocks out of 200 blocks\n16/09/07 17:58:45 INFO ShuffleBlockFetcherIterator: Getting 2 non-\n   empty blocks out of 200 blocks\n16/09/07 17:58:45 INFO ShuffleBlockFetcherIterator: Started 0 remote \n   fetches in 0 ms\n16/09/07 17:58:45 INFO ShuffleBlockFetcherIterator: Started 0 remote \n   fetches in 0 ms\n16/09/07 17:58:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-\n   empty blocks out of 10 blocks\n16/09/07 17:58:45 INFO ShuffleBlockFetcherIterator: Getting 1 non-\n   empty blocks out of 10 blocks\n16/09/07 17:58:45 INFO ShuffleBlockFetcherIterator: Started 0 remote \n   fetches in 0 ms\n16/09/07 17:58:45 INFO ShuffleBlockFetcherIterator: Started 0 remote \n   fetches in 0 ms\nRoot-mean-square error = 2.1487554400294777\n\n```", "```scala\nval actualRating = moviesForUser.take(1)(0)\n\n```", "```scala\nactualRating: org.apache.spark.mllib.recommendation.Rating = \n   Rating(789,1012,4.0)\n\n```", "```scala\nval predictedRating = model.predict(789, actualRating.product)\n\n```", "```scala\n...\n14/04/13 13:01:15 INFO SparkContext: Job finished: lookup at MatrixFactorizationModel.scala:46, took 0.025404 s\npredictedRating: Double = 4.001005374200248\n\n```", "```scala\nval squaredError = math.pow(predictedRating - actualRating.rating, \n   2.0)\n\n```", "```scala\nsquaredError: Double = 1.010777282523947E-6\n\n```", "```scala\nval usersProducts = ratings.map{ case Rating(user, product, \n   rating)  => (user, product)} \nval predictions = model.predict(usersProducts).map{ \n    case Rating(user, product, rating) => ((user, product), \n   rating) \n}\n\n```", "```scala\nval ratingsAndPredictions = ratings.map{ \n  case Rating(user, product, rating) => ((user, product), rating) \n}.join(predictions)\n\n```", "```scala\nval MSE = ratingsAndPredictions.map{ \n    case ((user, product), (actual, predicted)) =>  math.pow((actual - predicted), 2) \n}.reduce(_ + _) / ratingsAndPredictions.count \nprintln(\"Mean Squared Error = \" + MSE)\n\n```", "```scala\nMean Squared Error = 0.08231947642632852\n\n```", "```scala\nval RMSE = math.sqrt(MSE) \nprintln(\"Root Mean Squared Error = \" + RMSE)\n\n```", "```scala\nRoot Mean Squared Error = 0.2869137090247319\n\n```", "```scala\ndef avgPrecisionK(actual: Seq[Int], predicted: Seq[Int], k: Int): \n   Double = { \n    val predK = predicted.take(k) \n    var score = 0.0 \n    var numHits = 0.0 \n    for ((p, i) <- predK.zipWithIndex) { \n      if (actual.contains(p)) { \n        numHits += 1.0 \n        score += numHits / (i.toDouble + 1.0) \n      } \n    } \n    if (actual.isEmpty) { \n      1.0 \n    } else { \n      score / scala.math.min(actual.size, k).toDouble \n    } \n  }\n\n```", "```scala\nval actualMovies = moviesForUser.map(_.product)\n\n```", "```scala\nactualMovies: Seq[Int] = ArrayBuffer(1012, 127, 475, 93, 1161, 286, \n   293, 9, 50, 294, 181, 1, 1008, 508, 284, 1017, 137, 111, 742, 248, \n   249, 1007, 591, 150, 276, 151, 129, 100, 741, 288, 762, 628, 124)\n\n```", "```scala\nval predictedMovies = topKRecs.map(_.product)\n\n```", "```scala\npredictedMovies: Array[Int] = Array(27, 497, 633, 827, 602, 849, 401, \n   584, 1035, 1014)\n\n```", "```scala\nval apk10 = avgPrecisionK(actualMovies, predictedMovies, 10)\n\n```", "```scala\napk10: Double = 0.0\n\n```", "```scala\nval itemFactors = model.productFeatures.map { case (id, factor) => \n   factor }.collect() \nval itemMatrix = new DoubleMatrix(itemFactors) \nprintln(itemMatrix.rows, itemMatrix.columns)\n\n```", "```scala\n(1682,50)\n\n```", "```scala\nval imBroadcast = sc.broadcast(itemMatrix)\n\n```", "```scala\n14/04/13 21:02:01 INFO MemoryStore: ensureFreeSpace(672960) called \n   with curMem=4006896, maxMem=311387750\n14/04/13 21:02:01 INFO MemoryStore: Block broadcast_21 stored as \n   values to memory (estimated size 657.2 KB, free 292.5 MB)\nimBroadcast: \n   org.apache.spark.broadcast.Broadcast[org.jblas.DoubleMatrix] = \n   Broadcast(21)\n\n```", "```scala\nval allRecs = model.userFeatures.map{ case (userId, array) => \n  val userVector = new DoubleMatrix(array) \n  val scores = imBroadcast.value.mmul(userVector) \n  val sortedWithId = scores.data.zipWithIndex.sortBy(-_._1) \n  val recommendedIds = sortedWithId.map(_._2 + 1).toSeq \n  (userId, recommendedIds) \n}\n\n```", "```scala\nallRecs: org.apache.spark.rdd.RDD[(Int, Seq[Int])] = MappedRDD[269] \n   at map at <console>:29\n\n```", "```scala\nval userMovies = ratings.map{ case Rating(user, product, rating) \n   => (user, product) }.groupBy(_._1)\n\n```", "```scala\nuserMovies: org.apache.spark.rdd.RDD[(Int, Seq[(Int, Int)])] = \n  MapPartitionsRDD[277] at groupBy at <console>:21\n\n```", "```scala\nval K = 10 \nval MAPK = allRecs.join(userMovies).map{ case (userId, (predicted, actualWithIds)) => \n  val actual = actualWithIds.map(_._2).toSeq \n  avgPrecisionK(actual, predicted, K) \n}.reduce(_ + _) / allRecs.count \nprintln(\"Mean Average Precision at K = \" + MAPK)\n\n```", "```scala\nMean Average Precision at K = 0.030486963254725705\n\n```", "```scala\nimport org.apache.spark.mllib.evaluation.RegressionMetrics \nval predictedAndTrue = ratingsAndPredictions.map { case ((user, \n   product), (predicted, actual)) => (predicted, actual) } \nval regressionMetrics = new RegressionMetrics(predictedAndTrue)\n\n```", "```scala\nprintln(\"Mean Squared Error = \" + \n   regressionMetrics.meanSquaredError) \nprintln(\"Root Mean Squared Error = \" + \n   regressionMetrics.rootMeanSquaredError)\n\n```", "```scala\nMean Squared Error = 0.08231947642632852\nRoot Mean Squared Error = 0.2869137090247319\n\n```", "```scala\nimport org.apache.spark.mllib.evaluation.RankingMetrics \nval predictedAndTrueForRanking = allRecs.join(userMovies).map{ \n   case (userId, (predicted, actualWithIds)) => \n    val actual = actualWithIds.map(_._2) \n    (predicted.toArray, actual.toArray) \n} \nval rankingMetrics = new \n   RankingMetrics(predictedAndTrueForRanking) \nprintln(\"Mean Average Precision = \" + \n   rankingMetrics.meanAveragePrecision)\n\n```", "```scala\nMean Average Precision = 0.07171412913757183\n\n```", "```scala\nval MAPK2000 = allRecs.join(userMovies).map{ case (userId, \n   (predicted, actualWithIds)) => \n  val actual = actualWithIds.map(_._2).toSeq \n  avgPrecisionK(actual, predicted, 2000) \n}.reduce(_ + _) / allRecs.count \nprintln(\"Mean Average Precision = \" + MAPK2000)\n\n```", "```scala\nMean Average Precision = 0.07171412913757186.\n\n```", "```scala\nval transactions = Seq( \n      \"r z h k p\", \n      \"z y x w v u t s\", \n      \"s x o n r\", \n      \"x z y m t s q e\", \n      \"z\", \n      \"x z y r q t p\") \n      .map(_.split(\" \"))\n\n```", "```scala\nval sc = new SparkContext(\"local[2]\", \"Chapter 5 App\")\n\n```", "```scala\nval rdd = sc.parallelize(transactions, 2).cache()\n\n```", "```scala\nval fpg = new FPGrowth()\n\n```", "```scala\nval model = fpg.setMinSupport(0.2).setNumPartitions(1).run(rdd)\n\n```", "```scala\nmodel.freqItemsets.collect().foreach { \nitemset => \n        println(itemset.items.mkString( \n\"[\", \",\", \"]\") + \", \" + itemset.freq \n  )\n\n```", "```scala\n[s], 3\n[s,x], 3\n[s,x,z], 2\n[s,z], 2\n[r], 3\n[r,x], 2\n[r,z], 2\n[y], 3\n[y,s], 2\n[y,s,x], 2\n[y,s,x,z], 2\n[y,s,z], 2\n[y,x], 3\n[y,x,z], 3\n[y,t], 3\n[y,t,s], 2\n[y,t,s,x], 2\n[y,t,s,x,z], 2\n[y,t,s,z], 2\n[y,t,x], 3\n[y,t,x,z], 3\n[y,t,z], 3\n[y,z], 3\n[q], 2\n[q,y], 2\n[q,y,x], 2\n[q,y,x,z], 2\n[q,y,t], 2\n[q,y,t,x], 2\n[q,y,t,x,z], 2\n[q,y,t,z], 2\n[q,y,z], 2\n[q,x], 2\n[q,x,z], 2\n[q,t], 2\n[q,t,x], 2\n[q,t,x,z], 2\n[q,t,z], 2\n[q,z], 2\n[x], 4\n[x,z], 3\n[t], 3\n[t,s], 2\n[t,s,x], 2\n[t,s,x,z], 2\n[t,s,z], 2\n[t,x], 3\n[t,x,z], 3\n[t,z], 3\n[p], 2\n[p,r], 2\n[p,r,z], 2\n[p,z], 2\n[z], 5\n\n```", "```scala\n        val sc = Util.sc \n        val rawData = Util.getUserData() \n        rawData.first()\n\n```", "```scala\n        val rawRatings = rawData.map(_.split(\"t\").take(3)) \n        rawRatings.first() \n        val ratings = rawRatings.map { case Array(user, movie, \n           rating) => \n        Rating(user.toInt, movie.toInt, rating.toDouble) } \n            val ratingsFirst = ratings.first() \n        println(ratingsFirst)\n\n```", "```scala\n        val movies = Util.getMovieData() \n        val titles = movies.map(line => \n        line.split(\"|\").take(2)).map(array \n        => (array(0).toInt, array(1))).collectAsMap() \n        titles(123)\n\n```", "```scala\n        val model = fpg \n              .setMinSupport(0.1) \n              .setNumPartitions(1) \n              .run(rddx)\n\n```", "```scala\n            var eRDD = sc.emptyRDD \n            var z = Seq[String]() \n\n            val l = ListBuffer() \n            val aj = new Array[String](400) \n            var i = 0 \n            for( a <- 501 to 900) { \n              val moviesForUserX = ratings.keyBy(_.user). \n                lookup(a) \n             val moviesForUserX_10 = \n               moviesForUserX.sortBy(-_.rating).take(10) \n             val moviesForUserX_10_1 = moviesForUserX_10.map\n               (r => r.product) \n             var temp = \"\" \n             for( x <- moviesForUserX_10_1){ \n                if(temp.equals(\"\")) \n                  temp = x.toString \n                else { \n                  temp =  temp + \" \" + x \n                } \n             } \n             aj(i) = temp \n             i += 1 \n            } \n            z = aj \n            val transaction = z.map(_.split(\" \")) \n            val rddx = sc.parallelize(transaction, 2).cache() \n            val fpg = new FPGrowth() \n            val model = fpg \n              .setMinSupport(0.1) \n              .setNumPartitions(1) \n              .run(rddx) \n            model.freqItemsets.collect().foreach { itemset => \n              println(itemset.items.mkString(\"[\", \",\", \"]\") \n                + \", \" + itemset.freq) \n            } \n            sc.stop()\n\n```", "```scala\n        [302], 40\n [258], 59\n [100], 49\n [286], 50\n [181], 45\n [127], 60\n [313], 59\n [300], 49\n [50], 94\n\n```"]