["```scala\n>unzip ml-100k.zip\ninflating: ml-100k/allbut.pl \ninflating: ml-100k/mku.sh \ninflating: ml-100k/README\n ...\ninflating: ml-100k/ub.base \ninflating: ml-100k/ub.test\n\n```", "```scala\n >cd ml-100k\n\n```", "```scala\n$ head -5 u.user\n 1|24|M|technician|85711\n 2|53|F|other|94043\n 3|23|M|writer|32067\n 4|24|M|technician|43537\n 5|33|F|other|15213\n\n```", "```scala\n$head -5 u.item\n 1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-\n exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\n 2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-\n exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n 3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-\n exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\n 4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-\n exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0\n 5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-\n exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0\n\n```", "```scala\nmovie id | movie title | release date | video release date | IMDb \n URL | unknown | Action | Adventure | Animation | Children's | \n Comedy | Crime | Documentary | Drama | Fantasy | Film-Noir | \n Horror | Musical | Mystery | Romance | Sci-Fi | Thriller | War | \n Western |\n\n```", "```scala\nuser id | item id | rating | timestamp\n\n```", "```scala\n>head -5 u.data\n1962423881250949\n1863023891717742\n223771878887116\n244512880606923\n1663461886397596\n\n```", "```scala\n\u251c\u2500\u2500 1.6.2\n\u2502   \u251c\u2500\u2500 com\n\u2502   \u2502   \u251c\u2500\u2500 __init__.py\n\u2502   \u2502   \u2514\u2500\u2500 sparksamples\n\u2502   \u2502       \u251c\u2500\u2500 __init__.py\n\u2502   \u2502       \u251c\u2500\u2500 movie_data.py\n\u2502   \u2502       \u251c\u2500\u2500 plot_user_ages.py\n\u2502   \u2502       \u251c\u2500\u2500 plot_user_occupations.py\n\u2502   \u2502       \u251c\u2500\u2500 rating_data.py\n\u2502   \u2502       \u251c\u2500\u2500 user_data.py\n\u2502   \u2502       \u251c\u2500\u2500 util.py\n\u2502   \u2502       \n\u2502   \u2514\u2500\u2500 __init__.py\n\u251c\u2500\u2500 2.0.0\n\u2502   \u2514\u2500\u2500 com\n\u2502       \u251c\u2500\u2500 __init__.py\n\u2502       \u2514\u2500\u2500 sparksamples\n\u2502           \u251c\u2500\u2500 __init__.py\n\u2502           \u251c\u2500\u2500 movie_data.py\n\u2502           \u251c\u2500\u2500 plot_user_ages.py\n\u2502           \u251c\u2500\u2500 plot_user_occupations.py\n\u2502           \u251c\u2500\u2500 rating_data.py\n\u2502           \u251c\u2500\u2500 spark-warehouse\n\u2502           \u251c\u2500\u2500 user_data.py\n\u2502           \u251c\u2500\u2500 util.py\n\u2502           \n\n```", "```scala\n\u251c\u2500\u2500 1.6.2\n\u2502   \u251c\u2500\u2500 build.sbt\n\u2502   \u251c\u2500\u2500 spark-warehouse\n\u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2514\u2500\u2500 main\n\u2502   \u2502       \u2514\u2500\u2500 scala\n\u2502   \u2502           \u2514\u2500\u2500 org\n\u2502   \u2502               \u2514\u2500\u2500 sparksamples\n\u2502   \u2502                   \u251c\u2500\u2500 CountByRatingChart.scala\n\u2502   \u2502                   \u251c\u2500\u2500 exploredataset\n\u2502   \u2502                   \u2502   \u251c\u2500\u2500 explore_movies.scala\n\u2502   \u2502                   \u2502   \u251c\u2500\u2500 explore_ratings.scala\n\u2502   \u2502                   \u2502   \u2514\u2500\u2500 explore_users.scala\n\u2502   \u2502                   \u251c\u2500\u2500 featureext\n\u2502   \u2502                   \u2502   \u251c\u2500\u2500 ConvertWordsToVectors.scala\n\u2502   \u2502                   \u2502   \u251c\u2500\u2500 StandardScalarSample.scala\n\u2502   \u2502                   \u2502   \u2514\u2500\u2500 TfIdfSample.scala\n\u2502   \u2502                   \u251c\u2500\u2500 MovieAgesChart.scala\n\u2502   \u2502                   \u251c\u2500\u2500 MovieDataFillingBadValues.scala\n\u2502   \u2502                   \u251c\u2500\u2500 MovieData.scala\n\u2502   \u2502                   \u251c\u2500\u2500 RatingData.scala\n\u2502   \u2502                   \u251c\u2500\u2500 UserAgesChart.scala\n\u2502   \u2502                   \u251c\u2500\u2500 UserData.scala\n\u2502   \u2502                   \u251c\u2500\u2500 UserOccupationChart.scala\n\u2502   \u2502                   \u251c\u2500\u2500 UserRatingsChart.scala\n\u2502   \u2502                   \u2514\u2500\u2500 Util.scala\n\n```", "```scala\n\u251c\u2500\u2500 2.0.0\n\u2502   \u251c\u2500\u2500 build.sbt\n\u2502   \u251c\u2500\u2500 src\n\u2502   \u2502   \u2514\u2500\u2500 main\n\u2502   \u2502       \u2514\u2500\u2500 scala\n\u2502   \u2502           \u2514\u2500\u2500 org\n\u2502   \u2502               \u2514\u2500\u2500 sparksamples\n\u2502   \u2502                   \u251c\u2500\u2500 CountByRatingChart.scala\n\u2502   \u2502                   \u251c\u2500\u2500 df\n\u2502   \u2502                   \u251c\u2500\u2500 exploredataset\n\u2502   \u2502                   \u2502   \u251c\u2500\u2500 explore_movies.scala\n\u2502   \u2502                   \u2502   \u251c\u2500\u2500 explore_ratings.scala\n\u2502   \u2502                   \u2502   \u2514\u2500\u2500 explore_users.scala\n\u2502   \u2502                   \u251c\u2500\u2500 featureext\n\u2502   \u2502                   \u2502   \u251c\u2500\u2500 ConvertWordsToVectors.scala\n\u2502   \u2502                   \u2502   \u251c\u2500\u2500 StandardScalarSample.scala\n\u2502   \u2502                   \u2502   \u2514\u2500\u2500 TfIdfSample.scala\n\u2502   \u2502                   \u251c\u2500\u2500 MovieAgesChart.scala\n\u2502   \u2502                   \u251c\u2500\u2500 MovieDataFillingBadValues.scala\n\u2502   \u2502                   \u251c\u2500\u2500 MovieData.scala\n\u2502   \u2502                   \u251c\u2500\u2500 RatingData.scala\n\u2502   \u2502                   \u251c\u2500\u2500 UserAgesChart.scala\n\u2502   \u2502                   \u251c\u2500\u2500 UserData.scala\n\u2502   \u2502                   \u251c\u2500\u2500 UserOccupationChart.scala\n\u2502   \u2502                   \u251c\u2500\u2500 UserRatingsChart.scala\n\u2502   \u2502                   \u2514\u2500\u2500 Util.scala\n\n```", "```scala\n $ cd /MYPATH/spark-ml/Chapter_04/scala/2.0.0\n $ sbt compile\n $ sbt run\n\n```", "```scala\ndef get_user_data(): \n  custom_schema = StructType([ \n  StructField(\"no\", StringType(), True), \n  StructField(\"age\", IntegerType(), True), \n  StructField(\"gender\", StringType(), True), \n  StructField(\"occupation\", StringType(), True), \n  StructField(\"zipCode\", StringType(), True) \n]) \nfrompyspark.sql import SQLContext \nfrompyspark.sql.types import * \n\nsql_context = SQLContext(sc) \n\nuser_df = sql_context.read  \n  .format('com.databricks.spark.csv')  \n  .options(header='false', delimiter='|')  \n  .load(\"%s/ml-100k/u.user\"% PATH, schema =  \ncustom_schema) \nreturnuser_df\n\n```", "```scala\nuser_data = get_user_data() \nprint(user_data.first)\n\n```", "```scala\nu'1|24|M|technician|85711'\n\n```", "```scala\n\nval customSchema = StructType(Array( \nStructField(\"no\", IntegerType, true), \nStructField(\"age\", StringType, true), \nStructField(\"gender\", StringType, true), \nStructField(\"occupation\", StringType, true), \nStructField(\"zipCode\", StringType, true))); \nval spConfig = (new \n SparkConf).setMaster(\"local\").setAppName(\"SparkApp\") \nval spark = SparkSession \n  .builder() \n  .appName(\"SparkUserData\").config(spConfig) \n  .getOrCreate() \n\nval user_df = spark.read.format(\"com.databricks.spark.csv\") \n  .option(\"delimiter\", \"|\").schema(customSchema) \n  .load(\"/home/ubuntu/work/ml-resources/spark-ml/data/ml-\n 100k/u.user\") \nval first = user_df.first() \nprintln(\"First Record : \" + first)\n\n```", "```scala\nu'1|24|M|technician|85711'\n\n```", "```scala\nnum_users = user_data.count() \nnum_genders = \n len(user_data.groupBy(\"gender\").count().collect()) \nnum_occupation = \n len(user_data.groupBy(\"occupation\").count().collect()) \nnum_zipcodes = \n len(user_data.groupby(\"zipCode\").count().collect()) \nprint(\"Users: \"+ str(num_users)) \nprint(\"Genders: \"+ str(num_genders)) \nprint(\"Occupation: \"+ str(num_occupation)) \nprint(\"ZipCodes: \"+ str(num_zipcodes))\n\n```", "```scala\nUsers: 943\nGenders: 2\nOccupations: 21\nZIPCodes: 795\n\n```", "```scala\nval num_genders = user_df.groupBy(\"gender\").count().count() \nval num_occupations = \n user_df.groupBy(\"occupation\").count().count() \nval num_zipcodes = user_df.groupBy(\"zipCode\").count().count() \n\nprintln(\"num_users : \"+ user_df.count()) \nprintln(\"num_genders : \"+ num_genders) \nprintln(\"num_occupations : \"+ num_occupations) \nprintln(\"num_zipcodes: \"+ num_zipcodes) \nprintln(\"Distribution by Occupation\") \nprintln(user_df.groupBy(\"occupation\").count().show())\n\n```", "```scala\nnum_users: 943\nnum_genders: 2\nnum_occupations: 21\nnum_zipcodes: 795\n\n```", "```scala\nuser_data = get_user_data() \nuser_ages = user_data.select('age').collect() \nuser_ages_list = [] \nuser_ages_len = len(user_ages) \nfor i in range(0, (user_ages_len - 1)): \n    user_ages_list.append(user_ages[i].age) \nplt.hist(user_ages_list, bins=20, color='lightblue', normed=True) \nfig = matplotlib.pyplot.gcf() \nfig.set_size_inches(16, 10) \nplt.show()\n\n```", "```scala\nval userDataFrame = Util.getUserFieldDataFrame() \nval ages_array = userDataFrame.select(\"age\").collect() \n\nval min = 0 \nval max = 80 \nval bins = 16 \nval step = (80/bins).toInt \nvar mx = Map(0 ->0) \nfor (i <- step until (max + step) by step) { \n  mx += (i -> 0) \n} \nfor( x <- 0 until ages_array.length) { \n  val age = Integer.parseInt( \n    ages_array(x)(0).toString) \n  for(j <- 0 until (max + step) by step) { \n    if(age >= j && age < (j + step)){ \n      mx = mx + (j -> (mx(j) + 1)) \n    } \n  } \n} \n\nval mx_sorted =  ListMap(mx.toSeq.sortBy(_._1):_*) \nval ds = new org.jfree.data.category.DefaultCategoryDataset \nmx_sorted.foreach{ case (k,v) => ds.addValue(v,\"UserAges\", k)} \nval chart = ChartFactories.BarChart(ds) \nchart.show() \nUtil.sc.stop()\n\n```", "```scala\nuser_data = get_user_data() \nuser_occ = user_data.groupby(\"occupation\").count().collect() \n\nuser_occ_len = len(user_occ) \nuser_occ_list = [] \nfor i in range(0, (user_occ_len - 1)): \nelement = user_occ[i] \ncount = element. __getattr__('count') \ntup = (element.occupation, count) \n    user_occ_list.append(tup) \n\nx_axis1 = np.array([c[0] for c in user_occ_list]) \ny_axis1 = np.array([c[1] for c in user_occ_list]) \nx_axis = x_axis1[np.argsort(y_axis1)] \ny_axis = y_axis1[np.argsort(y_axis1)] \n\npos = np.arange(len(x_axis)) \nwidth = 1.0 \n\nax = plt.axes() \nax.set_xticks(pos + (width / 2)) \nax.set_xticklabels(x_axis) \n\nplt.bar(pos, y_axis, width, color='lightblue') \nplt.xticks(rotation=30) \nfig = matplotlib.pyplot.gcf() \nfig.set_size_inches(20, 10) \nplt.show()\n\n```", "```scala\n        userDataFrame.select(\"occupation\")\n\n```", "```scala\n        val occupation_groups =\n          userDataFrame.groupBy(\"occupation\").count()\n\n```", "```scala\n        val occupation_groups_sorted = \n          occupation_groups.sort(\"count\")\n\n```", "```scala\n        val userDataFrame = Util.getUserFieldDataFrame() \n        val occupation = userDataFrame.select(\"occupation\") \n        val occupation_groups = \n         userDataFrame.groupBy(\"occupation\").count() \n        val occupation_groups_sorted = occupation_groups.sort(\"count\") \n        occupation_groups_sorted.show() \n        val occupation_groups_collection = \n         occupation_groups_sorted.collect() \n\n        val ds = new org.jfree.data.category.DefaultCategoryDataset \n        val mx = scala.collection.immutable.ListMap() \n\n        for( x <- 0 until occupation_groups_collection.length) { \n          val occ = occupation_groups_collection(x)(0) \n          val count = Integer.parseInt(\n            occupation_groups_collection(x)(1).toString) \n          ds.addValue(count,\"UserAges\", occ.toString) \n        } \n\n        val chart = ChartFactories.BarChart(ds) \n        val font = new Font(\"Dialog\", Font.PLAIN,5); \n\n        chart.peer.getCategoryPlot.getDomainAxis(). \n        setCategoryLabelPositions(CategoryLabelPositions.UP_90); \n        chart.peer.getCategoryPlot.getDomainAxis.setLabelFont(font) \n        chart.show() \n        Util.sc.stop()\n\n```", "```scala\ndef getMovieDataDF() : DataFrame = { \n  val customSchema = StructType(Array( \n  StructField(\"id\", StringType, true), \n  StructField(\"name\", StringType, true), \n  StructField(\"date\", StringType, true), \n  StructField(\"url\", StringType, true))); \n  val movieDf = spark.read.format(\n    \"com.databricks.spark.csv\") \n     .option(\"delimiter\", \"|\").schema(customSchema) \n     .load(PATH_MOVIES) \n  return movieDf \n}\n\n```", "```scala\ndef getMovieYearsCountSorted(): scala.Array[(Int,String)] = { \n  val movie_data_df = Util.getMovieDataDF() \n  movie_data_df.createOrReplaceTempView(\"movie_data\") \n  movie_data_df.printSchema() \n\n  Util.spark.udf.register(\"convertYear\", Util.convertYear _) \n  movie_data_df.show(false) \n\n  val movie_years = Util.spark.sql(\n    \"select convertYear(date) as year from movie_data\") \n  val movie_years_count = movie_years.groupBy(\"year\").count() \n  movie_years_count.show(false) \n  val movie_years_count_rdd = movie_years_count.rdd.map(\n   row => (Integer.parseInt(row(0).toString), row(1).toString)) \n  val movie_years_count_collect = movie_years_count_rdd.collect() \n  val movie_years_count_collect_sort = \n  movie_years_count_collect.sortBy(_._1) \n} \n\ndef main(args: Array[String]) { \n  val movie_years = MovieData.getMovieYearsCountSorted() \n  for( a <- 0 to (movie_years.length -1)){ \n    print(movie_years(a)) \n  } \n}\n\n```", "```scala\n(1900,1)\n(1922,1)\n(1926,1)\n(1930,1)\n(1931,1)\n(1932,1)\n(1933,2)\n(1934,4)\n(1935,4)\n(1936,2)\n(1937,4)\n(1938,3)\n(1939,7)\n(1940,8)\n(1941,5)\n(1942,2)\n(1943,4)\n(1944,5)\n(1945,4)\n(1946,5)\n(1947,5)\n(1948,3)\n(1949,4)\n(1950,7)\n(1951,5)\n(1952,3)\n(1953,2)\n(1954,7)\n(1955,5)\n(1956,4)\n(1957,8)\n(1958,9)\n(1959,4)\n(1960,5)\n(1961,3)\n(1962,5)\n(1963,6)\n(1964,2)\n(1965,5)\n(1966,2)\n(1967,5)\n(1968,6)\n(1969,4)\n(1970,3)\n(1971,7)\n(1972,3)\n(1973,4)\n(1974,8)\n(1975,6)\n(1976,5)\n(1977,4)\n(1978,4)\n(1979,9)\n(1980,8)\n(1981,12)\n(1982,13)\n(1983,5)\n(1984,8)\n(1985,7)\n(1986,15)\n(1987,13)\n(1988,11)\n(1989,15)\n(1990,24)\n(1991,22)\n(1992,37)\n(1993,126)\n(1994,214)\n(1995,219)\n(1996,355)\n(1997,286)\n(1998,65)\n\n```", "```scala\nobject MovieAgesChart { \n  def main(args: Array[String]) { \n    val movie_years_count_collect_sort =            \n    MovieData.getMovieYearsCountSorted() \n\n    val ds = new \n      org.jfree.data.category.DefaultCategoryDataset \n    for(i <- movie_years_count_collect_sort){ \n      ds.addValue(i._2.toDouble,\"year\", i._1) \n    } \n    val  chart = ChartFactories.BarChart(ds) \n    chart.show() \n    Util.sc.stop() \n  } \n}\n\n```", "```scala\nobject RatingData { \n  def main(args: Array[String]) { \n    val customSchema = StructType(Array( \n      StructField(\"user_id\", IntegerType, true), \n      StructField(\"movie_id\", IntegerType, true), \n      StructField(\"rating\", IntegerType, true), \n      StructField(\"timestamp\", IntegerType, true))) \n\n    val spConfig = (new SparkConf).setMaster(\"local\").\n      setAppName(\"SparkApp\") \n    val spark = SparkSession.builder() \n      .appName(\"SparkRatingData\").config(spConfig) \n      .getOrCreate() \n\n    val rating_df = spark.read.format(\"com.databricks.spark.csv\") \n     .option(\"delimiter\", \"t\").schema(customSchema) \n     .load(\"../../data/ml-100k/u.data\") \n    rating_df.createOrReplaceTempView(\"df\") \n    val num_ratings = rating_df.count() \n    val num_movies = Util.getMovieDataDF().count() \n    val first = rating_df.first() \n    println(\"first:\" + first) \n    println(\"num_ratings:\" + num_ratings) \n  } \n}\n\n```", "```scala\nFirst: 196 242 3 881250949\nnum_ratings:100000\n\n```", "```scala\nval max = Util.spark.sql(\"select max(rating)  from df\") \nmax.show() \n\nval min = Util.spark.sql(\"select min(rating)  from df\") \nmin.show() \n\nval avg = Util.spark.sql(\"select avg(rating)  from df\") \navg.show()\n\n```", "```scala\n+----------------+\n|.  max(rating)  |\n+----------------+\n|              5 |\n+----------------+\n\n+----------------+\n|.  min(rating)  |\n+----------------+\n|              1 |\n+----------------+\n\n+-----------------+\n|.  avg(rating)   |\n+-----------------+\n|         3.52986 |\n+-----------------+\n\n```", "```scala\nobject CountByRatingChart { \n  def main(args: Array[String]) { \n    val customSchema = StructType(Array( \n      StructField(\"user_id\", IntegerType, true), \n      StructField(\"movie_id\", IntegerType, true), \n      StructField(\"rating\", IntegerType, true), \n      StructField(\"timestamp\", IntegerType, true))) \n\n   val  spConfig = (new SparkConf).setMaster(\"local\").\n     setAppName(\"SparkApp\") \n   val  spark = SparkSession \n      .builder() \n      .appName(\"SparkRatingData\").config(spConfig) \n      .getOrCreate() \n   val rating_df = spark.read.format(\"com.databricks.spark.csv\") \n      .option(\"delimiter\", \"t\").schema(customSchema) \n\n   val rating_df_count = rating_df.groupBy(\"rating\").\n     count().sort(\"rating\") \n\n   rating_df_count.show() \n   val rating_df_count_collection = rating_df_count.collect() \n\n   val ds = new org.jfree.data.category.DefaultCategoryDataset \n   val mx = scala.collection.immutable.ListMap() \n\n   for( x <- 0 until rating_df_count_collection.length) { \n      val occ = rating_df_count_collection(x)(0) \n      val count = Integer.parseInt( \n        rating_df_count_collection(x)(1).toString) \n      ds.addValue(count,\"UserAges\", occ.toString) \n    } \n\n    val chart = ChartFactories.BarChart(ds) \n    val font = new Font(\"Dialog\", Font.PLAIN,5); \n    chart.peer.getCategoryPlot.getDomainAxis(). \n    setCategoryLabelPositions(CategoryLabelPositions.UP_90); \n    chart.peer.getCategoryPlot.getDomainAxis.setLabelFont(font) \n    chart.show() \n    Util.sc.stop() \n  } \n}\n\n```", "```scala\nobject UserRatingsChart { \n  def main(args: Array[String]) { \n\n  } \n}\n\n```", "```scala\nval customSchema = StructType(Array( \n  StructField(\"user_id\", IntegerType, true), \n  StructField(\"movie_id\", IntegerType, true), \n  StructField(\"rating\", IntegerType, true), \n  StructField(\"timestamp\", IntegerType, true))) \n\nval spConfig = (new      \n    SparkConf).setMaster(\"local\").setAppName(\"SparkApp\") \nval spark = SparkSession \n   .builder() \n   .appName(\"SparkRatingData\").config(spConfig) \n   .getOrCreate() \n\nval rating_df = spark.read.format(\"com.databricks.spark.csv\") \n   .option(\"delimiter\", \"t\").schema(customSchema) \n   .load(\"../../data/ml-100k/u.data\") \n\nval rating_nos_by_user =       \n    rating_df.groupBy(\"user_id\").count().sort(\"count\") \nval ds = new org.jfree.data.category.DefaultCategoryDataset \n  rating_nos_by_user.show(rating_nos_by_user.collect().length)\n\n```", "```scala\n+-------+-----+\n|user_id|count|\n+-------+-----+\n|    636|   20|\n|    572|   20|\n|    926|   20|\n|    824|   20|\n|    166|   20|\n|    685|   20|\n|    812|   20|\n|    418|   20|\n|    732|   20|\n|    364|   20|\n....\n 222|  387|\n|    293|  388|\n|     92|  388|\n|    308|  397|\n|    682|  399|\n|     94|  400|\n|      7|  403|\n|    846|  405|\n|    429|  414|\n|    279|  434|\n|    181|  435|\n|    393|  448|\n|    234|  480|\n|    303|  484|\n|    537|  490|\n|    416|  493|\n|    276|  518|\n|    450|  540|\n|     13|  636|\n|    655|  685|\n|    405|  737|\n+-------+-----+\n\n```", "```scala\nval step = (max/bins).toInt \nfor(i <- step until (max + step) by step) { \n  mx += (i -> 0); \n} \nfor( x <- 0 until rating_nos_by_user_collect.length) { \n  val user_id =\n    Integer.parseInt(rating_nos_by_user_collect(x)(0).toString) \n  val count = \n    Integer.parseInt(rating_nos_by_user_collect(x)(1).toString) \n  ds.addValue(count,\"Ratings\", user_id) \n} \n\nval chart = ChartFactories.BarChart(ds) \nchart.peer.getCategoryPlot.getDomainAxis().setVisible(false) \n\nchart.show() \nUtil.sc.stop()\n\n```", "```scala\nUtil.spark.udf.register(\"convertYear\", Util.convertYear _) \nmovie_data_df.show(false) \n\nval movie_years = Util.spark.sql(\"select convertYear(date) as year from   movie_data\") \n\nmovie_years.createOrReplaceTempView(\"movie_years\") \nUtil.spark.udf.register(\"replaceEmptyStr\", replaceEmptyStr _) \n\nval years_replaced =  Util.spark.sql(\"select replaceEmptyStr(year) \n  as r_year from movie_years\")\n\n```", "```scala\ndef replaceEmptyStr(v : Int): Int = { \n  try { \n    if(v.equals(\"\") ) { \n      return 1900 \n    } else { \n      returnv \n    } \n  }catch{ \n    case e: Exception => println(e) \n     return 1900 \n  } \n}\n\n```", "```scala\nval movie_years_filtered = movie_years.filter(x =>(x == 1900) ) \nval years_filtered_valid = years_replaced.filter(x => (x != \n  1900)).collect() \nval years_filtered_valid_int = new \n  Array[Int](years_filtered_valid.length) \nfor( i <- 0 until years_filtered_valid.length -1){ \nval x = Integer.parseInt(years_filtered_valid(i)(0).toString) \n  years_filtered_valid_int(i) = x \n} \nval years_filtered_valid_int_sorted = \n  years_filtered_valid_int.sorted \n\nval years_replaced_int = new Array[Int] \n  (years_replaced.collect().length) \n\nval years_replaced_collect = years_replaced.collect() \n\nfor( i <- 0 until years_replaced.collect().length -1){ \n  val x = Integer.parseInt(years_replaced_collect(i)(0).toString) \n  years_replaced_int(i) = x \n} \n\nval years_replaced_rdd = Util.sc.parallelize(years_replaced_int) \n\nval num = years_filtered_valid.length \nvar sum_y = 0 \nyears_replaced_int.foreach(sum_y += _) \nprintln(\"Total sum of Entries:\"+ sum_y) \nprintln(\"Total No of Entries:\"+ num) \nval mean = sum_y/num \nval median_v = median(years_filtered_valid_int_sorted) \nUtil.sc.broadcast(mean) \nprintln(\"Mean value of Year:\"+ mean) \nprintln(\"Median value of Year:\"+ median_v) \nval years_x = years_replaced_rdd.map(v => replace(v , median_v)) \nprintln(\"Total Years after conversion:\"+ years_x.count()) \nvar count = 0 \nUtil.sc.broadcast(count) \nval years_with1900 = years_x.map(x => (if(x == 1900) {count +=1})) \nprintln(\"Count of 1900: \"+ count)\n\n```", "```scala\nTotal sum of Entries:3344062\nTotal No of Entries:1682\nMean value of Year:1988\nMedian value of Year:1995\nTotal Years after conversion:1682\nCount of 1900: 0\nCount of 1900: 0\n\n```", "```scala\nval ratings_grouped = rating_df.groupBy(\"rating\") \nratings_grouped.count().show() \nval ratings_byuser_local = rating_df.groupBy(\"user_id\").count() \nval count_ratings_byuser_local = ratings_byuser_local.count() \nratings_byuser_local.show(ratings_byuser_local.collect().length) \nval movie_fields_df = Util.getMovieDataDF() \nval user_data_df = Util.getUserFieldDataFrame() \nval occupation_df = user_data_df.select(\"occupation\").distinct() \noccupation_df.sort(\"occupation\").show() \nval occupation_df_collect = occupation_df.collect() \n\nvar all_occupations_dict_1:Map[String, Int] = Map() \nvar idx = 0; \n// for loop execution with a range \nfor( idx <- 0 to (occupation_df_collect.length -1)){ \n  all_occupations_dict_1 += \n    occupation_df_collect(idx)(0).toString() -> idx \n} \n\nprintln(\"Encoding of 'doctor : \" + \n all_occupations_dict_1(\"doctor\")) \nprintln(\"Encoding of 'programmer' : \" + \n all_occupations_dict_1(\"programmer\"))\n\n```", "```scala\nEncoding of 'doctor : 20\nEncoding of 'programmer' : 5\n\nvar k = all_occupations_dict_1.size \nvar binary_x = DenseVector.zeros[Double](k) \nvar k_programmer = all_occupations_dict_1(\"programmer\") \nbinary_x(k_programmer) = 1 \nprintln(\"Binary feature vector: %s\" + binary_x) \nprintln(\"Length of binary vector: \" + k)\n\n```", "```scala\nBinary feature vector: %sDenseVector(0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0)\nLength of binary vector: 21\n\n```", "```scala\ndef getCurrentHour(dateStr: String) : Integer = { \n  var currentHour = 0 \n  try { \n    val date = new Date(dateStr.toLong) \n    return int2Integer(date.getHours) \n  } catch { \n    case _ => return currentHour \n  } \n  return 1 \n}\n\n```", "```scala\nTimestamps DataFrame is extracted from rating_df by creating a TempView df and running a select statement.\n\n```", "```scala\nval customSchema = StructType(Array( \nStructField(\"user_id\", IntegerType, true), \nStructField(\"movie_id\", IntegerType, true), \nStructField(\"rating\", IntegerType, true), \nStructField(\"timestamp\", IntegerType, true))) \n\nval spConfig = (new \n SparkConf).setMaster(\"local\").setAppName(\"SparkApp\") \nval spark = SparkSession \n  .builder() \n  .appName(\"SparkRatingData\").config(spConfig) \n  .getOrCreate() \n\nval rating_df = spark.read.format(\"com.databricks.spark.csv\") \n  .option(\"delimiter\", \"t\").schema(customSchema) \n  .load(\"../../data/ml-100k/u.data\") \nrating_df.createOrReplaceTempView(\"df\") \nUtil.spark.udf.register(\"getCurrentHour\", getCurrentHour _) \n\nval timestamps_df = \n Util.spark.sql(\"select getCurrentHour(timestamp) as hour from \n df\") \ntimestamps_df.show()\n\n```", "```scala\ndef assignTod(hr : Integer) : String = { \nif(hr >= 7 && hr < 12){ \nreturn\"morning\" \n}else if ( hr >= 12 && hr < 14) { \nreturn\"lunch\" \n  } else if ( hr>= 14 && hr < 18) { \nreturn\"afternoon\" \n  } else if ( hr>= 18 && hr.<(23)) { \nreturn\"evening\" \n  } else if ( hr>= 23 && hr <= 24) { \nreturn\"night\" \n  } else if (  hr< 7) { \nreturn\"night\" \n  } else { \nreturn\"error\" \n  } \n}\n\n```", "```scala\nUtil.spark.udf.register(\"assignTod\", assignTod _) \ntimestamps_df.createOrReplaceTempView(\"timestamps\") \nval tod = Util.spark.sql(\"select assignTod(hour) as tod from \n timestamps\") \ntod.show()\n\n```", "```scala\ndef processRegex(input:String):String= { \n  val pattern = \"^[^(]*\".r \n  val output = pattern.findFirstIn(input) \n  return output.get \n}\n\n```", "```scala\nval raw_title = \n org.sparksamples.Util.getMovieDataDF().select(\"name\"\n raw_title.show() \nraw_title.createOrReplaceTempView(\"titles\") \nUtil.spark.udf.register(\"processRegex\", processRegex _) \nval processed_titles = Util.spark.sql( \n\"select processRegex(name) from titles\") \nprocessed_titles.show() \nval titles_rdd = processed_titles.rdd.map(r => r(0).toString) \ntitles_rdd.take(5).foreach(println)\n\n```", "```scala\n//Output of raw_title.show()\n+--------------------+\n|           UDF(name)|\n+--------------------+\n|          Toy Story |\n|          GoldenEye |\n|         Four Rooms |\n|         Get Shorty |\n|            Copycat |\n|     Shanghai Triad |\n|     Twelve Monkeys |\n|               Babe |\n|   Dead Man Walking |\n|        Richard III |\n|              Seven |\n|Usual Suspects, The |\n|   Mighty Aphrodite |\n|        Postino, Il |\n| Mr. Holland's Opus |\n|       French Twist |\n|From Dusk Till Dawn |\n| White Balloon, The |\n|     Antonia's Line |\n| Angels and Insects |\n+--------------------+\n\n//titles_rdd.take(5).foreach(println)\nToy Story\nGoldenEye\nFour Rooms\nGet Shorty\nCopycat\n\n```", "```scala\nval title_terms = titles_rdd.map(x => x.split(\"\")) \ntitle_terms.take(5).foreach(_.foreach(println)) \nprintln(title_terms.count())\n\n```", "```scala\nToy\nStory\nGoldenEye\nFour\nRooms\nGet\nShorty\nCopycat\n\n```", "```scala\nval all_terms_dic = new ListBuffer[String]() \nval all_terms = title_terms.flatMap(title_terms => title_terms).distinct().collect() \nfor (term <- all_terms){ \n  all_terms_dic += term \n} \n\nprintln(all_terms_dic.length) \nprintln(all_terms_dic.indexOf(\"Dead\")) \nprintln(all_terms_dic.indexOf(\"Rooms\"))\n\n```", "```scala\nTotal number of terms: 2645\nIndex of term 'Dead': 147\nIndex of term 'Rooms': 1963\n\n```", "```scala\nval all_terms_withZip = title_terms.flatMap(title_terms =>\n  title_terms).distinct().zipWithIndex().collectAsMap() \nprintln(all_terms_withZip.get(\"Dead\")) \nprintln(all_terms_withZip.get(\"Rooms\"))\n\n```", "```scala\nIndex of term 'Dead': 147\nIndex of term 'Rooms': 1963\n\n```", "```scala\ndef create_vector(title_terms:Array[String], \n  all_terms_dic:ListBuffer[String]): CSCMatrix[Int] = { \n  var idx = 0 \n  val x = CSCMatrix.zeros[Int](1, all_terms_dic.length) \n  title_terms.foreach(i => { \n    if (all_terms_dic.contains(i)) { \n      idx = all_terms_dic.indexOf(i) \n      x.update(0, idx, 1) \n    } \n  }) \n  return x \n} \n\nval term_vectors = title_terms.map(title_terms =>\n create_vector(title_terms, all_terms_dic)) \nterm_vectors.take(5).foreach(println)\n\n```", "```scala\n1 x 2453 CSCMatrix\n(0,622) 1\n(0,1326) 1\n1 x 2453 CSCMatrix\n(0,418) 1\n1 x 2453 CSCMatrix\n(0,729) 1\n(0,996) 1\n1 x 2453 CSCMatrix\n(0,433) 1\n(0,1414) 1\n1 x 2453 CSCMatrix\n(0,1559) 1\n\n```", "```scala\n//val vector = DenseVector.rand(10) \nval vector = DenseVector(0.49671415, -0.1382643, \n0.64768854,1.52302986, -0.23415337, -0.23413696, 1.57921282, \n  0.76743473, -0.46947439, 0.54256004) \nval norm_fact = norm(vector) \nval vec = vector/norm_fact \nprintln(norm_fact) \nprintln(vec)\n\n```", "```scala\n2.5908023998401077\nDenseVector(0.19172212826059407, -0.053367366036303286, \n 0.24999534508690138, 0.5878602938201672, -0.09037870661786127, -\n 0.09037237267282516, 0.6095458380374597, 0.2962150760889223, -\n 0.18120810372453483, 0.20941776186153152)\n\n```", "```scala\nfrom pyspark.mllib.feature import Normalizer \nnormalizer = Normalizer() \nvector = sc.parallelize([x])\n\n```", "```scala\nnormalized_x_mllib = \n  normalizer.transform(vector).first().toArray()\n\n```", "```scala\nprint\"x:n%s\" % x \nprint\"2-Norm of x: %2.4f\" % norm_x_2 \nprint\"Normalized x MLlib:n%s\" % normalized_x_mllib \nprint\"2-Norm of normalized_x_mllib: %2.4f\" % \n np.linalg.norm(normalized_x_mllib)\n\n```", "```scala\nobject FeatureNormalizer { \n  def main(args: Array[String]): Unit = { \n    val v = Vectors.dense(0.49671415, -0.1382643, 0.64768854, \n      1.52302986, -0.23415337, -0.23413696, 1.57921282, \n      0.76743473, -0.46947439, 0.54256004) \n    val normalizer = new Normalizer(2) \n    val norm_op = normalizer.transform(v) \n    println(norm_op) \n  } \n}\n\n```", "```scala\n[0.19172212826059407,-\n 0.053367366036303286,0.24999534508690138,0.5878602938201672,-\n 0.09037870661786127,-\n 0.09037237267282516,0.6095458380374597,0.2962150760889223,-\n 0.18120810372453483,0.20941776186153152]\n\n```", "```scala\nobject TfIdfSample{ \n  def main(args: Array[String]) { \n    // TODO replace with path specific to your machine \n    val file = Util.SPARK_HOME + \"/README.md\" \n    val spConfig = (new        \n      SparkConf).setMaster(\"local\").setAppName(\"SparkApp\") \n    val sc = new SparkContext(spConfig) \n    val documents: RDD[Seq[String]] =      \n      sc.textFile(file).map(_.split(\"\").toSeq) \n    print(\"Documents Size:\" + documents.count) \n    val hashingTF = new HashingTF() \n    val tf = hashingTF.transform(documents) \n    for(tf_ <- tf) { \n      println(s\"$tf_\") \n    } \n    tf.cache() \n    val idf = new IDF().fit(tf) \n    val tfidf = idf.transform(tf) \n    println(\"tfidf size : \" + tfidf.count) \n    for(tfidf_ <- tfidf) { \n      println(s\"$tfidf_\") \n    } \n  } \n}\n\n```", "```scala\nobject ConvertWordsToVectors{ \n  def main(args: Array[String]) { \n    val file =  \n      \"/home/ubuntu/work/ml-resources/\" + \n      \"spark-ml/Chapter_04/data/text8_10000\" \n    val conf = new SparkConf().setMaster(\"local\").\n      setAppName(\"Word2Vector\") \n    val sc = new SparkContext(conf) \n    val input = sc.textFile(file).map(line => line.split(\"\").toSeq) \n    val word2vec = new Word2Vec() \n    val model = word2vec.fit(input) \n    val vectors = model.getVectors \n    vectors foreach (  \n      (t2) =>println (t2._1 + \"-->\" + t2._2.mkString(\"\")) \n    ) \n  } \n}\n\n```", "```scala\nideas-->0.0036772825 -9.474439E-4 0.0018383651 -6.24215E-4 -\n 0.0042944895 -5.839545E-4 -0.004661157 -0.0024960344 0.0046632644 -\n 0.00237432 -5.5691406E-5 -0.0033026629 0.0032463844 -0.0019799764 -\n 0.0016042799 0.0016129494 -4.099998E-4 0.0031266063 -0.0051537985 \n 0.004354736 -8.4361364E-4 0.0016157745 -0.006367187 0.0037806155 -\n 4.4071436E-4 8.62155E-4 0.0051918332 0.004437387 -0.0012511226 -\n 8.7162864E-4 -0.0035564564 -4.2263913E-4 -0.0020519749 -\n 0.0034343079 0.0035128237 -0.0014698022 -7.263344E-4 -0.0030510207 \n -1.05513E-4 0.003316195 0.001853326 -0.003090298 -7.3562167E-4 -\n 0.004879414 -0.007057088 1.1937474E-4 -0.0017973455 0.0034448127 \n 0.005289607 9.6152216E-4 0.002103868 0.0016721261 -9.6310966E-4 \n 0.0041839285 0.0035658625 -0.0038187192 0.005523701 -1.8146896E-4 -\n 0.006257453 6.5041234E-4 -0.006894542 -0.0013860351 -4.7463065E-4 \n 0.0044280654 -7.142674E-4 -0.005085546 -2.7047616E-4 0.0026938762 -\n 0.0020157609 0.0051508015 -0.0027767695 0.003554946 -0.0052921847 \n 0.0020432177 -0.002188367 -0.0010223344 -0.0031813548 -0.0032866944 \n 0.0020323955 -0.0015844131 -0.0041034482 0.0044767153 -2.5071128E-4 \n 0.0022343954 0.004051373 -0.0021706335 8.161181E-4 0.0042591896 \n 0.0036099665 -0.0024891358 -0.0043153367 -0.0037649528 -\n 0.0033249175 -9.5358933E-4 -0.0041675125 0.0029751007 -0.0017840122 \n -5.3287676E-4 1.983675E-4 -1.9737136E-5\n\n```", "```scala\nobject StandardScalarSample { \n  def main(args: Array[String]) { \n    val conf = new SparkConf().setMaster(\"local\"). \n     setAppName(\"Word2Vector\") \n    val sc = new SparkContext(conf) \n    val data = MLUtils.loadLibSVMFile( sc, \n      org.sparksamples.Util.SPARK_HOME +         \n      \"/data/mllib/sample_libsvm_data.txt\") \n\n    val scaler1 = new StandardScaler().fit(data.map(x => x.features) \n    val scaler2 = new StandardScaler(withMean = true, \n      withStd = true).fit(data.map(x => x.features)) \n    // scaler3 is an identical model to scaler2, and will produce   \n    //identical transformations \n    val scaler3 = new StandardScalerModel(scaler2.std, scaler2.mean) \n\n    // data1 will be unit variance. \n    val data1 = data.map(x => \n      (x.label, scaler1.transform(x.features))) \n    println(data1.first())\n    // Without converting the features into dense vectors, \n    //transformation with zero mean will raise \n    // exception on sparse vector. \n    // data2 will be unit variance and zero mean. \n    val data2 = data.map(x => (x.label,       \n      scaler2.transform(Vectors.dense(x.features.toArray)))) \n    println(data2.first()) \n  } \n}\n\n```"]