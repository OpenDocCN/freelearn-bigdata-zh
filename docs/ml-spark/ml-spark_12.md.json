["```scala\nimport org.apache.spark.ml.classification.LogisticRegression \nimport org.apache.spark.ml.linalg.{Vector, Vectors} \nimport org.apache.spark.ml.param.ParamMap \nimport org.apache.spark.sql.Row \nimport org.utils.StandaloneSpark \n\nobject PipelineComponentExample { \n\n  def main(args: Array[String]): Unit = { \n    val spark = StandaloneSpark.getSparkInstance() \n\n    // Prepare training data from a list of (label, features) tuples. \n    val training = spark.createDataFrame(Seq( \n      (1.0, Vectors.dense(0.0, 1.1, 0.1)), \n      (0.0, Vectors.dense(2.0, 1.0, -1.0)), \n      (0.0, Vectors.dense(2.0, 1.3, 1.0)), \n      (1.0, Vectors.dense(0.0, 1.2, -0.5)) \n    )).toDF(\"label\", \"features\") \n\n    // Create a LogisticRegression instance. This instance is an Estimator. \n    val lr = new LogisticRegression() \n    // Print out the parameters, documentation, and any default values. \n    println(\"LogisticRegression parameters:n\" + lr.explainParams() + \"n\") \n\n    // We may set parameters using setter methods. \n    lr.setMaxIter(10) \n      .setRegParam(0.01) \n\n    // Learn a LogisticRegression model.\n    // This uses the parameters stored in lr. \n    val model1 = lr.fit(training) \n    // Since model1 is a Model (i.e., a Transformer produced by an Estimator), \n    // we can view the parameters it used during fit(). \n    // This prints the parameter (name: value) pairs,\n    // where names are unique IDs for this \n    // LogisticRegression instance. \n    println(\"Model 1 was fit using parameters: \" + \n    model1.parent.extractParamMap) \n\n    // We may alternatively specify parameters using a ParamMap, \n    // which supports several methods for specifying parameters. \n    val paramMap = ParamMap(lr.maxIter -> 20) \n    .put(lr.maxIter, 30) // Specify 1 Param.\n    // This overwrites the original maxIter. \n    .put(lr.regParam -> 0.1, lr.threshold -> 0.55) // Specify multiple Params. \n\n    // One can also combine ParamMaps. \n    val paramMap2 = ParamMap(lr.probabilityCol ->             \n      \"myProbability\") \n    // Change output column name. \n    val paramMapCombined = paramMap ++ paramMap2 \n\n    // Now learn a new model using the paramMapCombined parameters. \n    lr.set* methods. \n    val model2 = lr.fit(training, paramMapCombined) \n    println(\"Model 2 was fit using parameters: \" + \n      model2.parent.extractParamMap) \n\n    // Prepare test data. \n    val test = spark.createDataFrame(Seq( \n      (1.0, Vectors.dense(-1.0, 1.5, 1.3)), \n      (0.0, Vectors.dense(3.0, 2.0, -0.1)), \n      (1.0, Vectors.dense(0.0, 2.2, -1.5)) \n        )).toDF(\"label\", \"features\") \n\n    // Make predictions on test data using the \n    // Transformer.transform() method. \n    // LogisticRegression.transform will only use the 'features' \n    // column. \n    // Note that model2.transform() outputs a 'myProbability' \n    // column instead of the usual \n    // 'probability' column since we renamed the       \n    lr.probabilityCol \n    parameter previously. \n    model2.transform(test) \n      .select(\"features\", \"label\", \"myProbability\", \n      \"prediction\") \n      .collect() \n      .foreach { case Row(features: Vector, label: Double, prob: \n        Vector, prediction: Double) => \n        println(s\"($features, $label) -> prob=$prob, \n        prediction=$prediction\") \n      } \n   } \n} \n\n```", "```scala\nModel 2 was fit using parameters: {\nlogreg_158888baeffa-elasticNetParam: 0.0,\nlogreg_158888baeffa-featuresCol: features,\nlogreg_158888baeffa-fitIntercept: true,\nlogreg_158888baeffa-labelCol: label,\nlogreg_158888baeffa-maxIter: 30,\nlogreg_158888baeffa-predictionCol: prediction,\nlogreg_158888baeffa-probabilityCol: myProbability,\nlogreg_158888baeffa-rawPredictionCol: rawPrediction,\nlogreg_158888baeffa-regParam: 0.1,\nlogreg_158888baeffa-standardization: true,\nlogreg_158888baeffa-threshold: 0.55,\nlogreg_158888baeffa-tol: 1.0E-6\n}\n17/02/12 12:32:49 INFO Instrumentation: LogisticRegression-\nlogreg_158888baeffa-268961738-2: training finished\n17/02/12 12:32:49 INFO CodeGenerator: Code generated in 26.525405    \nms\n17/02/12 12:32:49 INFO CodeGenerator: Code generated in 11.387162   \nms\n17/02/12 12:32:49 INFO SparkContext: Invoking stop() from shutdown \nhook\n([-1.0,1.5,1.3], 1.0) -> \nprob=[0.05707304171033984,0.9429269582896601], prediction=1.0\n([3.0,2.0,-0.1], 0.0) -> \nprob=[0.9238522311704088,0.0761477688295912], prediction=0.0\n([0.0,2.2,-1.5], 1.0) -> \nprob=[0.10972776114779145,0.8902722388522085], prediction=1.0\n\n```", "```scala\npackage org.textclassifier \n\nimport org.apache.spark.ml.{Pipeline, PipelineModel} \nimport org.apache.spark.ml.classification.LogisticRegression \nimport org.apache.spark.ml.feature.{HashingTF, Tokenizer} \nimport org.apache.spark.ml.linalg.Vector \nimport org.utils.StandaloneSpark \n\n/** \n* Created by manpreet.singh on 12/02/17\\. \n */ \nobject TextClassificationPipeline { \n\n  def main(args: Array[String]): Unit = { \n    val spark = StandaloneSpark.getSparkInstance() \n\n   // Prepare training documents from a list of (id, text, label) \n   // tuples. \n   val training = spark.createDataFrame(Seq( \n     (0L, \"a b c d e spark\", 1.0), \n     (1L, \"b d\", 0.0), \n     (2L, \"spark f g h\", 1.0), \n     (3L, \"hadoop mapreduce\", 0.0) \n    )).toDF(\"id\", \"text\", \"label\") \n\n    // Configure an ML pipeline, which consists of three stages: \n    // tokenizer, hashingTF, and lr. \n    val tokenizer = new Tokenizer() \n      .setInputCol(\"text\") \n      .setOutputCol(\"words\") \n    val hashingTF = new HashingTF() \n      .setNumFeatures(1000) \n      .setInputCol(tokenizer.getOutputCol) \n      .setOutputCol(\"features\") \n    val lr = new LogisticRegression() \n      .setMaxIter(10) \n      .setRegParam(0.001) \n    val pipeline = new Pipeline() \n      .setStages(Array(tokenizer, hashingTF, lr)) \n\n    // Fit the pipeline to training documents. \n    val model = pipeline.fit(training)\n\n    // Now we can optionally save the fitted pipeline to disk \n    model.write.overwrite().save(\"/tmp/spark-logistic-regression-\n      model\") \n\n    // We can also save this unfit pipeline to disk \n    pipeline.write.overwrite().save(\"/tmp/unfit-lr-model\") \n\n    // And load it back in during production \n    val sameModel = PipelineModel.load(\"/tmp/spark-logistic-\n      regression-model\") \n\n    // Prepare test documents, which are unlabeled (id, text) tuples. \n    val test = spark.createDataFrame(Seq( \n      (4L, \"spark i j k\"), \n      (5L, \"l m n\"), \n      (6L, \"spark hadoop spark\"), \n      (7L, \"apache hadoop\") \n    )).toDF(\"id\", \"text\") \n\n    // Make predictions on test documents. \n    model.transform(test) \n      .select(\"id\", \"text\", \"probability\", \"prediction\") \n      .collect() \n      .foreach { case Row(id: Long, text: String, prob: Vector, \n        prediction: Double) => \n        println(s\"($id, $text) --> prob=$prob, \n        prediction=$prediction\") \n      } \n    } \n } \n\n```", "```scala\n17/02/12 12:46:22 INFO Executor: Finished task 0.0 in stage \n30.0    \n(TID \n30). 1494 bytes result sent to driver\n17/02/12 12:46:22 INFO TaskSetManager: Finished task 0.0 in stage \n30.0 (TID 30) in 84 ms on localhost (1/1)\n17/02/12 12:46:22 INFO TaskSchedulerImpl: Removed TaskSet 30.0,    \nwhose tasks have all completed, from pool \n17/02/12 12:46:22 INFO DAGScheduler: ResultStage 30 (head at \nLogisticRegression.scala:683) finished in 0.084 s\n17/02/12 12:46:22 INFO DAGScheduler: Job 29 finished: head at \nLogisticRegression.scala:683, took 0.091814 s\n17/02/12 12:46:22 INFO CodeGenerator: Code generated in 5.88911 ms\n17/02/12 12:46:22 INFO CodeGenerator: Code generated in 8.320754 ms\n17/02/12 12:46:22 INFO CodeGenerator: Code generated in 9.082379 ms\n(4, spark i j k) --> \nprob=[0.15964077387874084,0.8403592261212592], \nprediction=1.0\n(5, l m n) --> prob=[0.8378325685476612,0.16216743145233883], \nprediction=0.0\n(6, spark hadoop spark) --> prob=    \n[0.06926633132976247,0.9307336686702374], prediction=1.0 (7, apache hadoop) --> prob=   \n[0.9821575333444208,0.01784246665557917], \nprediction=0.0\n\n```", "```scala\ncase \"LR\" =>             \n  LogisticRegressionPipeline.logisticRegressionPipeline(\n  vectorAssembler, dataFrame) \n\ncase \"DT\" =>       \n  DecisionTreePipeline.decisionTreePipeline(vectorAssembler, \n  dataFrame) \n\ncase \"RF\" => \n  RandomForestPipeline.randomForestPipeline(vectorAssembler, \n  dataFrame) \n\ncase\n  GradientBoostedTreePipeline.gradientBoostedTreePipeline\n  (vectorAssembler, dataFrame) \n\ncase \"NB\" =>                   \n  NaiveBayesPipeline.naiveBayesPipeline(vectorAssembler, \n  dataFrame) \n\ncase \"SVM\" => SVMPipeline.svmPipeline(sparkContext) \n\n```", "```scala\npackage org.stumbleuponclassifier \n\nimport org.apache.log4j.Logger \nimport org.apache.spark.ml.classification.DecisionTreeClassifier \nimport org.apache.spark.ml.evaluation.MulticlassClassification\n  Evaluator \nimport org.apache.spark.ml.feature.{StringIndexer,       \n  VectorAssembler} \nimport org.apache.spark.ml.{Pipeline, PipelineStage} \nimport org.apache.spark.sql.DataFrame \nimport scala.collection.mutable \n\n/** \n  * Created by manpreet.singh on 01/05/16\\. \n  */ \nobject DecisionTreePipeline { \n  @transient lazy val logger = Logger.getLogger(getClass.getName) \n\n  def decisionTreePipeline(vectorAssembler: VectorAssembler, \n    dataFrame: DataFrame) = { \n    val Array(training, test) = dataFrame.randomSplit(Array(0.9, \n      0.1), seed = 12345) \n\n    // Set up Pipeline \n    val stages = new mutable.ArrayBuffer[PipelineStage]() \n\n    val labelIndexer = new StringIndexer() \n      .setInputCol(\"label\") \n      .setOutputCol(\"indexedLabel\") \n    stages += labelIndexer \n\n    val dt = new DecisionTreeClassifier() \n      .setFeaturesCol(vectorAssembler.getOutputCol) \n      .setLabelCol(\"indexedLabel\") \n      .setMaxDepth(5) \n      .setMaxBins(32) \n      .setMinInstancesPerNode(1) \n      .setMinInfoGain(0.0) \n      .setCacheNodeIds(false) \n      .setCheckpointInterval(10) \n\n    stages += vectorAssembler \n    stages += dt \n    val pipeline = new Pipeline().setStages(stages.toArray) \n\n    // Fit the Pipeline \n    val startTime = System.nanoTime() \n    //val model = pipeline.fit(training) \n    val model = pipeline.fit(dataFrame) \n    val elapsedTime = (System.nanoTime() - startTime) / 1e9 \n    println(s\"Training time: $elapsedTime seconds\") \n\n    //val holdout = \n    // model.transform(test).select(\"prediction\",\"label\") \n    val holdout = \n      model.transform(dataFrame).select(\"prediction\",\"label\") \n\n    // Select (prediction, true label) and compute test error \n    val evaluator = new MulticlassClassificationEvaluator() \n      .setLabelCol(\"label\") \n      .setPredictionCol(\"prediction\") \n      .setMetricName(\"accuracy\") \n    val mAccuracy = evaluator.evaluate(holdout) \n    println(\"Test set accuracy = \" + mAccuracy) \n  } \n} \n\n```", "```scala\nAccuracy: 0.3786163522012579  \n\n```", "```scala\npackage org.stumbleuponclassifier \n\nimport org.apache.log4j.Logger \nimport org.apache.spark.ml.classification.NaiveBayes \nimport org.apache.spark.ml.evaluation.MulticlassClassification\n  Evaluator \nimport org.apache.spark.ml.feature.{StringIndexer, \n  VectorAssembler} \nimport org.apache.spark.ml.{Pipeline, PipelineStage} \nimport org.apache.spark.sql.DataFrame \nimport scala.collection.mutable \n\n/** \n  * Created by manpreet.singh on 01/05/16\\. \n  */ \nobject NaiveBayesPipeline { \n  @transient lazy val logger = \n  Logger.getLogger(getClass.getName) \n\n  def naiveBayesPipeline(vectorAssembler: VectorAssembler, \n    dataFrame: DataFrame) = { \n    val Array(training, test) = dataFrame.randomSplit(Array(0.9, \n      0.1), seed = 12345) \n\n    // Set up Pipeline \n    val stages = new mutable.ArrayBuffer[PipelineStage]() \n\n    val labelIndexer = new StringIndexer() \n      .setInputCol(\"label\") \n      .setOutputCol(\"indexedLabel\") \n    stages += labelIndexer \n\n    val nb = new NaiveBayes() \n\n    stages += vectorAssembler \n    stages += nb \n    val pipeline = new Pipeline().setStages(stages.toArray) \n\n    // Fit the Pipeline \n    val startTime = System.nanoTime() \n    // val model = pipeline.fit(training) \n    val model = pipeline.fit(dataFrame) \n    val elapsedTime = (System.nanoTime() - startTime) / 1e9 \n    println(s\"Training time: $elapsedTime seconds\") \n\n    // val holdout = \n    // model.transform(test).select(\"prediction\",\"label\") \n    val holdout = \n      model.transform(dataFrame).select(\"prediction\",\"label\") \n\n    // Select (prediction, true label) and compute test error \n    val evaluator = new MulticlassClassificationEvaluator() \n      .setLabelCol(\"label\") \n      .setPredictionCol(\"prediction\") \n      .setMetricName(\"accuracy\") \n    val mAccuracy = evaluator.evaluate(holdout) \n    println(\"Test set accuracy = \" + mAccuracy) \n  } \n} \n\n```", "```scala\nTraining time: 2.114725642 seconds\nAccuracy: 0.5660377358490566\n\n```", "```scala\npackage org.stumbleuponclassifier \n\nimport org.apache.log4j.Logger \nimport org.apache.spark.ml.classification.GBTClassifier \nimport org.apache.spark.ml.feature.{StringIndexer, \n   VectorAssembler} \nimport org.apache.spark.ml.{Pipeline, PipelineStage} \nimport org.apache.spark.mllib.evaluation.{MulticlassMetrics, \n   RegressionMetrics} \nimport org.apache.spark.sql.DataFrame \n\nimport scala.collection.mutable \n\n/** \n  * Created by manpreet.singh on 01/05/16\\. \n  */ \nobject GradientBoostedTreePipeline { \n  @transient lazy val logger = \n    Logger.getLogger(getClass.getName) \n    def gradientBoostedTreePipeline(vectorAssembler: \n      VectorAssembler, dataFrame: DataFrame) = { \n      val Array(training, test) = dataFrame.randomSplit(Array(0.9, \n      0.1), seed = 12345) \n\n    // Set up Pipeline \n    val stages = new mutable.ArrayBuffer[PipelineStage]() \n\n      val labelIndexer = new StringIndexer() \n      .setInputCol(\"label\") \n      .setOutputCol(\"indexedLabel\") \n    stages += labelIndexer \n\n    val gbt = new GBTClassifier() \n      .setFeaturesCol(vectorAssembler.getOutputCol) \n      .setLabelCol(\"indexedLabel\") \n      .setMaxIter(10) \n\n    stages += vectorAssembler \n    stages += gbt \n    val pipeline = new Pipeline().setStages(stages.toArray) \n\n    // Fit the Pipeline \n    val startTime = System.nanoTime() \n    //val model = pipeline.fit(training) \n    val model = pipeline.fit(dataFrame) \n    val elapsedTime = (System.nanoTime() - startTime) / 1e9 \n    println(s\"Training time: $elapsedTime seconds\") \n\n    // val holdout = \n    // model.transform(test).select(\"prediction\",\"label\") \n    val holdout = \n    model.transform(dataFrame).select(\"prediction\",\"label\") \n\n    // have to do a type conversion for RegressionMetrics \n    val rm = new RegressionMetrics(holdout.rdd.map(x => \n      (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))) \n\n    logger.info(\"Test Metrics\") \n    logger.info(\"Test Explained Variance:\") \n    logger.info(rm.explainedVariance) \n    logger.info(\"Test R^2 Coef:\") \n    logger.info(rm.r2) \n    logger.info(\"Test MSE:\") \n    logger.info(rm.meanSquaredError) \n    logger.info(\"Test RMSE:\") \n    logger.info(rm.rootMeanSquaredError) \n\n    val predictions = model.transform(test).select(\"prediction\")\n    .rdd.map(_.getDouble(0)) \n    val labels = model.transform(test).select(\"label\")\n    .rdd.map(_.getDouble(0)) \n    val accuracy = new \n      MulticlassMetrics(predictions.zip(labels)).precision \n    println(s\"  Accuracy : $accuracy\") \n  } \n\n  def savePredictions(predictions:DataFrame, testRaw:DataFrame, \n    regressionMetrics: RegressionMetrics, filePath:String) = { \n    predictions \n      .coalesce(1) \n      .write.format(\"com.databricks.spark.csv\") \n      .option(\"header\", \"true\") \n      .save(filePath) \n  } \n\n} \n\n```", "```scala\nAccuracy: 0.3647\n\n```"]