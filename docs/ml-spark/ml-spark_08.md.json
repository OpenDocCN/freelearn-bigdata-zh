["```scala\n      val ratings = spark.sparkContext \n      .textFile(DATA_PATH + \"/u.data\") \n      .map(_.split(\"\\t\")) \n      .map(lineSplit => Rating(lineSplit(0).toInt, \n        lineSplit(1).toInt,  lineSplit(2).toFloat, \n        lineSplit(3).toLong)) \n      .toDF()\n\n```", "```scala\n      val Array(training, test) =  \n        ratings.randomSplit(Array(0.8, 0.2))\n\n```", "```scala\n      val als = new ALS() \n        .setMaxIter(5) \n        .setRegParam(0.01) \n        .setUserCol(\"userId\") \n        .setItemCol(\"movieId\") \n        .setRatingCol(\"rating\")\n\n```", "```scala\n      val model = als.fit(training) \n      val predictions = model.transform(test)\n\n```", "```scala\n      val itemFactors = model.itemFactors \n      itemFactors.show() \n\n      val userFactors = model.userFactors \n      userFactors.show()\n\n```", "```scala\n      val itemFactorsOrdererd = itemFactors.orderBy(\"id\") \n      val itemFactorLibSVMFormat = \n        itemFactorsOrdererd.rdd.map(x => x(0) + \" \" + \n        getDetails(x(1).asInstanceOf\n          [scala.collection.mutable.WrappedArray[Float]])) \n      println(\"itemFactorLibSVMFormat.count() : \" + \n        itemFactorLibSVMFormat.count()) \n      print(\"itemFactorLibSVMFormat.first() : \" + \n        itemFactorLibSVMFormat.first()) \n\n      itemFactorLibSVMFormat.coalesce(1)\n        .saveAsTextFile(output + \"/\" + date_time + \n        \"/movie_lens_items_libsvm\")\n\n```", "```scala\n          1 1:0.44353345 2:-0.7453435 3:-0.55146646 4:-0.40894786 \n          5:-0.9921601 6:1.2012635 7:0.50330496 8:-0.23256435     \n          9:0.55483425 10:-1.4781344\n 2 1:0.34384087 2:-1.0242497 3:-0.20907198 4:-0.102892995 \n          5:-1.0616653 6:1.1338154 7:0.5742042 8:-0.46505615  \n          9:0.3823278 10:-1.0695107 3 1:-0.04743084 2:-0.6035447  \n          3:-0.7999673 4:0.16897096    \n          5:-1.0216197 6:0.3304353 7:1.5495727 8:0.2972699  \n          9:-0.6855238 \n          10:-1.5391738\n 4 1:0.24745995 2:-0.33971268 3:0.025664425 4:0.16798466 \n          5:-0.8462472 6:0.6734541 7:0.7537076 8:-0.7119413  \n          9:0.7475001 \n          10:-1.965572\n 5 1:0.30903652 2:-0.8523586 3:-0.54090345 4:-0.7004097 \n          5:-1.0383878 6:1.1784278 7:0.5125761 8:0.2566347         \n          9:-0.020201845   \n          10:-1.118083\n ....\n 1681 1:-0.14603947 2:-0.4475343 3:-0.50514024 4:-0.7221697 \n          5:-0.7997808 6:0.21069092 7:0.22631708 8:-0.32458723 \n          9:0.20187362 10:-1.2734087\n 1682 1:0.21975909 2:0.45303428 3:-0.73912954 4:-0.40584692 \n          5:-0.5299451 6:0.79586357 7:0.5154468 8:-0.4033669  \n          9:0.2220822 \n          10:-0.70235217\n\n```", "```scala\n      var itemFactorsXY = itemFactorsOrdererd.rdd.map( \n        x => getXY(x(1).asInstanceOf\n        [scala.collection.mutable.WrappedArray[Float]])) \n      itemFactorsXY.first() \n      itemFactorsXY.coalesce(1).saveAsTextFile(output + \"/\" + \n        date_time + \"/movie_lens_items_xy\")\n\n```", "```scala\n          2.254384458065033, 0.5487040132284164\n          -2.0540390759706497, 0.5557805597782135\n          -2.303591560572386, -0.047419726848602295\n          -0.7448508385568857, -0.5028514862060547\n          -2.8230229914188385, 0.8093537855893373\n          -1.4274845123291016, 1.4835840165615082\n          -1.3214656114578247, 0.09438827633857727\n          -2.028286747634411, 1.0806758720427752\n          -0.798517256975174, 0.8371041417121887\n          -1.556841880083084, -0.8985426127910614\n          -1.0867036543786526, 1.7443277575075626\n          -1.4234793484210968, 0.6246072947978973\n          -0.04958712309598923, 0.14585793018341064\n\n```", "```scala\n      val userFactorsOrdererd = userFactors.orderBy(\"id\") \n      val userFactorLibSVMFormat = \n        userFactorsOrdererd.rdd.map(x => x(0) + \" \" + \n        getDetails(x(1).asInstanceOf\n          [scala.collection.mutable.WrappedArray[Float]])) \n      println(\"userFactorLibSVMFormat.count() : \" + \n        userFactorLibSVMFormat.count()) \n      print(\"userFactorLibSVMFormat.first() : \" + \n        userFactorLibSVMFormat.first()) \n\n      userFactorLibSVMFormat.coalesce(1)\n        .saveAsTextFile(output + \"/\" + date_time + \n        \"/movie_lens_users_libsvm\")\n\n```", "```scala\n 1 1:0.75239724 2:0.31830165 3:0.031550772 4:-0.63495475 \n          5:-0.719721 6:0.5437525 7:0.59800273 8:-0.4264512  \n          9:0.6661331 \n          10:-0.9702077\n 2 1:-0.053673547 2:-0.24080916 3:-0.6896337 4:-0.3918436   \n          5:-0.4108574 6:0.663401 7:0.1975566 8:0.43086317 9:1.0833738 \n          10:-0.9398713\n 3 1:0.6261427 2:0.58282375 3:-0.48752788 4:-0.36584544 \n          5:-1.1869227 6:0.14955235 7:-0.17821303 8:0.3922112 \n          9:0.5596394 10:-0.83293504\n 4 1:1.0485783 2:0.2569924 3:-0.48094323 4:-1.8882223 \n          5:-1.4912299 6:0.50734115 7:1.2781366 8:0.028034585 \n          9:1.1323715 10:0.4267411\n 5 1:0.31982875 2:0.13479441 3:0.5392742 4:0.33915272 \n          5:-1.1892766 6:0.33669636 7:0.38314193 8:-0.9331541 \n          9:0.531006 10:-1.0546529\n 6 1:-0.5351592 2:0.1995535 3:-0.9234565 4:-0.5741345 \n          5:-0.4506062 6:0.35505387 7:0.41615438 8:-0.32665777 \n          9:0.22966743 10:-1.1040379\n 7 1:0.41014928 2:-0.32102737 3:-0.73221415 4:-0.4017513 \n          5:-0.87815255 6:0.3717881 7:-0.070220165 8:-0.5443932 \n          9:0.24361002 10:-1.2957898\n 8 1:0.2991327 2:0.3574251 3:-0.03855041 4:-0.1719838 \n          5:-0.840421 6:0.89891523 7:0.024321048 8:-0.9811069 \n          9:0.57676667 10:-1.2015694\n 9 1:-1.4988179 2:0.42335498 3:0.5973782 4:-0.11305857 \n          5:-1.3311529 6:0.91228217 7:1.461522 8:1.4502159 9:0.5554214 \n          10:-1.5014526\n 10 1:0.5876411 2:-0.26684982 3:-0.30273324 4:-0.78348076 \n          5:-0.61448336 6:0.5506227 7:0.2809167 8:-0.08864456 \n          9:0.57811487 10:-1.1085391\n\n```", "```scala\n      var userFactorsXY = userFactorsOrdererd.rdd.map( \n        x => getXY(x(1).asInstanceOf\n        [scala.collection.mutable.WrappedArray[Float]])) \n      userFactorsXY.first() \n      userFactorsXY.coalesce(1).saveAsTextFile(output + \"/\" + \n        date_time + \"/movie_lens_user_xy\")\n\n```", "```scala\n          -0.2524261102080345, 0.4112294316291809\n -1.7868174277245998, 1.435323253273964\n -0.8313295543193817, 0.09025487303733826\n -2.55482479929924, 3.3726249802857637\n 0.14377352595329285, -0.736962765455246\n -2.283802881836891, -0.4298199713230133\n -1.9229961037635803, -1.2950050458312035\n -0.39439742639660835, -0.682673366740346\n -1.9222962260246277, 2.8779889345169067\n -1.3799060583114624, 0.21247059851884842\n\n```", "```scala\n      val spConfig = (new \n        SparkConf).setMaster(\"local[1]\").setAppName(\"SparkApp\"). \n        set(\"spark.driver.allowMultipleContexts\", \"true\") \n\n      val spark = SparkSession \n        .builder() \n        .appName(\"Spark SQL Example\") \n        .config(spConfig) \n        .getOrCreate() \n\n      val datasetUsers = spark.read.format(\"libsvm\").load( \n        \"./OUTPUT/11_10_2016_10_28_56/movie_lens_users_libsvm/part-\n        00000\") \n      datasetUsers.show(3)\n\n```", "```scala\n          +-----+--------------------+\n |label|            features|\n +-----+--------------------+\n |  1.0|(10,[0,1,2,3,4,5,...|\n |  2.0|(10,[0,1,2,3,4,5,...|\n |  3.0|(10,[0,1,2,3,4,5,...|\n +-----+--------------------+\n only showing top 3 rows\n\n```", "```scala\n      val kmeans = new KMeans().setK(5).setSeed(1L) \n      val modelUsers = kmeans.fit(datasetUsers)\n\n```", "```scala\n      val modelUsers = kmeans.fit(datasetUsers)\n\n```", "```scala\n      val predictedUserClusters = modelUsers.transform(datasetUsers) \n      predictedUserClusters.show(5)\n\n```", "```scala\n+-----+--------------------+----------+\n|label|            features|prediction|\n+-----+--------------------+----------+\n|  1.0|(10,[0,1,2,3,4,5,...|         2|\n|  2.0|(10,[0,1,2,3,4,5,...|         0|\n|  3.0|(10,[0,1,2,3,4,5,...|         0|\n|  4.0|(10,[0,1,2,3,4,5,...|         2|\n|  5.0|(10,[0,1,2,3,4,5,...|         2|\n+-----+--------------------+----------+\nonly showing top 5 rows\n\n```", "```scala\nCluster : 0\n--------------------------\n+--------------------+\n|                name|\n+--------------------+\n|    GoldenEye (1995)|\n|   Four Rooms (1995)|\n|Shanghai Triad (Y...|\n|Twelve Monkeys (1...|\n|Dead Man Walking ...|\n|Usual Suspects, T...|\n|Mighty Aphrodite ...|\n|Antonia's Line (1...|\n|   Braveheart (1995)|\n|  Taxi Driver (1976)|\n+--------------------+\nonly showing top 10 rows\n\nCluster : 1\n--------------------------\n+--------------------+\n|                name|\n+--------------------+\n|     Bad Boys (1995)|\n|Free Willy 2: The...|\n|        Nadja (1994)|\n|     Net, The (1995)|\n|       Priest (1994)|\n|While You Were Sl...|\n|Ace Ventura: Pet ...|\n|   Free Willy (1993)|\n|Remains of the Da...|\n|Sleepless in Seat...|\n+--------------------+\nonly showing top 10 rows\n\nCluster : 2\n--------------------------\n+--------------------+\n|                name|\n+--------------------+\n|    Toy Story (1995)|\n|   Get Shorty (1995)|\n|      Copycat (1995)|\n|  Richard III (1995)|\n|Seven (Se7en) (1995)|\n|Mr. Holland's Opu...|\n|From Dusk Till Da...|\n|Brothers McMullen...|\n|Batman Forever (1...|\n|   Disclosure (1994)|\n+--------------------+\nonly showing top 10 rows\n\nCluster : 3\n--------------------------\n+--------------------+\n|                name|\n+--------------------+\n|         Babe (1995)|\n|  Postino, Il (1994)|\n|White Balloon, Th...|\n|Muppet Treasure I...|\n|Rumble in the Bro...|\n|Birdcage, The (1996)|\n|    Apollo 13 (1995)|\n|Belle de jour (1967)|\n| Crimson Tide (1995)|\n|To Wong Foo, Than...|\n+--------------------+\nonly showing top 10 rows\n\n```", "```scala\nobject MovieLensKMeansPersist { \n\n  val BASE= \"./data/movie_lens_libsvm_2f\" \n  val time = System.currentTimeMillis() \n  val formatter = new SimpleDateFormat(\"dd_MM_yyyy_hh_mm_ss\") \n\n  import java.util.Calendar \n  val calendar = Calendar.getInstance() \n  calendar.setTimeInMillis(time) \n  val date_time = formatter.format(calendar.getTime()) \n\n  def main(args: Array[String]): Unit = { \n\n    val spConfig = ( \n    new SparkConf).setMaster(\"local[1]\"). \n    setAppName(\"SparkApp\"). \n      set(\"spark.driver.allowMultipleContexts\", \"true\") \n\n    val spark = SparkSession \n      .builder() \n      .appName(\"Spark SQL Example\") \n      .config(spConfig) \n      .getOrCreate() \n\n    val datasetUsers = spark.read.format(\"libsvm\").load( \n      BASE + \"/movie_lens_2f_users_libsvm/part-00000\") \n    datasetUsers.show(3) \n\n    val kmeans = new KMeans().setK(5).setSeed(1L) \n    val modelUsers = kmeans.fit(datasetUsers) \n\n    // Evaluate clustering by computing Within  \n    //Set Sum of Squared Errors. \n\n    val predictedDataSetUsers = modelUsers.transform(datasetUsers) \n    print(predictedDataSetUsers.first()) \n    print(predictedDataSetUsers.count()) \n    val predictionsUsers = \n    predictedDataSetUsers.select(\"prediction\"). \n    rdd.map(x=> x(0)) \n    predictionsUsers.saveAsTextFile( \n    BASE + \"/prediction/\" + date_time + \"/users\") \n\n    val datasetItems = spark.read.format(\"libsvm\").load( \n      BASE + \"/movie_lens_2f_items_libsvm/part-00000\") \n    datasetItems.show(3) \n\n    val kmeansItems = new KMeans().setK(5).setSeed(1L) \n    val modelItems = kmeansItems.fit(datasetItems) \n    // Evaluate clustering by computing Within  \n    //Set Sum of Squared Errors. \n    val WSSSEItems = modelItems.computeCost(datasetItems) \n    println(s\"Items :  Within Set Sum of Squared Errors = \n      $WSSSEItems\") \n\n    // Shows the result. \n    println(\"Items - Cluster Centers: \") \n    modelUsers.clusterCenters.foreach(println) \n    val predictedDataSetItems = modelItems.transform(datasetItems) \n    val predictionsItems = predictedDataSetItems. \n      select(\"prediction\").rdd.map(x=> x(0)) \n    predictionsItems.saveAsTextFile(BASE + \"/prediction/\" +  \n      date_time + \"/items\") \n    spark.stop() \n  }\n\n```", "```scala\nval WSSSEUsers = modelUsers.computeCost(datasetUsers) \nprintln(s\"Users :  Within Set Sum of Squared Errors = $WSSSEUsers\") \nval WSSSEItems = modelItems.computeCost(datasetItems)   \nprintln(s\"Items :  Within Set Sum of Squared Errors = $WSSSEItems\")\n\n```", "```scala\nUsers :  Within Set Sum of Squared Errors = 2261.3086181660324\nItems :  Within Set Sum of Squared Errors = 5647.825222497311\n\n```", "```scala\nobject MovieLensKMeansMetrics { \n  case class RatingX(userId: Int, movieId: Int, rating: Float, \n    timestamp: Long) \n  val DATA_PATH= \"../../../data/ml-100k\" \n  val PATH_MOVIES = DATA_PATH + \"/u.item\" \n  val dataSetUsers = null \n\n  def main(args: Array[String]): Unit = { \n\n    val spConfig = (new \n      SparkConf).setMaster(\"local[1]\").setAppName(\"SparkApp\"). \n      set(\"spark.driver.allowMultipleContexts\", \"true\") \n\n    val spark = SparkSession \n      .builder() \n      .appName(\"Spark SQL Example\") \n      .config(spConfig) \n      .getOrCreate() \n\n    val datasetUsers = spark.read.format(\"libsvm\").load( \n      \"./data/movie_lens_libsvm/movie_lens_users_libsvm/part-\n      00000\") \n    datasetUsers.show(3) \n\n    val k = 5 \n    val itr = Array(1,10,20,50,75,100) \n    val result = new Array[String](itr.length) \n    for(i <- 0 until itr.length){ \n      val w = calculateWSSSE(spark,datasetUsers,itr(i),5,1L) \n      result(i) = itr(i) + \",\" + w \n    } \n    println(\"----------Users----------\") \n    for(j <- 0 until itr.length) { \n      println(result(j)) \n    } \n    println(\"-------------------------\") \n\n    val datasetItems = spark.read.format(\"libsvm\").load( \n      \"./data/movie_lens_libsvm/movie_lens_items_libsvm/\"+     \n      \"part-00000\") \n\n    val resultItems = new Array[String](itr.length) \n    for(i <- 0 until itr.length){ \n      val w = calculateWSSSE(spark,datasetItems,itr(i),5,1L) \n      resultItems(i) = itr(i) + \",\" + w \n    } \n\n    println(\"----------Items----------\") \n    for(j <- 0 until itr.length) { \n      println(resultItems(j)) \n    } \n    println(\"-------------------------\") \n\n    spark.stop() \n  } \n\n  import org.apache.spark.sql.DataFrame \n\n  def calculateWSSSE(spark : SparkSession, dataset : DataFrame,  \n    iterations : Int, k : Int, seed : Long) : Double = { \n    val x = dataset.columns \n\n    val kmeans =  \n      new KMeans().setK(k).setSeed(seed).setMaxIter(iterations) \n\n    val model = kmeans.fit(dataset) \n    val WSSSEUsers = model.computeCost(dataset) \n    return WSSSEUsers \n\n  }\n\n```", "```scala\n----------Users----------\n1,2429.214784372865\n10,2274.362593105573\n20,2261.3086181660324\n50,2261.015660051977\n75,2261.015660051977\n100,2261.015660051977\n-------------------------\n\n----------Items----------\n1,5851.444935665099\n10,5720.505597821477\n20,5647.825222497311\n50,5637.7439669472005\n75,5637.7439669472005\n100,5637.7439669472005\n\n```", "```scala\n        val spConfig = (new                         \n        SparkConf).setMaster(\"local[1]\").setAppName(\"SparkApp\"). \n        set(\"spark.driver.allowMultipleContexts\", \"true\") \n\n        val spark = SparkSession \n          .builder() \n          .appName(\"Spark SQL Example\") \n          .config(spConfig) \n          .getOrCreate() \n\n        val datasetUsers = spark.read.format(\"libsvm\").load( \n          BASE + \"/movie_lens_2f_users_libsvm/part-00000\") \n        datasetUsers.show(3)\n\n```", "```scala\n +-----+--------------------+\n |label|            features|\n +-----+--------------------+\n |  1.0|(2,[0,1],[0.37140...|\n |  2.0|(2,[0,1],[-0.2131...|\n |  3.0|(2,[0,1],[0.28579...|\n +-----+--------------------+\n only showing top 3 rows\n\n```", "```scala\n          val bKMeansUsers = new BisectingKMeans() \n          bKMeansUsers.setMaxIter(10) \n          bKMeansUsers.setMinDivisibleClusterSize(5)\n\n```", "```scala\n          val modelUsers = bKMeansUsers.fit(datasetUsers) \n\n          val movieDF = Util.getMovieDataDF(spark) \n          val predictedUserClusters = \n            modelUsers.transform(datasetUsers) \n          predictedUserClusters.show(5)\n\n```", "```scala\n          +-----+--------------------+----------+\n |label|            features|prediction|\n +-----+--------------------+----------+\n |  1.0|(2,[0,1],[0.37140...|         3|\n |  2.0|(2,[0,1],[-0.2131...|         3|\n |  3.0|(2,[0,1],[0.28579...|         3|\n |  4.0|(2,[0,1],[-0.6541...|         1|\n |  5.0|(2,[0,1],[0.90333...|         2|\n +-----+--------------------+----------+\n only showing top 5 rows\n\n```", "```scala\n        val joinedMovieDFAndPredictedCluster = \n          movieDF.join(predictedUserClusters,predictedUserClusters\n          (\"label\") === movieDF(\"id\")) \n        print(joinedMovieDFAndPredictedCluster.first()) \n        joinedMovieDFAndPredictedCluster.show(5)\n\n```", "```scala\n +--+---------------+-----------+-----+--------------------+----------+\n |id|          name|       date|label|      features|prediction|\n +--+---------------+-----------+-----+--------------------+----------+\n | 1| Toy Story (1995)  |01-Jan-1995|  1.0|(2,[0,1],[0.37140...|3|\n | 2| GoldenEye (1995)  |01-Jan-1995|  2.0|(2,[0,1],[-0.2131...|3|\n | 3|Four Rooms (1995)  |01-Jan-1995|  3.0|(2,[0,1],[0.28579...|3|\n | 4| Get Shorty (1995) |01-Jan-1995|  4.0|(2,[0,1],[-0.6541...|1|\n | 5| Copycat (1995)    |01-Jan-1995|  5.0|(2,[0,1],[0.90333...|2|\n +--+----------------+-----------+-----+--------------------+----------+\n only showing top 5 rows\n\n```", "```scala\n        for(i <- 0 until 5) { \n          val prediction0 =     \n          joinedMovieDFAndPredictedCluster.filter(\"prediction == \" + i) \n          println(\"Cluster : \" + i) \n          println(\"--------------------------\") \n          prediction0.select(\"name\").show(10) \n        }\n\n```", "```scala\n          Cluster : 0\n +--------------------+\n |                name|\n +--------------------+\n |Antonia's Line (1...|\n |Angels and Insect...|\n |Rumble in the Bro...|\n |Doom Generation, ...|\n |     Mad Love (1995)|\n | Strange Days (1995)|\n |       Clerks (1994)|\n |  Hoop Dreams (1994)|\n |Legends of the Fa...|\n |Professional, The...|\n +--------------------+\n only showing top 10 rows\n\n Cluster : 1\n --------------------------\n\n +--------------------+\n |                name|\n +--------------------+\n |   Get Shorty (1995)|\n |Dead Man Walking ...|\n |  Richard III (1995)|\n |Seven (Se7en) (1995)|\n |Usual Suspects, T...|\n |Mighty Aphrodite ...|\n |French Twist (Gaz...|\n |Birdcage, The (1996)|\n |    Desperado (1995)|\n |Free Willy 2: The...|\n +--------------------+\n only showing top 10 rows\n\n Cluster : 2\n --------------------------\n +--------------------+\n          |                name|\n +--------------------+\n          |      Copycat (1995)|\n          |Shanghai Triad (Y...|\n |  Postino, Il (1994)|\n          |From Dusk Till Da...|\n          |   Braveheart (1995)|\n |Batman Forever (1...|\n |        Crumb (1994)|\n          |To Wong Foo, Than...|\n |Billy Madison (1995)|\n |Dolores Claiborne...|\n          +--------------------+\n only showing top 10 rows\n\n          Cluster : 3\n          --------------------------\n          +--------------------+\n |                name|\n          +--------------------+\n          |    Toy Story (1995)|\n |    GoldenEye (1995)|\n |   Four Rooms (1995)|\n |Twelve Monkeys (1...|\n          |         Babe (1995)|\n |Mr. Holland's Opu...|\n |White Balloon, Th...|\n |Muppet Treasure I...|\n          |  Taxi Driver (1976)|\n          |Brothers McMullen...|\n +--------------------+\n          only showing top 10 rows\n\n```", "```scala\n          val WSSSEUsers = modelUsers.computeCost(datasetUsers) \n          println(s\"Users : Within Set Sum of Squared Errors =                      $WSSSEUsers\") \n\n          println(\"Users : Cluster Centers: \") \n          modelUsers.clusterCenters.foreach(println)\n\n```", "```scala\n          Users : Within Set Sum of Squared Errors = 220.213984126387\n          Users : Cluster Centers: \n          [-0.5152650631965345,-0.17908608684257435]\n          [-0.7330009110582011,0.5699292831746033]\n          [0.4657482296168242,0.07541218866995708]\n          [0.07297392612510972,0.7292946749843259]\n\n```", "```scala\n          val datasetItems = spark.read.format(\"libsvm\").load( \n            BASE + \"/movie_lens_2f_items_libsvm/part-00000\") \n          datasetItems.show(3) \n\n          val kmeansItems = new BisectingKMeans().setK(5).setSeed(1L) \n          val modelItems = kmeansItems.fit(datasetItems) \n\n          // Evaluate clustering by computing Within Set \n          // Sum of Squared Errors. \n          val WSSSEItems = modelItems.computeCost(datasetItems) \n          println(s\"Items : Within Set Sum of Squared Errors = \n            $WSSSEItems\") \n\n          // Shows the result. \n          println(\"Items - Cluster Centers: \") \n          modelUsers.clusterCenters.foreach(println) \n\n          Items: within Set Sum of Squared Errors = 538.4272487824393 \n          Items - Cluster Centers:  \n            [-0.5152650631965345,-0.17908608684257435] \n            [-0.7330009110582011,0.5699292831746033] \n            [0.4657482296168242,0.07541218866995708] \n            [0.07297392612510972,0.7292946749843259]\n\n```", "```scala\n          object BisectingKMeansPersist { \n            val PATH = \"/home/ubuntu/work/spark-2.0.0-bin-hadoop2.7/\" \n            val BASE = \"./data/movie_lens_libsvm_2f\" \n\n            val time = System.currentTimeMillis() \n            val formatter = new \n              SimpleDateFormat(\"dd_MM_yyyy_hh_mm_ss\") \n\n            import java.util.Calendar \n            val calendar = Calendar.getInstance() \n            calendar.setTimeInMillis(time) \n            val date_time = formatter.format(calendar.getTime()) \n\n            def main(args: Array[String]): Unit = { \n\n              val spConfig = (new     \n                SparkConf).setMaster(\"local[1]\")\n                .setAppName(\"SparkApp\"). \n              set(\"spark.driver.allowMultipleContexts\", \"true\") \n\n              val spark = SparkSession \n                .builder() \n                .appName(\"Spark SQL Example\") \n                .config(spConfig) \n                .getOrCreate() \n\n              val datasetUsers = spark.read.format(\"libsvm\").load( \n                BASE + \"/movie_lens_2f_users_libsvm/part-00000\") \n\n              val bKMeansUsers = new BisectingKMeans() \n              bKMeansUsers.setMaxIter(10) \n              bKMeansUsers.setMinDivisibleClusterSize(5) \n\n              val modelUsers = bKMeansUsers.fit(datasetUsers) \n              val predictedUserClusters = \n                modelUsers.transform(datasetUsers) \n\n              modelUsers.clusterCenters.foreach(println) \n              val predictedDataSetUsers = \n                modelUsers.transform(datasetUsers) \n              val predictionsUsers =       \n                predictedDataSetUsers.select(\"prediction\")\n                .rdd.map(x=> x(0)) \n               predictionsUsers.saveAsTextFile(BASE + \n                 \"/prediction/\" +      \n               date_time + \"/bkmeans_2f_users\")    \n\n               val datasetItems = \n                 spark.read.format(\"libsvm\").load(BASE + \n                 \"/movie_lens_2f_items_libsvm/part-00000\") \n\n               val kmeansItems = new \n                 BisectingKMeans().setK(5).setSeed(1L) \n               val modelItems = kmeansItems.fit(datasetItems) \n\n               val predictedDataSetItems = \n                 modelItems.transform(datasetItems) \n               val predictionsItems =      \n                 predictedDataSetItems.select(\"prediction\")\n                 .rdd.map(x=> x(0)) \n                 predictionsItems.saveAsTextFile(BASE + \n                 \"/prediction/\" +         \n               date_time + \"/bkmeans_2f_items\") \n               spark.stop() \n            } \n          }\n\n```", "```scala\nobject BisectingKMeansMetrics { \n  case class RatingX(userId: Int, movieId: Int,  \n    rating: Float, timestamp: Long) \n  val DATA_PATH= \"../../../data/ml-100k\" \n  val PATH_MOVIES = DATA_PATH + \"/u.item\" \n  val dataSetUsers = null \n\n  def main(args: Array[String]): Unit = { \n\n    val spConfig = ( \n      new SparkConf).setMaster(\"local[1]\").setAppName(\"SparkApp\"). \n      set(\"spark.driver.allowMultipleContexts\", \"true\") \n\n    val spark = SparkSession \n      .builder() \n      .appName(\"Spark SQL Example\") \n      .config(spConfig) \n      .getOrCreate() \n\n    val datasetUsers = spark.read.format(\"libsvm\").load( \n      \"./data/movie_lens_libsvm/movie_lens_users_libsvm/part-\n      00000\") \n    datasetUsers.show(3) \n\n    val k = 5 \n    val itr = Array(1,10,20,50,75,100) \n    val result = new Array[String](itr.length) \n    for(i <- 0 until itr.length){ \n      val w = calculateWSSSE(spark,datasetUsers,itr(i),5) \n      result(i) = itr(i) + \",\" + w \n    } \n    println(\"----------Users----------\") \n    for(j <- 0 until itr.length) { \n      println(result(j)) \n    } \n    println(\"-------------------------\") \n\n    val datasetItems = spark.read.format(\"libsvm\").load( \n      \"./data/movie_lens_libsvm/movie_lens_items_libsvm/part-\n      00000\") \n    val resultItems = new Array[String](itr.length) \n    for(i <- 0 until itr.length){ \n      val w = calculateWSSSE(spark,datasetItems,itr(i),5) \n      resultItems(i) = itr(i) + \",\" + w \n    } \n\n    println(\"----------Items----------\") \n    for(j <- 0 until itr.length) { \n      println(resultItems(j)) \n    } \n    println(\"-------------------------\") \n\n    spark.stop() \n  } \n\n  import org.apache.spark.sql.DataFrame \n\n  def calculateWSSSE(spark : SparkSession, dataset : DataFrame, \n    iterations : Int, k : Int) : Double = \n  { \n    val x = dataset.columns \n\n    val bKMeans = new BisectingKMeans() \n    bKMeans.setMaxIter(iterations) \n    bKMeans.setMinDivisibleClusterSize(k) \n\n    val model = bKMeans.fit(dataset) \n    val WSSSE = model.computeCost(dataset) \n    return WSSSE \n\n  } \n}\n\n```", "```scala\n       final val featuresCol: Param[String] \n       Param for features column name. \n       final val k: IntParam \n       Number of independent Gaussians in the mixture model. \n       final val \n       maxIter: IntParam \n       Param for maximum number of iterations (>= 0). \n       final val predictionCol: Param[String] \n       Param for prediction column name. \n       final val probabilityCol: Param[String] \n       Param for Column name for predicted class conditional \n       probabilities. \n       final val seed: LongParam \n       Param for random seed. \n       final val tol: DoubleParam\n\n```", "```scala\n       val gmmUsers = new GaussianMixture().setK(5).setSeed(1L)\n\n```", "```scala\n       Print Covariance and Mean\n      for (i <- 0 until modelUsers.gaussians.length) { \n        println(\"Users: weight=%f\\ncov=%s\\nmean=\\n%s\\n\" format \n          (modelUsers.weights(i), modelUsers.gaussians(i).cov,                           \n          modelUsers.gaussians(i).mean)) \n      }\n\n```", "```scala\n          object GMMClustering { \n\n            def main(args: Array[String]): Unit = { \n              val spConfig = (new SparkConf).setMaster(\"local[1]\"). \n                setAppName(\"SparkApp\"). \n                set(\"spark.driver.allowMultipleContexts\", \"true\") \n\n              val spark = SparkSession \n                .builder() \n                .appName(\"Spark SQL Example\") \n                .config(spConfig) \n                .getOrCreate() \n\n              val datasetUsers = spark.read.format(\"libsvm\").                \n               load(\"./data/movie_lens_libsvm/movie_lens_users_libsvm/\"\n               + \"part-00000\") \n              datasetUsers.show(3) \n\n              val gmmUsers = new GaussianMixture().setK(5).setSeed(1L) \n              val modelUsers = gmmUsers.fit(datasetUsers) \n\n              for (i <- 0 until modelUsers.gaussians.length) { \n                println(\"Users : weight=%f\\ncov=%s\\nmean=\\n%s\\n\" \n                   format (modelUsers.weights(i),  \n                   modelUsers.gaussians(i).cov,  \n                   modelUsers.gaussians(i).mean)) \n                } \n\n              val dataSetItems = spark.read.format(\"libsvm\").load( \n                \"./data/movie_lens_libsvm/movie_lens_items_libsvm/\" + \n                \"part-00000\") \n\n              val gmmItems = new \n                  GaussianMixture().setK(5).setSeed(1L) \n              val modelItems = gmmItems.fit(dataSetItems) \n\n              for (i <- 0 until modelItems.gaussians.length) { \n                println(\"Items : weight=%f\\ncov=%s\\nmean=\\n%s\\n\" \n                   format (modelUsers.weights(i), \n                   modelUsers.gaussians(i).cov, \n                   modelUsers.gaussians(i).mean)) \n              } \n              spark.stop() \n            }\n\n```"]