["```scala\n{ \n val sc = new SparkContext(\"local[1]\", \"Classification\") \n\n  // get StumbleUpon dataset 'https://www.kaggle.com/c/stumbleupon' \n  val records = sc.textFile( \n   SparkConstants.PATH + \"data/train_noheader.tsv\").map( \n   line => line.split(\"\\t\")) \n\n  val data_persistent = records.map { r => \n    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\")) \n    val label = trimmed(r.size - 1).toInt \n    val features = trimmed.slice(4, r.size - 1).map( \n     d => if (d == \"?\") 0.0 else d.toDouble) \n    val len = features.size.toInt \n    val len_2 = math.floor(len/2).toInt \n    val x = features.slice(0,len_2) \n\n    val y = features.slice(len_2 -1 ,len ) \n    var i=0 \n    var sum_x = 0.0 \n    var sum_y = 0.0 \n    while (i < x.length) { \n    sum_x += x(i) \n    i += 1 \n } \n\n i = 0 \n while (i < y.length) { \n   sum_y += y(i) \n   i += 1 \n } \n\n if (sum_y != 0.0) { \n    if(sum_x != 0.0) { \n       math.log(sum_x) + \",\" + math.log(sum_y) \n    }else { \n       sum_x + \",\" + math.log(sum_y) \n    }   \n }else { \n    if(sum_x != 0.0) { \n         math.log(sum_x) + \",\" + 0.0 \n    }else { \n        sum_x + \",\" + 0.0 \n    } \n  } \n\n} \nval dataone = data_persistent.first() \n  data_persistent.saveAsTextFile(SparkConstants.PATH + \n   \"/results/raw-input-log\") \n  sc.stop() \n\n} \n\n```", "```scala\n  > sed 1d train.tsv > train_noheader.tsv \n\n```", "```scala\n  >./bin/spark-shell --driver-memory 4g  \n\n```", "```scala\nval rawData = sc.textFile(\"/PATH/train_noheader.tsv\") \nval records = rawData.map(line => line.split(\"\\t\")) \nrecords.first() \n\n```", "```scala\nArray[String] = Array(\"http://www.bloomberg.com/news/2010-12-23/ibm-predicts-holographic-calls-air-breathing-batteries-by-2015.html\", \"4042\", ...  \n\n```", "```scala\nimport org.apache.spark.mllib.regression.LabeledPoint \nimport org.apache.spark.mllib.linalg.Vectors \nval data = records.map { r => \n  val trimmed = r.map(_.replaceAll(\"\\\"\", \"\")) \n  val label = trimmed(r.size - 1).toInt \n  val features = trimmed.slice(4, r.size - 1).map(d => if (d ==   \"?\") 0.0 else d.toDouble) \n  LabeledPoint(label, Vectors.dense(features)) \n} \n\n```", "```scala\ndata.cache \nval numData = data.count \n\n```", "```scala\nval nbData = records.map { r => \n  val trimmed = r.map(_.replaceAll(\"\\\"\", \"\")) \n  val label = trimmed(r.size - 1).toInt \n  val features = trimmed.slice(4, r.size - 1).map(d => if (d ==  \n  \"?\") 0.0 else d.toDouble).map(d => if (d < 0) 0.0 else d) \n  LabeledPoint(label, Vectors.dense(features)) \n}  \n\n```", "```scala\ncase \"LR\" => LogisticRegressionPipeline.logisticRegressionPipeline(vectorAssembler, dataFrame) \n\ncase \"DT\" => DecisionTreePipeline.decisionTreePipeline(vectorAssembler, dataFrame) \n\ncase \"RF\" => RandomForestPipeline.randomForestPipeline(vectorAssembler, dataFrame) \n\ncase \"GBT\" => GradientBoostedTreePipeline.gradientBoostedTreePipeline(vectorAssembler, dataFrame) \n\ncase \"NB\" => NaiveBayesPipeline.naiveBayesPipeline(vectorAssembler, dataFrame) \n\ncase \"SVM\" => SVMPipeline.svmPipeline(sparkContext) \n\n```", "```scala\n// create logisitic regression object \nval lr = new LogisticRegression() \n\n```", "```scala\n-------------------------------------------------------------------------------------------\norg.apache.spark.ml.tuning \nClass ParamGridBuilder \nBuilder for a param grid used in grid search-based model selection. \n-------------------------------------------------------------------------------------------\n\n// set params using ParamGrid builder \nval paramGrid = new ParamGridBuilder() \n  .addGrid(lr.regParam, Array(0.1, 0.01)) \n  .addGrid(lr.fitIntercept) \n  .addGrid(lr.elasticNetParam, Array(0.0, 0.25, 0.5, 0.75, 1.0)) \n  .build() \n\n// set pipeline to run the vector assembler and logistic regression // estimator \nval pipeline = new Pipeline().setStages(Array(vectorAssembler, \n lr)) \n\n```", "```scala\n-------------------------------------------------------------------------------------------\norg.apache.spark.ml.tuning \nClass TraiValidationSplit \nValidation for hyper-parameter tuning. Randomly splits the input dataset into train and validation sets. \n-------------------------------------------------------------------------------------------\n\n// use train validation split and regression evaluator for //evaluation \nval trainValidationSplit = new TrainValidationSplit() \n  .setEstimator(pipeline) \n  .setEvaluator(new RegressionEvaluator) \n  .setEstimatorParamMaps(paramGrid) \n  .setTrainRatio(0.8) \n\nval Array(training, test) = dataFrame.randomSplit(Array(0.8, 0.2), seed = 12345) \n\n// run the estimator \nval model = trainValidationSplit.fit(training) \n\nval holdout = model.transform(test).select(\"prediction\",\"label\") \n\n// have to do a type conversion for RegressionMetrics \nval rm = new RegressionMetrics(holdout.rdd.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))) \n\nlogger.info(\"Test Metrics\") \nlogger.info(\"Test Explained Variance:\") \nlogger.info(rm.explainedVariance) \nlogger.info(\"Test R^2 Coef:\") \nlogger.info(rm.r2) \nlogger.info(\"Test MSE:\") \nlogger.info(rm.meanSquaredError) \nlogger.info(\"Test RMSE:\") \nlogger.info(rm.rootMeanSquaredError) \n\nval totalPoints = test.count() \nval lrTotalCorrect = holdout.rdd.map(\n  x => if (x(0).asInstanceOf[Double] == x(1).asInstanceOf[Double]) \n  1 else 0).sum() \nval accuracy = lrTotalCorrect/totalPoints \nprintln(\"Accuracy of LogisticRegression is: \", accuracy) \n\n```", "```scala\nAccuracy of LogisticRegression is: ,0.6374918354016982\nMean Squared Error:,0.3625081645983018\nRoot Mean Squared Error:,0.6020865092312747  \n\n```", "```scala\n// read stumble upon dataset as rdd \nval records = sc.textFile(\"/home/ubuntu/work/ml-resources/spark-ml/train_noheader.tsv\").map(line => line.split(\"\\t\")) \n\n// get features and label from the rdd \nval data = records.map { r => \n    val trimmed = r.map(_.replaceAll(\"\\\"\", \"\")) \n    val label = trimmed(r.size - 1).toInt \n    val features = trimmed.slice(4, r.size - 1).map(d => if (d == \"?\") 0.0 else d.toDouble) \n    LabeledPoint(label, Vectors.dense(features)) \n  } \n\n  // params for SVM \n  val numIterations = 10 \n\n  // Run training algorithm to build the model \n  val svmModel = SVMWithSGD.train(data, numIterations) \n\n  // Clear the default threshold. \n  svmModel.clearThreshold() \n\n  val svmTotalCorrect = data.map { point => \n    if(svmModel.predict(point.features) == point.label) 1 else 0 \n  }.sum() \n\n  // calculate accuracy \n  val svmAccuracy = svmTotalCorrect / data.count() \n  println(svmAccuracy) \n} \n\n```", "```scala\n Area under ROC = 1.0  \n\n```", "```scala\n// split data randomly into training and testing dataset \nval Array(training, test) = dataFrame.randomSplit(Array(0.8, 0.2), seed = 12345) \n\n// Set up Pipeline \nval stages = new mutable.ArrayBuffer[PipelineStage]() \n\nval labelIndexer = new StringIndexer() \n  .setInputCol(\"label\") \n  .setOutputCol(\"indexedLabel\") \nstages += labelIndexer \n\n// create naive bayes model \nval nb = new NaiveBayes() \n\nstages += vectorAssembler \nstages += nb \nval pipeline = new Pipeline().setStages(stages.toArray) \n\n// Fit the Pipeline \nval startTime = System.nanoTime() \nval model = pipeline.fit(training) \nval elapsedTime = (System.nanoTime() - startTime) / 1e9 \nprintln(s\"Training time: $elapsedTime seconds\") \n\nval holdout = model.transform(test).select(\"prediction\",\"label\") \n\n// Select (prediction, true label) and compute test error \nval evaluator = new MulticlassClassificationEvaluator() \n  .setLabelCol(\"label\") \n  .setPredictionCol(\"prediction\") \n  .setMetricName(\"accuracy\") \nval mAccuracy = evaluator.evaluate(holdout) \nprintln(\"Test set accuracy = \" + mAccuracy) \n\n```", "```scala\nTraining time: 2.114725642 seconds\nAccuracy: 0.5660377358490566   \n\n```", "```scala\n// split data randomly into training and testing dataset \nval Array(training, test) = dataFrame.randomSplit(Array(0.8, 0.2), seed = 12345) \n\n// Set up Pipeline \nval stages = new mutable.ArrayBuffer[PipelineStage]() \n\nval labelIndexer = new StringIndexer() \n  .setInputCol(\"label\") \n  .setOutputCol(\"indexedLabel\") \nstages += labelIndexer \n\n// create Decision Tree Model \nval dt = new DecisionTreeClassifier() \n  .setFeaturesCol(vectorAssembler.getOutputCol) \n  .setLabelCol(\"indexedLabel\") \n  .setMaxDepth(5) \n  .setMaxBins(32) \n  .setMinInstancesPerNode(1) \n  .setMinInfoGain(0.0) \n  .setCacheNodeIds(false) \n  .setCheckpointInterval(10) \n\nstages += vectorAssembler \nstages += dt \nval pipeline = new Pipeline().setStages(stages.toArray) \n\n// Fit the Pipeline \nval startTime = System.nanoTime() \nval model = pipeline.fit(training) \nval elapsedTime = (System.nanoTime() - startTime) / 1e9 \nprintln(s\"Training time: $elapsedTime seconds\") \n\nval holdout = model.transform(test).select(\"prediction\",\"label\") \n\n// Select (prediction, true label) and compute test error \nval evaluator = new MulticlassClassificationEvaluator() \n  .setLabelCol(\"label\") \n  .setPredictionCol(\"prediction\") \n  .setMetricName(\"accuracy\") \nval mAccuracy = evaluator.evaluate(holdout) \nprintln(\"Test set accuracy = \" + mAccuracy) \n\n```", "```scala\nAccuracy: 0.3786163522012579  \n\n```", "```scala\n// split data randomly into training and testing dataset \nval Array(training, test) = dataFrame.randomSplit(Array(0.8, 0.2), seed = 12345) \n\n// Set up Pipeline \nval stages = new mutable.ArrayBuffer[PipelineStage]() \n\nval labelIndexer = new StringIndexer() \n  .setInputCol(\"label\") \n  .setOutputCol(\"indexedLabel\") \nstages += labelIndexer \n\n// create Random Forest Model \nval rf = new RandomForestClassifier() \n  .setFeaturesCol(vectorAssembler.getOutputCol) \n  .setLabelCol(\"indexedLabel\") \n  .setNumTrees(20) \n  .setMaxDepth(5) \n  .setMaxBins(32) \n  .setMinInstancesPerNode(1) \n  .setMinInfoGain(0.0) \n  .setCacheNodeIds(false) \n  .setCheckpointInterval(10) \n\nstages += vectorAssembler \nstages += rf \nval pipeline = new Pipeline().setStages(stages.toArray) \n\n// Fit the Pipeline \nval startTime = System.nanoTime() \nval model = pipeline.fit(training) \nval elapsedTime = (System.nanoTime() - startTime) / 1e9 \nprintln(s\"Training time: $elapsedTime seconds\") \n\nval holdout = model.transform(test).select(\"prediction\",\"label\") \n\n// Select (prediction, true label) and compute test error \nval evaluator = new MulticlassClassificationEvaluator() \n  .setLabelCol(\"label\") \n  .setPredictionCol(\"prediction\") \n  .setMetricName(\"accuracy\") \nval mAccuracy = evaluator.evaluate(holdout) \nprintln(\"Test set accuracy = \" + mAccuracy) \n\n```", "```scala\nAccuracy: 0.348  \n\n```", "```scala\n// split data randomly into training and testing dataset \nval Array(training, test) = dataFrame.randomSplit(Array(0.8, 0.2), seed = 12345) \n\n// Set up Pipeline \nval stages = new mutable.ArrayBuffer[PipelineStage]() \n\nval labelIndexer = new StringIndexer() \n  .setInputCol(\"label\") \n  .setOutputCol(\"indexedLabel\") \nstages += labelIndexer \n\n// create GBT Model \nval gbt = new GBTClassifier() \n  .setFeaturesCol(vectorAssembler.getOutputCol) \n  .setLabelCol(\"indexedLabel\") \n  .setMaxIter(10) \n\nstages += vectorAssembler \nstages += gbt \nval pipeline = new Pipeline().setStages(stages.toArray) \n\n// Fit the Pipeline \nval startTime = System.nanoTime() \nval model = pipeline.fit(training) \nval elapsedTime = (System.nanoTime() - startTime) / 1e9 \nprintln(s\"Training time: $elapsedTime seconds\") \n\nval holdout = model.transform(test).select(\"prediction\",\"label\") \n\n// have to do a type conversion for RegressionMetrics \nval rm = new RegressionMetrics(holdout.rdd.map(x => (x(0).asInstanceOf[Double], x(1).asInstanceOf[Double]))) \n\nlogger.info(\"Test Metrics\") \nlogger.info(\"Test Explained Variance:\") \nlogger.info(rm.explainedVariance) \nlogger.info(\"Test R^2 Coef:\") \nlogger.info(rm.r2) \nlogger.info(\"Test MSE:\") \nlogger.info(rm.meanSquaredError) \nlogger.info(\"Test RMSE:\") \nlogger.info(rm.rootMeanSquaredError) \n\nval predictions = model.transform(test).select(\"prediction\").rdd.map(_.getDouble(0)) \nval labels = model.transform(test).select(\"label\").rdd.map(_.getDouble(0)) \nval accuracy = new MulticlassMetrics(predictions.zip(labels)).precision \nprintln(s\"  Accuracy : $accuracy\") \n\n```", "```scala\nAccuracy: 0.3647  \n\n```", "```scala\npackage org.sparksamples.classification.stumbleupon \n\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassifier \nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator \nimport org.apache.spark.sql.SparkSession \n\n// set VM Option as -Dspark.master=local[1] \nobject MultilayerPerceptronClassifierExample { \n\n  def main(args: Array[String]): Unit = { \n    val spark = SparkSession \n      .builder \n      .appName(\"MultilayerPerceptronClassifierExample\") \n      .getOrCreate() \n\n    // Load the data stored in LIBSVM format as a DataFrame. \n    val data = spark.read.format(\"libsvm\") \n      .load(\"/Users/manpreet.singh/Sandbox/codehub/github/machinelearning/spark-ml/Chapter_06/2.0.0/scala-spark-app/src/main/scala/org/sparksamples/classification/dataset/spark-data/sample_multiclass_classification_data.txt\") \n\n    // Split the data into train and test \n    val splits = data.randomSplit(Array(0.8, 0.2), seed = 1234L) \n    val train = splits(0) \n    val test = splits(1) \n\n    // specify layers for the neural network: \n    // input layer of size 4 (features),  \n    //two intermediate of size 5 and 4 \n    // and output of size 3 (classes) \n    val layers = Array[Int](4, 5, 4, 3) \n\n    // create the trainer and set its parameters \n    val trainer = new MultilayerPerceptronClassifier() \n      .setLayers(layers) \n      .setBlockSize(128) \n      .setSeed(1234L) \n      .setMaxIter(100) \n\n    // train the model \n    val model = trainer.fit(train) \n\n    // compute accuracy on the test set \n    val result = model.transform(test) \n    val predictionAndLabels = result.select(\"prediction\", \"label\") \n    val evaluator = new MulticlassClassificationEvaluator() \n      .setMetricName(\"accuracy\") \n\n    println(\"Test set accuracy = \" + \n     evaluator.evaluate(predictionAndLabels)) \n\n    spark.stop() \n  } \n} \n\n```", "```scala\nPrecision = 1.0  \n\n```", "```scala\ncase class LabeledPoint(label: Double, features: Vector) \n\n```", "```scala\nimport  \norg.apache.spark.mllib.classification.LogisticRegressionWithSGD \nimport org.apache.spark.mllib.classification.SVMWithSGD \nimport org.apache.spark.mllib.classification.NaiveBayes \nimport org.apache.spark.mllib.tree.DecisionTree \nimport org.apache.spark.mllib.tree.configuration.Algo \nimport org.apache.spark.mllib.tree.impurity.Entropy  \nval numIterations = 10 \nval maxTreeDepth = 5 \n\n```", "```scala\nval lrModel = LogisticRegressionWithSGD.train(data, numIterations) \n\n```", "```scala\n...\n14/12/06 13:41:47 INFO DAGScheduler: Job 81 finished: reduce at RDDFunctions.scala:112, took 0.011968 s\n14/12/06 13:41:47 INFO GradientDescent: GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses 0.6931471805599474, 1196521.395699124, Infinity, 1861127.002201189, Infinity, 2639638.049627607, Infinity, Infinity, Infinity, Infinity\nlrModel: org.apache.spark.mllib.classification.LogisticRegressionModel = (weights=[-0.11372778986947886,-0.511619752777837, \n...  \n\n```", "```scala\nval svmModel = SVMWithSGD.train(data, numIterations) \n\n```", "```scala\n...\n14/12/06 13:43:08 INFO DAGScheduler: Job 94 finished: reduce at RDDFunctions.scala:112, took 0.007192 s\n14/12/06 13:43:08 INFO GradientDescent: GradientDescent.runMiniBatchSGD finished. Last 10 stochastic losses 1.0, 2398226.619666797, 2196192.9647478117, 3057987.2024311484, 271452.9038284356, 3158131.191895948, 1041799.350498323, 1507522.941537049, 1754560.9909073508, 136866.76745605646\nsvmModel: org.apache.spark.mllib.classification.SVMModel = (weights=[-0.12218838697834929,-0.5275107581589767,\n...  \n\n```", "```scala\nval nbModel = NaiveBayes.train(nbData) \n\n```", "```scala\n...\n14/12/06 13:44:48 INFO DAGScheduler: Job 95 finished: collect at NaiveBayes.scala:120, took 0.441273 s\nnbModel: org.apache.spark.mllib.classification.NaiveBayesModel = org.apache.spark.mllib.classification.NaiveBayesModel@666ac612 \n...  \n\n```", "```scala\nval dtModel = DecisionTree.train(data, Algo.Classification, Entropy, maxTreeDepth) \n\n```", "```scala\n...\n14/12/06 13:46:03 INFO DAGScheduler: Job 104 finished: collectAsMap at DecisionTree.scala:653, took 0.031338 s\n...\ntotal: 0.343024\nfindSplitsBins: 0.119499\nfindBestSplits: 0.200352\nchooseSplits: 0.199705\ndtModel: org.apache.spark.mllib.tree.model.DecisionTreeModel = DecisionTreeModel classifier of depth 5 with 61 nodes \n...  \n\n```", "```scala\nval dataPoint = data.first \nval prediction = lrModel.predict(dataPoint.features) \n\n```", "```scala\nprediction: Double = 1.0  \n\n```", "```scala\nval trueLabel = dataPoint.label \n\n```", "```scala\ntrueLabel: Double = 0.0  \n\n```", "```scala\nval predictions = lrModel.predict(data.map(lp => lp.features)) \npredictions.take(5) \n\n```", "```scala\nArray[Double] = Array(1.0, 1.0, 1.0, 1.0, 1.0)  \n\n```", "```scala\nval lrTotalCorrect = data.map { point => \n  if (lrModel.predict(point.features) == point.label) 1 else 0 \n}.sum  \nval lrAccuracy = lrTotalCorrect / data.count \n\n```", "```scala\nlrAccuracy: Double = 0.5146720757268425  \n\n```", "```scala\nval svmTotalCorrect = data.map { point => \n  if (svmModel.predict(point.features) == point.label) 1 else 0 \n}.sum \nval nbTotalCorrect = nbData.map { point => \n  if (nbModel.predict(point.features) == point.label) 1 else 0 \n}.sum \n\n```", "```scala\nval dtTotalCorrect = data.map { point => \n  val score = dtModel.predict(point.features) \n  val predicted = if (score > 0.5) 1 else 0  \n  if (predicted == point.label) 1 else 0 \n}.sum  \n\n```", "```scala\nval svmAccuracy = svmTotalCorrect / numData \n\n```", "```scala\nsvmAccuracy: Double = 0.5146720757268425  \n\n```", "```scala\nval nbAccuracy = nbTotalCorrect / numData \n\n```", "```scala\nnbAccuracy: Double = 0.5803921568627451  \n\n```", "```scala\nval dtAccuracy = dtTotalCorrect / numData \n\n```", "```scala\ndtAccuracy: Double = 0.6482758620689655  \n\n```", "```scala\nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics \nval metrics = Seq(lrModel, svmModel).map { model =>  \n  val scoreAndLabels = data.map { point => \n    (model.predict(point.features), point.label) \n  } \n  val metrics = new BinaryClassificationMetrics(scoreAndLabels) \n  (model.getClass.getSimpleName, metrics.areaUnderPR, metrics.areaUnderROC) \n} \n\n```", "```scala\nval nbMetrics = Seq(nbModel).map{ model => \n  val scoreAndLabels = nbData.map { point => \n    val score = model.predict(point.features) \n    (if (score > 0.5) 1.0 else 0.0, point.label) \n  } \n  val metrics = new BinaryClassificationMetrics(scoreAndLabels) \n  (model.getClass.getSimpleName, metrics.areaUnderPR,  \n  metrics.areaUnderROC) \n} \n\n```", "```scala\nval dtMetrics = Seq(dtModel).map{ model => \n  val scoreAndLabels = data.map { point => \n    val score = model.predict(point.features) \n    (if (score > 0.5) 1.0 else 0.0, point.label) \n  } \n  val metrics = new BinaryClassificationMetrics(scoreAndLabels) \n  (model.getClass.getSimpleName, metrics.areaUnderPR,  \n  metrics.areaUnderROC) \n} \nval allMetrics = metrics ++ nbMetrics ++ dtMetrics \nallMetrics.foreach{ case (m, pr, roc) =>  \n  println(f\"$m, Area under PR: ${pr * 100.0}%2.4f%%, Area under  \n  ROC: ${roc * 100.0}%2.4f%%\")  \n} \n\n```", "```scala\nLogisticRegressionModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\nSVMModel, Area under PR: 75.6759%, Area under ROC: 50.1418%\nNaiveBayesModel, Area under PR: 68.0851%, Area under ROC: 58.3559%\nDecisionTreeModel, Area under PR: 74.3081%, Area under ROC: 64.8837%  \n\n```", "```scala\nimport org.apache.spark.mllib.linalg.distributed.RowMatrix \nval vectors = data.map(lp => lp.features) \nval matrix = new RowMatrix(vectors) \nval matrixSummary = matrix.computeColumnSummaryStatistics() \n\n```", "```scala\nprintln(matrixSummary.mean) \n\n```", "```scala\n[0.41225805299526636,2.761823191986623,0.46823047328614004, ...  \n\n```", "```scala\nprintln(matrixSummary.min) \n\n```", "```scala\n[0.0,0.0,0.0,0.0,0.0,0.0,0.0,-1.0,0.0,0.0,0.0,0.045564223,-1.0, ...  \n\n```", "```scala\nprintln(matrixSummary.max) \n\n```", "```scala\n[0.999426,363.0,1.0,1.0,0.980392157,0.980392157,21.0,0.25,0.0,0.444444444, ...  \n\n```", "```scala\nprintln(matrixSummary.variance) \n\n```", "```scala\n[0.1097424416755897,74.30082476809638,0.04126316989120246, ...  \n\n```", "```scala\nprintln(matrixSummary.numNonzeros) \n\n```", "```scala\n[5053.0,7354.0,7172.0,6821.0,6160.0,5128.0,7350.0,1257.0,0.0, ...  \n\n```", "```scala\nimport org.apache.spark.mllib.feature.StandardScaler \nval scaler = new StandardScaler(withMean = true, withStd = true).fit(vectors) \nval scaledData = data.map(lp => LabeledPoint(lp.label, scaler.transform(lp.features))) \n\n```", "```scala\nprintln(data.first.features) \n\n```", "```scala\n[0.789131,2.055555556,0.676470588,0.205882353,  \n\n```", "```scala\nprintln(scaledData.first.features) \n\n```", "```scala\n[1.1376439023494747,-0.08193556218743517,1.025134766284205,-0.0558631837375738,  \n\n```", "```scala\nprintln((0.789131 - 0.41225805299526636)/ math.sqrt(0.1097424416755897)) \n\n```", "```scala\n1.137647336497682  \n\n```", "```scala\nval lrModelScaled = LogisticRegressionWithSGD.train(scaledData, numIterations) \nval lrTotalCorrectScaled = scaledData.map { point => \n  if (lrModelScaled.predict(point.features) == point.label) 1 else  \n  0 \n}.sum \nval lrAccuracyScaled = lrTotalCorrectScaled / numData \nval lrPredictionsVsTrue = scaledData.map { point =>  \n  (lrModelScaled.predict(point.features), point.label)  \n} \nval lrMetricsScaled = new BinaryClassificationMetrics(lrPredictionsVsTrue) \nval lrPr = lrMetricsScaled.areaUnderPR \nval lrRoc = lrMetricsScaled.areaUnderROC \nprintln(f\"${lrModelScaled.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaled * 100}%2.4f%%\\nArea under PR: ${lrPr * 100.0}%2.4f%%\\nArea under ROC: ${lrRoc * 100.0}%2.4f%%\")  \n\n```", "```scala\nLogisticRegressionModel\nAccuracy: 62.0419%\nArea under PR: 72.7254%\nArea under ROC: 61.9663%   \n\n```", "```scala\nval categories = records.map(r => r(3)).distinct.collect.zipWithIndex.toMap \nval numCategories = categories.size \nprintln(categories) \n\n```", "```scala\nMap(\"weather\" -> 0, \"sports\" -> 6, \"unknown\" -> 4, \"computer_internet\" -> 12, \"?\" -> 11, \"culture_politics\" -> 3, \"religion\" -> 8, \"recreation\" -> 2, \"arts_entertainment\" -> 9, \"health\" -> 5, \"law_crime\" -> 10, \"gaming\" -> 13, \"business\" -> 1, \"science_technology\" -> 7)  \n\n```", "```scala\nprintln(numCategories) \n\n```", "```scala\n14  \n\n```", "```scala\nval dataCategories = records.map { r => \n  val trimmed = r.map(_.replaceAll(\"\\\"\", \"\")) \n  val label = trimmed(r.size - 1).toInt \n  val categoryIdx = categories(r(3)) \n  val categoryFeatures = Array.ofDim[Double](numCategories) \n  categoryFeatures(categoryIdx) = 1.0 \n  val otherFeatures = trimmed.slice(4, r.size - 1).map(d => if   (d == \"?\") 0.0 else d.toDouble) \n  val features = categoryFeatures ++ otherFeatures \n  LabeledPoint(label, Vectors.dense(features)) \n} \nprintln(dataCategories.first) \n\n```", "```scala\nLabeledPoint(0.0[0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556,0.676470588,0.205882353,0.047058824,0.023529412,0.443783175,0.0,0.0,0.09077381,0.0,0.245831182,0.003883495,1.0,1.0,24.0,0.0,5424.0,170.0,8.0,0.152941176,0.079129575])  \n\n```", "```scala\nval scalerCats = new StandardScaler(withMean = true, withStd = true).fit(dataCategories.map(lp => lp.features)) \nval scaledDataCats = dataCategories.map(lp => LabeledPoint(lp.label, scalerCats.transform(lp.features))) \n\n```", "```scala\nprintln(dataCategories.first.features) \n\n```", "```scala\n0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.789131,2.055555556 ...  \n\n```", "```scala\nprintln(scaledDataCats.first.features) \n\n```", "```scala\n[-0.023261105535492967,2.720728254208072,-0.4464200056407091,-0.2205258360869135, ...  \n\n```", "```scala\nval lrModelScaledCats = LogisticRegressionWithSGD.train(scaledDataCats, numIterations) \nval lrTotalCorrectScaledCats = scaledDataCats.map { point => \n  if (lrModelScaledCats.predict(point.features) == point.label) 1 else 0 \n}.sum \nval lrAccuracyScaledCats = lrTotalCorrectScaledCats / numData \nval lrPredictionsVsTrueCats = scaledDataCats.map { point =>  \n  (lrModelScaledCats.predict(point.features), point.label)  \n} \nval lrMetricsScaledCats = new BinaryClassificationMetrics(lrPredictionsVsTrueCats) \nval lrPrCats = lrMetricsScaledCats.areaUnderPR \nval lrRocCats = lrMetricsScaledCats.areaUnderROC \nprintln(f\"${lrModelScaledCats.getClass.getSimpleName}\\nAccuracy: ${lrAccuracyScaledCats * 100}%2.4f%%\\nArea under PR: ${lrPrCats * 100.0}%2.4f%%\\nArea under ROC: ${lrRocCats * 100.0}%2.4f%%\")  \n\n```", "```scala\nLogisticRegressionModel\nAccuracy: 66.5720%\nArea under PR: 75.7964%\nArea under ROC: 66.5483%  \n\n```", "```scala\nval dataNB = records.map { r => \n  val trimmed = r.map(_.replaceAll(\"\\\"\", \"\")) \n  val label = trimmed(r.size - 1).toInt \n  val categoryIdx = categories(r(3)) \n  val categoryFeatures = Array.ofDim[Double](numCategories) \n  categoryFeatures(categoryIdx) = 1.0 \n  LabeledPoint(label, Vectors.dense(categoryFeatures)) \n} \n\n```", "```scala\nval nbModelCats = NaiveBayes.train(dataNB) \nval nbTotalCorrectCats = dataNB.map { point => \n  if (nbModelCats.predict(point.features) == point.label) 1 else 0 \n}.sum \nval nbAccuracyCats = nbTotalCorrectCats / numData \nval nbPredictionsVsTrueCats = dataNB.map { point =>  \n  (nbModelCats.predict(point.features), point.label)  \n} \nval nbMetricsCats = new BinaryClassificationMetrics(nbPredictionsVsTrueCats) \nval nbPrCats = nbMetricsCats.areaUnderPR \nval nbRocCats = nbMetricsCats.areaUnderROC \nprintln(f\"${nbModelCats.getClass.getSimpleName}\\nAccuracy: ${nbAccuracyCats * 100}%2.4f%%\\nArea under PR: ${nbPrCats * 100.0}%2.4f%%\\nArea under ROC: ${nbRocCats * 100.0}%2.4f%%\") \n\n```", "```scala\nNaiveBayesModel\nAccuracy: 60.9601%\nArea under PR: 74.0522%\nArea under ROC: 60.5138%\n\n```", "```scala\nclass LogisticRegressionWithSGD private ( \n  private var stepSize: Double, \n  private var numIterations: Int, \n  private var regParam: Double, \n  private var miniBatchFraction: Double) \n  extends GeneralizedLinearAlgorithm[LogisticRegressionModel] ... \n\n```", "```scala\nprivate val gradient = new LogisticGradient() \nprivate val updater = new SimpleUpdater() \noverride val optimizer = new GradientDescent(gradient, updater) \n  .setStepSize(stepSize) \n  .setNumIterations(numIterations) \n  .setRegParam(regParam) \n  .setMiniBatchFraction(miniBatchFraction) \n\n```", "```scala\nimport org.apache.spark.rdd.RDD \nimport org.apache.spark.mllib.optimization.Updater \nimport org.apache.spark.mllib.optimization.SimpleUpdater \nimport org.apache.spark.mllib.optimization.L1Updater \nimport org.apache.spark.mllib.optimization.SquaredL2Updater \nimport org.apache.spark.mllib.classification.ClassificationModel \n\n```", "```scala\ndef trainWithParams(input: RDD[LabeledPoint], regParam: Double, numIterations: Int, updater: Updater, stepSize: Double) = { \n  val lr = new LogisticRegressionWithSGD \n  lr.optimizer.setNumIterations(numIterations).  \n  setUpdater(updater).setRegParam(regParam).setStepSize(stepSize) \n  lr.run(input) \n} \n\n```", "```scala\ndef createMetrics(label: String, data: RDD[LabeledPoint], model: ClassificationModel) = { \n  val scoreAndLabels = data.map { point => \n    (model.predict(point.features), point.label) \n  } \n  val metrics = new BinaryClassificationMetrics(scoreAndLabels) \n  (label, metrics.areaUnderROC) \n} \n\n```", "```scala\nscaledDataCats.cache \n\n```", "```scala\nval iterResults = Seq(1, 5, 10, 50).map { param => \n  val model = trainWithParams(scaledDataCats, 0.0, param, new  \nSimpleUpdater, 1.0) \n  createMetrics(s\"$param iterations\", scaledDataCats, model) \n} \niterResults.foreach { case (param, auc) => println(f\"$param, AUC =  \n${auc * 100}%2.2f%%\") } \n\n```", "```scala\n1 iterations, AUC = 64.97%\n5 iterations, AUC = 66.62%\n10 iterations, AUC = 66.55%\n50 iterations, AUC = 66.81%  \n\n```", "```scala\nval stepResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param => \n  val model = trainWithParams(scaledDataCats, 0.0, numIterations, new SimpleUpdater, param) \n  createMetrics(s\"$param step size\", scaledDataCats, model) \n} \nstepResults.foreach { case (param, auc) => println(f\"$param, AUC =  \n${auc * 100}%2.2f%%\") } \n\n```", "```scala\n0.001 step size, AUC = 64.95%\n0.01 step size, AUC = 65.00%\n0.1 step size, AUC = 65.52%\n1.0 step size, AUC = 66.55%\n10.0 step size, AUC = 61.92%\n\n```", "```scala\nval regResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param => \n  val model = trainWithParams(scaledDataCats, param, numIterations, new SquaredL2Updater, 1.0) \n  createMetrics(s\"$param L2 regularization parameter\",  \nscaledDataCats, model) \n} \nregResults.foreach { case (param, auc) => println(f\"$param, AUC =  \n${auc * 100}%2.2f%%\") } \n\n```", "```scala\n0.001 L2 regularization parameter, AUC = 66.55%\n0.01 L2 regularization parameter, AUC = 66.55%\n0.1 L2 regularization parameter, AUC = 66.63%\n1.0 L2 regularization parameter, AUC = 66.04%\n10.0 L2 regularization parameter, AUC = 35.33%  \n\n```", "```scala\nimport org.apache.spark.mllib.tree.impurity.Impurity \nimport org.apache.spark.mllib.tree.impurity.Entropy \nimport org.apache.spark.mllib.tree.impurity.Gini \n\ndef trainDTWithParams(input: RDD[LabeledPoint], maxDepth: Int, impurity: Impurity) = { \n  DecisionTree.train(input, Algo.Classification, impurity, maxDepth) \n} \n\n```", "```scala\nval dtResultsEntropy = Seq(1, 2, 3, 4, 5, 10, 20).map { param => \n  val model = trainDTWithParams(data, param, Entropy) \n  val scoreAndLabels = data.map { point => \n    val score = model.predict(point.features) \n    (if (score > 0.5) 1.0 else 0.0, point.label) \n  } \n  val metrics = new BinaryClassificationMetrics(scoreAndLabels) \n  (s\"$param tree depth\", metrics.areaUnderROC) \n} \ndtResultsEntropy.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\") } \n\n```", "```scala\n1 tree depth, AUC = 59.33%\n2 tree depth, AUC = 61.68%\n3 tree depth, AUC = 62.61%\n4 tree depth, AUC = 63.63%\n5 tree depth, AUC = 64.88%\n10 tree depth, AUC = 76.26%\n20 tree depth, AUC = 98.45%  \n\n```", "```scala\n1 tree depth, AUC = 59.33%\n2 tree depth, AUC = 61.68%\n3 tree depth, AUC = 62.61%\n4 tree depth, AUC = 63.63%\n5 tree depth, AUC = 64.89%\n10 tree depth, AUC = 78.37%\n20 tree depth, AUC = 98.87%  \n\n```", "```scala\ndef trainNBWithParams(input: RDD[LabeledPoint], lambda: Double) = { \n  val nb = new NaiveBayes \n  nb.setLambda(lambda) \n  nb.run(input) \n} \nval nbResults = Seq(0.001, 0.01, 0.1, 1.0, 10.0).map { param => \n  val model = trainNBWithParams(dataNB, param) \n  val scoreAndLabels = dataNB.map { point => \n    (model.predict(point.features), point.label) \n  } \n  val metrics = new BinaryClassificationMetrics(scoreAndLabels) \n  (s\"$param lambda\", metrics.areaUnderROC) \n} \nnbResults.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.2f%%\")  \n} \n\n```", "```scala\n0.001 lambda, AUC = 60.51%\n0.01 lambda, AUC = 60.51%\n0.1 lambda, AUC = 60.51%\n1.0 lambda, AUC = 60.51%\n10.0 lambda, AUC = 60.51%  \n\n```", "```scala\nval trainTestSplit = scaledDataCats.randomSplit(Array(0.6, 0.4), 123) \nval train = trainTestSplit(0) \nval test = trainTestSplit(1) \n\n```", "```scala\nval regResultsTest = Seq(0.0, 0.001, 0.0025, 0.005, 0.01).map { param => \n  val model = trainWithParams(train, param, numIterations, new SquaredL2Updater, 1.0) \n  createMetrics(s\"$param L2 regularization parameter\", test, model) \n} \nregResultsTest.foreach { case (param, auc) => println(f\"$param, AUC = ${auc * 100}%2.6f%%\")  \n} \n\n```", "```scala\n0.0 L2 regularization parameter, AUC = 66.480874%\n0.001 L2 regularization parameter, AUC = 66.480874%\n0.0025 L2 regularization parameter, AUC = 66.515027%\n0.005 L2 regularization parameter, AUC = 66.515027%\n0.01 L2 regularization parameter, AUC = 66.549180%  \n\n```", "```scala\n0.0 L2 regularization parameter, AUC = 66.260311%\n0.001 L2 regularization parameter, AUC = 66.260311%\n0.0025 L2 regularization parameter, AUC = 66.260311%\n0.005 L2 regularization parameter, AUC = 66.238294%\n0.01 L2 regularization parameter, AUC = 66.238294%  \n\n```"]