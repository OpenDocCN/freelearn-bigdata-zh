["```scala\n>tar xfvz lfw-a.tgz\n\n```", "```scala\nChapter_09\n|-- 2.0.x\n|   |-- python\n|   |-- scala\n|-- data\n\n```", "```scala\nscala\n|-- src\n|   |-- main\n|   |   |-- java\n|   |   |-- resources\n|   |   |-- scala\n|   |   |   |-- org\n|   |   |       |-- sparksamples\n|   |   |           |-- ImageProcessing.scala\n|   |   |           |-- Util.scala\n|   |   |-- scala-2.11\n|   |-- test\n\n```", "```scala\nval spConfig = (new SparkConf).setMaster(\"local[1]\")\n  .setAppName(\"SparkApp\")\n  .set(\"spark.driver.allowMultipleContexts\", \"true\") \nval sc = new SparkContext(spConfig) \nval path = PATH +  \"/lfw/*\" \nval rdd = sc.wholeTextFiles(path) \nval first = rdd.first \nprintln(first)\n\n```", "```scala\nfirst: (String, String) =  (file:/PATH/lfw/Aaron_Eckhart /Aaron_Eckhart_0001.jpg,??JFIF????? ...\n\n```", "```scala\nval files = rdd.map { case (fileName, content) =>\n  fileName.replace(\"file:\", \"\") }\n\n```", "```scala\n/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\n\n```", "```scala\nprintln(files.count)\n\n```", "```scala\n..., /PATH/lfw/Azra_Akin/Azra_Akin_0003.jpg:0+19927,\n  /PATH/lfw/Azra_Akin/Azra_Akin_0004.jpg:0+16030\n...\n14/09/18 20:36:25 INFO SparkContext: Job finished:\n  count at  <console>:19, took 1.151955 s\n1055\n\n```", "```scala\n>ipython notebook\n\n```", "```scala\n>ipython --pylab\n\n```", "```scala\nfrom PIL import Image, ImageFilter \npath = PATH + \"/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\" \nim = Image.open(path) \nim.show()\n\n```", "```scala\nimport java.awt.image.BufferedImage \ndef loadImageFromFile(path: String): BufferedImage = { \n  ImageIO.read(new File(path)) \n}\n\n```", "```scala\nval aePath = \"/PATH/lfw/Aaron_Eckhart/Aaron_Eckhart_0001.jpg\" \nval aeImage = loadImageFromFile(aePath)\n\n```", "```scala\naeImage: java.awt.image.BufferedImage = BufferedImage@f41266e: \ntype =  5 ColorModel: #pixelBits = 24 numComponents = 3 color space =  java.awt.color.ICC_ColorSpace@7e420794 transparency = 1 has \nalpha =  false isAlphaPre = false ByteInterleavedRaster: \nwidth = 250 height =  250 #numDataElements 3 dataOff[0] = 2\n\n```", "```scala\ndef processImage(image: BufferedImage, width: Int, height: Int): \n  BufferedImage = { \n    val bwImage = new BufferedImage(width, height, \n    BufferedImage.TYPE_BYTE_GRAY) \n    val g = bwImage.getGraphics() \n    g.drawImage(image, 0, 0, width, height, null) \n    g.dispose() \n    bwImage \n  }\n\n```", "```scala\nval grayImage = processImage(aeImage, 100, 100)\n\n```", "```scala\ngrayImage: java.awt.image.BufferedImage = BufferedImage@21f8ea3b:  \ntype = 10 ColorModel: #pixelBits = 8 numComponents = 1 color space =  java.awt.color.ICC_ColorSpace@5cd9d8e9 transparency = 1 has \nalpha =  false isAlphaPre = false ByteInterleavedRaster: \nwidth = 100 height =  100 #numDataElements 1 dataOff[0] = 0\n\n```", "```scala\nimport javax.imageio.ImageIO \nimport java.io.File \nImageIO.write(grayImage, \"jpg\", new File(\"/tmp/aeGray.jpg\"))\n\n```", "```scala\ntmp_path = PATH + \"/aeGray.jpg\"\nae_gary = Image.open(tmp_path)\nae_gary.show()\n\n```", "```scala\ndef getPixelsFromImage(image: BufferedImage): Array[Double] = { \n  val width = image.getWidth \n  val height = image.getHeight \n  val pixels = Array.ofDim[Double](width * height) \n  image.getData.getPixels(0, 0, width, height, pixels) \n}\n\n```", "```scala\ndef extractPixels(path: String, width: Int, height: Int):\n  Array[Double] = { \n    val raw = loadImageFromFile(path) \n    val processed = processImage(raw, width, height) \n    getPixelsFromImage(processed) \n  }\n\n```", "```scala\nval pixels = files.map(f => extractPixels(f, 50, 50)) \nprintln(pixels.take(10).map(_.take(10).mkString   (\"\", \",\", \", \n  ...\")).mkString(\"n\"))\n\n```", "```scala\n0.0,0.0,0.0,0.0,0.0,0.0,1.0,1.0,0.0,0.0, ...\n241.0,243.0,245.0,244.0,231.0,205.0,177.0,160.0,150.0,147.0, ...\n253.0,253.0,253.0,253.0,253.0,253.0,254.0,254.0,253.0,253.0, ...\n244.0,244.0,243.0,242.0,241.0,240.0,239.0,239.0,237.0,236.0, ...\n44.0,47.0,47.0,49.0,62.0,116.0,173.0,223.0,232.0,233.0, ...\n0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ...\n1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,0.0,0.0, ...\n26.0,26.0,27.0,26.0,24.0,24.0,25.0,26.0,27.0,27.0, ...\n240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0,240.0, ...\n0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0, ...\n\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectors \nval vectors = pixels.map(p => Vectors.dense(p)) \n// the setName method create a human-readable name that is \n// displayed in the Spark Web UI \nvectors.setName(\"image-vectors\") \n// remember to cache the vectors to speed up computation \nvectors.cache\n\n```", "```scala\nimport org.apache.spark.mllib.linalg.Matrix \nimport org.apache.spark.mllib.linalg.distributed.RowMatrix \nimport org.apache.spark.mllib.feature.StandardScaler \nval scaler = new StandardScaler(withMean = true, withStd = false)\n  .fit(vectors)\n\n```", "```scala\nclass StandardScaler @Since(\"1.1.0\") (withMean: Boolean,\n  withStd: Boolean) extends Logging\n\n```", "```scala\n...\n14/09/21 11:46:58 INFO SparkContext: Job finished: reduce at  \nRDDFunctions.scala:111, took 0.495859 s\nscaler: org.apache.spark.mllib.feature.StandardScalerModel =  org.apache.spark.mllib.feature.StandardScalerModel@6bb1a1a1\n\n```", "```scala\nval scaledVectors = vectors.map(v => scaler.transform(v))\n\n```", "```scala\nimport org.apache.spark.mllib.linalg.Matrix \nimport org.apache.spark.mllib.linalg.distributed.RowMatrix \nval matrix = new RowMatrix(scaledVectors) \nval K = 10 \nval pc = matrix.computePrincipalComponents(K)\n\n```", "```scala\npc: org.apache.spark.mllib.linalg.Matrix = \n-0.023183157256614906  -0.010622723054037303  ... (10 total)\n-0.023960537953442107  -0.011495966728461177  ...\n-0.024397470862198022  -0.013512219690177352  ...\n-0.02463158818330343   -0.014758658113862178  ...\n-0.024941633606137027  -0.014878858729655142  ...\n-0.02525998879466241   -0.014602750644394844  ...\n-0.025494722450369593  -0.014678013626511024  ...\n-0.02604194423255582   -0.01439561589951032   ...\n-0.025942214214865228  -0.013907665261197633  ...\n-0.026151551334429365  -0.014707035797934148  ...\n-0.026106572186134578  -0.016701471378568943  ...\n-0.026242986173995755  -0.016254664123732318  ...\n-0.02573628754284022   -0.017185663918352894  ...\n-0.02545319635905169   -0.01653357295561698   ...\n-0.025325893980995124  -0.0157082218373399...\n\n```", "```scala\nval rows = pc.numRows \nval cols = pc.numCols \nprintln(rows, cols)\n\n```", "```scala\n(2500,10)\n\n```", "```scala\nimport breeze.linalg.DenseMatrix \nval pcBreeze = new DenseMatrix(rows, cols, pc.toArray)\n\n```", "```scala\nimport breeze.linalg.csvwrite \ncsvwrite(new File(\"/tmp/pc.csv\"), pcBreeze)\n\n```", "```scala\npcs = np.loadtxt(PATH + \"/pc.csv\", delimiter=\",\") \nprint(pcs.shape)\n\n```", "```scala\n(2500, 10)\n\n```", "```scala\ndef plot_gallery(images, h, w, n_row=2, n_col=5): \n        \"\"\"Helper function to plot a gallery of portraits\"\"\" \n        plt.figure(figsize=(1.8 * n_col, 2.4 * n_row)) \n        plt.subplots_adjust(bottom=0, left=.01, right=.99, top=.90,\n          hspace=.35) \n        for i in range(n_row * n_col): \n            plt.subplot(n_row, n_col, i + 1) \n            plt.imshow(images[:, i].reshape((h, w)),  \n                cmap=plt.cm.gray) \n            plt.title(\"Eigenface %d\" % (i + 1), size=12) \n            plt.xticks(()) \n            plt.yticks(()) \n\n  plt.show()\n\n```", "```scala\nplot_gallery(pcs, 50, 50)\n\n```", "```scala\nval projected = matrix.multiply(pc) \nprintln(projected.numRows, projected.numCols)\n\n```", "```scala\n(1055,10)\n\n```", "```scala\nprintln(projected.rows.take(5).mkString(\"n\"))\n\n```", "```scala\n[2648.9455749636277,1340.3713412351376,443.67380716760965, -353.0021423043161,52.53102289832631,423.39861446944354, 413.8429065865399,-484.18122999722294,87.98862070273545, -104.62720604921965]\n[172.67735747311974,663.9154866829355,261.0575622447282, -711.4857925259682,462.7663154755333,167.3082231097332, -71.44832640530836,624.4911488194524,892.3209964031695, -528.0056327351435]\n[-1063.4562028554978,388.3510869550539,1508.2535609357597, 361.2485590837186,282.08588829583596,-554.3804376922453, 604.6680021092125,-224.16600191143075,-228.0771984153961, -110.21539201855907]\n[-4690.549692385103,241.83448841252638,-153.58903325799685, -28.26215061165965,521.8908276360171,-442.0430200747375, -490.1602309367725,-456.78026845649435,-78.79837478503592, 70.62925170688868]\n[-2766.7960144161225,612.8408888724891,-405.76374113178616, -468.56458995613974,863.1136863614743,-925.0935452709143, 69.24586949009642,-777.3348492244131,504.54033662376435, 257.0263568009851]\n\n```", "```scala\nval svd = matrix.computeSVD(10, computeU = true) \nprintln(s\"U dimension: (${svd.U.numRows}, ${svd.U.numCols})\") \nprintln(s\"S dimension: (${svd.s.size}, )\") \nprintln(s\"V dimension: (${svd.V.numRows}, ${svd.V.numCols})\")\n\n```", "```scala\nU dimension: (1055, 10)\nS dimension: (10, )\nV dimension: (2500, 10)\n\n```", "```scala\ndef approxEqual(array1: Array[Double], array2: Array[Double],    \ntolerance: Double = 1e-6): Boolean = { \n  // note we ignore sign of the principal component / \n  // singular vector elements \n  val bools = array1.zip(array2).map { case (v1, v2) => if    \n    (math.abs(math.abs(v1) - math.abs(v2)) > 1e-6) false else true } \n  bools.fold(true)(_ & _) \n}\n\n```", "```scala\nprintln(approxEqual(Array(1.0, 2.0, 3.0), Array(1.0, 2.0, 3.0)))\n\n```", "```scala\ntrue\n\n```", "```scala\nprintln(approxEqual(Array(1.0, 2.0, 3.0), Array(3.0, 2.0, 1.0)))\n\n```", "```scala\nfalse\n\n```", "```scala\nprintln(approxEqual(svd.V.toArray, pc.toArray))\n\n```", "```scala\ntrue\n\n```", "```scala\nval breezeS = breeze.linalg.DenseVector(svd.s.toArray) \nval projectedSVD = svd.U.rows.map { v =>  \n  val breezeV = breeze.linalg.DenseVector(v.toArray) \n  val multV = breezeV :* breezeS \n  Vectors.dense(multV.data) \n} \nprojected.rows.zip(projectedSVD).map { case (v1, v2) =>\n  approxEqual(v1.toArray, v2.toArray) }.filter(b => true).count\n\n```", "```scala\nval sValues = (1 to 5).map { \n  i => matrix.computeSVD(i,  computeU = false).s \n} \nsValues.foreach(println)\n\n```", "```scala\n[54091.00997110354]\n[54091.00997110358,33757.702867982436]\n[54091.00997110357,33757.70286798241,24541.193694775946]\n[54091.00997110358,33757.70286798242,24541.19369477593, 23309.58418888302]\n[54091.00997110358,33757.70286798242,24541.19369477593, 23309.584188882982,21803.09841158358]\n\n```", "```scala\nval svd300 = matrix.computeSVD(300, computeU = false) \nval sMatrix = new DenseMatrix(1, 300, svd300.s.toArray) \nprintln(sMatrix) \ncsvwrite(new File(\"/home/ubuntu/work/ml-resources/\n  spark-ml/Chapter_09/data/s.csv\"), sMatrix)\n\n```", "```scala\nfile_name = '/home/ubuntu/work/ml-resources/spark-ml/Chapter_09/data/s.csv' \ndata = np.genfromtxt(file_name, delimiter=',')  \nplt.plot(data) \nplt.suptitle('Variation 300 Singular Values ') \nplt.xlabel('Singular Value No') \nplt.ylabel('Variation') \nplt.show()\n\n```", "```scala\nplt.plot(cumsum(data)) \nplt.yscale('log') \nplt.suptitle('Cumulative Variation 300 Singular Values ') \nplt.xlabel('Singular Value No') \nplt.ylabel('Cumulative Variation') \nplt.show()\n\n```"]