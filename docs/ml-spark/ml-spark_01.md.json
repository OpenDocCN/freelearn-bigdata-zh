["```scala\n $ tar xfvz spark-2.0.0-bin-hadoop2.7.tgz\n $ cd spark-2.0.0-bin-hadoop2.7\n\n```", "```scala\n $ bin/run-example SparkPi 100\n\n```", "```scala\n...\n16/11/24 14:41:58 INFO Executor: Finished task 99.0 in stage 0.0 \n    (TID 99). 872 bytes result sent to driver\n16/11/24 14:41:58 INFO TaskSetManager: Finished task 99.0 in stage \n    0.0 (TID 99) in 59 ms on localhost (100/100)\n16/11/24 14:41:58 INFO DAGScheduler: ResultStage 0 (reduce at \n    SparkPi.scala:38) finished in 1.988 s\n16/11/24 14:41:58 INFO TaskSchedulerImpl: Removed TaskSet 0.0, \n    whose tasks have all completed, from pool \n16/11/24 14:41:58 INFO DAGScheduler: Job 0 finished: reduce at \n    SparkPi.scala:38, took 2.235920 s\nPi is roughly 3.1409527140952713\n\n```", "```scala\n $ ./bin/spark-submit  --class org.apache.spark.examples.SparkPi \n --master local[2] ./examples/jars/spark-examples_2.11-2.0.0.jar 100 \n\n```", "```scala\n $ MASTER=spark://IP:PORT --class org.apache.spark.examples.SparkPi \n ./examples/jars/spark-examples_2.11-2.0.0.jar 100\n\n```", "```scala\nval conf = new SparkConf() \n.setAppName(\"Test Spark App\") \n.setMaster(\"local[4]\") \nval sc = new SparkContext(conf)\n\n```", "```scala\nval sc = new SparkContext(\"local[4]\", \"Test Spark App\")\n\n```", "```scala\nval spConfig = (new SparkConf).setMaster(\"local\").setAppName(\"SparkApp\")\n val spark = SparkSession\n   .builder()\n   .appName(\"SparkUserData\").config(spConfig)\n   .getOrCreate()\n\n```", "```scala\nval user_df = spark.read.format(\"com.databricks.spark.csv\")\n   .option(\"delimiter\", \"|\").schema(customSchema)\n   .load(\"/home/ubuntu/work/ml-resources/spark-ml/data/ml-100k/u.user\")\nval first = user_df.first()\n\n```", "```scala\n$ ~/work/spark-2.0.0-bin-hadoop2.7/bin/spark-shell \nUsing Spark's default log4j profile: org/apache/spark/log4j-\n    defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n16/08/06 22:14:25 WARN NativeCodeLoader: Unable to load native-\n    hadoop library for your platform... using builtin-java classes \n    where applicable\n16/08/06 22:14:25 WARN Utils: Your hostname, ubuntu resolves to a \n    loopback address: 127.0.1.1; using 192.168.22.180 instead (on \n    interface eth1)\n16/08/06 22:14:25 WARN Utils: Set SPARK_LOCAL_IP if you need to \n    bind to another address\n16/08/06 22:14:26 WARN Utils: Service 'SparkUI' could not bind on \n    port 4040\\. Attempting port 4041.\n16/08/06 22:14:27 WARN SparkContext: Use an existing SparkContext, \n    some configuration may not take effect.\nSpark context Web UI available at http://192.168.22.180:4041\nSpark context available as 'sc' (master = local[*], app id = local-\n    1470546866779).\nSpark session available as 'spark'.\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _ / _ / ______/ __/  '_/\n /___/ .__/_,_/_/ /_/_   version 2.0.0\n /_/\n\nUsing Scala version 2.11.8 (Java HotSpot(TM) 64-Bit Server VM, \n    Java 1.7.0_60)\nType in expressions to have them evaluated.\nType :help for more information.\n\nscala> \n\n```", "```scala\n~/work/spark-2.0.0-bin-hadoop2.7/bin/pyspark \nPython 2.7.6 (default, Jun 22 2015, 17:58:13) \n[GCC 4.8.2] on linux2\nType \"help\", \"copyright\", \"credits\" or \"license\" for more \n    information.\nUsing Spark's default log4j profile: org/apache/spark/log4j-\n    defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n16/08/06 22:16:15 WARN NativeCodeLoader: Unable to load native-\n    hadoop library for your platform... using builtin-java classes \n    where applicable\n16/08/06 22:16:15 WARN Utils: Your hostname, ubuntu resolves to a \n    loopback address: 127.0.1.1; using 192.168.22.180 instead (on \n    interface eth1)\n16/08/06 22:16:15 WARN Utils: Set SPARK_LOCAL_IP if you need to \n    bind to another address\n16/08/06 22:16:16 WARN Utils: Service 'SparkUI' could not bind on \n    port 4040\\. Attempting port 4041.\nWelcome to\n ____              __\n / __/__  ___ _____/ /__\n _ / _ / ______/ __/  '_/\n /__ / .__/_,_/_/ /_/_   version 2.0.0\n /_/\n\nUsing Python version 2.7.6 (default, Jun 22 2015 17:58:13)\nSparkSession available as 'spark'.\n>>> \n\n```", "```scala\n$ ~/work/spark-2.0.0-bin-hadoop2.7/bin/sparkR\nR version 3.0.2 (2013-09-25) -- \"Frisbee Sailing\"\nCopyright (C) 2013 The R Foundation for Statistical Computing\nPlatform: x86_64-pc-linux-gnu (64-bit)\n\nR is free software and comes with ABSOLUTELY NO WARRANTY.\nYou are welcome to redistribute it under certain conditions.\nType 'license()' or 'licence()' for distribution details.\n\n Natural language support but running in an English locale\n\nR is a collaborative project with many contributors.\nType 'contributors()' for more information and\n'citation()' on how to cite R or R packages in publications.\n\nType 'demo()' for some demos, 'help()' for on-line help, or\n'help.start()' for an HTML browser interface to help.\nType 'q()' to quit R.\n\nLaunching java with spark-submit command /home/ubuntu/work/spark- \n    2.0.0-bin-hadoop2.7/bin/spark-submit   \"sparkr-shell\" \n    /tmp/RtmppzWD8S/backend_porta6366144af4f \nUsing Spark's default log4j profile: org/apache/spark/log4j-\n    defaults.properties\nSetting default log level to \"WARN\".\nTo adjust logging level use sc.setLogLevel(newLevel).\n16/08/06 22:26:22 WARN NativeCodeLoader: Unable to load native-\n    hadoop library for your platform... using builtin-java classes \n    where applicable\n16/08/06 22:26:22 WARN Utils: Your hostname, ubuntu resolves to a \n    loopback address: 127.0.1.1; using 192.168.22.186 instead (on \n    interface eth1)\n16/08/06 22:26:22 WARN Utils: Set SPARK_LOCAL_IP if you need to \n    bind to another address\n16/08/06 22:26:22 WARN Utils: Service 'SparkUI' could not bind on \n    port 4040\\. Attempting port 4041.\n\n Welcome to\n ____              __ \n / __/__  ___ _____/ /__ \n _ / _ / _ ____/ __/  '_/ \n /___/ .__/_,_/_/ /_/_   version  2.0.0 \n /_/ \n SparkSession available as 'spark'.\nDuring startup - Warning message:\npackage 'SparkR' was built under R version 3.1.1 \n> \n\n```", "```scala\nval collection = List(\"a\", \"b\", \"c\", \"d\", \"e\") \nval rddFromCollection = sc.parallelize(collection)\n\n```", "```scala\nval rddFromTextFile = sc.textFile(\"LICENSE\")\n\n```", "```scala\nrddFromTextFile: org.apache.spark.rdd.RDD[String] = LICENSE   \nMapPartitionsRDD[1] at textFile at <console>:24\n\n```", "```scala\nval rddFromTextFileHDFS = sc.textFile(\"hdfs://input/LICENSE \")\n\n```", "```scala\nval rddFromTextFileS3 = sc.textFile(\"s3n://input/LICENSE \")\n\n```", "```scala\nval intsFromStringsRDD = rddFromTextFile.map(line => line.size)\n\n```", "```scala\nintsFromStringsRDD: org.apache.spark.rdd.RDD[Int] = \nMapPartitionsRDD[2] at map at <console>:26\n\n```", "```scala\nintsFromStringsRDD.count\n\n```", "```scala\nres0: Long = 299\n\n```", "```scala\nval sumOfRecords = intsFromStringsRDD.sum \nval numRecords = intsFromStringsRDD.count \nval aveLengthOfRecord = sumOfRecords / numRecords\n\n```", "```scala\nscala> intsFromStringsRDD.count\nres0: Long = 299\n\nscala> val sumOfRecords = intsFromStringsRDD.sum\nsumOfRecords: Double = 17512.0\n\nscala> val numRecords = intsFromStringsRDD.count\nnumRecords: Long = 299\n\nscala> val aveLengthOfRecord = sumOfRecords / numRecords\naveLengthOfRecord: Double = 58.5685618729097\n\n```", "```scala\nval aveLengthOfRecordChained = rddFromTextFile.map(line => line.size).sum / rddFromTextFile.count\n\n```", "```scala\nval transformedRDD = rddFromTextFile.map(line => line.size).filter(size => size > 10).map(size => size * 2)\n\n```", "```scala\ntransformedRDD: org.apache.spark.rdd.RDD[Int] = \nMapPartitionsRDD[6] at map at <console>:26\n\n```", "```scala\nval computation = transformedRDD.sum\n\n```", "```scala\ncomputation: Double = 35006.0\n\n```", "```scala\nrddFromTextFile.cache\nres0: rddFromTextFile.type = MapPartitionsRDD[1] at textFile at \n<console>:27\n\n```", "```scala\nval aveLengthOfRecordChained = rddFromTextFile.map(line => \nline.size).sum / rddFromTextFile.count\n\n```", "```scala\nval broadcastAList = sc.broadcast(List(\"a\", \"b\", \"c\", \"d\", \"e\"))\n\n```", "```scala\nsc.parallelize(List(\"1\", \"2\", \"3\")).map(x => broadcastAList.value ++  \n  x).collect\n\n```", "```scala\n...\nres1: Array[List[Any]] = Array(List(a, b, c, d, e, 1), List(a, b, \nc, d, e, 2), List(a, b, c, d, e, 3))\n\n```", "```scala\nimport org.apache.spark.sql.SparkSession \nval spark = SparkSession.builder().appName(\"Spark SQL\").config(\"spark.some.config.option\", \"\").getOrCreate() \nimport spark.implicits._\n\n```", "```scala\nscala> val df = spark.read.json(\"/home/ubuntu/work/ml-resources\n  /spark-ml/Chapter_01/data/example_one.json\")\n\n```", "```scala\norg.apache.spark.sql\nClass SparkSession.implicits$\nObject org.apache.spark.sql.SQLImplicits\nEnclosing class: [SparkSession](https://spark.apache.org/docs/2.0.0/api/java/org/apache/spark/sql/SparkSession.html)\n\n```", "```scala\ndf: org.apache.spark.sql.DataFrame = [address: struct<city: \nstring, state: string>, name: string]\n\n```", "```scala\nscala> df.show\n+-----------------+-------+\n|          address|   name|\n+-----------------+-------+\n|  [Columbus,Ohio]|    Yin|\n|[null,California]|Michael|\n+-----------------+-------+\n\n```", "```scala\nJohn,iPhone Cover,9.99\nJohn,Headphones,5.49\nJack,iPhone Cover,9.99\nJill,Samsung Galaxy Cover,8.95\nBob,iPad Cover,5.49\n\n```", "```scala\nname := \"scala-spark-app\" \nversion := \"1.0\" \nscalaVersion := \"2.11.7\" \nlibraryDependencies += \"org.apache.spark\" %% \"spark-core\" % \"2.0.0\"\n\n```", "```scala\nimport org.apache.spark.SparkContext \nimport org.apache.spark.SparkContext._ \n\n/** \n * A simple Spark app in Scala \n */ \nobject ScalaApp {\n\n```", "```scala\ndef main(args: Array[String]) { \n  val sc = new SparkContext(\"local[2]\", \"First Spark App\") \n  // we take the raw data in CSV format and convert it into a \n   set of records of the form (user, product, price) \n  val data = sc.textFile(\"data/UserPurchaseHistory.csv\") \n    .map(line => line.split(\",\")) \n    .map(purchaseRecord => (purchaseRecord(0), \n     purchaseRecord(1), purchaseRecord(2)))\n\n```", "```scala\n// let's count the number of purchases \nval numPurchases = data.count() \n// let's count how many unique users made purchases \nval uniqueUsers = data.map{ case (user, product, price) => user \n}.distinct().count() \n// let's sum up our total revenue \nval totalRevenue = data.map{ case (user, product, price) => \nprice.toDouble }.sum() \n// let's find our most popular product \nval productsByPopularity = data \n  .map{ case (user, product, price) => (product, 1) } \n  .reduceByKey(_ + _) \n  .collect() \n  .sortBy(-_._2)     \nval mostPopular = productsByPopularity(0)\n\n```", "```scala\n    println(\"Total purchases: \" + numPurchases) \n    println(\"Unique users: \" + uniqueUsers) \n    println(\"Total revenue: \" + totalRevenue) \n    println(\"Most popular product: %s with %d \n    purchases\".format(mostPopular._1, mostPopular._2)) \n  } \n}\n\n```", "```scala\n...\n[info] Compiling 1 Scala source to ...\n[info] Running ScalaApp\n...\nTotal purchases: 5\nUnique users: 4\nTotal revenue: 39.91\nMost popular product: iPhone Cover with 2 purchases\n\n```", "```scala\nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.JavaSparkContext; \nimport scala.Tuple2; \nimport java.util.*; \nimport java.util.stream.Collectors; \n\n/** \n * A simple Spark app in Java \n */ \npublic class JavaApp { \n  public static void main(String[] args) {\n\n```", "```scala\nJavaSparkContext sc = new JavaSparkContext(\"local[2]\", \n     \"First Spark App\"); \n// we take the raw data in CSV format and convert it into a \n// set of records of the form (user, product, price) \nJavaRDD<String[]> data =   sc.textFile(\"data/UserPurchaseHistory.csv\").map(s ->         s.split(\",\"));\n\n```", "```scala\n// let's count the number of purchases \nlong numPurchases = data.count(); \n// let's count how many unique users made purchases \nlong uniqueUsers = data.map(strings ->  \n      strings[0]).distinct().count(); \n// let's sum up our total revenue \nDouble totalRevenue = data.map(strings ->  \n      Double.parseDouble(strings[2])).reduce((Double v1,  \nDouble v2) -> new Double(v1.doubleValue() + v2.doubleValue()));\n\n```", "```scala\n// let's find our most popular product \nList<Tuple2<String, Integer>> pairs = data.mapToPair(strings -> new Tuple2<String, Integer>(strings[1], 1)).reduceByKey((Integer i1, Integer i2) -> i1 + i2).collect(); \n\nMap<String, Integer> sortedData = new HashMap<>(); \nIterator it = pairs.iterator(); \nwhile (it.hasNext()) { \n    Tuple2<String, Integer> o = (Tuple2<String, Integer>) it.next(); \n    sortedData.put(o._1, o._2); \n} \nList<String> sorted = sortedData.entrySet() \n        .stream() \n        .sorted(Comparator.comparing((Map.Entry<String, Integer> \n          entry) -> entry.getValue()).reversed())\n         .map(Map.Entry::getKey) \n        .collect(Collectors.toList()); \nString mostPopular = sorted.get(0); \n            int purchases = sortedData.get(mostPopular); \n    System.out.println(\"Total purchases: \" + numPurchases); \n    System.out.println(\"Unique users: \" + uniqueUsers); \n    System.out.println(\"Total revenue: \" + totalRevenue); \n    System.out.println(String.format(\"Most popular product:\n     %s with %d purchases\", mostPopular, purchases)); \n  } \n}\n\n```", "```scala\n  $ mvn exec:java -Dexec.mainClass=\"JavaApp\"\n\n```", "```scala\n...\n14/01/30 17:02:43 INFO spark.SparkContext: Job finished: collect \nat JavaApp.java:46, took 0.039167 s\nTotal purchases: 5\nUnique users: 4\nTotal revenue: 39.91\nMost popular product: iPhone Cover with 2 purchases\n\n```", "```scala\nfrom pyspark import SparkContext\n\nsc = SparkContext(\"local[2]\", \"First Spark App\")\n# we take the raw data in CSV format and convert it into a set of \n    records of the form (user, product, price)\ndata = sc.textFile(\"data/UserPurchaseHistory.csv\").map(lambda \n    line: line.split(\",\")).map(lambda record: (record[0], record[1], \n    record[2]))\n# let's count the number of purchases\nnumPurchases = data.count()\n# let's count how many unique users made purchases\nuniqueUsers = data.map(lambda record: record[0]).distinct().count()\n# let's sum up our total revenue\ntotalRevenue = data.map(lambda record: float(record[2])).sum()\n# let's find our most popular product\nproducts = data.map(lambda record: (record[1], \n    1.0)).reduceByKey(lambda a, b: a + b).collect()\nmostPopular = sorted(products, key=lambda x: x[1], reverse=True)[0]\n\nprint \"Total purchases: %d\" % numPurchases\nprint \"Unique users: %d\" % uniqueUsers\nprint \"Total revenue: %2.2f\" % totalRevenue\nprint \"Most popular product: %s with %d purchases\" % \n    (mostPopular[0], mostPopular[1])\n\n```", "```scala\n $SPARK_HOME/bin/spark-submit pythonapp.py\n\n```", "```scala\n...\n14/01/30 11:43:47 INFO SparkContext: Job finished: collect at \npythonapp.py:14, took 0.050251 s\nTotal purchases: 5\nUnique users: 4\nTotal revenue: 39.91\nMost popular product: iPhone Cover with 2 purchases\n\n```", "```scala\nSys.setenv(SPARK_HOME = \"/PATH/spark-2.0.0-bin-hadoop2.7\") \n.libPaths(c(file.path(Sys.getenv(\"SPARK_HOME\"), \"R\", \"lib\"), \n .libPaths())) \n#load the Sparkr library \nlibrary(SparkR) \nsc <- sparkR.init(master = \"local\", sparkPackages=\"com.databricks:spark-csv_2.10:1.3.0\") \nsqlContext <- sparkRSQL.init(sc) \n\nuser.purchase.history <- \"/PATH/ml-resources/spark-ml/Chapter_01/r-spark-app/data/UserPurchaseHistory.csv\" \ndata <- read.df(sqlContext, user.purchase.history, \"com.databricks.spark.csv\", header=\"false\") \nhead(data) \ncount(data) \n\nparseFields <- function(record) { \n  Sys.setlocale(\"LC_ALL\", \"C\") # necessary for strsplit() to work correctly \n  parts <- strsplit(as.character(record), \",\") \n  list(name=parts[1], product=parts[2], price=parts[3]) \n} \n\nparsedRDD <- SparkR:::lapply(data, parseFields) \ncache(parsedRDD) \nnumPurchases <- count(parsedRDD) \n\nsprintf(\"Number of Purchases : %d\", numPurchases) \ngetName <- function(record){ \n  record[1] \n} \n\ngetPrice <- function(record){ \n  record[3] \n} \n\nnameRDD <- SparkR:::lapply(parsedRDD, getName) \nnameRDD = collect(nameRDD) \nhead(nameRDD) \n\nuniqueUsers <- unique(nameRDD) \nhead(uniqueUsers) \n\npriceRDD <- SparkR:::lapply(parsedRDD, function(x) { as.numeric(x$price[1])}) \ntake(priceRDD,3) \n\ntotalRevenue <- SparkR:::reduce(priceRDD, \"+\") \n\nsprintf(\"Total Revenue : %.2f\", s) \n\nproducts <- SparkR:::lapply(parsedRDD, function(x) { list( toString(x$product[1]), 1) }) \ntake(products, 5) \nproductCount <- SparkR:::reduceByKey(products, \"+\", 2L) \nproductsCountAsKey <- SparkR:::lapply(productCount, function(x) { list( as.integer(x[2][1]), x[1][1])}) \n\nproductCount <- count(productsCountAsKey) \nmostPopular <- toString(collect(productsCountAsKey)[[productCount]][[2]]) \nsprintf(\"Most Popular Product : %s\", mostPopular)\n\n```", "```scala\n  $ Rscript r-script-01.R \n\n```", "```scala\n> sprintf(\"Number of Purchases : %d\", numPurchases)\n[1] \"Number of Purchases : 5\"\n\n> uniqueUsers <- unique(nameRDD)\n> head(uniqueUsers)\n[[1]]\n[[1]]$name\n[[1]]$name[[1]]\n[1] \"John\"\n[[2]]\n[[2]]$name\n[[2]]$name[[1]]\n[1] \"Jack\"\n[[3]]\n[[3]]$name\n[[3]]$name[[1]]\n[1] \"Jill\"\n[[4]]\n[[4]]$name\n[[4]]$name[[1]]\n[1] \"Bob\"\n\n> sprintf(\"Total Revenue : %.2f\", totalRevenueNum)\n[1] \"Total Revenue : 39.91\"\n\n> sprintf(\"Most Popular Product : %s\", mostPopular)\n[1] \"Most Popular Product : iPad Cover\"\n\n```", "```scala\n>./ec2/spark-ec2 \n\n```", "```scala\nUsage: spark-ec2 [options] <action> <cluster_name>\n<action> can be: launch, destroy, login, stop, start, get-master\n\nOptions:\n...\n\n```", "```scala\n  $ chmod 600 spark.pem\n $ export AWS_ACCESS_KEY_ID=\"...\"\n $ export AWS_SECRET_ACCESS_KEY=\"...\"\n\n```", "```scala\n $  cd ec2\n $ ./spark-ec2 --key-pair=rd_spark-user1 --identity-file=spark.pem  \n    --region=us-east-1 --zone=us-east-1a launch my-spark-cluster\n\n```", "```scala\nSetting up security groups...\nCreating security group my-spark-cluster-master\nCreating security group my-spark-cluster-slaves\nSearching for existing cluster my-spark-cluster in region \n    us-east-1...\nSpark AMI: ami-5bb18832\nLaunching instances...\nLaunched 1 slave in us-east-1a, regid = r-5a893af2\nLaunched master in us-east-1a, regid = r-39883b91\nWaiting for AWS to propagate instance metadata...\nWaiting for cluster to enter 'ssh-ready' state...........\nWarning: SSH connection error. (This could be temporary.)\nHost: ec2-52-90-110-128.compute-1.amazonaws.com\nSSH return code: 255\nSSH output: ssh: connect to host ec2-52-90-110-128.compute- \n    1.amazonaws.com port 22: Connection refused\nWarning: SSH connection error. (This could be temporary.)\nHost: ec2-52-90-110-128.compute-1.amazonaws.com\nSSH return code: 255\nSSH output: ssh: connect to host ec2-52-90-110-128.compute-\n    1.amazonaws.com port 22: Connection refused\nWarnig: SSH connection error. (This could be temporary.)\nHost: ec2-52-90-110-128.compute-1.amazonaws.com\nSSH return code: 255\nSSH output: ssh: connect to host ec2-52-90-110-128.compute-\n    1.amazonaws.com port 22: Connection refused\nCluster is now in 'ssh-ready' state. Waited 510 seconds.\n\n```", "```scala\n./tachyon/setup.sh: line 5: /root/tachyon/bin/tachyon: \n    No such file or directory\n./tachyon/setup.sh: line 9: /root/tachyon/bin/tachyon-start.sh: \n    No such file or directory\n[timing] tachyon setup:  00h 00m 01s\nSetting up rstudio\nspark-ec2/setup.sh: line 110: ./rstudio/setup.sh: \n    No such file or directory\n[timing] rstudio setup:  00h 00m 00s\nSetting up ganglia\nRSYNC'ing /etc/ganglia to slaves...\nec2-52-91-214-206.compute-1.amazonaws.com\nShutting down GANGLIA gmond:                               [FAILED]\nStarting GANGLIA gmond:                                    [  OK  ]\nShutting down GANGLIA gmond:                               [FAILED]\nStarting GANGLIA gmond:                                    [  OK  ]\nConnection to ec2-52-91-214-206.compute-1.amazonaws.com closed.\nShutting down GANGLIA gmetad:                              [FAILED]\nStarting GANGLIA gmetad:                                   [  OK  ]\nStopping httpd:                                            [FAILED]\nStarting httpd: httpd: Syntax error on line 154 of /etc/httpd\n    /conf/httpd.conf: Cannot load /etc/httpd/modules/mod_authz_core.so \n    into server: /etc/httpd/modules/mod_authz_core.so: cannot open \n    shared object file: No such file or directory              [FAILED]\n[timing] ganglia setup:  00h 00m 03s\nConnection to ec2-52-90-110-128.compute-1.amazonaws.com closed.\nSpark standalone cluster started at \n    http://ec2-52-90-110-128.compute-1.amazonaws.com:8080\nGanglia started at http://ec2-52-90-110-128.compute-\n    1.amazonaws.com:5080/ganglia\nDone!\nubuntu@ubuntu:~/work/spark-1.6.0-bin-hadoop2.6/ec2$\n\n```", "```scala\n  $ ssh -i spark.pem root@ ec2-52-90-110-128.compute-1.amazonaws.com\n\n```", "```scala\n  $ ./spark-ec2 -i spark.pem get-master test-cluster\n\n```", "```scala\n  $ cd spark\n $ MASTER=local[2] ./bin/run-example SparkPi\n\n```", "```scala\n...\n14/01/30 20:20:21 INFO SparkContext: Job finished: reduce at \nSparkPi.scala:35, took 0.864044012 s\nPi is roughly 3.14032\n...\n\n```", "```scala\n    `$ MASTER=spark://` ec2-52-90-110-128.compute-\n      1.amazonaws.com:`7077 ./bin/run-example SparkPi` \n\n```", "```scala\n...\n14/01/30 20:26:17 INFO client.Client$ClientActor: Connecting to \n    master spark://ec2-54-220-189-136.eu-\n    west-1.compute.amazonaws.com:7077\n14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend: \n    Connected to Spark cluster with app ID app-20140130202617-0001\n14/01/30 20:26:17 INFO client.Client$ClientActor: Executor added: \n    app-20140130202617-0001/0 on worker-20140130201049-\n    ip-10-34-137-45.eu-west-1.compute.internal-57119 \n    (ip-10-34-137-45.eu-west-1.compute.internal:57119) with 1 cores\n14/01/30 20:26:17 INFO cluster.SparkDeploySchedulerBackend:\n    Granted executor ID app-20140130202617-0001/0 on hostPort \n    ip-10-34-137-45.eu-west-1.compute.internal:57119 with 1 cores, \n    2.4 GB RAM\n14/01/30 20:26:17 INFO client.Client$ClientActor: \n    Executor updated: app-20140130202617-0001/0 is now RUNNING\n14/01/30 20:26:18 INFO spark.SparkContext: Starting job: reduce at \n    SparkPi.scala:39\n...\n\n```", "```scala\n  **$ ./bin/spark-shell --master spark://** ec2-52-90-110-128.compute-\n    1.amazonaws.com**:7077**\n\n```", "```scala\n  **$ ./bin/pyspark --master spark://** ec2-52-90-110-128.compute-\n    1.amazonaws.com**:7077**\n\n```", "```scala\n  $ ./ec2/spark-ec2 -k spark -i spark.pem destroy test-cluster\n\n```", "```scala\nAre you sure you want to destroy the cluster test-cluster?\nThe following instances will be terminated:\nSearching for existing cluster test-cluster...\nFound 1 master(s), 1 slaves\n> ec2-54-227-127-14.compute-1.amazonaws.com\n> ec2-54-91-61-225.compute-1.amazonaws.com\nALL DATA ON ALL NODES WILL BE LOST!!\nDestroy cluster test-cluster (y/N): y\nTerminating master...\nTerminating slaves...\n\n```", "```scala\n **$ ssh -i rd_spark-user1.pem**\n   hadoop@ec2-52-3-242-138.compute-1.amazonaws.com \n\n```", "```scala\n     Last login: Wed Jan 13 10:46:26 2016\n\n __|  __|_  )\n _|  (     /   Amazon Linux AMI\n ___|___|___|\n\n https://aws.amazon.com/amazon-linux-ami/2015.09-release-notes/\n 23 package(s) needed for security, out of 49 available\n Run \"sudo yum update\" to apply all updates.\n [hadoop@ip-172-31-2-31 ~]$ \n\n```", "```scala\n      [hadoop@ip-172-31-2-31 ~]$ spark-shell\n 16/01/13 10:49:36 INFO SecurityManager: Changing view acls to: \n          hadoop\n 16/01/13 10:49:36 INFO SecurityManager: Changing modify acls to: \n          hadoop\n 16/01/13 10:49:36 INFO SecurityManager: SecurityManager: \n          authentication disabled; ui acls disabled; users with view \n          permissions: Set(hadoop); users with modify permissions: \n          Set(hadoop)\n 16/01/13 10:49:36 INFO HttpServer: Starting HTTP Server\n 16/01/13 10:49:36 INFO Utils: Successfully started service 'HTTP \n          class server' on port 60523.\n Welcome to\n ____              __\n / __/__  ___ _____/ /__\n _ / _ / _ &grave;/ __/  '_/\n /___/ .__/_,_/_/ /_/_   version 1.5.2\n /_/\n scala> sc\n\n```", "```scala\n    scala> val textFile = sc.textFile(\"s3://elasticmapreduce/samples\n      /hive-ads/tables/impressions/dt=2009-04-13-08-05\n      /ec2-0-51-75-39.amazon.com-2009-04-13-08-05.log\")\n scala> val linesWithCartoonNetwork = textFile.filter(line =>  \n      line.contains(\"cartoonnetwork.com\")).count()\n\n```", "```scala\n     linesWithCartoonNetwork: Long = 9\n\n```", "```scala\n  $ gcloud beta dataproc --project=rd-spark-1 jobs wait 1ed4d07f-\n    55fc-45fe-a565-290dcd1978f7\n\n```", "```scala\nWaiting for job output...\n16/01/28 10:04:29 INFO akka.event.slf4j.Slf4jLogger: Slf4jLogger \n    started\n16/01/28 10:04:29 INFO Remoting: Starting remoting\n...\nSubmitted application application_1453975062220_0001\nPi is roughly 3.14157732 \n\n```"]