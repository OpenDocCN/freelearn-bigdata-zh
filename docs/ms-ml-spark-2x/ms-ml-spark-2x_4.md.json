["```scala\nexport SPARK_HOME=\"<path to your Spark2.0 distribution\"\nexport SPARKLING_WATER_VERSION=\"2.1.12\"\nexport SPARK_PACKAGES=\\\n\"ai.h2o:sparkling-water-core_2.11:${SPARKLING_WATER_VERSION},\\\nai.h2o:sparkling-water-repl_2.11:${SPARKLING_WATER_VERSION},\\\nai.h2o:sparkling-water-ml_2.11:${SPARKLING_WATER_VERSION},\\\ncom.packtpub:mastering-ml-w-spark-utils:1.0.0\"\n$SPARK_HOME/bin/spark-shell \\\n--master 'local[*]' \\\n--driver-memory 10g \\\n--executor-memory 10g \\\n--confspark.executor.extraJavaOptions=-XX:MaxPermSize=384M \\\n--confspark.driver.extraJavaOptions=-XX:MaxPermSize=384M \\\n--packages \"$SPARK_PACKAGES\" \"$@\"\n```", "```scala\nval positiveReviews= spark.sqlContext.read.textFile(\"../data/aclImdb/train/pos/*.txt\") \n   .toDF(\"reviewText\") \nprintln(s\"Number of positive reviews: ${positiveReviews.count}\") \nNumber of positive reviews: 12500\n```", "```scala\nprintln(\"Positive reviews:\")\npositiveReviews.show(5, truncate = true)\n```", "```scala\nval negativeReviews= spark.sqlContext.read.textFile(\"../data/aclImdb/train/neg/*.txt\")\n                .toDF(\"reviewText\")\nprintln(s\"Number of negative reviews: ${negativeReviews.count}\")\n```", "```scala\nimport org.apache.spark.sql.functions._\nval pos= positiveReviews.withColumn(\"label\", lit(1.0))\nval neg= negativeReviews.withColumn(\"label\", lit(0.0))\nvar movieReviews= pos.union(neg).withColumn(\"row_id\", monotonically_increasing_id)\nprintln(\"All reviews:\")\nmovieReviews.show(5)\n```", "```scala\nimport org.apache.spark.ml.feature.StopWordsRemover \nval stopWords= StopWordsRemover.loadDefaultStopWords(\"english\") ++ Array(\"ax\", \"arent\", \"re\")\n```", "```scala\nval *MIN_TOKEN_LENGTH* = 3\nval *toTokens*= (minTokenLen: Int, stopWords: Array[String], \n    review: String) =>\n      review.split(\"\"\"\\W+\"\"\")\n            .map(_.toLowerCase.replaceAll(\"[^\\\\p{IsAlphabetic}]\", \"\"))\n            .filter(w =>w.length>minTokenLen)\n            .filter(w => !stopWords.contains(w))\n```", "```scala\nimport spark.implicits._ \nval toTokensUDF= udf(toTokens.curried(MIN_TOKEN_LENGTH)(stopWords)) \nmovieReviews= movieReviews.withColumn(\"reviewTokens\", \n                                      toTokensUDF('reviewText)) \n```", "```scala\nval RARE_TOKEN = 2\nval rareTokens= movieReviews.select(\"reviewTokens\")\n               .flatMap(r =>r.getAs[Seq[String]](\"reviewTokens\"))\n               .map((v:String) => (v, 1))\n               .groupByKey(t => t._1)\n               .reduceGroups((a,b) => (a._1, a._2 + b._2))\n               .map(_._2)\n               .filter(t => t._2 <RARE_TOKEN)\n               .map(_._1)\n               .collect()\n```", "```scala\nprintln(s\"Rare tokens count: ${rareTokens.size}\")\nprintln(s\"Rare tokens: ${rareTokens.take(10).mkString(\", \")}\")\n```", "```scala\nval rareTokensFilter= (rareTokens: Array[String], tokens: Seq[String]) =>tokens.filter(token => !rareTokens.contains(token)) \nval rareTokensFilterUDF= udf(rareTokensFilter.curried(rareTokens)) \n\nmovieReviews= movieReviews.withColumn(\"reviewTokens\", rareTokensFilterUDF('reviewTokens)) \n\nprintln(\"Movie reviews tokens:\") \nmovieReviews.show(5) \n```", "```scala\n{\"acting\": 1, \"all\": 2, \"brilliant\": 3, \"cast\": 4, \"goodfellas\": 5, \"great\": 6, \"lover\": 7, \"money\": 8, \"movie\": 9, \"must\": 10, \"plot\": 11, \"riveting\": 12, \"see\": 13, \"spent\": 14, \"well\": 15, \"with\": 16, \"worth\": 17}\n```", "```scala\n[[1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1]\n[0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0]]\n```", "```scala\nval hashingTF= new HashingTF hashingTF.setInputCol(\"reviewTokens\")\n                   .setOutputCol(\"tf\")\n                   .setNumFeatures(1 <<12) // 2^12\n                   .setBinary(false)\nval tfTokens= hashingTF.transform(movieReviews)\nprintln(\"Vectorized movie reviews:\")\ntfTokens.show(5)\n```", "```scala\nimport org.apache.spark.ml.feature.IDF\nval idf= new IDF idf.setInputCol(hashingTF.getOutputCol)\n                    .setOutputCol(\"tf-idf\")\nval idfModel= idf.fit(tfTokens)\n```", "```scala\nval tfIdfTokens= idfModel.transform(tfTokens)\nprintln(\"Vectorized and scaled movie reviews:\")\ntfIdfTokens.show(5)\n```", "```scala\nimport org.apache.spark.ml.linalg.Vector\nval vecTf= tfTokens.take(1)(0).getAs[Vector](\"tf\").toSparse\nval vecTfIdf= tfIdfTokens.take(1)(0).getAs[Vector](\"tf-idf\").toSparse\nprintln(s\"Both vectors contains the same layout of non-zeros: ${java.util.Arrays.equals(vecTf.indices, vecTfIdf.indices)}\")\n```", "```scala\nprintln(s\"${vecTf.values.zip(vecTfIdf.values).take(5).mkString(\"\\n\")}\")\n```", "```scala\nvalsplits = tfIdfTokens.select(\"row_id\", \"label\", idf.getOutputCol).randomSplit(Array(0.7, 0.1, 0.1, 0.1), seed = 42)\nval(trainData, testData, transferData, validationData) = (splits(0), splits(1), splits(2), splits(3))\nSeq(trainData, testData, transferData, validationData).foreach(_.cache())\n```", "```scala\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel\nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator\nimport java.io.File\nval dtModelPath = s\" $ MODELS_DIR /dtModel\"\nval dtModel= {\n  val dtGridSearch = for (\n    dtImpurity<- Array(\"entropy\", \"gini\");\n    dtDepth<- Array(3, 5))\n    yield {\n      println(s\"Training decision tree: impurity $dtImpurity,\n              depth: $dtDepth\")\n      val dtModel = new DecisionTreeClassifier()\n          .setFeaturesCol(idf.getOutputCol)\n          .setLabelCol(\"label\")\n          .setImpurity(dtImpurity)\n          .setMaxDepth(dtDepth)\n          .setMaxBins(10)\n          .setSeed(42)\n          .setCacheNodeIds(true)\n          .fit(trainData)\n      val dtPrediction = dtModel.transform(testData)\n      val dtAUC = new BinaryClassificationEvaluator().setLabelCol(\"label\")\n          .evaluate(dtPrediction)\n      println(s\" DT AUC on test data: $dtAUC\")\n      ((dtImpurity, dtDepth), dtModel, dtAUC)\n    }\n    println(dtGridSearch.sortBy(-_._3).take(5).mkString(\"\\n\"))\n    val bestModel = dtGridSearch.sortBy(-_._3).head._2\n    bestModel.write.overwrite.save(dtModelPath)\n    bestModel\n  }\n```", "```scala\nval dtModel= if (new File(dtModelPath).exists()) {\n  DecisionTreeClassificationModel.load(dtModelPath)\n} else { /* do training */ }\n```", "```scala\nimport org.apache.spark.ml.classification.{NaiveBayes, NaiveBayesModel}\nval nbModelPath= s\"$MODELS_DIR/nbModel\"\nval nbModel= {\n  val model = new NaiveBayes()\n      .setFeaturesCol(idf.getOutputCol)\n      .setLabelCol(\"label\")\n      .setSmoothing(1.0)\n      .setModelType(\"multinomial\") // Note: input data are multinomial\n      .fit(trainData)\n  val nbPrediction = model.transform(testData)\n  val nbAUC = new BinaryClassificationEvaluator().setLabelCol(\"label\")\n                 .evaluate(nbPrediction)\n  println(s\"Naive Bayes AUC: $nbAUC\")\n  model.write.overwrite.save(nbModelPath)\n  model\n}\n```", "```scala\nimport org.apache.spark.ml.classification.{RandomForestClassifier, RandomForestClassificationModel}\nval rfModelPath= s\"$MODELS_DIR/rfModel\"\nval rfModel= {\n  val rfGridSearch = for (\n    rfNumTrees<- Array(10, 15);\n    rfImpurity<- Array(\"entropy\", \"gini\");\n    rfDepth<- Array(3, 5))\n    yield {\n      println( s\"Training random forest: numTrees: $rfNumTrees, \n              impurity $rfImpurity, depth: $rfDepth\")\n     val rfModel = new RandomForestClassifier()\n         .setFeaturesCol(idf.getOutputCol)\n         .setLabelCol(\"label\")\n         .setNumTrees(rfNumTrees)\n         .setImpurity(rfImpurity)\n         .setMaxDepth(rfDepth)\n         .setMaxBins(10)\n         .setSubsamplingRate(0.67)\n         .setSeed(42)\n         .setCacheNodeIds(true)\n         .fit(trainData)\n     val rfPrediction = rfModel.transform(testData)\n     val rfAUC = new BinaryClassificationEvaluator()\n                 .setLabelCol(\"label\")\n                 .evaluate(rfPrediction)\n     println(s\" RF AUC on test data: $rfAUC\")\n     ((rfNumTrees, rfImpurity, rfDepth), rfModel, rfAUC)\n   }\n   println(rfGridSearch.sortBy(-_._3).take(5).mkString(\"\\n\"))\n   val bestModel = rfGridSearch.sortBy(-_._3).head._2 \n   // Stress that the model is minimal because of defined gird space^\n   bestModel.write.overwrite.save(rfModelPath)\n   bestModel\n}\n```", "```scala\nimport org.apache.spark.ml.classification.{GBTClassifier, GBTClassificationModel}\nval gbmModelPath= s\"$MODELS_DIR/gbmModel\"\nval gbmModel= {\n  val model = new GBTClassifier()\n      .setFeaturesCol(idf.getOutputCol)\n      .setLabelCol(\"label\")\n      .setMaxIter(20)\n      .setMaxDepth(6)\n      .setCacheNodeIds(true)\n      .fit(trainData)\n  val gbmPrediction = model.transform(testData)\n  gbmPrediction.show()\n  val gbmAUC = new BinaryClassificationEvaluator()\n      .setLabelCol(\"label\")\n      .setRawPredictionCol(model.getPredictionCol)\n      .evaluate(gbmPrediction)\n  println(s\" GBM AUC on test data: $gbmAUC\")\n  model.write.overwrite.save(gbmModelPath)\n  model\n}\n```", "```scala\nimport org.apache.spark.ml.PredictionModel \nimport org.apache.spark.sql.DataFrame \n\nval models = Seq((\"NB\", nbModel), (\"DT\", dtModel), (\"RF\", rfModel), (\"GBM\", gbmModel)) \ndef mlData(inputData: DataFrame, responseColumn: String, baseModels: Seq[(String, PredictionModel[_, _])]): DataFrame= { \nbaseModels.map{ case(name, model) => \nmodel.transform(inputData) \n     .select(\"row_id\", model.getPredictionCol ) \n     .withColumnRenamed(\"prediction\", s\"${name}_prediction\") \n  }.reduceLeft((a, b) =>a.join(b, Seq(\"row_id\"), \"inner\")) \n   .join(inputData.select(\"row_id\", responseColumn), Seq(\"row_id\"), \"inner\") \n} \nval mlTrainData= mlData(transferData, \"label\", models).drop(\"row_id\") \nmlTrainData.show() \n```", "```scala\nval mlTestData = mlData(validationData, \"label\", models).drop(\"row_id\") \n```", "```scala\nimport org.apache.spark.h2o._ \nval hc= H2OContext.getOrCreate(sc) \nval mlTrainHF= hc.asH2OFrame(mlTrainData, \"metaLearnerTrain\") \nval mlTestHF= hc.asH2OFrame(mlTestData, \"metaLearnerTest\") \n```", "```scala\nimportwater.fvec.Vec\nval toEnumUDF= (name: String, vec: Vec) =>vec.toCategoricalVec\nmlTrainHF(toEnumUDF, 'label).update()\nmlTestHF(toEnumUDF, 'label).update()\n```", "```scala\nval metaLearningModel= new H2ODeepLearning()(hc, spark.sqlContext)\n      .setTrainKey(mlTrainHF.key)\n      .setValidKey(mlTestHF.key)\n      .setResponseColumn(\"label\")\n      .setEpochs(10)\n      .setHidden(Array(100, 100, 50))\n      .fit(null)\n```", "```scala\nimport org.apache.spark.ml.{Pipeline, UnaryTransformer} \nimport org.apache.spark.sql.types._ \nimport org.apache.spark.ml.param.ParamMap\nimport org.apache.spark.ml.util.{MLWritable, MLWriter} \n\nclass UDFTransformer[T, U](override valuid: String, \n                           f: T =>U, inType: DataType, \n                           outType: DataType) \nextendsUnaryTransformer[T, U, UDFTransformer[T, U]] with MLWritable { \n\noverride protected defcreateTransformFunc: T =>U = f \n\noverride protected defvalidateInputType(inputType: DataType): Unit = require(inputType == inType) \n\noverride protected defoutputDataType: DataType = outType \n\noverride defwrite: MLWriter = new MLWriter { \noverride protected defsaveImpl(path: String): Unit = {} \n } \n} \n```", "```scala\nval tokenizerTransformer= new UDFTransformer[String, Array[String]](\n  \"tokenizer\", toTokens.curried(MIN_TOKEN_LENGTH)(stopWords),\n  StringType, new ArrayType(StringType, true))\n```", "```scala\nval rareTokensFilterTransformer= new UDFTransformer[Seq[String], Seq[String]](\n  \"rareWordsRemover\",\n  rareTokensFilter.curried(rareTokens),\n  newArrayType(StringType, true), new ArrayType(StringType, true))\n```", "```scala\nimport org.apache.spark.ml.Transformer \nclass ColumnSelector(override valuid: String, valcolumnsToSelect: Array[String]) extends Transformer with MLWritable { \n\n  override deftransform(dataset: Dataset[_]): DataFrame= { \n    dataset.select(columnsToSelect.map(dataset.col): _*) \n  } \n\n  override deftransformSchema(schema: StructType): StructType = { \n    StructType(schema.fields.filter(col=>columnsToSelect\n                            .contains(col.name))) \n  } \n\n  override defcopy(extra: ParamMap): ColumnSelector = defaultCopy(extra) \n\n  override defwrite: MLWriter = new MLWriter { \n    override protected defsaveImpl(path: String): Unit = {} \n  } \n} \n```", "```scala\nval columnSelector= new ColumnSelector( \n  \"columnSelector\",  Array(s\"DT_${dtModel.getPredictionCol}\", \n  s\"NB_${nbModel.getPredictionCol}\", \n  s\"RF_${rfModel.getPredictionCol}\", \n  s\"GBM_${gbmModel.getPredictionCol}\") \n```", "```scala\nval superLearnerPipeline = new Pipeline() \n .setStages(Array( \n// Tokenize \ntokenizerTransformer \n     .setInputCol(\"reviewText\") \n     .setOutputCol(\"allReviewTokens\"), \n// Remove rare items \nrareTokensFilterTransformer \n     .setInputCol(\"allReviewTokens\") \n     .setOutputCol(\"reviewTokens\"), \nhashingTF, \nidfModel, \ndtModel \n     .setPredictionCol(s\"DT_${dtModel.getPredictionCol}\") \n     .setRawPredictionCol(s\"DT_${dtModel.getRawPredictionCol}\") \n     .setProbabilityCol(s\"DT_${dtModel.getProbabilityCol}\"), \nnbModel \n     .setPredictionCol(s\"NB_${nbModel.getPredictionCol}\") \n     .setRawPredictionCol(s\"NB_${nbModel.getRawPredictionCol}\") \n     .setProbabilityCol(s\"NB_${nbModel.getProbabilityCol}\"), \nrfModel \n     .setPredictionCol(s\"RF_${rfModel.getPredictionCol}\") \n     .setRawPredictionCol(s\"RF_${rfModel.getRawPredictionCol}\") \n     .setProbabilityCol(s\"RF_${rfModel.getProbabilityCol}\"), \ngbmModel// Note: GBM does not have full API of PredictionModel \n.setPredictionCol(s\"GBM_${gbmModel.getPredictionCol}\"), \ncolumnSelector, \nmetaLearningModel \n )) \n```", "```scala\nval superLearnerModel= superLearnerPipeline.fit(pos)\n```", "```scala\nval review = \"Although I love this movie, I can barely watch it, it is so real.....\"\nval reviewToScore= sc.parallelize(Seq(review)).toDF(\"reviewText\")\nval reviewPrediction= superLearnerModel.transform(reviewToScore)\n```", "```scala\nreviewPrediction.printSchema()\n```", "```scala\nreviewPrediction.show()\n```"]