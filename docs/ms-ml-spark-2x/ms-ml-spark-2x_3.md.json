["```scala\n199.38 0 NaN 34.1875 1.54285 7.86975 5.88674 1.57679 7.65264 5.84959 -0.0855996 ... 1 0 0 0 \n199.39 11 NaN 34.1875 1.46513 7.94554 5.80834 1.5336 7.81914 5.92477 -0.0907069 ...  1 0 0 0 \n199.4 11 NaN 34.1875 1.41585 7.82933 5.5001 1.56628 8.03042 6.01488 -0.0399161 ...  1 0 0 0 \n```", "```scala\nexport SPARKLING_WATER_VERSION=\"2.1.12\" \nexport SPARK_PACKAGES=\\ \n\"ai.h2o:sparkling-water-core_2.11:${SPARKLING_WATER_VERSION},\\ \nai.h2o:sparkling-water-repl_2.11:${SPARKLING_WATER_VERSION},\\ \nai.h2o:sparkling-water-ml_2.11:${SPARKLING_WATER_VERSION},\\ \ncom.packtpub:mastering-ml-w-spark-utils:1.0.0\" \n\n$SPARK_HOME/bin/spark-shell \\ \n        --master 'local[*]' \\ \n        --driver-memory 8g \\ \n        --executor-memory 8g \\ \n        --conf spark.executor.extraJavaOptions=-XX:MaxPermSize=384M\n        \\ \n        --conf spark.driver.extraJavaOptions=-XX:MaxPermSize=384M \\ \n        --packages \"$SPARK_PACKAGES\" \n```", "```scala\nval path = s\"${sys.env.get(\"DATADIR\").getOrElse(\"data\")}/subject*\"\nval dataFiles = sc.wholeTextFiles(path)\nprintln(s\"Number of input files: ${dataFiles.count}\")\n```", "```scala\nval allColumnNames = Array( \n  \"timestamp\", \"activityId\", \"hr\") ++ Array( \n  \"hand\", \"chest\", \"ankle\").flatMap(sensor => \n    Array( \n      \"temp\",  \n      \"accel1X\", \"accel1Y\", \"accel1Z\", \n      \"accel2X\", \"accel2Y\", \"accel2Z\", \n      \"gyroX\", \"gyroY\", \"gyroZ\", \n      \"magnetX\", \"magnetY\", \"magnetZ\", \n      \"orientX\", \"orientY\", \"orientZ\"). \n    map(name => s\"${sensor}_${name}\")) \n```", "```scala\nval ignoredColumns =  \n  Array(0,  \n    3 + 13, 3 + 14, 3 + 15, 3 + 16, \n    20 + 13, 20 + 14, 20 + 15, 20 + 16, \n    37 + 13, 37 + 14, 37 + 15, 37 + 16) \n```", "```scala\nval rawData = dataFiles.flatMap { case (path, content) =>  \n  content.split(\"\\n\") \n}.map { row =>  \n  row.split(\" \").map(_.trim). \n  zipWithIndex. \n  map(v => if (v.toUpperCase == \"NAN\") Double.NaN else v.toDouble). \n  collect {  \n    case (cell, idx) if !ignoredColumns.contains(idx) => cell \n  } \n} \nrawData.cache() \n\nprintln(s\"Number of rows: ${rawData.count}\") \n```", "```scala\nimport org.apache.spark.utils.Tabulizer._\n val columnNames = allColumnNames.\n   zipWithIndex.\n   filter { case (_, idx) => !ignoredColumns.contains(idx) }.\n   map { case (name, _) => name }\n\n println(s\"Column names:${table(columnNames, 4, None)}\") \n```", "```scala\nval activities = Map( \n  1 -> \"lying\", 2 -> \"sitting\", 3 -> \"standing\", 4 -> \"walking\",  \n  5 -> \"running\", 6 -> \"cycling\", 7 -> \"Nordic walking\",  \n  9 -> \"watching TV\", 10 -> \"computer work\", 11 -> \"car driving\", \n 12 -> \"ascending stairs\", 13 -> \"descending stairs\",  \n 16 -> \"vacuum cleaning\", 17 -> \"ironing\", \n 18 -> \"folding laundry\", 19 -> \"house cleaning\", \n 20 -> \"playing soccer\", 24 -> \"rope jumping\", 0 -> \"other\") \n\n```", "```scala\nval dataActivityId = rawData.map(l => l(0).toInt)\n\n val activityIdCounts = dataActivityId.\n   map(n => (n, 1)).\n   reduceByKey(_ + _)\n\n val activityCounts = activityIdCounts.\n   collect.\n   sortBy { case (activityId, count) =>\n     -count\n }.map { case (activityId, count) =>\n   (activitiesMap(activityId), count)\n }\n\n println(s\"Activities distribution:${table({activityCounts})}\")\n```", "```scala\nval nanCountPerRow = rawData.map { row => \n  row.foldLeft(0) { case (acc, v) =>  \n    acc + (if (v.isNaN) 1 else 0)  \n  } \n} \nval nanTotalCount = nanCount.sum \n\nval ncols = rawData.take(1)(0).length \nval nrows = rawData.count \n\nval nanRatio = 100.0 * nanTotalCount / (ncols * nrows)  \n\nprintln(f\"\"\"|NaN count = ${nanTotalCount}%.0f \n            |NaN ratio = ${nanRatio}%.2f %%\"\"\".stripMargin) \n```", "```scala\nval nanRowDistribution = nanCountPerRow.\n   map( count => (count, 1)).\n   reduceByKey(_ + _).sortBy(-_._1).collect\n\n println(s\"${table(Seq(\"#NaN\",\"#Rows\"), nanRowDistribution, Map.empty[Int, String])}\") \n```", "```scala\nval nanRowThreshold = 26 \nval badRows = nanCountPerRow.zipWithIndex.zip(rawData).filter(_._1._1 > nanRowThreshold).sortBy(-_._1._1) \nprintln(s\"Bad rows (#NaN, Row Idx, Row):\\n${badRows.collect.map(x => (x._1, x._2.mkString(\",\"))).mkString(\"\\n\")}\") \n```", "```scala\nval nanCountPerColumn = rawData.map { row =>\n   row.map(v => if (v.isNaN) 1 else 0)\n }.reduce((v1, v2) => v1.indices.map(i => v1(i) + v2(i)).toArray)\n\n println(s\"\"\"Number of missing values per column:\n      ^${table(columnNames.zip(nanCountPerColumn).map(t => (t._1, t._2, \"%.2f%%\".format(100.0 * t._2 / nrows))).sortBy(-_._2))}\n      ^\"\"\".stripMargin('^')) \n```", "```scala\nval heartRateColumn = rawData. \n  map(row => row(1)). \n  filter(_.isNaN). \n  map(_.toInt) \n\nval heartRateValues = heartRateColumn.collect \nval meanHeartRate = heartRateValues.sum / heartRateValues.count \nscala.util.Sorting.quickSort(heartRateValues) \nval medianHeartRate = heartRateValues(heartRateValues.length / 2) \n\nprintln(s\"Mean heart rate: ${meanHeartRate}\") \nprintln(s\"Median heart rate: ${medianHeartRate}\") \n```", "```scala\ndef inc[K,V](l: Seq[(K, V)], v: (K, V)) // (3)\n             (implicit num: Numeric[V]): Seq[(K,V)] =\n if (l.exists(_._1 == v._1)) l.map(e => e match {\n   case (v._1, n) => (v._1, num.plus(n, v._2))\n   case t => t\n }) else l ++ Seq(v)\n\n val distribTemplate = activityIdCounts.collect.map { case (id, _) => (id, 0) }.toSeq\n val nanColumnDistribV1 = rawData.map { row => // (1)\n   val activityId = row(0).toInt\n   row.drop(1).map { v =>\n     if (v.isNaN) inc(distribTemplate, (activityId, 1)) else distribTemplate\n   } // Tip: Make sure that we are returning same type\n }.reduce { (v1, v2) =>  // (2)\n   v1.indices.map(idx => v1(idx).foldLeft(v2(idx))(inc)).toArray\n }\n\n println(s\"\"\"\n         ^NaN Column x Response distribution V1:\n         ^${table(Seq(distribTemplate.map(v => activitiesMap(v._1)))\n                  ++ columnNames.drop(1).zip(nanColumnDistribV1).map(v => Seq(v._1) ++ v._2.map(_._2)), true)}\n           \"\"\".stripMargin('^')) \n\n```", "```scala\nval nanColumnDistribV2 = rawData.map(row => {\n   val activityId = row(0).toInt\n   (activityId, row.drop(1).map(v => if (v.isNaN) 1 else 0))\n }).reduceByKey( (v1, v2) =>\n   v1.indices.map(idx => v1(idx) + v2(idx)).toArray\n ).map { case (activityId, d) =>\n   (activitiesMap(activityId), d)\n }.collect\n\n println(s\"\"\"\n         ^NaN Column x Response distribution V2:\n         ^${table(Seq(columnNames.toSeq) ++ nanColumnDistribV2.map(v => Seq(v._1) ++ v._2), true)}\n         \"\"\".stripMargin('^'))\n```", "```scala\nval imputedValues = columnNames.map { \n  _ match { \n    case \"hr\" => 60.0 \n    case _ => 0.0 \n  } \n} \n```", "```scala\nimport org.apache.spark.rdd.RDD \ndef imputeNaN( \n  data: RDD[Array[Double]],  \n  values: Array[Double]): RDD[Array[Double]] = { \n    data.map { row => \n      row.indices.map { i => \n        if (row(i).isNaN) values(i) \n        else row(i) \n      }.toArray \n    } \n} \n```", "```scala\ndef filterBadRows( \n  rdd: RDD[Array[Double]], \n  nanCountPerRow: RDD[Int], \n  nanThreshold: Int): RDD[Array[Double]] = { \n    rdd.zip(nanCountPerRow).filter { case (row, nanCount) => \n      nanCount > nanThreshold \n  }.map { case (row, _) => \n        row \n  } \n} \n```", "```scala\nval activityId2Idx = activityIdCounts. \n  map(_._1). \n  collect. \n  zipWithIndex. \n  toMap \n```", "```scala\nval processedRawData = imputeNaN( \n  filterBadRows(rawData, nanCountPerRow, nanThreshold = 26), \n  imputedValues) \n```", "```scala\nprintln(s\"Number of rows before/after: ${rawData.count} / ${ processedRawData.count}\") \n```", "```scala\nimport org.apache.spark.mllib \nimport org.apache.spark.mllib.regression.LabeledPoint \nimport org.apache.spark.mllib.linalg.Vectors \nimport org.apache.spark.mllib.tree.RandomForest \nimport org.apache.spark.mllib.util.MLUtils \n\nval data = processedRawData.map { r =>  \n    val activityId = r(0) \n    val activityIdx = activityId2Idx(activityId) \n    val features = r.drop(1) \n    LabeledPoint(activityIdx, Vectors.dense(features)) \n} \n\n```", "```scala\nval splits = data.randomSplit(Array(0.8, 0.2)) \nval (trainingData, testData) =  \n    (splits(0), splits(1)) \n```", "```scala\nimport org.apache.spark.mllib.tree.configuration._ \nimport org.apache.spark.mllib.tree.impurity._ \nval rfStrategy = new Strategy( \n  algo = Algo.Classification, \n  impurity = Entropy, \n  maxDepth = 10, \n  maxBins = 20, \n  numClasses = activityId2Idx.size, \n  categoricalFeaturesInfo = Map[Int, Int](), \n  subsamplingRate = 0.68) \n\nval rfModel = RandomForest.trainClassifier( \n    input = trainingData,  \n    strategy = rfStrategy, \n    numTrees = 50,  \n    featureSubsetStrategy = \"auto\",  \n    seed = 42) \n```", "```scala\nimport org.apache.spark.mllib.evaluation._ \nimport org.apache.spark.mllib.tree.model._ \ndef getMetrics(model: RandomForestModel, data: RDD[LabeledPoint]): \n    MulticlassMetrics = { \n        val predictionsAndLabels = data.map(example => \n            (model.predict(example.features), example.label) \n        ) \n        new MulticlassMetrics(predictionsAndLabels) \n} \n```", "```scala\nval rfModelMetrics = getMetrics(rfModel, testData) \n```", "```scala\nprintln(s\"\"\"|Confusion matrix: \n  |${rfModelMetrics.confusionMatrix}\"\"\".stripMargin) \n```", "```scala\ndef idx2Activity(idx: Double): String =  \n  activityId2Idx. \n  find(e => e._2 == idx.asInstanceOf[Int]). \n  map(e => activitiesMap(e._1)). \n  getOrElse(\"UNKNOWN\") \n\nval rfCMLabels = rfModelMetrics.labels.map(idx2Activity(_)) \nprintln(s\"\"\"|Labels: \n  |${rfCMLabels.mkString(\", \")}\"\"\".stripMargin) \n```", "```scala\nval rfCM = rfModelMetrics.confusionMatrix \nval rfCMTotal = rfCM.toArray.sum \nval rfAccuracy = (0 until rfCM.numCols).map(i => rfCM(i,i)).sum / rfCMTotal \nprintln(f\"RandomForest accuracy = ${rfAccuracy*100}%.2f %%\") \n```", "```scala\nimport org.apache.spark.mllib.linalg.Matrix\n def colSum(m: Matrix, colIdx: Int) = (0 until m.numRows).map(m(_, colIdx)).sum\n def rowSum(m: Matrix, rowIdx: Int) = (0 until m.numCols).map(m(rowIdx, _)).sum\n val rfCMActDist = (0 until rfCM.numRows).map(rowSum(rfCM, _)/rfCMTotal)\n val rfCMPredDist = (0 until rfCM.numCols).map(colSum(rfCM, _)/rfCMTotal)\n\n println(s\"\"\"^Class distribution\n             ^${table(Seq(\"Class\", \"Actual\", \"Predicted\"),\n                      rfCMLabels.zip(rfCMActDist.zip(rfCMPredDist)).map(p => (p._1, p._2._1, p._2._2)),\n                      Map(1 -> \"%.2f\", 2 -> \"%.2f\"))}\n           \"\"\".stripMargin('^')) \n```", "```scala\ndef rfPrecision(m: Matrix, feature: Int) = m(feature, feature) / colSum(m, feature)\n def rfRecall(m: Matrix, feature: Int) = m(feature, feature) / rowSum(m, feature)\n def rfF1(m: Matrix, feature: Int) = 2 * rfPrecision(m, feature) * rfRecall(m, feature) / (rfPrecision(m, feature) + rfRecall(m, feature))\n\n val rfPerClassSummary = rfCMLabels.indices.map { i =>\n   (rfCMLabels(i), rfRecall(rfCM, i), rfPrecision(rfCM, i), rfF1(rfCM, i))\n }\n\n println(s\"\"\"^Per class summary:\n             ^${table(Seq(\"Label\", \"Recall\", \"Precision\", \"F-1\"),\n                      rfPerClassSummary,\n                      Map(1 -> \"%.4f\", 2 -> \"%.4f\", 3 -> \"%.4f\"))}\n           \"\"\".stripMargin('^')) \n```", "```scala\nval rfPerClassSummary2 = rfCMLabels.indices.map { i =>  \n    (rfCMLabels(i), rfModelMetrics.recall(i), rfModelMetrics.precision(i), rfModelMetrics.fMeasure(i))  \n} \n```", "```scala\nval rfMacroRecall = rfCMLabels.indices.map(i => rfRecall(rfCM, i)).sum/rfCMLabels.size \nval rfMacroPrecision = rfCMLabels.indices.map(i => rfPrecision(rfCM, i)).sum/rfCMLabels.size \nval rfMacroF1 = rfCMLabels.indices.map(i => rfF1(rfCM, i)).sum/rfCMLabels.size \n\nprintln(f\"\"\"|Macro statistics \n  |Recall, Precision, F-1 \n  |${rfMacroRecall}%.4f, ${rfMacroPrecision}%.4f, ${rfMacroF1}%.4f\"\"\".stripMargin) \n```", "```scala\nprintln(f\"\"\"|Weighted statistics \n  |Recall, Precision, F-1 \n  |${rfModelMetrics.weightedRecall}%.4f, ${rfModelMetrics.weightedPrecision}%.4f, ${rfModelMetrics.weightedFMeasure}%.4f \n  |\"\"\".stripMargin) \n```", "```scala\nimport org.apache.spark.mllib.linalg.Matrices \nval rfOneVsAll = rfCMLabels.indices.map { i => \n    val icm = rfCM(i,i) \n    val irowSum = rowSum(rfCM, i) \n    val icolSum = colSum(rfCM, i) \n    Matrices.dense(2,2,  \n      Array( \n        icm, irowSum - icm, \n        icolSum - icm, rfCMTotal - irowSum - icolSum + icm)) \n  } \nprintln(rfCMLabels.indices.map(i => s\"${rfCMLabels(i)}\\n${rfOneVsAll(i)}\").mkString(\"\\n\")) \n```", "```scala\nval rfOneVsAllCM = rfOneVsAll.foldLeft(Matrices.zeros(2,2))((acc, m) => \n  Matrices.dense(2, 2,  \n    Array(acc(0, 0) + m(0, 0),  \n          acc(1, 0) + m(1, 0), \n          acc(0, 1) + m(0, 1), \n          acc(1, 1) + m(1, 1))) \n) \nprintln(s\"Sum of oneVsAll CM:\\n${rfOneVsAllCM}\") \n```", "```scala\nprintln(f\"Average accuracy: ${(rfOneVsAllCM(0,0) + rfOneVsAllCM(1,1))/rfOneVsAllCM.toArray.sum}%.4f\") \n```", "```scala\nprintln(f\"Micro-averaged metrics: ${rfOneVsAllCM(0,0)/(rfOneVsAllCM(0,0)+rfOneVsAllCM(1,0))}%.4f\") \n```", "```scala\nimport org.apache.spark.h2o._ \nval h2oContext = H2OContext.getOrCreate(sc) \n\nval trainHF = h2oContext.asH2OFrame(trainingData, \"trainHF\") \ntrainHF.setNames(columnNames) \ntrainHF.update() \nval testHF = h2oContext.asH2OFrame(testData, \"testHF\") \ntestHF.setNames(columnNames) \ntestHF.update() \n```", "```scala\nprintln(s\"\"\"^Distribution of activityId:\n             ^${table(Seq(\"activityId\", \"Count\"),\n                      testData.map(row => (row.label, 1)).reduceByKey(_ + _).collect.sortBy(_._1),\n                      Map.empty[Int, String])}\n             \"\"\".stripMargin('^')) \n```", "```scala\ntrainHF.replace(0, trainHF.vec(0).toCategoricalVec).remove \ntrainHF.update \ntestHF.replace(0, testHF.vec(0).toCategoricalVec).remove \ntestHF.update \n```", "```scala\nval domain = trainHF.vec(0).domain.map(i => idx2Activity(i.toDouble)) \ntrainHF.vec(0).setDomain(domain) \nwater.DKV.put(trainHF.vec(0)) \ntestHF.vec(0).setDomain(domain) \nwater.DKV.put(testHF.vec(0)) \n```", "```scala\nimport _root_.hex.tree.drf.DRF \nimport _root_.hex.tree.drf.DRFModel \nimport _root_.hex.tree.drf.DRFModel.DRFParameters \nimport _root_.hex.ScoreKeeper._ \nimport _root_.hex.ConfusionMatrix \nimport water.Key.make \n\nval drfParams = new DRFParameters \ndrfParams._train = trainHF._key \ndrfParams._valid = testHF._key \ndrfParams._response_column = \"activityId\" \ndrfParams._max_depth = 20 \ndrfParams._ntrees = 50 \ndrfParams._score_each_iteration = true \ndrfParams._stopping_rounds = 2 \ndrfParams._stopping_metric = StoppingMetric.misclassification \ndrfParams._stopping_tolerance = 1e-3 \ndrfParams._seed = 42 \ndrfParams._nbins = 20 \ndrfParams._nbins_cats = 1024 \n\nval drfModel = new DRF(drfParams, make[DRFModel](\"drfModel\")).trainModel.get \n```", "```scala\nprintln(s\"Number of trees: ${drfModel._output._ntrees}\") \n```", "```scala\nval drfCM = drfModel._output._validation_metrics.cm \ndef h2oCM2SparkCM(h2oCM: ConfusionMatrix): Matrix = { \n  Matrices.dense(h2oCM.size, h2oCM.size, h2oCM._cm.flatMap(x => x)) \n} \nval drfSparkCM = h2oCM2SparkCM(drfCM) \n```", "```scala\nval drfPerClassSummary = drfCM._domain.indices.map { i =>\n   (drfCM._domain(i), rfRecall(drfSparkCM, i), rfPrecision(drfSparkCM, i), rfF1(drfSparkCM, i))\n }\n\n println(s\"\"\"^Per class summary\n             ^${table(Seq(\"Label\", \"Recall\", \"Precision\", \"F-1\"),\n                      drfPerClassSummary,\n                      Map(1 -> \"%.4f\", 2 -> \"%.4f\", 3 -> \"%.4f\"))}\n           \"\"\".stripMargin('^')) \n```", "```scala\ndrfParams._ntrees = 20 \ndrfParams._stopping_rounds = 0 \ndrfParams._checkpoint = drfModel._key \n\nval drfModel20 = new DRF(drfParams, make[DRFModel](\"drfMode20\")).trainModel.get \nprintln(s\"Number of trees: ${drfModel20._output._ntrees}\") \n```"]