["```scala\nimport org.apache.spark.graphx._\n```", "```scala\n\"org.apache.spark\" %% \"spark-graphx\" % \"2.1.1\"\n```", "```scala\nclass Graph[VD, ED] {\n  val vertices: VertexRDD[VD]\n  val edges: EdgeRDD[ED]\n}\n```", "```scala\ncase class Edge[ED] (\n  var srcId: VertexId,\n  var dstId: VertexId,\n  var attr: ED\n)\n```", "```scala\nimport org.apache.spark.rdd.RDD\nval vertices: RDD[(VertexId, String)] = sc.parallelize(\n  Array((1L, \"Anne\"),\n    (2L, \"Bernie\"),\n    (3L, \"Chris\"),\n    (4L, \"Don\"),\n    (5L, \"Edgar\")))\n```", "```scala\nval edges: RDD[Edge[String]] = sc.parallelize(\n  Array(Edge(1L, 2L, \"likes\"),\n    Edge(2L, 3L, \"trusts\"),\n    Edge(3L, 4L, \"believes\"),\n    Edge(4L, 5L, \"worships\"),\n    Edge(1L, 3L, \"loves\"),\n    Edge(4L, 1L, \"dislikes\")))\n```", "```scala\nval friendGraph: Graph[String, String] = Graph(vertices, edges)\n```", "```scala\nfriendGraph.vertices.collect.foreach(println)\n```", "```scala\nfriendGraph.edges.map( e => e.srcId > e.dstId ).filter(_ == true).count\n```", "```scala\nval mappedEdgeGraph: Graph[String, Boolean] = \n  friendGraph.mapEdges( e => e.srcId > e.dstId )\n```", "```scala\n3212,221,1347929725\n3212,3301,1347923714\n3212,1801,1347714310\n3212,1491,1347924000\n3212,1483,1347923691\n3212,1872,1347939690\n1486,1783,1346181381\n2382,3350,1346675417\n2382,1783,1342925318\n2159,349,1347911999\n```", "```scala\nval edges: RDD[Edge[Long]] =\n  sc.textFile(\"./rt_occupywallstnyc.edges\").map { line =>\n    val fields = line.split(\",\")\n    Edge(fields(0).toLong, fields(1).toLong, fields(2).toLong)\n  }\n```", "```scala\nval rtGraph: Graph[String, Long] = Graph.fromEdges(edges, defaultValue =  \"\")\n```", "```scala\nval order = rtGraph.numVertices\nval degree = rtGraph.numEdges\n```", "```scala\nval avgDegree = rtGraph.degrees.map(_._2).reduce(_ + _) / order.toDouble\n```", "```scala\nval vertexDegrees: VertexRDD[Int] = rtGraph.degrees\nval degrees: RDD[Int] = vertexDegrees.map(v => v._2)\nval sumDegrees: Int = degrees.reduce((v1, v2) => v1 + v2 )\nval avgDegreeAlt = sumDegrees / order.toDouble\n```", "```scala\nval maxInDegree: (Long, Int) = rtGraph.inDegrees.reduce(\n  (v1,v2) => if (v1._2 > v2._2) v1 else v2\n)\n```", "```scala\nrtGraph.edges.filter(e => e.dstId == 1783).map(_.srcId).distinct()\n```", "```scala\nval minOutDegree: (Long, Int) = rtGraph.outDegrees.reduce(\n  (v1,v2) => if (v1._2 < v2._2) v1 else v2\n)\n```", "```scala\nval triplets: RDD[EdgeTriplet[String, Long]] = rtGraph.triplets\n```", "```scala\nval tweetStrings = triplets.map(\n  t => t.dstId + \" retweeted \" + t.attr + \" from \" + t.srcId\n)\ntweetStrings.take(5)\n```", "```scala\nval vertexIdData: Graph[Long, Long] = rtGraph.mapVertices( (id, _) => id)\n```", "```scala\nval mappedTripletsGraph = rtGraph.mapTriplets(\n  t => t.dstId + \" retweeted \" + t.attr + \" from \" + t.srcId\n)\n```", "```scala\ndef outerJoinVertices[U, VD2](other: RDD[(VertexId, U)])\n  (mapFunc: (VertexId, VD, Option[U]) => VD2): Graph[VD2, ED]\n```", "```scala\nval outDegreeGraph: Graph[Long, Long] =\n  rtGraph.outerJoinVertices[Int, Long](rtGraph.outDegrees)(\n    mapFunc = (id, origData, outDeg) => outDeg.getOrElse(0).toLong\n  )\n```", "```scala\ndef subgraph(\n  epred: EdgeTriplet[VD,ED] => Boolean = (x => true),\n  vpred: (VertexId, VD) => Boolean = ((v, d) => true)): Graph[VD, ED]\n```", "```scala\nval tenOrMoreRetweets = outDegreeGraph.subgraph(\n  vpred = (id, deg) => deg >= 10\n)\ntenOrMoreRetweets.vertices.count\ntenOrMoreRetweets.edges.count\n```", "```scala\nval lessThanTenRetweets = rtGraph.mask(tenOrMoreRetweets)\n```", "```scala\n1 3\n5 3\n4 2\n3 2\n1 5\n```", "```scala\nimport org.apache.spark.graphx.GraphLoader\nval edgeListGraph = GraphLoader.edgeListFile(sc, \"./edge_list.txt\")\n```", "```scala\nval rawEdges: RDD[(VertexId, VertexId)] = sc.textFile(\"./edge_list.txt\").map { \n  line =>\n    val field = line.split(\" \")\n    (field(0).toLong, field(1).toLong)\n}\nval edgeTupleGraph = Graph.fromEdgeTuples(\n  rawEdges=rawEdges, defaultValue=\"\")\n```", "```scala\nimport org.apache.spark.graphx.util.GraphGenerators\n```", "```scala\nval starGraph = GraphGenerators.starGraph(sc, 11)\n```", "```scala\nval gridGraph = GraphGenerators.gridGraph(sc, 5, 5)\n```", "```scala\nval logNormalGraph  = GraphGenerators.logNormalGraph(\n  sc, numVertices = 20, mu=1, sigma = 3\n)\n```", "```scala\nlogNormalGraph.outDegrees.map(_._2).collect().sorted\n```", "```scala\n<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<gexf  version=\"1.2\">\n    <meta lastmodifieddate=\"2009-03-20\">\n        <creator>Gexf.net</creator>\n        <description>A hello world! file</description>\n    </meta>\n    <graph mode=\"static\" defaultedgetype=\"directed\">\n        <nodes>\n            <node id=\"0\" label=\"Hello\" />\n            <node id=\"1\" label=\"Word\" />\n        </nodes>\n        <edges>\n            <edge id=\"0\" source=\"0\" target=\"1\" />\n        </edges>\n    </graph>\n</gexf>\n```", "```scala\ndef toGexf[VD, ED](g: Graph[VD, ED]): String = {\n  val header =\n    \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n      |<gexf  version=\"1.2\">\n      |  <meta>\n      |    <description>A gephi graph in GEXF format</description>\n      |  </meta>\n      |    <graph mode=\"static\" defaultedgetype=\"directed\">\n    \"\"\".stripMargin\n\n  val vertices = \"<nodes>\\n\" + g.vertices.map(\n    v => s\"\"\"<node id=\\\"${v._1}\\\" label=\\\"${v._2}\\\"/>\\n\"\"\"\n  ).collect.mkString + \"</nodes>\\n\"\n\n  val edges = \"<edges>\\n\" + g.edges.map(\n    e => s\"\"\"<edge source=\\\"${e.srcId}\\\" target=\\\"${e.dstId}\\\" label=\\\"${e.attr}\\\"/>\\n\"\"\"\n  ).collect.mkString + \"</edges>\\n\"\n\n  val footer = \"</graph>\\n</gexf>\"\n\n  header + vertices + edges + footer\n}\n```", "```scala\nimport java.io.PrintWriter\nimport org.apache.spark._\nimport org.apache.spark.graphx._\nimport org.apache.spark.rdd.RDD\n\nobject GephiApp {\n  def main(args: Array[String]) {\n\n    val conf = new SparkConf()\n      .setAppName(\"Gephi Test Writer\")\n      .setMaster(\"local[4]\")\n    val sc = new SparkContext(conf)\n\n    val vertices: RDD[(VertexId, String)] = sc.parallelize(\n      Array((1L, \"Anne\"),\n        (2L, \"Bernie\"),\n        (3L, \"Chris\"),\n        (4L, \"Don\"),\n        (5L, \"Edgar\")))\n\n    val edges: RDD[Edge[String]] = sc.parallelize(\n      Array(Edge(1L, 2L, \"likes\"),\n        Edge(2L, 3L, \"trusts\"),\n        Edge(3L, 4L, \"believes\"),\n        Edge(4L, 5L, \"worships\"),\n        Edge(1L, 3L, \"loves\"),\n        Edge(4L, 1L, \"dislikes\")))\n\n    val graph: Graph[String, String] = Graph(vertices, edges)\n\n    val pw = new PrintWriter(\"./graph.gexf\")\n    pw.write(toGexf(graph))\n    pw.close()\n  }\n}\n```", "```scala\ndef aggregateMessages[Msg: ClassTag](\n  sendMsg: EdgeContext[VD, ED, Msg] => Unit,\n  mergeMsg: (Msg, Msg) => Msg,\n  tripletFields: TripletFields = TripletFields.All\n): VertexRDD[Msg]\n```", "```scala\ndef sendToSrc(msg: A): Unit\ndef sendToDst(msg: A): Unit\n```", "```scala\nval inDegVertexRdd: VertexRDD[Int] = friendGraph.aggregateMessages[Int](\n  sendMsg = ec => ec.sendToDst(1),\n  mergeMsg = (msg1, msg2) => msg1+msg2\n)\nassert(inDegVertexRdd.collect.deep == friendGraph.inDegrees.collect.deep)\n```", "```scala\nobject Pregel {\n  def apply[VD: ClassTag, ED: ClassTag, A: ClassTag]\n    (graph: Graph[VD, ED],\n     initialMsg: A,\n     maxIterations: Int = Int.MaxValue,\n     activeDirection: EdgeDirection = EdgeDirection.Either)\n    (vprog: (VertexId, VD, A) => VD,\n     sendMsg: EdgeTriplet[VD, ED] => Iterator[(VertexId, A)],\n     mergeMsg: (A, A) => A)\n  : Graph[VD, ED]\n}\n```", "```scala\nimport org.apache.spark.graphx._\nimport scala.reflect.ClassTag\n\nobject ConnectedComponents extends Serializable {\n\n  def run[VD: ClassTag, ED: ClassTag](graph: Graph[VD, ED],\n                                      maxIterations: Int)\n  : Graph[VertexId, ED] = {\n\n    val idGraph: Graph[VertexId, ED] = graph.mapVertices((id, _) => id)\n\n    def vprog(id: VertexId, attr: VertexId, msg: VertexId): VertexId = {\n      math.min(attr, msg)\n    }\n\n    def sendMsg(edge: EdgeTriplet[VertexId, ED]): Iterator[(VertexId, VertexId)] = {\n      if (edge.srcAttr < edge.dstAttr) {\n        Iterator((edge.dstId, edge.srcAttr))\n      } else if (edge.srcAttr > edge.dstAttr) {\n        Iterator((edge.srcId, edge.dstAttr))\n      } else {\n        Iterator.empty\n      }\n    }\n\n    def mergeMsg(v1: VertexId, v2: VertexId): VertexId = math.min(v1, v2)\n\n    Pregel(\n      graph = idGraph,\n      initialMsg = Long.MaxValue,\n      maxIterations,\n      EdgeDirection.Either)(\n      vprog,\n      sendMsg,\n      mergeMsg)\n  }\n}\n```", "```scala\nval ccGraph = ConnectedComponents.run(rtGraph, 5)\ncc.vertices.map(_._2).distinct.count\n```", "```scala\ndef run[VD: ClassTag, ED: ClassTag](graph: Graph[VD, ED])\n: Graph[VertexId, ED] = {\n  run(graph, Int.MaxValue)\n}\n```", "```scala\nval ccGraph = ConnectedComponents.run(rtGraph)\n```", "```scala\nimport org.graphframes._\n\nval friendGraphFrame = GraphFrame.fromGraphX(friendGraph)\nval graph = friendGraphFrame.toGraphX\n```", "```scala\nfriendGraphFrame.find(\"(v1)-[e1]->(v2); (v2)-[e2]->(v3)\").filter(\n  \"e1.attr = 'trusts' OR v3.attr = 'Chris'\"\n).collect.foreach(println)\n```", "```scala\nval actorGraph = GraphLoader.edgeListFile(sc, \"./ca-hollywood-2009.txt\")\nactorGraph.edges.count()\n```", "```scala\nval actorComponents = actorGraph.connectedComponents().cache \nactorComponents.vertices.map(_._2).distinct().count\n```", "```scala\nval clusterSizes =actorComponents.vertices.map(\n  v => (v._2, 1)).reduceByKey(_ + _)\nclusterSizes.map(_._2).max\nclusterSizes.map(_._2).min\n```", "```scala\nval strongComponents = actorGraph.stronglyConnectedComponents(numIter = 1)\nstrongComponents.vertices.map(_._2).distinct().count\n```", "```scala\nval canonicalGraph = actorGraph.mapEdges(\n  e => 1).removeSelfEdges().convertToCanonicalEdges()\n```", "```scala\nval partitionedGraph = canonicalGraph.partitionBy(PartitionStrategy.RandomVertexCut)\n```", "```scala\nimport org.apache.spark.graphx.lib.TriangleCount\nval triangles = TriangleCount.runPreCanonicalized(partitionedGraph)\n```", "```scala\nactorGraph.triangleCount()\n```", "```scala\nimport org.apache.spark.graphx.lib.TriangleCount\nTriangleCount.run(actorGraph)\n```", "```scala\nfriendGraph.staticPageRank(numIter = 10).vertices.collect.foreach(println)\n```", "```scala\n (1,0.42988729103845036)\n (2,0.3308390977362031)\n (3,0.6102873825386869)\n (4,0.6650182732476072)\n (5,0.42988729103845036)\n```", "```scala\nfriendGraph.pageRank(tol = 0.0001, resetProb = 0.15)\n```", "```scala\nval actorPrGraph: Graph[Double, Double] = actorGraph.pageRank(0.0001)\nactorPrGraph.vertices.reduce((v1, v2) => {\n  if (v1._2 > v2._2) v1 else v2\n})\n```", "```scala\nactorPrGraph.inDegrees.filter(v => v._1 == 33024L).collect.foreach(println)\n```", "```scala\nactorPrGraph.inDegrees.map(_._2).collect().sorted.takeRight(10)\n```", "```scala\nactorPrGraph.inDegrees.map(_._2).filter(_ >= 62).count\n```"]