["```scala\nobject Chapter8 extends App {\n\nval spark = SparkSession.builder()\n     .master(\"local[*]\")\n     .appName(\"Chapter8\")\n     .getOrCreate()\n\nval sc = spark.sparkContext\nsc.setLogLevel(\"WARN\")\nscript(spark, sc, spark.sqlContext)\n\ndef script(spark: SparkSession, sc: SparkContext, sqlContext: SQLContext): Unit = {\n      // ...code of application\n}\n}\n\n```", "```scala\nobject Chapter8Library {\n    // ...code of library\n  }\n```", "```scala\nval DATASET_DIR = s\"${sys.env.get(\"DATADIR\").getOrElse(\"data\")}\" val DATASETS = Array(\"LoanStats3a.CSV\", \"LoanStats3b.CSV\")\nimport java.net.URI\n\nimport water.fvec.H2OFrame\nval loanDataHf = new H2OFrame(DATASETS.map(name => URI.create(s\"${DATASET_DIR}/${name}\")):_*)\n```", "```scala\nimport com.packtpub.mmlwspark.utils.Tabulizer.table\nval idColumns = Seq(\"id\", \"member_id\")\nprintln(s\"Columns with Ids: ${table(idColumns, 4, None)}\")\n\n```", "```scala\nval constantColumns = loanDataHf.names().indices\n   .filter(idx => loanDataHf.vec(idx).isConst || loanDataHf.vec(idx).isBad)\n   .map(idx => loanDataHf.name(idx))\nprintln(s\"Constant and bad columns: ${table(constantColumns, 4, None)}\")\n```", "```scala\nval stringColumns = loanDataHf.names().indices\n   .filter(idx => loanDataHf.vec(idx).isString)\n   .map(idx => loanDataHf.name(idx))\nprintln(s\"String columns:${table(stringColumns, 4, None)}\")\n```", "```scala\nval loanProgressColumns = Seq(\"funded_amnt\", \"funded_amnt_inv\", \"grade\", \"initial_list_status\",\n\"issue_d\", \"last_credit_pull_d\", \"last_pymnt_amnt\", \"last_pymnt_d\",\n\"next_pymnt_d\", \"out_prncp\", \"out_prncp_inv\", \"pymnt_plan\",\n\"recoveries\", \"sub_grade\", \"total_pymnt\", \"total_pymnt_inv\",\n\"total_rec_int\", \"total_rec_late_fee\", \"total_rec_prncp\")\n```", "```scala\nval columnsToRemove = (idColumns ++ constantColumns ++ stringColumns ++ loanProgressColumns)\n```", "```scala\nval categoricalColumns = loanDataHf.names().indices\n  .filter(idx => loanDataHf.vec(idx).isCategorical)\n  .map(idx => (loanDataHf.name(idx), loanDataHf.vec(idx).cardinality()))\n  .sortBy(-_._2)\n\nprintln(s\"Categorical columns:${table(tblize(categoricalColumns, true, 2))}\")\n```", "```scala\nimport org.apache.spark.sql.functions._\nval toNumericMnths = (replacementValue: Float) => (mnths: String) => {\nif (mnths != null && !mnths.trim.isEmpty) mnths.trim.toFloat else replacementValue\n}\nval toNumericMnthsUdf = udf(toNumericMnths(0.0f))\n```", "```scala\nimport org.apache.spark.sql.functions._\nval toNumericRate = (rate: String) => {\nval num = if (rate != null) rate.stripSuffix(\"%\").trim else \"\"\nif (!num.isEmpty) num.toFloat else Float.NaN\n}\nval toNumericRateUdf = udf(toNumericRate)\n```", "```scala\nval naColumns = loanDataHf.names().indices\n   .filter(idx => loanDataHf.vec(idx).naCnt() >0)\n   .map(idx =>\n          (loanDataHf.name(idx),\n            loanDataHf.vec(idx).naCnt(),\nf\"${100*loanDataHf.vec(idx).naCnt()/loanDataHf.numRows().toFloat}%2.1f%%\")\n   ).sortBy(-_._2)\nprintln(s\"Columns with NAs (#${naColumns.length}):${table(naColumns)}\")\n```", "```scala\ndef basicDataCleanup(loanDf: DataFrame, colsToDrop: Seq[String] = Seq()) = {\n   (\n     (if (loanDf.columns.contains(\"int_rate\"))\n       loanDf.withColumn(\"int_rate\", toNumericRateUdf(col(\"int_rate\")))\nelse loanDf)\n       .withColumn(\"revol_util\", toNumericRateUdf(col(\"revol_util\")))\n       .withColumn(\"mo_sin_old_il_acct\", toNumericMnthsUdf(col(\"mo_sin_old_il_acct\")))\n       .withColumn(\"mths_since_last_delinq\", toNumericMnthsUdf(col(\"mths_since_last_delinq\")))\n       .withColumn(\"mths_since_last_record\", toNumericMnthsUdf(col(\"mths_since_last_record\")))\n       .withColumn(\"mths_since_last_major_derog\", toNumericMnthsUdf(col(\"mths_since_last_major_derog\")))\n       .withColumn(\"mths_since_recent_bc\", toNumericMnthsUdf(col(\"mths_since_recent_bc\")))\n       .withColumn(\"mths_since_recent_bc_dlq\", toNumericMnthsUdf(col(\"mths_since_recent_bc_dlq\")))\n       .withColumn(\"mths_since_recent_inq\", toNumericMnthsUdf(col(\"mths_since_recent_inq\")))\n       .withColumn(\"mths_since_recent_revol_delinq\", toNumericMnthsUdf(col(\"mths_since_recent_revol_delinq\")))\n   ).drop(colsToDrop.toArray :_*)\n }\n```", "```scala\nval toBinaryLoanStatus = (status: String) => status.trim.toLowerCase() match {\ncase \"fully paid\" =>\"good loan\"\ncase _ =>\"bad loan\"\n}\nval toBinaryLoanStatusUdf = udf(toBinaryLoanStatus)\n```", "```scala\nimport com.packtpub.mmlwspark.chapter8.Chapter8Library._\nval loanDataDf = h2oContext.asDataFrame(loanDataHf)(sqlContext)\nval loanStatusBaseModelDf = basicDataCleanup(\n   loanDataDf\n     .where(\"loan_status is not null\")\n     .withColumn(\"loan_status\", toBinaryLoanStatusUdf($\"loan_status\")),\n   colsToDrop = Seq(\"title\") ++ columnsToRemove)\n```", "```scala\nval loanStatusDfSplits = loanStatusBaseModelDf.randomSplit(Array(0.7, 0.3), seed = 42)\n\nval trainLSBaseModelHf = toHf(loanStatusDfSplits(0).drop(\"emp_title\", \"desc\"), \"trainLSBaseModelHf\")(h2oContext)\nval validLSBaseModelHf = toHf(loanStatusDfSplits(1).drop(\"emp_title\", \"desc\"), \"validLSBaseModelHf\")(h2oContext)\ndef toHf(df: DataFrame, name: String)(h2oContext: H2OContext): H2OFrame = {\nval hf = h2oContext.asH2OFrame(df, name)\nval allStringColumns = hf.names().filter(name => hf.vec(name).isString)\n     hf.colToEnum(allStringColumns)\n     hf\n }\n```", "```scala\n\nimport _root_.hex.tree.drf.DRFModel.DRFParameters\nimport _root_.hex.tree.drf.{DRF, DRFModel}\nimport _root_.hex.ScoreKeeper.StoppingMetric\nimport com.packtpub.mmlwspark.utils.Utils.let\n\nval loanStatusBaseModelParams = let(new DRFParameters) { p =>\n   p._response_column = \"loan_status\" p._train = trainLSBaseModelHf._key\np._ignored_columns = Array(\"int_rate\")\n   p._stopping_metric = StoppingMetric.logloss\np._stopping_rounds = 1\np._stopping_tolerance = 0.1\np._ntrees = 100\np._balance_classes = true p._score_tree_interval = 20\n}\nval loanStatusBaseModel1 = new DRF(loanStatusBaseModelParams, water.Key.make[DRFModel](\"loanStatusBaseModel1\"))\n   .trainModel()\n   .get()\n```", "```scala\nloanStatusBaseModelParams._ignored_columns = Array(\"int_rate\", \"collection_recovery_fee\", \"zip_code\")\nval loanStatusBaseModel2 = new DRF(loanStatusBaseModelParams, water.Key.make[DRFModel](\"loanStatusBaseModel2\"))\n   .trainModel()\n   .get()\n```", "```scala\nimport _root_.hex.ModelMetrics\nval lsBaseModelPredHf = loanStatusBaseModel2.score(validLSBaseModelHf)\nprintln(ModelMetrics.getFromDKV(loanStatusBaseModel2, validLSBaseModelHf))\n```", "```scala\ndef profitMoneyLoss = (predThreshold: Double) =>\n     (act: String, predGoodLoanProb: Double, loanAmount: Int, intRate: Double, term: String) => {\nval termInMonths = term.trim match {\ncase \"36 months\" =>36\ncase \"60 months\" =>60\n}\nval intRatePerMonth = intRate / 12 / 100\nif (predGoodLoanProb < predThreshold && act == \"good loan\") {\n         termInMonths*loanAmount*intRatePerMonth / (1 - Math.pow(1+intRatePerMonth, -termInMonths)) - loanAmount\n       } else 0.0\n}\n```", "```scala\nval loanMoneyLoss = (act: String, predGoodLoanProb: Double, predThreshold: Double, loanAmount: Int) => {\nif (predGoodLoanProb > predThreshold /* good loan predicted */\n&& act == \"bad loan\" /* actual is bad loan */) loanAmount else 0\n}\n```", "```scala\nimport org.apache.spark.sql.Row\ndef totalLoss(actPredDf: DataFrame, threshold: Double): (Double, Double, Long, Double, Long, Double) = {\n\nval profitMoneyLossUdf = udf(profitMoneyLoss(threshold))\nval loanMoneyLossUdf = udf(loanMoneyLoss(threshold))\n\nval lostMoneyDf = actPredDf\n     .where(\"loan_status is not null and loan_amnt is not null\")\n     .withColumn(\"profitMoneyLoss\", profitMoneyLossUdf($\"loan_status\", $\"good loan\", $\"loan_amnt\", $\"int_rate\", $\"term\"))\n     .withColumn(\"loanMoneyLoss\", loanMoneyLossUdf($\"loan_status\", $\"good loan\", $\"loan_amnt\"))\n\n   lostMoneyDf\n     .agg(\"profitMoneyLoss\" ->\"sum\", \"loanMoneyLoss\" ->\"sum\")\n     .collect.apply(0) match {\ncase Row(profitMoneyLossSum: Double, loanMoneyLossSum: Double) =>\n       (threshold,\n         profitMoneyLossSum, lostMoneyDf.where(\"profitMoneyLoss > 0\").count,\n         loanMoneyLossSum, lostMoneyDf.where(\"loanMoneyLoss > 0\").count,\n         profitMoneyLossSum + loanMoneyLossSum\n       )\n   }\n }\n```", "```scala\nimport _root_.hex.AUC2.ThresholdCriterion\nval predVActHf: Frame = lsBaseModel2PredHf.add(validLSBaseModelHf)\n water.DKV.put(predVActHf)\nval predVActDf = h2oContext.asDataFrame(predVActHf)(sqlContext)\nval DEFAULT_THRESHOLDS = Array(0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95)\n\nprintln(\ntable(Array(\"Threshold\", \"Profit Loss\", \"Count\", \"Loan loss\", \"Count\", \"Total loss\"),\n         (DEFAULT_THRESHOLDS :+\n               ThresholdCriterion.min_per_class_accuracy.max_criterion(lsBaseModel2PredModelMetrics.auc_obj()))\n          .map(threshold =>totalLoss(predVActDf, threshold)),\nMap(1 ->\"%,.2f\", 3 ->\"%,.2f\", 5 ->\"%,.2f\")))\n```", "```scala\n// @Snippet\ndef findMinLoss(model: DRFModel,\n                 validHf: H2OFrame,\n                 defaultThresholds: Array[Double]): (Double, Double, Double, Double) = {\nimport _root_.hex.ModelMetrics\nimport _root_.hex.AUC2.ThresholdCriterion\n// Score model\nval modelPredHf = model.score(validHf)\nval modelMetrics = ModelMetrics.getFromDKV(model, validHf)\nval predVActHf: Frame = modelPredHf.add(validHf)\n   water.DKV.put(predVActHf)\n//\nval predVActDf = h2oContext.asDataFrame(predVActHf)(sqlContext)\nval min = (DEFAULT_THRESHOLDS :+ ThresholdCriterion.min_per_class_accuracy.max_criterion(modelMetrics.auc_obj()))\n     .map(threshold =>totalLoss(predVActDf, threshold)).minBy(_._6)\n   ( /* Threshold */ min._1, /* Total loss */ min._6, /* Profit loss */ min._2, /* Loan loss */ min._4)\n }\nval minLossModel2 = findMinLoss(loanStatusBaseModel2, validLSBaseModelHf, DEFAULT_THRESHOLDS)\nprintln(f\"Min total loss for model 2: ${minLossModel2._2}%,.2f (threshold = ${minLossModel2._1})\")\n```", "```scala\nval unifyTextColumn = (in: String) => {\nif (in != null) in.toLowerCase.replaceAll(\"[^\\\\w ]|\", \"\") else null\n}\nval unifyTextColumnUdf = udf(unifyTextColumn)\n```", "```scala\nval ALL_NUM_REGEXP = java.util.regex.Pattern.compile(\"\\\\d*\")\nval tokenizeTextColumn = (minLen: Int) => (stopWords: Array[String]) => (w: String) => {\nif (w != null)\n     w.split(\" \").map(_.trim).filter(_.length >= minLen).filter(!ALL_NUM_REGEXP.matcher(_).matches()).filter(!stopWords.contains(_)).toSeq\nelse Seq.empty[String]\n }\nimport org.apache.spark.ml.feature.StopWordsRemover\nval tokenizeUdf = udf(tokenizeTextColumn(3)(StopWordsRemover.loadDefaultStopWords(\"english\")))\n```", "```scala\nval empTitleColumnDf = loanStatusBaseModelDf\n   .withColumn(\"emp_title\", unifyTextColumnUdf($\"emp_title\"))\n   .withColumn(\"emp_title_tokens\", tokenizeUdf($\"emp_title\"))\n```", "```scala\nprintln(\"Number of unique values in emp_title column: \" +\n        empTitleColumn.select(\"emp_title\").groupBy(\"emp_title\").count().count())\nprintln(\"Number of unique tokens with freq > 100 in emp_title column: \" +\n        empTitleColumn.rdd.flatMap(row => row.getSeq[String](1).map(w => (w, 1)))\n          .reduceByKey(_ + _).filter(_._2 >100).count)\n```", "```scala\nimport org.apache.spark.ml.feature.Word2Vec\nval empTitleW2VModel = new Word2Vec()\n  .setInputCol(\"emp_title_tokens\")\n  .setOutputCol(\"emp_title_w2vVector\")\n  .setMinCount(1)\n  .fit(empTitleColumn)\n```", "```scala\n\n val empTitleColumnWithW2V =   w2vModel.transform(empTitleW2VModel)\n empTitleColumnWithW2V.printSchema()\n```", "```scala\nimport org.apache.spark.ml.clustering.KMeans\nval K = 500\nval empTitleKmeansModel = new KMeans()\n  .setFeaturesCol(\"emp_title_w2vVector\")\n  .setK(K)\n  .setPredictionCol(\"emp_title_cluster\")\n  .fit(empTitleColumnWithW2V)\n```", "```scala\nval clustered = empTitleKmeansModel.transform(empTitleColumnWithW2V)\nclustered.printSchema()\n```", "```scala\nprintln(\ns\"\"\"Words in cluster '133':\n |${clustered.select(\"emp_title\").where(\"emp_title_cluster = 133\").take(10).mkString(\", \")}\n |\"\"\".stripMargin)\n```", "```scala\nimport org.apache.spark.ml.Pipeline\nimport org.apache.spark.sql.types._\n\nval empTitleTransformationPipeline = new Pipeline()\n   .setStages(Array(\nnew UDFTransformer(\"unifier\", unifyTextColumn, StringType, StringType)\n       .setInputCol(\"emp_title\").setOutputCol(\"emp_title_unified\"),\nnew UDFTransformer(\"tokenizer\",\n                        tokenizeTextColumn(3)(StopWordsRemover.loadDefaultStopWords(\"english\")),\n                        StringType, ArrayType(StringType, true))\n       .setInputCol(\"emp_title_unified\").setOutputCol(\"emp_title_tokens\"),\n     empTitleW2VModel,\n     empTitleKmeansModel,\nnew ColRemover().setKeep(false).setColumns(Array(\"emp_title\", \"emp_title_unified\", \"emp_title_tokens\", \"emp_title_w2vVector\"))\n   ))\n```", "```scala\nval empTitleTransformer = empTitleTransformationPipeline.fit(loanStatusBaseModelDf)\n```", "```scala\nval trainLSBaseModel3Df = empTitleTransformer.transform(loanStatusDfSplits(0))\nval validLSBaseModel3Df = empTitleTransformer.transform(loanStatusDfSplits(1))\nval trainLSBaseModel3Hf = toHf(trainLSBaseModel3Df.drop(\"desc\"), \"trainLSBaseModel3Hf\")(h2oContext)\nval validLSBaseModel3Hf = toHf(validLSBaseModel3Df.drop(\"desc\"), \"validLSBaseModel3Hf\")(h2oContext)\n```", "```scala\nloanStatusBaseModelParams._train = trainLSBaseModel3Hf._key\nval loanStatusBaseModel3 = new DRF(loanStatusBaseModelParams, water.Key.make[DRFModel](\"loanStatusBaseModel3\"))\n   .trainModel()\n   .get()\n```", "```scala\nval minLossModel3 = findMinLoss(loanStatusBaseModel3, validLSBaseModel3Hf, DEFAULT_THRESHOLDS)\nprintln(f\"Min total loss for model 3: ${minLossModel3._2}%,.2f (threshold = ${minLossModel3._1})\")\n```", "```scala\nimport org.apache.spark.sql.types._\nval descColUnifier = new UDFTransformer(\"unifier\", unifyTextColumn, StringType, StringType)\n   .setInputCol(\"desc\")\n.setOutputCol(\"desc_unified\")\n\nval descColTokenizer = new UDFTransformer(\"tokenizer\",\n                                           tokenizeTextColumn(3)(StopWordsRemover.loadDefaultStopWords(\"english\")),\n                                           StringType, ArrayType(StringType, true))\n.setInputCol(\"desc_unified\")\n.setOutputCol(\"desc_tokens\")\n```", "```scala\nimport org.apache.spark.ml.feature.CountVectorizer\nval descCountVectorizer = new CountVectorizer()\n   .setInputCol(\"desc_tokens\")\n   .setOutputCol(\"desc_vector\")\n   .setMinDF(1)\n   .setMinTF(1)\n```", "```scala\nimport org.apache.spark.ml.feature.IDF\nval descIdf = new IDF()\n   .setInputCol(\"desc_vector\")\n   .setOutputCol(\"desc_idf_vector\")\n   .setMinDocFreq(1)\n```", "```scala\nimport org.apache.spark.ml.Pipeline\nval descFreqPipeModel = new Pipeline()\n   .setStages(\nArray(descColUnifier,\n           descColTokenizer,\n           descCountVectorizer,\n           descIdf)\n   ).fit(loanStatusBaseModelDf)\n```", "```scala\nval descFreqDf = descFreqPipeModel.transform(loanStatusBaseModelDf)\nimport org.apache.spark.ml.feature.IDFModel\nimport org.apache.spark.ml.feature.CountVectorizerModel\nval descCountVectorizerModel = descFreqPipeModel.stages(2).asInstanceOf[CountVectorizerModel]\nval descIdfModel = descFreqPipeModel.stages(3).asInstanceOf[IDFModel]\nval descIdfScores = descIdfModel.idf.toArray\nval descVocabulary = descCountVectorizerModel.vocabulary\nprintln(\ns\"\"\"\n     ~Size of 'desc' column vocabulary: ${descVocabulary.length} ~Top ten highest scores:\n     ~${table(descVocabulary.zip(descIdfScores).sortBy(-_._2).take(10))}\n\"\"\".stripMargin('~'))\n```", "```scala\nimport org.apache.spark.ml.linalg.{Vector, Vectors}\nval rowAdder = (toVector: Row => Vector) => (r1: Row, r2: Row) => {\nRow(Vectors.dense((toVector(r1).toArray, toVector(r2).toArray).zipped.map((a, b) => a + b)))\n }\n\nval descTargetGoodLoan = descFreqDf\n   .where(\"loan_status == 'good loan'\")\n   .select(\"desc_vector\")\n   .reduce(rowAdder((row:Row) => row.getAs[Vector](0))).getAs[Vector](0).toArray\n\nval descTargetBadLoan = descFreqDf\n   .where(\"loan_status == 'bad loan'\")\n   .select(\"desc_vector\")\n   .reduce(rowAdder((row:Row) => row.getAs[Vector](0))).getAs[Vector](0).toArray\n```", "```scala\nval descTargetsWords = descTargetGoodLoan.zip(descTargetBadLoan)\n   .zip(descVocabulary.zip(descIdfScores)).map(t => (t._1._1, t._1._2, t._2._1, t._2._2))\nprintln(\ns\"\"\"\n      ~Words used only in description of good loans:\n      ~${table(descTargetsWords.filter(t => t._1 >0 && t._2 == 0).sortBy(-_._1).take(10))} ~\n      ~Words used only in description of bad loans:\n      ~${table(descTargetsWords.filter(t => t._1 == 0 && t._2 >0).sortBy(-_._1).take(10))}\n\"\"\".stripMargin('~'))\n```", "```scala\ndef descWordScore = (freqGoodLoan: Double, freqBadLoan: Double, wordIdfScore: Double) =>\n   Math.abs(freqGoodLoan - freqBadLoan) * wordIdfScore * wordIdfScore\n```", "```scala\nval numOfGoodLoans = loanStatusBaseModelDf.where(\"loan_status == 'good loan'\").count()\nval numOfBadLoans = loanStatusBaseModelDf.where(\"loan_status == 'bad loan'\").count()\n\nval descDiscriminatingWords = descTargetsWords.filter(t => t._1 >0 && t. _2 >0).map(t => {\nval freqGoodLoan = t._1 / numOfGoodLoans\nval freqBadLoan = t._2 / numOfBadLoans\nval word = t._3\nval idfScore = t._4\n       (word, freqGoodLoan*100, freqBadLoan*100, idfScore, descWordScore(freqGoodLoan, freqBadLoan, idfScore))\n     })\nprintln(\ntable(Seq(\"Word\", \"Freq Good Loan\", \"Freq Bad Loan\", \"Idf Score\", \"Score\"),\n     descDiscriminatingWords.sortBy(-_._5).take(100),\nMap(1 ->\"%.2f\", 2 ->\"%.2f\")))\n```", "```scala\nval descWordEncoder = (denominatingWords: Array[String]) => (desc: String) => {\nif (desc != null) {\nval unifiedDesc = unifyTextColumn(desc)\n       Vectors.dense(denominatingWords.map(w =>if (unifiedDesc.contains(w)) 1.0 else 0.0))\n     } else null }\n```", "```scala\nval trainLSBaseModel4Df = trainLSBaseModel3Df.withColumn(\"desc_denominating_words\", descWordEncoderUdf($\"desc\")).drop(\"desc\")\nval validLSBaseModel4Df = validLSBaseModel3Df.withColumn(\"desc_denominating_words\", descWordEncoderUdf($\"desc\")).drop(\"desc\")\nval trainLSBaseModel4Hf = toHf(trainLSBaseModel4Df, \"trainLSBaseModel4Hf\")\nval validLSBaseModel4Hf = toHf(validLSBaseModel4Df, \"validLSBaseModel4Hf\")\n loanStatusBaseModelParams._train = trainLSBaseModel4Hf._key\nval loanStatusBaseModel4 = new DRF(loanStatusBaseModelParams, water.Key.make[DRFModel](\"loanStatusBaseModel4\"))\n   .trainModel()\n   .get()\n```", "```scala\nval minLossModel4 = findMinLoss(loanStatusBaseModel4, validLSBaseModel4Hf, DEFAULT_THRESHOLDS)\nprintln(f\"Min total loss for model 4: ${minLossModel4._2}%,.2f (threshold = ${minLossModel4._1})\")\n```", "```scala\nprintln(\ns\"\"\"\n     ~Results:\n     ~${table(Seq(\"Threshold\", \"Total loss\", \"Profit loss\", \"Loan loss\"),\nSeq(minLossModel2, minLossModel3, minLossModel4),\nMap(1 ->\"%,.2f\", 2 ->\"%,.2f\", 3 ->\"%,.2f\"))}\n\"\"\".stripMargin('~'))\n```", "```scala\nval intRateDfSplits = loanStatusDfSplits.map(df => {\n   df\n     .where(\"loan_status == 'good loan'\")\n     .drop(\"emp_title\", \"desc\", \"loan_status\")\n     .withColumn(\"int_rate\", toNumericRateUdf(col(\"int_rate\")))\n })\nval trainIRHf = toHf(intRateDfSplits(0), \"trainIRHf\")(h2oContext)\nval validIRHf = toHf(intRateDfSplits(1), \"validIRHf\")(h2oContext)\n```", "```scala\nimport _root_.hex.tree.gbm.GBMModel.GBMParameters\nval intRateModelParam = let(new GBMParameters()) { p =>\n   p._train = trainIRHf._key\np._valid = validIRHf._key\np._response_column = \"int_rate\" p._score_tree_interval  = 20\n}\n```", "```scala\nimport _root_.hex.grid.{GridSearch}\nimport water.Key\nimport scala.collection.JavaConversions._\nval intRateHyperSpace: java.util.Map[String, Array[Object]] = Map[String, Array[AnyRef]](\n\"_ntrees\" -> (1 to 10).map(v => Int.box(100*v)).toArray,\n\"_max_depth\" -> (2 to 7).map(Int.box).toArray,\n\"_learn_rate\" ->Array(0.1, 0.01).map(Double.box),\n\"_col_sample_rate\" ->Array(0.3, 0.7, 1.0).map(Double.box),\n\"_learn_rate_annealing\" ->Array(0.8, 0.9, 0.95, 1.0).map(Double.box)\n )\n```", "```scala\nimport _root_.hex.grid.HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria\nval intRateHyperSpaceCriteria = let(new RandomDiscreteValueSearchCriteria) { c =>\n   c.set_stopping_metric(StoppingMetric.RMSE)\n   c.set_stopping_tolerance(0.1)\n   c.set_stopping_rounds(1)\n   c.set_max_runtime_secs(4 * 60 /* seconds */)\n }\n```", "```scala\nval intRateGrid = GridSearch.startGridSearch(Key.make(\"intRateGridModel\"),\n                                              intRateModelParam,\n                                              intRateHyperSpace,\nnew GridSearch.SimpleParametersBuilderFactory[GBMParameters],\n                                              intRateHyperSpaceCriteria).get()\n```", "```scala\nval intRateModel = intRateGrid.getModels.minBy(_._output._validation_metrics.rmse())\nprintln(intRateModel._output._validation_metrics)\n```", "```scala\nimport _root_.hex.tree.drf.DRFModel\ndef scoreLoan(df: DataFrame,\n                     empTitleTransformer: PipelineModel,\n                     loanStatusModel: DRFModel,\n                     goodLoanProbThreshold: Double,\n                     intRateModel: GBMModel)(h2oContext: H2OContext): DataFrame = {\nval inputDf = empTitleTransformer.transform(basicDataCleanup(df))\n     .withColumn(\"desc_denominating_words\", descWordEncoderUdf(col(\"desc\")))\n     .drop(\"desc\")\nval inputHf = toHf(inputDf, \"input_df_\" + df.hashCode())(h2oContext)\n// Predict loan status and int rate\nval loanStatusPrediction = loanStatusModel.score(inputHf)\nval intRatePrediction = intRateModel.score(inputHf)\nval probGoodLoanColName = \"good loan\" val inputAndPredictionsHf = loanStatusPrediction.add(intRatePrediction).add(inputHf)\n   inputAndPredictionsHf.update()\n// Prepare field loan_status based on threshold\nval loanStatus = (threshold: Double) => (predGoodLoanProb: Double) =>if (predGoodLoanProb < threshold) \"bad loan\" else \"good loan\" val loanStatusUdf = udf(loanStatus(goodLoanProbThreshold))\n   h2oContext.asDataFrame(inputAndPredictionsHf)(df.sqlContext).withColumn(\"loan_status\", loanStatusUdf(col(probGoodLoanColName)))\n }\n```", "```scala\nval prediction = scoreLoan(loanStatusDfSplits(0), \n                            empTitleTransformer, \n                            loanStatusBaseModel4, \n                            minLossModel4._4, \n                            intRateModel)(h2oContext)\n prediction.show(10)\n```", "```scala\nval MODELS_DIR = s\"${sys.env.get(\"MODELSDIR\").getOrElse(\"models\")}\" val destDir = new File(MODELS_DIR)\n empTitleTransformer.write.overwrite.save(new File(destDir, \"empTitleTransformer\").getAbsolutePath)\n```", "```scala\nloanStatusBaseModel4.getMojo.writeTo(new FileOutputStream(new File(destDir, \"loanStatusModel.mojo\")))\n intRateModel.getMojo.writeTo(new FileOutputStream(new File(destDir, \"intRateModel.mojo\")))\n```", "```scala\ndef saveSchema(schema: StructType, destFile: File, saveWithMetadata: Boolean = false) = {\nimport java.nio.file.{Files, Paths, StandardOpenOption}\n\nimport org.apache.spark.sql.types._\nval processedSchema = StructType(schema.map {\ncase StructField(name, dtype, nullable, metadata) =>StructField(name, dtype, nullable, if (saveWithMetadata) metadata else Metadata.empty)\ncase rec => rec\n    })\n\n   Files.write(Paths.get(destFile.toURI),\n               processedSchema.json.getBytes(java.nio.charset.StandardCharsets.UTF_8),\n               StandardOpenOption.TRUNCATE_EXISTING, StandardOpenOption.CREATE)\n }\n```", "```scala\nsaveSchema(loanDataDf.schema, new File(destDir, \"inputSchema.json\"))\n```", "```scala\nobject Chapter8StreamApp extends App {\n\nval spark = SparkSession.builder()\n     .master(\"local[*]\")\n     .appName(\"Chapter8StreamApp\")\n     .getOrCreate()\n\nscript(spark,\n          sys.env.get(\"MODELSDIR\").getOrElse(\"models\"),\n          sys.env.get(\"APPDATADIR\").getOrElse(\"appdata\"))\n\ndef script(ssc: SparkSession, modelDir: String, dataDir: String): Unit = {\n// ...\nval inputDataStream = spark.readStream/* (1) create stream */\n\nval outputDataStream = /* (2) transform inputDataStream */\n\n /* (3) export stream */ outputDataStream.writeStream.format(\"console\").start().awaitTermination()\n   }\n }\n```", "```scala\ndef loadSchema(srcFile: File): StructType = {\nimport org.apache.spark.sql.types.DataType\nStructType(\n     DataType.fromJson(scala.io.Source.fromFile(srcFile).mkString).asInstanceOf[StructType].map {\ncase StructField(name, dtype, nullable, metadata) =>StructField(name, dtype, true, metadata)\ncase rec => rec\n     }\n   )\n }\n```", "```scala\nval inputSchema = Chapter8Library.loadSchema(new File(modelDir, \"inputSchema.json\"))\n```", "```scala\nval inputDataStream = spark.readStream\n   .schema(inputSchema)\n   .option(\"timestampFormat\", \"MMM-yyy\")\n   .option(\"nullValue\", null)\n   .CSV(s\"${dataDir}/*.CSV\")\n```", "```scala\ninputDataStream.schema.printTreeString()\n```", "```scala\nval empTitleTransformer = PipelineModel.load(s\"${modelDir}/empTitleTransformer\")\n```", "```scala\nval loanStatusModel = MojoModel.load(new File(s\"${modelDir}/loanStatusModel.mojo\").getAbsolutePath)\nval intRateModel = MojoModel.load(new File(s\"${modelDir}/intRateModel.mojo\").getAbsolutePath)\n```", "```scala\nclass MojoTransformer(override val uid: String,\n                       mojoModel: MojoModel) extends Transformer {\n\ncase class BinomialPrediction(p0: Double, p1: Double)\ncase class RegressionPrediction(value: Double)\n\nimplicit def toBinomialPrediction(bmp: AbstractPrediction) =\nBinomialPrediction(bmp.asInstanceOf[BinomialModelPrediction].classProbabilities(0),\n                        bmp.asInstanceOf[BinomialModelPrediction].classProbabilities(1))\nimplicit def toRegressionPrediction(rmp: AbstractPrediction) =\nRegressionPrediction(rmp.asInstanceOf[RegressionModelPrediction].value)\n\nval modelUdf = {\nval epmw = new EasyPredictModelWrapper(mojoModel)\n     mojoModel._category match {\ncase ModelCategory.Binomial =>udf[BinomialPrediction, Row] { r: Row => epmw.predict(rowToRowData(r)) }\ncase ModelCategory.Regression =>udf[RegressionPrediction, Row] { r: Row => epmw.predict(rowToRowData(r)) }\n     }\n   }\n\nval predictStruct = mojoModel._category match {\ncase ModelCategory.Binomial =>StructField(\"p0\", DoubleType)::StructField(\"p1\", DoubleType)::Nil\ncase ModelCategory.Regression =>StructField(\"pred\", DoubleType)::Nil\n}\n\nval outputCol = s\"${uid}Prediction\" override def transform(dataset: Dataset[_]): DataFrame = {\nval inputSchema = dataset.schema\nval args = inputSchema.fields.map(f => dataset(f.name))\n     dataset.select(col(\"*\"), modelUdf(struct(args: _*)).as(outputCol))\n   }\n\nprivate def rowToRowData(row: Row): RowData = new RowData {\n     row.schema.fields.foreach(f => {\n       row.getAs[AnyRef](f.name) match {\ncase v: Number => put(f.name, v.doubleValue().asInstanceOf[Object])\ncase v: java.sql.Timestamp => put(f.name, v.getTime.toDouble.asInstanceOf[Object])\ncase null =>// nop\ncase v => put(f.name, v)\n       }\n     })\n   }\n\noverride def copy(extra: ParamMap): Transformer =  defaultCopy(extra)\n\noverride def transformSchema(schema: StructType): StructType =  {\nval outputFields = schema.fields :+ StructField(outputCol, StructType(predictStruct), false)\n     StructType(outputFields)\n   }\n }\n```", "```scala\nval loanStatusTransformer = new MojoTransformer(\"loanStatus\", loanStatusModel)\nval intRateTransformer = new MojoTransformer(\"intRate\", intRateModel)\n```", "```scala\nval outputDataStream =\n   intRateTransformer.transform(\n     loanStatusTransformer.transform(\n       empTitleTransformer.transform(\n         Chapter8Library.basicDataCleanup(inputDataStream))\n         .withColumn(\"desc_denominating_words\", descWordEncoderUdf(col(\"desc\"))))\n```", "```scala\noutputDataStream.schema.printTreeString()\n```", "```scala\noutputDataStream.writeStream.format(\"console\").start().awaitTermination()\n```"]