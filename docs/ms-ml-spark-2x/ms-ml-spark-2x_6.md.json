["```scala\ndef fpGrowth(tree: FPTree, i: Item):\n    if (tree consists of a single path P){\n        compute transformed prefix path P' of P\n        return all combinations p in P' joined with i\n    }\n    else{\n        for each item in tree {\n            newI = i joined with item\n            construct conditional pattern base and conditional FP-tree newTree\n            call fpGrowth(newTree, newI)\n        }\n    }\n```", "```scala\nimport org.apache.spark.mllib.fpm.FPGrowth\nimport org.apache.spark.rdd.RDD\n\nval transactions: RDD[Array[String]] = sc.parallelize(Array(\n  Array(\"a\", \"c\", \"d\", \"f\", \"g\", \"i\", \"m\", \"p\"),\n  Array(\"a\", \"b\", \"c\", \"f\", \"l\", \"m\", \"o\"),\n  Array(\"b\", \"f\", \"h\", \"j\", \"o\"),\n  Array(\"b\", \"c\", \"k\", \"s\", \"p\"),\n  Array(\"a\", \"c\", \"e\", \"f\", \"l\", \"m\", \"n\", \"p\")\n))\n\nval fpGrowth = new FPGrowth()\n  .setMinSupport(0.6)\n  .setNumPartitions(5)\nval model = fpGrowth.run(transactions)\n\nmodel.freqItemsets.collect().foreach { itemset =>\n  println(itemset.items.mkString(\"[\", \",\", \"]\") + \", \" + itemset.freq)\n}\n```", "```scala\nimport org.apache.spark.mllib.fpm.AssociationRules\nimport org.apache.spark.mllib.fpm.FPGrowth.FreqItemset\n\nval patterns: RDD[FreqItemset[String]] = sc.parallelize(Seq(\n  new FreqItemset(Array(\"m\"), 3L),\n  new FreqItemset(Array(\"m\", \"c\"), 3L),\n  new FreqItemset(Array(\"m\", \"c\", \"f\"), 3L), \n  new FreqItemset(Array(\"m\", \"a\"), 3L), \n  new FreqItemset(Array(\"m\", \"a\", \"c\"), 3L),\n  new FreqItemset(Array(\"m\", \"a\", \"c\", \"f\"), 3L),  \n  new FreqItemset(Array(\"m\", \"a\", \"f\"), 3L), \n  new FreqItemset(Array(\"m\", \"f\"), 3L), \n  new FreqItemset(Array(\"f\"), 4L), \n  new FreqItemset(Array(\"c\"), 4L), \n  new FreqItemset(Array(\"c\", \"f\"), 3L), \n  new FreqItemset(Array(\"p\"), 3L), \n  new FreqItemset(Array(\"p\", \"c\"), 3L), \n  new FreqItemset(Array(\"a\"), 3L), \n  new FreqItemset(Array(\"a\", \"c\"), 3L), \n  new FreqItemset(Array(\"a\", \"c\", \"f\"), 3L), \n  new FreqItemset(Array(\"a\", \"f\"), 3L), \n  new FreqItemset(Array(\"b\"), 3L)\n))\n\nval associationRules = new AssociationRules().setMinConfidence(0.7)\nval rules = associationRules.run(patterns)\n\nrules.collect().foreach { rule =>\n  println(\"[\" + rule.antecedent.mkString(\",\") + \"=>\"\n    + rule.consequent.mkString(\",\") + \"],\" + rule.confidence)\n}\n```", "```scala\nval patterns = model.freqItemsets\n```", "```scala\nval rules = model.generateAssociationRules(confidence = 0.7)\n```", "```scala\ndef prefixSpan(s: Prefix, l: Length, S: ProjectedDatabase):\n  S' = set of all s' in S|s if {\n    (s' appended to s is a sequential pattern) or\n    (<s'> appended to s is a sequential pattern)\n  }\n  for s' in S' {\n    s'' = s' appended to s\n    output s''\n    call prefixSpan(s'', l+1, S|s'')\n  }\n}\ncall prefixSpan(<>, 0, S)\n```", "```scala\nimport org.apache.spark.mllib.fpm.PrefixSpan\n\nval sequences:RDD[Array[Array[String]]] = sc.parallelize(Seq(\n  Array(Array(\"a\"), Array(\"a\", \"b\", \"c\"), Array(\"a\", \"c\"), Array(\"d\"), Array(\"c\", \"f\")),\n Array(Array(\"a\", \"d\"), Array(\"c\"), Array(\"b\", \"c\"), Array(\"a\", \"e\")),\n Array(Array(\"e\", \"f\"), Array(\"a\", \"b\"), Array(\"d\", \"f\"), Array(\"c\"), Array(\"b\")),\n Array(Array(\"e\"), Array(\"g\"), Array(\"a\", \"f\"), Array(\"c\"), Array(\"b\"), Array(\"c\")) ))\nval prefixSpan = new PrefixSpan()\n  .setMinSupport(0.7)\n  .setMaxPatternLength(5)\nval model = prefixSpan.run(sequences)\nmodel.freqSequences.collect().foreach {\n  freqSequence => println(freqSequence.sequence.map(_.mkString(\"[\", \", \", \"]\")).mkString(\"[\", \", \", \"]\") + \", \" + freqSequence.freq) }\n```", "```scala\n% Different categories found in input file:\n\nfrontpage news tech local opinion on-air misc weather msn-news health living business msn-sports sports summary bbs travel\n\n% Sequences:\n\n1 1 \n2 \n3 2 2 4 2 2 2 3 3 \n5 \n1 \n6 \n1 1 \n6 \n6 7 7 7 6 6 8 8 8 8 \n```", "```scala\nval transactions: RDD[Array[Int]] = sc.textFile(\"./msnbc990928.seq\") map { line =>\n  line.split(\" \").map(_.toInt)\n}\n```", "```scala\nval uniqueTransactions: RDD[Array[Int]] = transactions.map(_.distinct).cache()\n```", "```scala\nval fpGrowth = new FPGrowth().setMinSupport(0.05)\nval model = fpGrowth.run(uniqueTransactions)\nval count = uniqueTransactions.count()\n\nmodel.freqItemsets.collect().foreach { itemset =>\n    println(itemset.items.mkString(\"[\", \",\", \"]\") + \", \" + itemset.freq / count.toDouble )\n}\n```", "```scala\nmodel.freqItemsets.collect().foreach { itemset =>\n  if (itemset.items.length >= 3)\n    println(itemset.items.mkString(\"[\", \",\", \"]\") + \", \" + itemset.freq / count.toDouble )\n}\n```", "```scala\nval rules = model.generateAssociationRules(confidence = 0.4)\nrules.collect().foreach { rule =>\n  println(\"[\" + rule.antecedent.mkString(\",\") + \"=>\"\n    + rule.consequent.mkString(\",\") + \"],\" + (100 * rule.confidence).round / 100.0)\n}\n```", "```scala\nrules.count\nval frontPageConseqRules = rules.filter(_.consequent.head == 1)\nfrontPageConseqRules.count\nfrontPageConseqRules.filter(_.antecedent.contains(2)).count\n```", "```scala\nval sequences: RDD[Array[Array[Int]]] = transactions.map(_.map(Array(_))).cache()\n```", "```scala\nval prefixSpan = new PrefixSpan().setMinSupport(0.005).setMaxPatternLength(15)\nval psModel = prefixSpan.run(sequences)\n```", "```scala\npsModel.freqSequences.map(fs => (fs.sequence.length, 1))\n  .reduceByKey(_ + _)\n  .sortByKey()\n  .collect()\n  .foreach(fs => println(s\"${fs._1}: ${fs._2}\"))\n```", "```scala\npsModel.freqSequences\n  .map(fs => (fs.sequence.length, fs))\n  .groupByKey()\n  .map(group => group._2.reduce((f1, f2) => if (f1.freq > f2.freq) f1 else f2))\n  .map(_.sequence.map(_.mkString(\"[\", \", \", \"]\")).mkString(\"[\", \", \", \"]\"))\n  .collect.foreach(println)\n```", "```scala\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\nimport org.apache.spark.{SparkConf, SparkContext}\n\nval conf = new SparkConf()\n  .setAppName(\"MSNBC data first streaming example\")\n  .setMaster(\"local[2]\")\nval sc = new SparkContext(conf)\nval ssc = new StreamingContext(sc, batchDuration = Seconds(10))\n```", "```scala\nval transactions: RDD[Array[Int]] = sc.textFile(\"src/main/resources/msnbc990928.seq\") map { line =>\n  line.split(\" \").map(_.toInt)\n}\nval trainSequences = transactions.map(_.map(Array(_))).cache()\nval prefixSpan = new PrefixSpan().setMinSupport(0.005).setMaxPatternLength(15)\nval psModel = prefixSpan.run(trainSequences)\nval freqSequences = psModel.freqSequences.map(_.sequence).collect()\n```", "```scala\nval rawSequences: DStream[String] = ssc.socketTextStream(\"localhost\", 8000)\n```", "```scala\nval sequences: DStream[Array[Array[Int]]] = rawSequences\n .map(line => line.split(\" \").map(_.toInt))\n .map(_.map(Array(_)))\n```", "```scala\nprint(\">>> Analyzing new batch of data\")\nsequences.foreachRDD(\n rdd => rdd.foreach(\n   array => {\n     println(\">>> Sequence: \")\n     println(array.map(_.mkString(\"[\", \", \", \"]\")).mkString(\"[\", \", \", \"]\"))\n     freqSequences.count(_.deep == array.deep) match {\n       case count if count > 0 => println(\"is frequent!\")\n       case _ => println(\"is not frequent.\")\n     }\n   }\n )\n)\nprint(\">>> done\")\n```", "```scala\nssc.start()\nssc.awaitTermination()\n```", "```scala\nnc -lk 8000\n```", "```scala\nval rawEvents: DStream[String] = ssc.socketTextStream(\"localhost\", 9999)\nval events: DStream[(Int, String)] = rawEvents.map(line => line.split(\": \"))\n .map(kv => (kv(0).toInt, kv(1)))\n```", "```scala\nval duration = Seconds(20)\nval slide = Seconds(10)\n\nval rawSequencesWithIds: DStream[(Int, String)] = events\n  .reduceByKeyAndWindow((v1: String, v2: String) => v1 + \" \" + v2, duration, slide)\nval rawSequences = rawSequencesWithIds.map(_.2)\n// remainder as in previous example\n```", "```scala\nval countIds = events.map(e => (e._1, 1))\nval counts: DStream[(Int, Int)] = countIds.reduceByKey(_ + _)\n```", "```scala\ndef updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = {\n  Some(runningCount.getOrElse(0) + newValues.sum)\n}\nval runningCounts = countIds.updateStateByKey[Int](updateFunction _)\n```"]