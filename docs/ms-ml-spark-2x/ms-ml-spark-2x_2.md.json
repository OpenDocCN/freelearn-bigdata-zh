["```scala\ntar -xvf spark-2.1.1-bin-hadoop2.6.tgz \nexport SPARK_HOME=\"$(pwd)/spark-2.1.1-bin-hadoop2.6 \n```", "```scala\nexport SPARKLING_WATER_VERSION=\"2.1.12\"\nexport SPARK_PACKAGES=\\\n\"ai.h2o:sparkling-water-core_2.11:${SPARKLING_WATER_VERSION},\\\nai.h2o:sparkling-water-repl_2.11:${SPARKLING_WATER_VERSION},\\\nai.h2o:sparkling-water-ml_2.11:${SPARKLING_WATER_VERSION},\\\ncom.packtpub:mastering-ml-w-spark-utils:1.0.0\"\n\n$SPARK_HOME/bin/spark-shell \\      \n\n            --master 'local[*]' \\\n            --driver-memory 4g \\\n            --executor-memory 4g \\\n            --packages \"$SPARK_PACKAGES\"\n\n```", "```scala\nimport org.apache.spark.mllib \nimport org.apache.spark.mllib.regression.LabeledPoint \nimport org.apache.spark.mllib.linalg._ \nimport org.apache.spark.mllib.linalg.distributed.RowMatrix \nimport org.apache.spark.mllib.util.MLUtils \nimport org.apache.spark.mllib.evaluation._ \nimport org.apache.spark.mllib.tree._ \nimport org.apache.spark.mllib.tree.model._ \nimport org.apache.spark.rdd._ \n```", "```scala\nval rawData = sc.textFile(s\"${sys.env.get(\"DATADIR\").getOrElse(\"data\")}/higgs100k.csv\")\nprintln(s\"Number of rows: ${rawData.count}\") \n\n```", "```scala\nrawData.take(2) \n```", "```scala\nval data = rawData.map(line => line.split(',').map(_.toDouble)) \n\n```", "```scala\nval response: RDD[Int] = data.map(row => row(0).toInt)   \nval features: RDD[Vector] = data.map(line => Vectors.dense(line.slice(1, line.size))) \n```", "```scala\nval featuresMatrix = new RowMatrix(features) \nval featuresSummary = featuresMatrix.computeColumnSummaryStatistics() \n```", "```scala\n\nimport org.apache.spark.utils.Tabulizer._ \nprintln(s\"Higgs Features Mean Values = ${table(featuresSummary.mean, 8)}\")\n\n```", "```scala\nprintln(s\"Higgs Features Variance Values = ${table(featuresSummary.variance, 8)}\") \n\n```", "```scala\nval nonZeros = featuresSummary.numNonzeros \nprintln(s\"Non-zero values count per column: ${table(nonZeros, cols = 8, format = \"%.0f\")}\") \n```", "```scala\nval numRows = featuresMatrix.numRows\n val numCols = featuresMatrix.numCols\n val colsWithZeros = nonZeros\n   .toArray\n   .zipWithIndex\n   .filter { case (rows, idx) => rows != numRows }\n println(s\"Columns with zeros:\\n${table(Seq(\"#zeros\", \"column\"), colsWithZeros, Map.empty[Int, String])}\")\n```", "```scala\nval sparsity = nonZeros.toArray.sum / (numRows * numCols)\nprintln(f\"Data sparsity: ${sparsity}%.2f\") \n```", "```scala\nval responseValues = response.distinct.collect\n println(s\"Response values: ${responseValues.mkString(\", \")}\") \n```", "```scala\nval responseDistribution = response.map(v => (v,1)).countByKey\n println(s\"Response distribution:\\n${table(responseDistribution)}\") \n```", "```scala\nimport org.apache.spark.h2o._ \nval h2oContext = H2OContext.getOrCreate(sc) \n\n```", "```scala\nh2oContext: org.apache.spark.h2o.H2OContext =  \nSparkling Water Context: \n * H2O name: sparkling-water-user-303296214 \n * number of executors: 1 \n * list of used executors: \n  (executorId, host, port) \n  ------------------------ \n  (driver,192.168.1.65,54321) \n  ------------------------ \n  Open H2O Flow in browser: http://192.168.1.65:54321 (CMD + click in Mac OSX) \n```", "```scala\nval h2oResponse = h2oContext.asH2OFrame(response, \"response\")\n```", "```scala\nimport h2oContext.implicits._ \nval h2oResponse: H2OFrame = response \n```", "```scala\nval higgs = response.zip(features).map {  \ncase (response, features) =>  \nLabeledPoint(response, features) } \n\nhiggs.setName(\"higgs\").cache() \n```", "```scala\n(1.0, [0.123, 0.456, 0.567, 0.678, ..., 0.789]) \n```", "```scala\n// Create Train & Test Splits \nval trainTestSplits = higgs.randomSplit(Array(0.8, 0.2)) \nval (trainingData, testData) = (trainTestSplits(0), trainTestSplits(1)) \n```", "```scala\nval higgsHF = h2oContext.asH2OFrame(higgs.toDF, \"higgsHF\") \n```", "```scala\nval numClasses = 2 \nval categoricalFeaturesInfo = Map[Int, Int]() \nval impurity = \"gini\" \nval maxDepth = 5 \nval maxBins = 10 \n```", "```scala\nval dtreeModel = DecisionTree.trainClassifier( \ntrainingData,  \nnumClasses,  \ncategoricalFeaturesInfo, \nimpurity,  \nmaxDepth,  \nmaxBins) \n\n// Show the tree \nprintln(\"Decision Tree Model:\\n\" + dtreeModel.toDebugString) \n```", "```scala\nval treeLabelAndPreds = testData.map { point =>\n   val prediction = dtreeModel.predict(point.features)\n   (point.label.toInt, prediction.toInt)\n }\n\n val treeTestErr = treeLabelAndPreds.filter(r => r._1 != r._2).count.toDouble / testData.count()\n println(f\"Tree Model: Test Error = ${treeTestErr}%.3f\") \n```", "```scala\nval cm = treeLabelAndPreds.combineByKey( \n  createCombiner = (label: Int) => if (label == 0) (1,0) else (0,1),  \n  mergeValue = (v:(Int,Int), label:Int) => if (label == 0) (v._1 +1, v._2) else (v._1, v._2 + 1), \n  mergeCombiners = (v1:(Int,Int), v2:(Int,Int)) => (v1._1 + v2._1, v1._2 + v2._2)).collect \n```", "```scala\ncm: Array[(Int, (Int, Int))] = Array((0,(5402,4131)), (1,(2724,7846))) \n```", "```scala\nval (tn, tp, fn, fp) = (cm(0)._2._1, cm(1)._2._2, cm(1)._2._1, cm(0)._2._2) \nprintln(f\"\"\"Confusion Matrix \n  |   ${0}%5d ${1}%5d  ${\"Err\"}%10s \n  |0  ${tn}%5d ${fp}%5d ${tn+fp}%5d ${fp.toDouble/(tn+fp)}%5.4f \n  |1  ${fn}%5d ${tp}%5d ${fn+tp}%5d ${fn.toDouble/(fn+tp)}%5.4f \n  |   ${tn+fn}%5d ${fp+tp}%5d ${tn+fp+fn+tp}%5d ${(fp+fn).toDouble/(tn+fp+fn+tp)}%5.4f \n  |\"\"\".stripMargin) \n```", "```scala\ntype Predictor = {  \n  def predict(features: Vector): Double \n} \n\ndef computeMetrics(model: Predictor, data: RDD[LabeledPoint]): BinaryClassificationMetrics = { \n    val predAndLabels = data.map(newData => (model.predict(newData.features), newData.label)) \n      new BinaryClassificationMetrics(predAndLabels) \n} \n\nval treeMetrics = computeMetrics(dtreeModel, testData) \nprintln(f\"Tree Model: AUC on Test Data = ${treeMetrics.areaUnderROC()}%.3f\") \n```", "```scala\nval numClasses = 2 \nval categoricalFeaturesInfo = Map[Int, Int]() \nval numTrees = 10 \nval featureSubsetStrategy = \"auto\"  \nval impurity = \"gini\" \nval maxDepth = 5 \nval maxBins = 10 \nval seed = 42 \n\nval rfModel = RandomForest.trainClassifier(trainingData, numClasses, categoricalFeaturesInfo, \n  numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed) \n\n```", "```scala\ndef computeError(model: Predictor, data: RDD[LabeledPoint]): Double = {  \n  val labelAndPreds = data.map { point => \n    val prediction = model.predict(point.features) \n    (point.label, prediction) \n  } \n  labelAndPreds.filter(r => r._1 != r._2).count.toDouble/data.count \n} \nval rfTestErr = computeError(rfModel, testData) \nprintln(f\"RF Model: Test Error = ${rfTestErr}%.3f\") \n```", "```scala\n\nval rfMetrics = computeMetrics(rfModel, testData) \nprintln(f\"RF Model: AUC on Test Data = ${rfMetrics.areaUnderROC}%.3f\") \n```", "```scala\nval rfGrid =  \n    for ( \n    gridNumTrees <- Array(15, 20); \n    gridImpurity <- Array(\"entropy\", \"gini\"); \n    gridDepth <- Array(20, 30); \n    gridBins <- Array(20, 50)) \n        yield { \n    val gridModel = RandomForest.trainClassifier(trainingData, 2, Map[Int, Int](), gridNumTrees, \"auto\", gridImpurity, gridDepth, gridBins) \n    val gridAUC = computeMetrics(gridModel, testData).areaUnderROC \n    val gridErr = computeError(gridModel, testData) \n    ((gridNumTrees, gridImpurity, gridDepth, gridBins), gridAUC, gridErr) \n  } \n```", "```scala\n|(15,entropy,20,20)|0.697|0.302|\n```", "```scala\nval rfParamsMaxAUC = rfGrid.maxBy(g => g._2)\nprintln(f\"RF Model: Parameters ${rfParamsMaxAUC._1}%s producing max AUC = ${rfParamsMaxAUC._2}%.3f (error = ${rfParamsMaxAUC._3}%.3f)\") \n```", "```scala\nimport org.apache.spark.mllib.tree.GradientBoostedTrees\n import org.apache.spark.mllib.tree.configuration.BoostingStrategy\n import org.apache.spark.mllib.tree.configuration.Algo\n\n val gbmStrategy = BoostingStrategy.defaultParams(Algo.Classification)\n gbmStrategy.setNumIterations(10)\n gbmStrategy.setLearningRate(0.1)\n gbmStrategy.treeStrategy.setNumClasses(2)\n gbmStrategy.treeStrategy.setMaxDepth(10)\n gbmStrategy.treeStrategy.setCategoricalFeaturesInfo(java.util.Collections.emptyMap[Integer, Integer])\n\n val gbmModel = GradientBoostedTrees.train(trainingData, gbmStrategy)\n```", "```scala\nval gbmTestErr = computeError(gbmModel, testData) \nprintln(f\"GBM Model: Test Error = ${gbmTestErr}%.3f\") \nval gbmMetrics = computeMetrics(dtreeModel, testData) \nprintln(f\"GBM Model: AUC on Test Data = ${gbmMetrics.areaUnderROC()}%.3f\") \n```", "```scala\nval gbmGrid =  \nfor ( \n  gridNumIterations <- Array(5, 10, 50); \n  gridDepth <- Array(2, 3, 5, 7); \n  gridLearningRate <- Array(0.1, 0.01))  \nyield { \n  gbmStrategy.numIterations = gridNumIterations \n  gbmStrategy.treeStrategy.maxDepth = gridDepth \n  gbmStrategy.learningRate = gridLearningRate \n\n  val gridModel = GradientBoostedTrees.train(trainingData, gbmStrategy) \n  val gridAUC = computeMetrics(gridModel, testData).areaUnderROC \n  val gridErr = computeError(gridModel, testData) \n  ((gridNumIterations, gridDepth, gridLearningRate), gridAUC, gridErr) \n} \n```", "```scala\nprintln(\ns\"\"\"GBM Model: Grid results:\n      |${table(Seq(\"iterations, depth, learningRate\", \"AUC\", \"error\"), gbmGrid.sortBy(-_._2).take(10), format = Map(1 -> \"%.3f\", 2 -> \"%.3f\"))}\n\"\"\".stripMargin)\n```", "```scala\nval gbmParamsMaxAUC = gbmGrid.maxBy(g => g._2) \nprintln(f\"GBM Model: Parameters ${gbmParamsMaxAUC._1}%s producing max AUC = ${gbmParamsMaxAUC._2}%.3f (error = ${gbmParamsMaxAUC._3}%.3f)\") \n```", "```scala\nval trainingHF = h2oContext.asH2OFrame(trainingData.toDF, \"trainingHF\") \nval testHF = h2oContext.asH2OFrame(testData.toDF, \"testHF\") \n```", "```scala\ntrainingHF.replace(0, trainingHF.vecs()(0).toCategoricalVec).remove() \ntrainingHF.update() \n\ntestHF.replace(0, testHF.vecs()(0).toCategoricalVec).remove() \ntestHF.update() \n```", "```scala\nimport _root_.hex.deeplearning._ \nimport DeepLearningParameters.Activation \n\nval dlParams = new DeepLearningParameters() \ndlParams._train = trainingHF._key \ndlParams._valid = testHF._key \ndlParams._response_column = \"label\" \ndlParams._epochs = 1 \ndlParams._activation = Activation.RectifierWithDropout \ndlParams._hidden = Array[Int](500, 500, 500) \n```", "```scala\nval dl = new DeepLearning(dlParams) \nval dlModel = dl.trainModel.get \n```", "```scala\nprintln(s\"DL Model: ${dlModel}\") \n```", "```scala\nval testPredictions = dlModel.score(testHF) \n\ntestPredictions: water.fvec.Frame = \nFrame _95829d4e695316377f96db3edf0441ee (19912 rows and 3 cols): \n         predict                   p0                    p1 \n    min           0.11323123896925524  0.017864442175851737 \n   mean            0.4856033079851807    0.5143966920148184 \n stddev            0.1404849885490033   0.14048498854900326 \n    max            0.9821355578241482    0.8867687610307448 \nmissing                           0.0                   0.0 \n      0        1   0.3908680007591152    0.6091319992408847 \n      1        1   0.3339873797352686    0.6660126202647314 \n      2        1   0.2958578897481016    0.7041421102518984 \n      3        1   0.2952981947808155    0.7047018052191846 \n      4        0   0.7523906949762337   0.24760930502376632 \n      5        1   0.53559438105240... \n```", "```scala\nimport water.app.ModelMetricsSupport._ \nval dlMetrics = binomialMM(dlModel, testHF) \n\n```"]