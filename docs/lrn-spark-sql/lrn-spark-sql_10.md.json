["```scala\nsource /Users/aurobindosarkar/Downloads/BigDL-master/scripts/bigdl.sh\n```", "```scala\nbin/spark-shell --properties-file /Users/aurobindosarkar/Downloads/BigDL-master/spark/dist/target/bigdl-0.2.0-SNAPSHOT-spark-2.0.0-scala-2.11.8-mac-dist/conf/spark-bigdl.conf --jars /Users/aurobindosarkar/Downloads/BigDL-master/spark/dist/target/bigdl-0.2.0-SNAPSHOT-spark-2.0.0-scala-2.11.8-mac-dist/lib/bigdl-0.2.0-SNAPSHOT-jar-with-dependencies.jar\n```", "```scala\nscala> import com.intel.analytics.bigdl._\nscala> import com.intel.analytics.bigdl.dataset.DataSet\nscala> import com.intel.analytics.bigdl.dataset.image.{BytesToGreyImg, GreyImgNormalizer, GreyImgToBatch, GreyImgToSample}\nscala> import com.intel.analytics.bigdl.nn.{ClassNLLCriterion, Module}\nscala> import com.intel.analytics.bigdl.numeric.NumericFloat\nscala> import com.intel.analytics.bigdl.optim._\nscala> import com.intel.analytics.bigdl.utils.{Engine, T,\nscala> import com.intel.analytics.bigdl.nn._\nscala> import java.nio.ByteBuffer\nscala> import java.nio.file.{Files, Path, Paths}\nscala> import com.intel.analytics.bigdl.dataset.ByteRecord\nscala> import com.intel.analytics.bigdl.utils.File\n\nscala> val trainData = \"/Users/aurobindosarkar/Downloads/mnist/train-images-idx3-ubyte\"\nscala> val trainLabel = \"/Users/aurobindosarkar/Downloads/mnist/train-labels-idx1-ubyte\"\nscala> val validationData = \"/Users/aurobindosarkar/Downloads/mnist/t10k-images-idx3-ubyte\"\nscala> val validationLabel = \"/Users/aurobindosarkar/Downloads/mnist/t10k-labels-idx1-ubyte\"\n\nscala> val nodeNumber = 1 //Number of nodes\nscala> val coreNumber = 2 //Number of cores\n\nscala> Engine.init\n\nscala> val model = Sequential[Float]()\nmodel: com.intel.analytics.bigdl.nn.Sequential[Float] =\nnn.Sequential {\n[input -> -> output]\n}\n\nscala> val classNum = 10 //Number of classes (digits)\nscala> val batchSize = 12\n//The model uses the Tanh function for non-linearity.\n//It has two sets layers comprising of Convolution-Non-Linearity-Pooling\n//It uses a Softmax function to output the results\n\nscala> model.add(Reshape(Array(1, 28, 28))).add(SpatialConvolution(1, 6, 5, 5)).add(Tanh()).add(SpatialMaxPooling(2, 2, 2, 2)).add(Tanh()).add(SpatialConvolution(6, 12, 5, 5)).add(SpatialMaxPooling(2, 2, 2, 2)).add(Reshape(Array(12 * 4 * 4))).add(Linear(12 * 4 * 4, 100)).add(Tanh()).add(Linear(100, classNum)).add(LogSoftMax())\n\nres1: model.type =\nnn.Sequential {\n[input -> (1) -> (2) -> (3) -> (4) -> (5) -> (6) -> (7) -> (8) -> (9) -> (10) -> (11) -> (12) -> output]\n(1): nn.Reshape(1x28x28)\n(2): nn.SpatialConvolution(1 -> 6, 5 x 5, 1, 1, 0, 0)\n(3): nn.Tanh\n(4): nn.SpatialMaxPooling(2, 2, 2, 2, 0, 0)\n(5): nn.Tanh\n(6): nn.SpatialConvolution(6 -> 12, 5 x 5, 1, 1, 0, 0)\n(7): nn.SpatialMaxPooling(2, 2, 2, 2, 0, 0)\n(8): nn.Reshape(192)\n(9): nn.Linear(192 -> 100)\n(10): nn.Tanh\n(11): nn.Linear(100 -> 10)\n(12): nn.LogSoftMax\n}\n\n//The following is a private function in Utils.\nscala> def load(featureFile: String, labelFile: String): Array[ByteRecord] = {\n|    val featureBuffer = ByteBuffer.wrap(Files.readAllBytes(Paths.get(featureFile)))\n|    val labelBuffer = ByteBuffer.wrap(Files.readAllBytes(Paths.get(labelFile)));\n|    val labelMagicNumber = labelBuffer.getInt();\n|    require(labelMagicNumber == 2049);\n|    val featureMagicNumber = featureBuffer.getInt();\n|    require(featureMagicNumber == 2051);\n|    val labelCount = labelBuffer.getInt();\n|    val featureCount = featureBuffer.getInt();\n|    require(labelCount == featureCount);\n|    val rowNum = featureBuffer.getInt();\n|    val colNum = featureBuffer.getInt();\n|    val result = new Array[ByteRecord](featureCount);\n|    var i = 0;\n|    while (i < featureCount) {\n|       val img = new Array[Byte]((rowNum * colNum));\n|       var y = 0;\n|       while (y < rowNum) {\n|          var x = 0;\n|          while (x < colNum) {\n|             img(x + y * colNum) = featureBuffer.get();\n|             x += 1;\n|          }\n|          y += 1;\n|       }\n|       result(i) = ByteRecord(img, labelBuffer.get().toFloat + 1.0f);\n|       i += 1;\n|    }\n|    result;\n| }\n\nscala> val trainMean = 0.13066047740239506\nscala> val trainStd = 0.3081078\n\nscala> val trainSet = DataSet.array(load(trainData, trainLabel), sc) -> BytesToGreyImg(28, 28) -> GreyImgNormalizer(trainMean, trainStd) -> GreyImgToBatch(batchSize)\n\nscala> val optimizer = Optimizer(model = model, dataset = trainSet, criterion = ClassNLLCriterion[Float]())\n\nscala> val testMean = 0.13251460696903547\nscala> val testStd = 0.31048024\nscala> val maxEpoch = 2\n\nscala> val validationSet = DataSet.array(load(validationData, validationLabel), sc) -> BytesToGreyImg(28, 28) -> GreyImgNormalizer(testMean, testStd) -> GreyImgToBatch(batchSize)\n\nscala> optimizer.setEndWhen(Trigger.maxEpoch(2))\nscala> optimizer.setState(T(\"learningRate\" -> 0.05, \"learningRateDecay\" -> 0.0))\nscala> optimizer.setCheckpoint(\"/Users/aurobindosarkar/Downloads/mnist/checkpoint\", Trigger.severalIteration(500))\nscala> optimizer.setValidation(trigger = Trigger.everyEpoch, dataset = validationSet, vMethods = Array(new Top1Accuracy, new Top5Accuracy[Float], new Loss[Float]))\n\nscala> optimizer.optimize()\n\nscala> model.save(\"/Users/aurobindosarkar/Downloads/mnist/model\") //Save the trained model to disk.\nscala> val model = Module.load[Float](\"/Users/aurobindosarkar/Downloads/mnist/model\") //Retrieve the model from the disk\nscala> val partitionNum = 2\nscala> val rddData = sc.parallelize(load(validationData, validationLabel), partitionNum)\n\nscala> val transformer = BytesToGreyImg(28, 28) -> GreyImgNormalizer(testMean, testStd) -> GreyImgToSample()\n\nscala> val evaluationSet = transformer(rddData)\n\nscala> val result = model.evaluate(evaluationSet, Array(new Top1Accuracy[Float]), Some(batchSize))\n\nscala> result.foreach(r => println(s\"${r._2} is ${r._1}\"))\nTop1Accuracy is Accuracy(correct: 9831, count: 10000, accuracy: 0.9831)\n```", "```scala\nval model = Sequential[Float]()\n\n//The model has 3 sets of Convolution and Pooling layers.\nmodel.add(Reshape(Array(param.embeddingDim, 1, param.maxSequenceLength)))\nmodel.add(SpatialConvolution(param.embeddingDim, 128, 5, 1))\nmodel.add(ReLU())\nmodel.add(SpatialMaxPooling(5, 1, 5, 1))\nmodel.add(SpatialConvolution(128, 128, 5, 1))\nmodel.add(ReLU())\nmodel.add(SpatialMaxPooling(5, 1, 5, 1))\nmodel.add(SpatialConvolution(128, 128, 5, 1))\nmodel.add(ReLU())\nmodel.add(SpatialMaxPooling(35, 1, 35, 1))\nmodel.add(Reshape(Array(128)))\nmodel.add(Linear(128, 100))\nmodel.add(Linear(100, classNum))\nmodel.add(LogSoftMax())\n\n//The optimizer uses the Adagrad method\nval optimizer = Optimizer(\nmodel = buildModel(classNum),\nsampleRDD = trainingRDD,\ncriterion = new ClassNLLCriterion[Float](),\nbatchSize = param.batchSize\n)\n\noptimizer\n.setOptimMethod(new Adagrad(learningRate = 0.01, learningRateDecay = 0.0002))\n.setValidation(Trigger.everyEpoch, valRDD, Array(new Top1Accuracy[Float]), param.batchSize)\n.setEndWhen(Trigger.maxEpoch(20))\n.optimize()\n```", "```scala\nAurobindos-MacBook-Pro-2:BigDL aurobindosarkar$ /Users/aurobindosarkar/Downloads/BigDL-master/scripts/bigdl.sh -- /Users/aurobindosarkar/Downloads/spark-2.1.0-bin-hadoop2.7/bin/spark-submit --master \"local[2]\" --driver-memory 14g --class com.intel.analytics.bigdl.example.textclassification.TextClassifier /Users/aurobindosarkar/Downloads/BigDL-master/spark/dist/target/bigdl-0.2.0-SNAPSHOT-spark-2.0.0-scala-2.11.8-mac-dist/lib/bigdl-0.2.0-SNAPSHOT-jar-with-dependencies.jar --batchSize 128 -b /Users/aurobindosarkar/Downloads/textclassification -p 4\n```", "```scala\n17/08/16 14:50:07 INFO textclassification.TextClassifier$: Current parameters: TextClassificationParams(/Users/aurobindosarkar/Downloads/textclassification,1000,20000,0.8,128,100,4)\n17/08/16 14:50:07 INFO utils.ThreadPool$: Set mkl threads to 1 on thread 1\n17/08/16 14:50:09 INFO utils.Engine$: Auto detect executor number and executor cores number\n17/08/16 14:50:09 INFO utils.Engine$: Executor number is 1 and executor cores number is 2\n17/08/16 14:50:09 INFO utils.Engine$: Find existing spark context. Checking the spark conf...\n17/08/16 14:50:10 INFO utils.TextClassifier: Found 8000 texts.\n17/08/16 14:50:10 INFO utils.TextClassifier: Found 8 classes\n17/08/16 14:50:13 INFO utils.TextClassifier: Indexing word vectors.\n17/08/16 14:50:16 INFO utils.TextClassifier: Found 17424 word vectors.\n17/08/16 14:50:16 INFO optim.DistriOptimizer$: caching training rdd ...\n17/08/16 14:50:37 INFO optim.DistriOptimizer$: Cache thread models...\n17/08/16 14:50:37 INFO optim.DistriOptimizer$: model thread pool size is 1\n17/08/16 14:50:37 INFO optim.DistriOptimizer$: Cache thread models... done\n17/08/16 14:50:37 INFO optim.DistriOptimizer$: config {\nlearningRate: 0.01\nmaxDropPercentage: 0.0\ncomputeThresholdbatchSize: 100\nwarmupIterationNum: 200\nlearningRateDecay: 2.0E-4\ndropPercentage: 0.0\n}\n17/08/16 14:50:37 INFO optim.DistriOptimizer$: Shuffle data\n17/08/16 14:50:37 INFO optim.DistriOptimizer$: Shuffle data complete. Takes 0.012679728s\n17/08/16 14:50:38 INFO optim.DistriOptimizer$: [Epoch 1 0/6458][Iteration 1][Wall Clock 0.0s] Train 128 in 0.962042186seconds. Throughput is 133.0503 records/second. Loss is 2.0774076.\n17/08/16 14:50:40 INFO optim.DistriOptimizer$: [Epoch 1 128/6458][Iteration 2][Wall Clock 0.962042186s] Train 128 in 1.320501728seconds. Throughput is 96.93285 records/second. Loss is 4.793501.\n17/08/16 14:50:40 INFO optim.DistriOptimizer$: [Epoch 1 256/6458][Iteration 3][Wall Clock 2.282543914s] Train 128 in 0.610049842seconds. Throughput is 209.81892 records/second. Loss is 2.1110187.\n17/08/16 14:50:41 INFO optim.DistriOptimizer$: [Epoch 1 384/6458][Iteration 4][Wall Clock 2.892593756s] Train 128 in 0.609548069seconds. Throughput is 209.99164 records/second. Loss is 2.0820618.\n17/08/16 14:50:42 INFO optim.DistriOptimizer$: [Epoch 1 512/6458][Iteration 5][Wall Clock 3.502141825s] Train 128 in 0.607720212seconds. Throughput is 210.62325 records/second. Loss is 2.0860045.\n17/08/16 14:50:42 INFO optim.DistriOptimizer$: [Epoch 1 640/6458][Iteration 6][Wall Clock 4.109862037s] Train 128 in 0.607034064seconds. Throughput is 210.86131 records/second. Loss is 2.086178.\n.\n.\n.\n17/08/16 15:04:57 INFO optim.DistriOptimizer$: [Epoch 20 6144/6458][Iteration 1018][Wall Clock 855.715191033s] Train 128 in 0.771615991seconds. Throughput is 165.88562 records/second. Loss is 2.4244189E-4.\n17/08/16 15:04:58 INFO optim.DistriOptimizer$: [Epoch 20 6272/6458][Iteration 1019][Wall Clock 856.486807024s] Train 128 in 0.770584628seconds. Throughput is 166.10765 records/second. Loss is 0.04117684.\n17/08/16 15:04:59 INFO optim.DistriOptimizer$: [Epoch 20 6400/6458][Iteration 1020][Wall Clock 857.257391652s] Train 128 in 0.783425485seconds. Throughput is 163.38503 records/second. Loss is 3.2506883E-4.\n17/08/16 15:04:59 INFO optim.DistriOptimizer$: [Epoch 20 6400/6458][Iteration 1020][Wall Clock 857.257391652s] Epoch finished. Wall clock time is 861322.002763ms\n17/08/16 15:04:59 INFO optim.DistriOptimizer$: [Wall Clock 861.322002763s] Validate model...\n17/08/16 15:05:02 INFO optim.DistriOptimizer$: Top1Accuracy is Accuracy(correct: 1537, count: 1542, accuracy: 0.996757457846952)\n```", "```scala\nhead -n 8000 input.txt > val.txt\ntail -n +8000 input.txt > train.txt\n```", "```scala\nval model = Sequential[Float]()\n//The RNN is created with the time-related parameter.\nmodel.add(Recurrent[Float]()\n.add(RnnCell[Float](inputSize, hiddenSize, Tanh[Float]())))\n.add(TimeDistributed[Float](Linear[Float](hiddenSize, outputSize)))\n\n//The optimization method used is SGD.\nval optimMethod = if (param.stateSnapshot.isDefined) {\nOptimMethod.load[Float](param.stateSnapshot.get)\n} else {\n   new SGD[Float](learningRate = param.learningRate, learningRateDecay = 0.0, weightDecay = param.weightDecay, momentum = param.momentum, dampening = param.dampening)\n}\n\nval optimizer = Optimizer(\nmodel = model,\ndataset = trainSet,\ncriterion = TimeDistributedCriterion[Float](\n   CrossEntropyCriterion[Float](), sizeAverage = true)\n)\n\noptimizer\n.setValidation(Trigger.everyEpoch, validationSet, Array(new Loss[Float](TimeDistributedCriterion[Float](CrossEntropyCriterion[Float](), sizeAverage = true))))\n.setOptimMethod(optimMethod)\n.setEndWhen(Trigger.maxEpoch(param.nEpochs))\n.setCheckpoint(param.checkpoint.get, Trigger.everyEpoch)\n.optimize()\n```", "```scala\nAurobindos-MacBook-Pro-2:bigdl-rnn aurobindosarkar$ /Users/aurobindosarkar/Downloads/BigDL-master/scripts/bigdl.sh -- \\\n> /Users/aurobindosarkar/Downloads/spark-2.1.0-bin-hadoop2.7/bin/spark-submit \\\n> --master local[2] \\\n> --executor-cores 2 \\\n> --total-executor-cores 2 \\\n> --class com.intel.analytics.bigdl.models.rnn.Train \\\n> /Users/aurobindosarkar/Downloads/dist-spark-2.1.1-scala-2.11.8-mac-0.3.0-20170813.202825-21-dist/lib/bigdl-SPARK_2.1-0.3.0-SNAPSHOT-jar-with-dependencies.jar \\\n> -f /Users/aurobindosarkar/Downloads/bigdl-rnn/inputdata/ -s /Users/aurobindosarkar/Downloads/bigdl-rnn/saveDict/ --checkpoint /Users/aurobindosarkar/Downloads/bigdl-rnn/model/ --batchSize 12 -e 2\n```", "```scala\n17/08/16 21:32:38 INFO utils.ThreadPool$: Set mkl threads to 1 on thread 1\n17/08/16 21:32:39 INFO utils.Engine$: Auto detect executor number and executor cores number\n17/08/16 21:32:39 INFO utils.Engine$: Executor number is 1 and executor cores number is 2\n17/08/16 21:32:39 INFO utils.Engine$: Find existing spark context. Checking the spark conf...\n17/08/16 21:32:41 INFO text.Dictionary: 272304 words and32885 sentences processed\n17/08/16 21:32:41 INFO text.Dictionary: save created dictionary.txt and discard.txt to/Users/aurobindosarkar/Downloads/bigdl-rnn/saveDict\n17/08/16 21:32:41 INFO rnn.Train$: maxTrain length = 25, maxVal = 22\n17/08/16 21:32:42 INFO optim.DistriOptimizer$: caching training rdd ...\n17/08/16 21:32:42 INFO optim.DistriOptimizer$: Cache thread models...\n17/08/16 21:32:42 INFO optim.DistriOptimizer$: model thread pool size is 1\n17/08/16 21:32:42 INFO optim.DistriOptimizer$: Cache thread models... done\n17/08/16 21:32:42 INFO optim.DistriOptimizer$: config {\nmaxDropPercentage: 0.0\ncomputeThresholdbatchSize: 100\nwarmupIterationNum: 200\nisLayerwiseScaled: false\ndropPercentage: 0.0\n}\n17/08/16 21:32:42 INFO optim.DistriOptimizer$: Shuffle data\n17/08/16 21:32:42 INFO optim.DistriOptimizer$: Shuffle data complete. \nTakes 0.011933988s\n17/08/16 21:32:43 INFO optim.DistriOptimizer$: [Epoch 1 0/32885][Iteration 1][Wall Clock 0.0s] Train 12 in 0.642820037seconds. Throughput is 18.667744 records/second. Loss is 8.302014\\. Current learning rate is 0.1.\n17/08/16 21:32:43 INFO optim.DistriOptimizer$: [Epoch 1 12/32885][Iteration 2][Wall Clock 0.642820037s] Train 12 in 0.211497603seconds. Throughput is 56.73823 records/second. Loss is 8.134232\\. Current learning rate is 0.1.\n17/08/16 21:32:44 INFO optim.DistriOptimizer$: [Epoch 1 24/32885][Iteration 3][Wall Clock 0.85431764s] Train 12 in 0.337422962seconds. Throughput is 35.56367 records/second. Loss is 7.924248\\. Current learning rate is 0.1.\n17/08/16 21:32:44 INFO optim.DistriOptimizer$: [Epoch 1 36/32885][Iteration 4][Wall Clock 1.191740602s] Train 12 in 0.189710956seconds. Throughput is 63.25412 records/second. Loss is 7.6132483\\. Current learning rate is 0.1.\n17/08/16 21:32:44 INFO optim.DistriOptimizer$: [Epoch 1 48/32885][Iteration 5][Wall Clock 1.381451558s] Train 12 in 0.180944071seconds. Throughput is 66.31883 records/second. Loss is 7.095647\\. Current learning rate is 0.1.\n17/08/16 21:32:44 INFO optim.DistriOptimizer$: [Epoch 1 60/32885][Iteration 6][Wall Clock 1.562395629s] Train 12 in 0.184258125seconds. Throughput is 65.12603 records/second. Loss is 6.3607793\\. Current learning rate is 0.1..\n.\n.\n17/08/16 21:50:00 INFO optim.DistriOptimizer$: [Epoch 2 32856/32885][Iteration 5480][Wall Clock 989.905619531s] Train 12 in 0.19739412seconds. Throughput is 60.792084 records/second. Loss is 1.5389917\\. Current learning rate is 0.1.\n17/08/16 21:50:00 INFO optim.DistriOptimizer$: [Epoch 2 32868/32885][Iteration 5481][Wall Clock 990.103013651s] Train 12 in 0.192780994seconds. Throughput is 62.2468 records/second. Loss is 1.3890615\\. Current learning rate is 0.1.\n17/08/16 21:50:01 INFO optim.DistriOptimizer$: [Epoch 2 32880/32885][Iteration 5482][Wall Clock 990.295794645s] Train 12 in 0.197826032seconds. Throughput is 60.65936 records/second. Loss is 1.5320908\\. Current learning rate is 0.1.\n17/08/16 21:50:01 INFO optim.DistriOptimizer$: [Epoch 2 32880/32885][Iteration 5482][Wall Clock 990.295794645s] Epoch finished. Wall clock time is 1038274.610521ms\n17/08/16 21:50:01 INFO optim.DistriOptimizer$: [Wall Clock 1038.274610521s] Validate model...\n17/08/16 21:50:52 INFO optim.DistriOptimizer$: Loss is (Loss: 1923.4493, count: 1388, Average Loss: 1.3857704)\n[Wall Clock 1038.274610521s] Save model to /Users/aurobindosarkar/Downloads/bigdl-rnn/model//20170816_213242\n```", "```scala\nAurobindos-MacBook-Pro-2:bigdl-rnn aurobindosarkar$ /Users/aurobindosarkar/Downloads/BigDL-master/scripts/bigdl.sh -- \\\n> /Users/aurobindosarkar/Downloads/spark-2.1.0-bin-hadoop2.7/bin/spark-submit \\\n> --master local[2] \\\n> --executor-cores 1 \\\n> --total-executor-cores 2 \\\n> --class com.intel.analytics.bigdl.models.rnn.Test \\\n> /Users/aurobindosarkar/Downloads/dist-spark-2.1.1-scala-2.11.8-mac-0.3.0-20170813.202825-21-dist/lib/bigdl-SPARK_2.1-0.3.0-SNAPSHOT-jar-with-dependencies.jar \\\n> -f /Users/aurobindosarkar/Downloads/bigdl-rnn/saveDict --model /Users/aurobindosarkar/Downloads/bigdl-rnn/model/20170816_213242/model.5483 --words 20 --batchSize 12\n17/08/16 21:53:21 INFO utils.ThreadPool$: Set mkl threads to 1 on thread 1\n17/08/16 21:53:22 INFO utils.Engine$: Auto detect executor number and executor cores number\n17/08/16 21:53:22 INFO utils.Engine$: Executor number is 1 and executor cores number is 2\n17/08/16 21:53:22 INFO utils.Engine$: Find existing spark context. Checking the spark conf...\n17/08/16 21:53:24 WARN optim.Validator$: Validator(model, dataset) is deprecated. \n17/08/16 21:53:24 INFO optim.LocalValidator$: model thread pool size is 1\n17/08/16 21:53:24 INFO optim.LocalValidator$: [Validation] 12/13 Throughput is 84.44181986758397 record / sec\n17/08/16 21:53:24 INFO optim.LocalValidator$: [Validation] 13/13 Throughput is 115.81166197957567 record / sec\nLoss is (Loss: 11.877369, count: 3, Average Loss: 3.959123)\n```", "```scala\ntrain-images-idx3-ubyte.gz\ntrain-labels-idx1-ubyte.gz (the labels file is not actually used in this example)\n```", "```scala\ntrain-images-idx3-ubyte\ntrain-labels-idx1-ubyte\n```", "```scala\nval rowN = 28\nval colN = 28\nval featureSize = rowN * colN\nval classNum = 32\n//The following model uses ReLU\n\nval model = Sequential[Float]()\nmodel.add(new Reshape(Array(featureSize)))\nmodel.add(new Linear(featureSize, classNum))\nmodel.add(new ReLU[Float]())\nmodel.add(new Linear(classNum, featureSize))\nmodel.add(new Sigmoid[Float]())\n\nval optimMethod = new Adagrad[Float](learningRate = 0.01, learningRateDecay = 0.0, weightDecay = 0.0005)\n\nval optimizer = Optimizer(\n   model = model,\n   dataset = trainDataSet,\n   criterion = new MSECriterion[Float]()\n)\noptimizer.setOptimMethod(optimMethod).setEndWhen(Trigger.maxEpoch(param.maxEpoch)).optimize()\n```", "```scala\nAurobindos-MacBook-Pro-2:bigdl-rnn aurobindosarkar$ /Users/aurobindosarkar/Downloads/BigDL-master/scripts/bigdl.sh -- /Users/aurobindosarkar/Downloads/spark-2.1.0-bin-hadoop2.7/bin/spark-submit --master local[2] --class com.intel.analytics.bigdl.models.autoencoder.Train /Users/aurobindosarkar/Downloads/BigDL-master/spark/dist/target/bigdl-0.2.0-SNAPSHOT-spark-2.0.0-scala-2.11.8-mac-dist/lib/bigdl-0.2.0-SNAPSHOT-jar-with-dependencies.jar -b 150 -f /Users/aurobindosarkar/Downloads/mnist --maxEpoch 2 --checkpoint /Users/aurobindosarkar/Downloads/mnist\n```", "```scala\n17/08/16 22:52:16 INFO utils.ThreadPool$: Set mkl threads to 1 on thread 1\n17/08/16 22:52:17 INFO utils.Engine$: Auto detect executor number and executor cores number\n17/08/16 22:52:17 INFO utils.Engine$: Executor number is 1 and executor cores number is 2\n17/08/16 22:52:17 INFO utils.Engine$: Find existing spark context. Checking the spark conf...\n17/08/16 22:52:18 INFO optim.DistriOptimizer$: caching training rdd ...\n17/08/16 22:52:19 INFO optim.DistriOptimizer$: Cache thread models...\n17/08/16 22:52:19 INFO optim.DistriOptimizer$: model thread pool size is 1\n17/08/16 22:52:19 INFO optim.DistriOptimizer$: Cache thread models... done\n17/08/16 22:52:19 INFO optim.DistriOptimizer$: config {\nweightDecay: 5.0E-4\nlearningRate: 0.01\nmaxDropPercentage: 0.0\ncomputeThresholdbatchSize: 100\nmomentum: 0.9\nwarmupIterationNum: 200\ndampening: 0.0\ndropPercentage: 0.0\n}\n17/08/16 22:52:19 INFO optim.DistriOptimizer$: Shuffle data\n17/08/16 22:52:19 INFO optim.DistriOptimizer$: Shuffle data complete. Takes 0.013076416s\n17/08/16 22:52:19 INFO optim.DistriOptimizer$: [Epoch 1 0/60000][Iteration 1][Wall Clock 0.0s] Train 150 in 0.217233789seconds. Throughput is 690.5003 records/second. Loss is 1.2499084.\n17/08/16 22:52:20 INFO optim.DistriOptimizer$: [Epoch 1 150/60000][Iteration 2][Wall Clock 0.217233789s] Train 150 in 0.210093679seconds. Throughput is 713.9672 records/second. Loss is 1.1829382.\n17/08/16 22:52:20 INFO optim.DistriOptimizer$: [Epoch 1 300/60000][Iteration 3][Wall Clock 0.427327468s] Train 150 in 0.05808109seconds. Throughput is 2582.5962 records/second. Loss is 1.089432.\n17/08/16 22:52:20 INFO optim.DistriOptimizer$: [Epoch 1 450/60000][Iteration 4][Wall Clock 0.485408558s] Train 150 in 0.053720011seconds. Throughput is 2792.2556 records/second. Loss is 0.96986365.\n17/08/16 22:52:20 INFO optim.DistriOptimizer$: [Epoch 1 600/60000][Iteration 5][Wall Clock 0.539128569s] Train 150 in 0.052071024seconds. Throughput is 2880.681 records/second. Loss is 0.9202304.\n.\n.\n.\n17/08/16 22:52:45 INFO optim.DistriOptimizer$: [Epoch 2 59400/60000][Iteration 797][Wall Clock 26.151645532s] Train 150 in 0.026734804seconds. Throughput is 5610.6636 records/second. Loss is 0.5562006.\n17/08/16 22:52:45 INFO optim.DistriOptimizer$: [Epoch 2 59550/60000][Iteration 798][Wall Clock 26.178380336s] Train 150 in 0.031001227seconds. Throughput is 4838.518 records/second. Loss is 0.55211174.\n17/08/16 22:52:45 INFO optim.DistriOptimizer$: [Epoch 2 59700/60000][Iteration 799][Wall Clock 26.209381563s] Train 150 in 0.027455972seconds. Throughput is 5463.292 records/second. Loss is 0.5566905.\n17/08/16 22:52:45 INFO optim.DistriOptimizer$: [Epoch 2 59850/60000][Iteration 800][Wall Clock 26.236837535s] Train 150 in 0.037863017seconds. Throughput is 3961.6494 records/second. Loss is 0.55880654.\n17/08/16 22:52:45 INFO optim.DistriOptimizer$: [Epoch 2 59850/60000][Iteration 800][Wall Clock 26.236837535s] Epoch finished. Wall clock time is 26374.372173ms\n[Wall Clock 26.374372173s] Save model to /Users/aurobindosarkar/Downloads/mnist/20170816_225219\n```"]