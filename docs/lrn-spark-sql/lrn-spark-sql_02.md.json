["```scala\n      mysql> create database retailDB;\n      Connect to retailDB as follows:\n      mysql> use retailDB;\n```", "```scala\n      mysql>create table transactions(transactionID integer not null \n      auto_increment, invoiceNovarchar(20), stockCodevarchar(20), \n      description varchar(255), quantity integer, unitPrice double, \n      customerIDvarchar(20), country varchar(100), invoiceDate \n      Timestamp, primary key(transactionID));\n```", "```scala\nmysql> describe transactions;\n```", "```scala\n      mysql> CREATE USER 'retaildbuser'@'localhost' IDENTIFIED BY \n             'mypass';\n      mysql> GRANT ALL ON retailDB.* TO 'retaildbuser'@'localhost';\n```", "```scala\n      SPARK_CLASSPATH=/Users/aurobindosarkar/Downloads/mysql-connector-\n      java-5.1.38/mysql-connector-java-5.1.38-bin.jar bin/spark-shell\n```", "```scala\n      scala> import org.apache.spark.sql.types._\n      scala> import org.apache.spark.sql.Row\n      scala> import java.util.Properties\n\n      scala>val inFileRDD =       \n      sc.textFile(\"file:///Users/aurobindosarkar/Downloads/UCI Online  \n      Retail.txt\")\n```", "```scala\n      scala>val allRowsRDD = inFileRDD.map(line \n      =>line.split(\"\\t\").map(_.trim))\n      scala>val header = allRowsRDD.first\n      scala>val data = allRowsRDD.filter(_(0) != header(0))\n```", "```scala\n      scala>val fields = Seq(\n      | StructField(\"invoiceNo\", StringType, true),\n      | StructField(\"stockCode\", StringType, true),\n      | StructField(\"description\", StringType, true),\n      | StructField(\"quantity\", IntegerType, true),\n      | StructField(\"invoiceDate\", StringType, true),\n      | StructField(\"unitPrice\", DoubleType, true),\n      | StructField(\"customerID\", StringType, true),\n      | StructField(\"country\", StringType, true)\n      | )\n      scala>val schema = StructType(fields)\n```", "```scala\n      scala>val rowRDD = data.map(attributes => Row(attributes(0), \n      attributes(1), attributes(2), attributes(3).toInt, attributes(4), \n      attributes(5).toDouble, attributes(6), attributes(7)))\n\n      scala>val r1DF = spark.createDataFrame(rowRDD, schema)\n```", "```scala\n      scala>val ts = \n      unix_timestamp($\"invoiceDate\",\"dd/MM/yyHH:mm\").cast(\"timestamp\")\n      scala>val r2DF = r1DF.withColumn(\"ts\", ts)\n      scala>r2DF.show()\n```", "```scala\n      scala> r2DF.createOrReplaceTempView(\"retailTable\")\n      scala>val r3DF = spark.sql(\"select * from retailTable where ts< \n      '2011-12-01'\")\n      scala>val r4DF = spark.sql(\"select * from retailTable where ts>= \n      '2011-12-01'\")\n```", "```scala\n      scala>val selectData = r4DF.select(\"invoiceNo\", \"stockCode\", \n      \"description\", \"quantity\", \"unitPrice\", \"customerID\", \"country\", \n      \"ts\")\n```", "```scala\n      scala>val writeData = selectData.withColumnRenamed(\"ts\", \n      \"invoiceDate\")\n      scala>writeData.show()\n```", "```scala\n      scala>val dbUrl = \"jdbc:mysql://localhost:3306/retailDB\"\n      scala>val prop = new Properties()\n      scala>prop.setProperty(\"user\", \"retaildbuser\")\n      scala>prop.setProperty(\"password\", \"mypass\")\n      scala>writeData.write.mode(\"append\").jdbc(dbUrl, \"transactions\", \n      prop)\n```", "```scala\n      scala>val selectData = r3DF.select(\"invoiceNo\", \"stockCode\", \n      \"description\", \"quantity\", \"unitPrice\", \"customerID\", \"country\", \n      \"ts\")\n\n      scala>val writeData = selectData.withColumnRenamed(\"ts\", \n      \"invoiceDate\")\n      scala>writeData.select(\"*\").write.format(\"json\")\n      .save(\"hdfs://localhost:9000/Users/r3DF\")\n```", "```scala\n>use nycschoolsDB\nswitched to dbnycschoolsDB\n```", "```scala\n\nmongoimport --host localhost --port 27017 --username <your user name here> --password \"<your password here>\" --collection schools --db nycschoolsDB --file <your download file name here>\n```", "```scala\n>show collections\n schools\n >db.schools.findOne()\n```", "```scala\n./bin/spark-shell --jars /Users/aurobindosarkar/Downloads/mongo-spark-connector_2.11-2.2.0-assembly.jar\nscala> import org.apache.spark.sql.SQLContext\nscala> import org.apache.spark.{SparkConf, SparkContext}\nscala> import com.mongodb.spark.MongoSpark\nscala> import com.mongodb.spark.config.{ReadConfig, WriteConfig}\n```", "```scala\nscala>val readConfig = ReadConfig(Map(\"uri\" -> \"mongodb://localhost:27017/nycschoolsDB.schools?readPreference=primaryPreferred\"))\n\nscala>val writeConfig = WriteConfig(Map(\"uri\" -> \"mongodb://localhost:27017/nycschoolsDB.outCollection\"))\n```", "```scala\nscala>val schoolsDF = MongoSpark.load(sc, readConfig).toDF[School]\n\nscala>schoolsDF.take(1).foreach(println)\n```", "```scala\nscala>val reviewsDF = spark.read.json(\"file:///Users/aurobindosarkar/Downloads/reviews_Electronics_5.json\")\n```", "```scala\nscala> reviewsDF.printSchema()\n```", "```scala\nscala>reviewsDF.createOrReplaceTempView(\"reviewsTable\")\nscala>val selectedDF = spark.sql(\"SELECT asin, overall, reviewTime, reviewerID, reviewerName FROM reviewsTable WHERE overall >= 3\")\n```", "```scala\nscala> selectedDF.show()\n```", "```scala\nscala> val selectedJSONArrayElementDF = reviewsDF.select($\"asin\", $\"overall\", $\"helpful\").where($\"helpful\".getItem(0) < 3)\n\nscala>selectedJSONArrayElementDF.show()\n```", "```scala\nAurobindos-MacBook-Pro-2:spark-2.1.0-bin-hadoop2.7 aurobindosarkar$ bin/spark-shell --jars /Users/aurobindosarkar/Downloads/spark-avro_2.11-3.2.0.jar\n```", "```scala\nscala> import com.databricks.spark.avro._\nscala> val reviewsDF = spark.read.json(\"file:///Users/aurobindosarkar/Downloads/reviews_Electronics_5.json\")\n\nscala> reviewsDF.count()\nres4: Long = 1689188  \n```", "```scala\nscala> reviewsDF.filter(\"overall < 3\").coalesce(1).write.avro(\"file:///Users/aurobindosarkar/Downloads/amazon_reviews/avro\")\n\n```", "```scala\nscala> val reviewsAvroDF = spark.read.avro(\"file:///Users/aurobindosarkar/Downloads/amazon_reviews/avro/part-00000-c6b6b423-70d6-440f-acbe-0de65a6a7f2e.avro\")\n\nscala> reviewsAvroDF.count()\nres5: Long = 190864\n```", "```scala\nscala> reviewsAvroDF.select(\"asin\", \"helpful\", \"overall\", \"reviewTime\", \"reviewerID\", \"reviewerName\").show(5)\n```", "```scala\nscala> spark.conf.set(\"spark.sql.avro.compression.codec\", \"deflate\")\nscala> spark.conf.set(\"spark.sql.avro.deflate.level\", \"5\")\n```", "```scala\nscala> val reviewsAvroDF = spark.read.avro(\"file:////Users/aurobindosarkar/Downloads/amazon_reviews/avro/part-00000-c6b6b423-70d6-440f-acbe-0de65a6a7f2e.avro\")\n```", "```scala\nscala> reviewsAvroDF.write.partitionBy(\"overall\").avro(\"file:////Users/aurobindosarkar/Downloads/amazon_reviews/avro/partitioned\")\n```", "```scala\nscala> reviewsDF.filter(\"overall < 3\").coalesce(1).write.parquet(\"file:///Users/aurobindosarkar/Downloads/amazon_reviews/parquet\")\n```", "```scala\nscala> val reviewsParquetDF = spark.read.parquet(\"file:///Users/aurobindosarkar/Downloads/amazon_reviews/parquet/part-00000-3b512935-ec11-48fa-8720-e52a6a29416b.snappy.parquet\")\n```", "```scala\nscala> reviewsParquetDF.createOrReplaceTempView(\"reviewsTable\")\nscala> val reviews1RatingsDF = spark.sql(\"select asin, overall, reviewerID, reviewerName from reviewsTable where overall < 2\")\n```", "```scala\nscala> reviews1RatingsDF.show(5, false)\n```"]