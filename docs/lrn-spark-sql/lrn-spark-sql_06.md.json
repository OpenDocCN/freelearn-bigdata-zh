["```scala\nscala> val inDiaDataDF = spark.read.option(\"header\", true).csv(\"file:///Users/aurobindosarkar/Downloads/dataset_diabetes/diabetic_data.csv\").cache() \n```", "```scala\nscala> inDiaDataDF.printSchema() \n```", "```scala\nscala> inDiaDataDF.take(5).foreach(println)\n```", "```scala\nscala> inDiaDataDF.select(\"num_lab_procedures\", \"num_procedures\", \"num_medications\", \"number_diagnoses\").describe().show() \n\n+-------+------------------+------------------+------------------+------------------+ \n|summary|num_lab_procedures|    num_procedures|   num_medications|  number_diagnoses| \n+-------+------------------+------------------+------------------+------------------+ \n|  count|            101766|            101766|            101766|            101766| \n|   mean| 43.09564098028811| 1.339730361810428|16.021844230882614| 7.422606764538254| \n| stddev| 19.67436224914214|1.7058069791211583| 8.127566209167293|1.9336001449974298| \n|    min|                 1|                 0|                 1|                 1| \n|    max|                99|                 6|                 9|                 9| \n+-------+------------------+------------------+------------------+------------------+ \n\n```", "```scala\nscala> inDiaDataDF.select($\"weight\").groupBy($\"weight\").count().select($\"weight\", (($\"count\" / inDiaDataDF.count())*100).alias(\"percent_recs\")).where(\"weight = '?'\").show()\n+------+-----------------+ \n|weight|     percent_recs| \n+------+-----------------+ \n|     ?|96.85847925633315| \n+------+-----------------+ \n\nscala> inDiaDataDF.select($\"payer_code\").groupBy($\"payer_code\").count().select($\"payer_code\", (($\"count\" / inDiaDataDF.count())*100).alias(\"percent_recs\")).where(\"payer_code = '?'\").show() \n\n+----------+----------------+ \n|payer_code|    percent_recs| \n+----------+----------------+ \n|         ?|39.5574160328597| \n+----------+----------------+ \n\nscala> inDiaDataDF.select($\"medical_specialty\").groupBy($\"medical_specialty\").count().select($\"medical_specialty\", (($\"count\" / inDiaDataDF.count())*100).alias(\"percent_recs\")).where(\"medical_specialty = '?'\").show() \n\n+-----------------+-----------------+ \n|medical_specialty|     percent_recs| \n+-----------------+-----------------+ \n|                ?|49.08220820313268| \n+-----------------+-----------------+ \n```", "```scala\nscala> val diaDataDrpDF = inDiaDataDF.drop(\"weight\", \"payer_code\") \n```", "```scala\n\nscala> diaDataDrpDF.select($\"patient_nbr\").groupBy($\"patient_nbr\").count().where(\"count > 1\").show(5) \n\n+-----------+-----+ \n|patient_nbr|count| \n+-----------+-----+ \n|    4311585|    2| \n|    4624767|    2| \n|   24962301|    3| \n|   11889666|    2| \n|    2585367|    2| \n+-----------+-----+ \nonly showing top 5 rows \n\nscala>  diaDataDrpDF.select($\"patient_nbr\").groupBy($\"patient_nbr\").count().where(\"count > 1\").count() \nres67: Long = 16773 \n```", "```scala\nscala> val w = Window.partitionBy($\"patient_nbr\").orderBy($\"encounter_id\".desc) \n\nscala> val diaDataSlctFirstDF = diaDataDrpDF.withColumn(\"rn\", row_number.over(w)).where($\"rn\" === 1).drop(\"rn\") \n\nscala> diaDataSlctFirstDF.select($\"patient_nbr\").groupBy($\"patient_nbr\").count().where(\"count > 1\").show() \n+-----------+-----+ \n|patient_nbr|count| \n+-----------+-----+ \n+-----------+-----+ \n\nscala> diaDataSlctFirstDF.count() \nres35: Long = 71518 \n```", "```scala\nscala> val diaDataAdmttedDF = diaDataSlctFirstDF.filter($\"discharge_disposition_id\" =!= \"11\") \n\nscala> diaDataAdmttedDF.count() \nres16: Long = 69934 \n```", "```scala\nscala> joinDF.select(\"encounter_id\", \"dchrgDisp\", \"admType\", \"admission_source\").show() \n```", "```scala\nscala> joinDF.select(\"encounter_id\", \"dchrgDisp\").groupBy(\"dchrgDisp\").count().orderBy($\"count\".desc).take(10).foreach(println) \n```", "```scala\nscala> joinDF.select(\"encounter_id\", \"admType\").groupBy(\"admType\").count().orderBy($\"count\".desc).take(5).foreach(println) \n[Emergency,35988] \n[Elective,13698] \n[Urgent,12799] \n[NULL,4373] \n[Not Available,2752] \n\nscala> joinDF.select(\"encounter_id\", \"admission_source\").groupBy(\"admission_source\").count().orderBy($\"count\".desc).take(5).foreach(println) \n\n[ Emergency Room,37649]                                                          \n[ Physician Referral,21196] \n[NULL,4801] \n[Transfer from a hospital,2622] \n[ Transfer from another health care facility,1797] \n```", "```scala\n scala> diaDataAdmttedDF.select(\"medical_specialty\").where(\"medical_specialty = '?'\").groupBy(\"medical_specialty\").count().show() \n\n+-----------------+-----+ \n|medical_specialty|count| \n+-----------------+-----+ \n|                ?|33733| \n+-----------------+-----+ \n\nscala> val diaDataRplcMedSplDF = diaDataAdmttedDF.na.replace(\"medical_specialty\", Map(\"?\" -> \"Missing\")) \n```", "```scala\nscala> diaDataDrpColsDF.groupBy($\"A1Cresult\").count().show() \n\n+---------+-----+                                                                \n|A1Cresult|count| \n+---------+-----+ \n|     None|57645| \n|       >8| 5866| \n|     Norm| 3691| \n|       >7| 2732| \n+---------+-----+ \n\nscala> def udfA1CGrps() = udf[Double, String] { a => val x = a match { case \"None\" => 1.0; case \">8\" => 2.0; case \">7\" => 3.0; case \"Norm\" => 4.0;}; x;}  \n\nscala> val diaDataA1CResultsDF = diaDataDrpColsDF.withColumn(\"A1CResGrp\", udfA1CGrps()($\"A1Cresult\")) \n\nscala> diaDataA1CResultsDF.groupBy(\"A1CResGrp\").count().withColumn(\"Percent_of_Population\", ($\"count\" / diaDataA1CResultsDF.count())*100).withColumnRenamed(\"count\", \"Num_of_Encounters\").show() \n+--------------------+-----------------+---------------------+ \n|           A1CResGrp|Num_of_Encounters|Percent_of_Population| \n+--------------------+-----------------+---------------------+ \n|No test was perfo...|            57645|    82.42771756227299| \n|Result was high a...|             5866|    8.387908599536706| \n|Normal result of ...|             3691|     5.27783338576372| \n|Result was high b...|             2732|    3.906540452426573| \n+--------------------+-----------------+---------------------+ \n```", "```scala\nscala> def udfReAdmBins() = udf[String, String] { a => val x = a match { case \"<30\" => \"Readmitted\"; case \"NO\" => \"Not Readmitted\"; case \">30\" => \"Not Readmitted\";}; x;} \n\nscala> val diaDataReadmtdDF = diaDataA1CResultsDF.withColumn(\"Readmitted\", udfReAdmBins()($\"readmitted\")) \n```", "```scala\nscala> diaDataReadmtdDF.groupBy(\"race\").pivot(\"Readmitted\").agg(count(\"Readmitted\")).show() \n+---------------+--------------+----------+                                      \n|           race|Not Readmitted|Readmitted| \n+---------------+--------------+----------+ \n|      Caucasian|         49710|      2613| \n|          Other|          1095|        51| \n|AfricanAmerican|         12185|       423| \n|       Hispanic|          1424|        70| \n|          Asian|           478|        25| \n|              ?|          1795|        65| \n+---------------+--------------+----------+ \n\nscala> diaDataReadmtdDF.groupBy(\"A1CResGrp\").pivot(\"Readmitted\").agg(count(\"Readmitted\")).orderBy(\"A1CResGrp\").show() \n+--------------------+--------------+----------+                                 \n|           A1CResGrp|Not Readmitted|Readmitted| \n+--------------------+--------------+----------+ \n|No test was perfo...|         54927|      2718| \n|Normal result of ...|          3545|       146| \n|Result was high a...|          5618|       248| \n|Result was high b...|          2597|       135| \n+--------------------+--------------+----------+ \n\nscala> diaDataReadmtdDF.groupBy(\"gender\").pivot(\"Readmitted\").agg(count(\"Readmitted\")).show() \n+---------------+--------------+----------+                                      \n|         gender|Not Readmitted|Readmitted| \n+---------------+--------------+----------+ \n|         Female|         35510|      1701| \n|Unknown/Invalid|             3|      null| \n|           Male|         31174|      1546| \n+---------------+--------------+----------+  \n```", "```scala\nscala> def udfAgeBins() = udf[String, String] { a => val x = a match { case \"[0-10)\" => \"Young\"; case \"[10-20)\" => \"Young\"; case \"[20-30)\" => \"Young\"; case \"[30-40)\" => \"Middle\"; case \"[40-50)\" => \"Middle\"; case \"[50-60)\" => \"Middle\"; case \"[60-70)\" => \"Elder\";  case \"[70-80)\" => \"Elder\"; case \"[80-90)\" => \"Elder\"; case \"[90-100)\" => \"Elder\";}; x;} \n\nscala> val diaDataAgeBinsDF = diaDataReadmtdDF.withColumn(\"age_category\", udfAgeBins()($\"age\")) \n\nscala> val diaDataRmvGndrDF = diaDataAgeBinsDF.filter($\"gender\" =!= \"Unknown/Invalid\") \n```", "```scala\nscala> diaDataFinalDF.printSchema()\n```", "```scala\nscala> diaDataFinalDF.take(5).foreach(println)\n```", "```scala\nscala> val raceIndexer = new StringIndexer().setInputCol(\"race\").setOutputCol(\"raceCat\").fit(diaDataFinalDF) \n\nscala> raceIndexer.transform(diaDataFinalDF).select(\"race\", \"raceCat\").show() \n```", "```scala\nscala> raceIndexer.transform(diaDataFinalDF).select(\"race\", \"raceCat\").groupBy(\"raceCat\").count().show() \n+-------+-----+ \n|raceCat|count| \n+-------+-----+ \n|    0.0|52323| \n|    1.0|12608| \n|    4.0| 1145| \n|    3.0| 1494| \n|    2.0| 1858| \n|    5.0|  503| \n+-------+-----+ \n\nscala> val raceIndexer = new StringIndexer().setInputCol(\"race\").setOutputCol(\"raceCat\").fit(diaDataFinalDF) \n\nscala> val rDF = raceIndexer.transform(diaDataFinalDF) \n\n```", "```scala\nscala> val genderIndexer = new StringIndexer().setInputCol(\"gender\").setOutputCol(\"genderCat\").fit(rDF) \n\nscala> val gDF = genderIndexer.transform(rDF) \n\nscala>  val ageCategoryIndexer  = new StringIndexer().setInputCol(\"age\").setOutputCol(\"ageCat\").fit(gDF) \n\nscala> val acDF = ageCategoryIndexer.transform(gDF) \n\nscala>  val A1CresultIndexer  = new StringIndexer().setInputCol(\"A1CResGrp\").setOutputCol(\"A1CResGrpCat\").fit(acDF) \n\nscala> val a1crDF = A1CresultIndexer.transform(acDF) \n\nscala> val changeIndexer  = new StringIndexer().setInputCol(\"change\").setOutputCol(\"changeCat\").fit(a1crDF) \n\nscala> val cDF = changeIndexer.transform(a1crDF) \n\nscala> val diabetesMedIndexer  = new StringIndexer().setInputCol(\"diabetesMed\").setOutputCol(\"diabetesMedCat\").fit(cDF) \n\nscala> val dmDF = diabetesMedIndexer.transform(cDF)\n```", "```scala\nscala>  dmDF.printSchema() \n```", "```scala\nscala> val labelIndexer = new StringIndexer().setInputCol(\"Readmitted\").setOutputCol(\"indexedLabel\") \n```", "```scala\nscala> val stringIndexers = catFeatColNames.map { colName => \n     |   new StringIndexer() \n     |     .setInputCol(colName) \n     |     .setOutputCol(colName + \"Cat\") \n     |     .fit(diaDataFinalDF) \n     | }\n```", "```scala\nscala> val dataDF = dmDF.stat.sampleBy(\"Readmitted\", Map(\"Readmitted\" -> 1.0, \"Not Readmitted\" -> .030), 0) \n\nscala> val assembler = new VectorAssembler().setInputCols(Array(\"num_lab_procedures\", \"num_procedures\", \"num_medications\", \"number_outpatient\", \"number_emergency\", \"number_inpatient\", \"number_diagnoses\", \"admission_type_id\", \"discharge_disposition_id\", \"admission_source_id\", \"time_in_hospital\", \"raceCat\", \"genderCat\", \"ageCat\", \"A1CresultCat\", \"changeCat\", \"diabetesMedCat\")).setOutputCol(\"features\") \n```", "```scala\nscala> val numFeatNames = Seq(\"num_lab_procedures\", \"num_procedures\", \"num_medications\", \"number_outpatient\", \"number_emergency\", \"number_inpatient\", \"number_diagnoses\", \"admission_type_id\", \"discharge_disposition_id\", \"admission_source_id\", \"time_in_hospital\") \n\nscala> val catFeatNames = catFeatColNames.map(_ + \"Cat\") \n\nscala> val allFeatNames = numFeatNames ++ catFeatNames \n\nscala> val assembler = new VectorAssembler().setInputCols(Array(allFeatNames: _*)).setOutputCol(\"features\") \n```", "```scala\nscala> val df2 = assembler.transform(dataDF) \n\nscala> df2.select(\"Readmitted\", \"features\").take(5).foreach(println) \n```", "```scala\nscala> val featureIndexer = new VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(df2) \n```", "```scala\nscala> val rf = new RandomForestClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setNumTrees(10) \n\n```", "```scala\nscala> val dt = new DecisionTreeClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\") \n\nscala> val gbt = new GBTClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10)\n```", "```scala\nscala> val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, rf)) \n```", "```scala\nscala> val Array(trainingData, testData) = df2.randomSplit(Array(0.8, 0.2), 11L) \n```", "```scala\nscala> val model = pipeline.fit(trainingData) \n```", "```scala\nscala> val predictions = model.transform(testData) \n\nscala> predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(25) \n```", "```scala\nscala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n\nscala> val accuracy = evaluator.evaluate(predictions) \naccuracy: Double = 0.6483412322274882                                            \n\nscala> println(\"Test Error = \" + (1.0 - accuracy)) \nTest Error = 0.3516587677725118 \n\n```", "```scala\nscala> val rfModel = model.stages(2).asInstanceOf[RandomForestClassificationModel] \n\nscala> println(\"Learned classification forest model:\\n\" + rfModel.toDebugString) \n\n```", "```scala\nscala> val paramGrid = new ParamGridBuilder().addGrid(rf.maxBins, Array(25, 28, 31)).addGrid(rf.maxDepth, Array(4, 6, 8)).addGrid(rf.impurity, Array(\"entropy\", \"gini\")).build() \n```", "```scala\nscala> val evaluator = new BinaryClassificationEvaluator().setLabelCol(\"indexedLabel\")\n```", "```scala\nscala> val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(2) \n```", "```scala\nscala> val crossValidatorModel = cv.fit(df2) \n```", "```scala\nscala> val predictions = crossValidatorModel.transform(testData) \n\nscala> predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(25) \n```", "```scala\nscala>  val accuracy = evaluator.evaluate(predictions) \naccuracy: Double = 0.6823964115630783 \n\nscala>  println(\"Test Error = \" + (1.0 - accuracy)) \nTest Error = 0.3176035884369217 \n```", "```scala\nscala> val lr = new LogisticRegression().setMaxIter(10).setRegParam(0.3).setElasticNetParam(0.8).setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\") \n```", "```scala\nscala> val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, lr)) \n\nscala> val Array(trainingData, testData) = df2.randomSplit(Array(0.8, 0.2), 11L) \n\nscala> val model = pipeline.fit(trainingData) \n\nscala> val predictions = model.transform(testData) \n\nscala> predictions.select(\"A1CResGrpCat\", \"indexedLabel\", \"prediction\").show() \n+------------+------------+----------+ \n|A1CResGrpCat|indexedLabel|prediction| \n+------------+------------+----------+ \n|         0.0|         0.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         3.0|         0.0|       0.0| \n|         3.0|         0.0|       0.0| \n|         0.0|         1.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         0.0|         1.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         0.0|         1.0|       0.0| \n|         0.0|         1.0|       0.0| \n|         0.0|         1.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         0.0|         1.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         0.0|         0.0|       0.0| \n|         1.0|         0.0|       0.0| \n|         0.0|         0.0|       0.0| \n+------------+------------+----------+ \nonly showing top 20 rows \n\nscala> predictions.select($\"indexedLabel\", $\"prediction\").where(\"indexedLabel != prediction\").count() \nres104: Long = 407 \n```", "```scala\nscala> val pca = new PCA().setInputCol(\"features\").setOutputCol(\"pcaFeatures\").setK(3).fit(df2) \n\nscala> val result = pca.transform(df2).select(\"pcaFeatures\") \n\nscala> result.take(5).foreach(println) \n[[-52.49989457347012,-13.91558303051395,-0.9577895037038642]] \n[[-17.787698281398306,-2.3653156500575743,0.67773733633875]] \n[[-42.61350777796136,-8.019782413210889,4.744540532872854]] \n[[-36.62417236331611,-7.161756365322481,-0.06153645411567934]] \n[[-51.157132286686824,-2.6029561027003685,0.8995320464587268]] \n```", "```scala\n\nscala> val indexer = new StringIndexer().setInputCol(\"race\").setOutputCol(\"raceIndex\").fit(df2) \n\nscala> val indexed = indexer.transform(df2) \n\nscala> val encoder = new OneHotEncoder().setInputCol(\"raceIndex\").setOutputCol(\"raceVec\") \n\nscala> val encoded = encoder.transform(indexed) \n\nscala> encoded.select(\"raceVec\").show() \n```", "```scala\nscala> val splits = Array(Double.NegativeInfinity, 20.0, 40.0, 60.0, 80.0, 100.0, Double.PositiveInfinity) \n\nscala> val bucketizer = new Bucketizer().setInputCol(\"num_lab_procedures\").setOutputCol(\"bucketedLabProcs\").setSplits(splits) \n\nscala> // Transform original data into its bucket index. \n\nscala> val bucketedData = bucketizer.transform(df2) \n\nscala> println(s\"Bucketizer output with ${bucketizer.getSplits.length-1} buckets\") \nBucketizer output with 6 buckets \n```", "```scala\nscala> val slicer = new VectorSlicer().setInputCol(\"features\").setOutputCol(\"slicedfeatures\").setNames(Array(\"raceCat\", \"genderCat\", \"ageCat\", \"A1CResGrpCat\")) \n\nscala> val output = slicer.transform(df2) \n\nscala> output.select(\"slicedFeatures\").take(5).foreach(println) \n[(4,[1],[1.0])] \n[[0.0,1.0,0.0,0.0]] \n[(4,[],[])] \n[(4,[1],[1.0])] \n[[1.0,1.0,1.0,0.0]] \n\nscala> val slicer = new VectorSlicer().setInputCol(\"features\").setOutputCol(\"slicedfeatures\").setNames(Array(\"raceCat\", \"genderCat\", \"ageCat\")) \n\nscala> val output = slicer.transform(df2) \n\nscala> output.select(\"slicedFeatures\").take(5).foreach(println) \n[(3,[1],[1.0])] \n[[0.0,1.0,0.0]] \n[(3,[],[])] \n[(3,[1],[1.0])] \n[[1.0,1.0,1.0]] \n```", "```scala\nscala> def udfReAdmLabels() = udf[Double, String] { a => val x = a match { case \"Readmitted\" => 0.0; case \"Not Readmitted\" => 0.0;}; x;} \n\nscala> val df3 = df2.withColumn(\"reAdmLabel\", udfReAdmLabels()($\"Readmitted\")) \n\nscala> val selector = new ChiSqSelector().setNumTopFeatures(1).setFeaturesCol(\"features\").setLabelCol(\"reAdmLabel\").setOutputCol(\"selectedFeatures\") \n\nscala> val result = selector.fit(df3).transform(df3) \n\nscala> println(s\"ChiSqSelector output with top ${selector.getNumTopFeatures} features selected\") \nChiSqSelector output with top 1 features selected \n\nscala> result.select(\"selectedFeatures\").show() \n```", "```scala\nval normalizer = new Normalizer().setInputCol(\"raw_features \").setOutputCol(\"features\") \n```", "```scala\nscala> val labelIndexer = new StringIndexer().setInputCol(\"Readmitted\").setOutputCol(\"indexedLabel\").fit(df2) \n\nscala> val featureIndexer = new VectorIndexer().setInputCol(\"features\").setOutputCol(\"indexedFeatures\").setMaxCategories(4).fit(df2) \n\nscala> val Array(trainingData, testData) = df2.randomSplit(Array(0.7, 0.3)) \n\nscala> val gbt = new GBTClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"indexedFeatures\").setMaxIter(10) \n\nscala> val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels) \n\nscala> val pipeline = new Pipeline().setStages(Array(labelIndexer, featureIndexer, gbt, labelConverter)) \n\nscala> val model = pipeline.fit(trainingData) \n\nscala> val predictions = model.transform(testData) \n\nscala> predictions.select(\"predictedLabel\", \"indexedLabel\", \"features\").show(5) \n+--------------+------------+--------------------+ \n|predictedLabel|indexedLabel|            features| \n+--------------+------------+--------------------+ \n|    Readmitted|         0.0|(17,[0,2,5,6,7,8,...| \n|Not Readmitted|         1.0|[43.0,1.0,7.0,0.0...| \n|    Readmitted|         1.0|(17,[0,2,5,6,7,8,...| \n|    Readmitted|         0.0|(17,[0,1,2,6,7,8,...| \n|    Readmitted|         0.0|(17,[0,2,6,7,8,9,...| \n+--------------+------------+--------------------+ \nonly showing top 5 rows \n```", "```scala\nscala> def udfLabels() = udf[Integer, String] { a => val x = a match { case \"very_low\" => 0; case \"Very Low\" => 0; case \"Low\" => 0; case \"Middle\" => 1; case \"High\" => 1;}; x;} \n```", "```scala\nscala> val inDataDF = spark.read.option(\"header\", true).csv(\"file:///Users/aurobindosarkar/Downloads/Data_User_Modeling.csv\").withColumn(\"label\", udfLabels()($\"UNS\")) \n\nscala> inDataDF.select(\"label\").groupBy(\"label\").count().show() \n+-----+-----+ \n|label|count| \n+-----+-----+ \n|    1|  224| \n|    0|  179| \n+-----+-----+ \n\nscala> inDataDF.cache() \n```", "```scala\nscala> val inDataFinalDF = inDataDF.select($\"STG\".cast(DoubleType), $\"SCG\".cast(DoubleType), $\"STR\".cast(DoubleType), $\"LPR\".cast(DoubleType), $\"PEG\".cast(DoubleType), $\"UNS\", $\"label\") \n\nscala> inDataFinalDF.count() \nres2: Long = 403 \n\nscala> inDataFinalDF.take(5).foreach(println) \n[0.0,0.0,0.0,0.0,0.0,very_low,0] \n[0.08,0.08,0.1,0.24,0.9,High,1] \n[0.06,0.06,0.05,0.25,0.33,Low,0] \n[0.1,0.1,0.15,0.65,0.3,Middle,1] \n[0.08,0.08,0.08,0.98,0.24,Low,0] \n\nscala> inDataFinalDF.printSchema() \nroot \n |-- STG: double (nullable = true) \n |-- SCG: double (nullable = true) \n |-- STR: double (nullable = true) \n |-- LPR: double (nullable = true) \n |-- PEG: double (nullable = true) \n |-- UNS: string (nullable = true) \n |-- label: integer (nullable = true) \n\n```", "```scala\nscala> val allFeatNames = Seq(\"STG\", \"SCG\", \"STR\", \"LPR\", \"PEG\") \n\nscala> val assembler = new VectorAssembler().setInputCols(Array(allFeatNames: _*)).setOutputCol(\"features\") \n\nscala> val df2 = assembler.transform(inDataFinalDF) \n\nscala> df2.cache() \n\nscala> import org.apache.spark.ml.clustering.KMeans \n\nBelow, we create the k-means component with 2 clusters. \n\nscala> val kmeans = new KMeans().setK(2).setSeed(1L) \n\nscala> val model = kmeans.fit(df2) \n```", "```scala\nscala> println(kmeans.explainParams) \nfeaturesCol: features column name (default: features) \ninitMode: The initialization algorithm. Supported options: 'random' and 'k-means||'. (default: k-means||) \ninitSteps: The number of steps for k-means|| initialization mode. Must be > 0\\. (default: 2) \nk: The number of clusters to create. Must be > 1\\. (default: 2, current: 2)\nmaxIter: maximum number of iterations (>= 0) (default: 20)\npredictionCol: prediction column name (default: prediction)\nseed: random seed (default: -1689246527, current: 1)\ntol: the convergence tolerance for iterative algorithms (>= 0) (default: 1.0E-4)\n```", "```scala\nscala> val WSSSE = model.computeCost(df2) \nWSSSE: Double = 91.41199908476494 \n\nscala> println(s\"Within Set Sum of Squared Errors = $WSSSE\") \nWithin Set Sum of Squared Errors = 91.41199908476494 \n\nscala> model.clusterCenters.foreach(println) \n[0.310042654028436,0.31230331753554513,0.41459715639810407,0.4508104265402843,0.2313886255924172] \n[0.4005052083333334,0.40389583333333334,0.5049739583333334,0.4099479166666665,0.7035937499999997] \n\nscala> val transformed =  model.transform(df2) \n\nscala> transformed.take(5).foreach(println) \n[0.0,0.0,0.0,0.0,0.0,very_low,0,(5,[],[]),0] \n[0.08,0.08,0.1,0.24,0.9,High,1,[0.08,0.08,0.1,0.24,0.9],1] \n[0.06,0.06,0.05,0.25,0.33,Low,0,[0.06,0.06,0.05,0.25,0.33],0] \n[0.1,0.1,0.15,0.65,0.3,Middle,1,[0.1,0.1,0.15,0.65,0.3],0] \n[0.08,0.08,0.08,0.98,0.24,Low,0,[0.08,0.08,0.08,0.98,0.24],0] \n\n```", "```scala\nscala> transformed.select(\"prediction\").groupBy(\"prediction\").count().orderBy(\"prediction\").show() \n+----------+-----+ \n|prediction|count| \n+----------+-----+ \n|         0|  211| \n|         1|  192| \n+----------+-----+ \n\nscala> val y1DF = transformed.select($\"label\", $\"prediction\").where(\"label != prediction\") \n\nscala> y1DF.count() \nres14: Long = 34 \n```", "```scala\nscala> transformed.filter(\"prediction = 0\").show() \n```", "```scala\nscala> transformed.filter(\"prediction = 1\").show()\n```", "```scala\nscala> transformed.filter(\"prediction = 0\").select(\"STG\", \"SCG\", \"STR\", \"LPR\", \"PEG\").describe().show() \n```", "```scala\nscala> transformed.filter(\"prediction = 1\").select(\"STG\", \"SCG\", \"STR\", \"LPR\", \"PEG\").describe().show() \n```", "```scala\nscala> println(\"No. of mis-matches between predictions and labels =\" + y1DF.count()+\"\\nTotal no. of records=  \"+ transformed.count()+\"\\nCorrect predictions =  \"+ (1-(y1DF.count()).toDouble/transformed.count())+\"\\nMis-match = \"+ (y1DF.count()).toDouble/transformed.count()) \n\nNo. of mis-matches between predictions and labels =34\nTotal no. of records= 403\nCorrect predictions = 0.9156327543424317\nMis-match = 0.08436724565756824\n```", "```scala\nscala> val testDF = spark.createDataFrame(Seq((0.08,0.08,0.1,0.24,0.9, Vectors.dense(0.08,0.08,0.1,0.24,0.9)))).toDF(\"STG\", \"SCG\", \"STR\", \"LPR\", \"PEG\", \"features\") \n\nscala> model.transform(testDF).show() \n+----+----+---+----+---+--------------------+----------+ \n| STG| SCG|STR| LPR|PEG|            features|prediction| \n+----+----+---+----+---+--------------------+----------+ \n|0.08|0.08|0.1|0.24|0.9|[0.08,0.08,0.1,0....|         1| \n+----+----+---+----+---+--------------------+----------+ \n\nscala> val testDF = spark.createDataFrame(Seq((0.06,0.06,0.05,0.25,0.33, Vectors.dense(0.06,0.06,0.05,0.25,0.33)))).toDF(\"STG\", \"SCG\", \"STR\", \"LPR\", \"PEG\", \"features\")\n\nscala> model.transform(testDF).show() \n+----+----+----+----+----+--------------------+----------+ \n| STG| SCG| STR| LPR| PEG|            features|prediction| \n+----+----+----+----+----+--------------------+----------+ \n|0.06|0.06|0.05|0.25|0.33|[0.06,0.06,0.05,0...|         0| \n+----+----+----+----+----+--------------------+----------+ \n\n```"]