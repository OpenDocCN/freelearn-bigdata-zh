["```scala\nscala> import spark.implicits._ \nscala> import org.apache.spark.sql._ \nscala> import org.apache.spark.sql.types._ \nscala> import scala.util.matching.Regex \nscala> import org.apache.spark.ml.{Pipeline, PipelineModel} \nscala> import org.apache.spark.rdd.RDD \nscala> import scala.math \nscala> import org.apache.spark.ml.feature.{HashingTF, IDF, RegexTokenizer, Tokenizer, NGram, StopWordsRemover, CountVectorizer} \nscala> import org.apache.spark.sql.{Row, DataFrame} \nscala> import org.apache.spark.ml.feature.{VectorAssembler, StringIndexer, IndexToString} scala> import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier, LogisticRegression, NaiveBayes, NaiveBayesModel} \nscala> import org.apache.spark.ml.Pipeline \nscala> import org.apache.spark.ml.evaluation.{RegressionEvaluator, MulticlassClassificationEvaluator} \nscala> import org.apache.spark.ml.linalg.Vector \nscala> import org.apache.spark.ml.tuning.{CrossValidator, ParamGridBuilder, TrainValidationSplit} \nscala> import org.apache.spark.ml.clustering.{LDA} \nscala> import scala.collection.mutable.WrappedArray \nscala> import org.apache.spark.ml._ \n\n//The following package will be created later in this Chapter. \nscala> import org.chap9.edgar10k._ \n```", "```scala\nscala> val inputLines = sc.textFile(\"file:///Users/aurobindosarkar/Downloads/edgardata/0001193125-14-383437.txt\") \n\nscala> val linesToString = inputLines.toLocalIterator.mkString  \n```", "```scala\nscala> linesToString.length \nres0: Int = 11917240 \nprintln statements to display the input and output string lengths (representing pre- and post-processing lengths):\n```", "```scala\nscala> def deleteAbbrev(instr: String): String = { \n     |       //println(\"Input string length=\"+ instr.length()) \n     |       val pattern = new Regex(\"[A-Z]\\\\.([A-Z]\\\\.)+\") \n     |       val str = pattern.replaceAllIn(instr, \" \") \n     |       //println(\"Output string length =\"+ str.length()) \n     |       //println(\"String length reduced by=\"+ (instr.length - str.length())) \n     |       str \n     | } \n\nscala> val lineRemAbbrev = deleteAbbrev(linesToString) \n```", "```scala\nscala> def deleteDocTypes(instr: String): String = { \n     |       //println(\"Input string length=\"+ instr.length()) \n     |       val pattern = new Regex(\"(?s)<TYPE>(GRAPHIC|EXCEL|PDF|ZIP|COVER|CORRESP|EX-10[01].INS|EX-99.SDR [KL].INS|EX-10[01].SCH|EX-99.SDR [KL].SCH|EX-10[01].CAL|EX-99.SDR [KL].CAL|EX-10[01].DEF|EX-99.SDR [KL].LAB|EX-10[01].LAB|EX-99.SDR [KL].LAB|EX-10[01].PRE|EX-99.SDR [KL].PRE|EX-10[01].PRE|EX-99.SDR [KL].PRE).*?</TEXT>\")    \n     |       val str = pattern.replaceAllIn(instr, \" \") \n     |       //println(\"Output string length =\"+ str.length()) \n     |       //println(\"String length reduced by=\"+ (instr.length - str.length())) \n     |       str \n     | } \n\nscala> val lineRemDocTypes = deleteDocTypes(lineRemAbbrev)\n```", "```scala\nscala> def deleteMetaData(instr: String): String = { \n     |       val pattern1 = new Regex(\"<HEAD>.*?</HEAD>\") \n     |       val str1 = pattern1.replaceAllIn(instr, \" \") \n     |       val pattern2 = new Regex(\"(?s)<TYPE>.*?<SEQUENCE>.*?<FILENAME>.*?<DESCRIPTION>.*?\") \n     |       val str2 = pattern2.replaceAllIn(str1, \" \") \n     |       str2 \n     | } \n\nscala> val lineRemMetaData = deleteMetaData(lineRemDocTypes)\n```", "```scala\nscala> def deleteTablesNHTMLElem(instr: String): String = { \n     |       val pattern1 = new Regex(\"(?s)(?i)<Table.*?</Table>\") \n     |       val str1 = pattern1.replaceAllIn(instr, \" \") \n     |       val pattern2 = new Regex(\"(?s)<[^>]*>\") \n     |       val str2 = pattern2.replaceAllIn(str1, \" \") \n     |       str2 \n     | } \n\nscala> val lineRemTabNHTML = deleteTablesNHTMLElem(lineRemMetaData) \n```", "```scala\nscala> def deleteExtCharset(instr: String): String = { \n     |       val pattern1 = new Regex(\"(?s)( |&nbsp;|&#x(A|a)0;)\") \n     |       val str1 = pattern1.replaceAllIn(instr, \" \") \n     |       val pattern2 = new Regex(\"(\u2019|\u2019)\") \n     |       val str2 = pattern2.replaceAllIn(str1, \"'\") \n     |       val pattern3 = new Regex(\"x\") \n     |       val str3 = pattern3.replaceAllIn(str2, \" \") \n     |       val pattern4 = new Regex(\"(\u00a8|\u00a7|&reg;|\u2122|&copy;)\") \n     |       val str4 = pattern4.replaceAllIn(str3, \" \") \n     |       val pattern5 = new Regex(\"(\u201c|\u201d|\u201c|\u201d)\") \n     |       val str5 = pattern5.replaceAllIn(str4, \"\\\"\") \n     |       val pattern6 = new Regex(\"&amp;\") \n     |       val str6 = pattern6.replaceAllIn(str5, \"&\") \n     |       val pattern7 = new Regex(\"(\u2013|\u2014|\u2013)\") \n     |       val str7 = pattern7.replaceAllIn(str6, \"-\") \n     |       val pattern8 = new Regex(\"\u2044\") \n     |       val str8 = pattern8.replaceAllIn(str7, \"/\") \n     |       str8 \n     | } \n\nscala> val lineRemExtChrst = deleteExtCharset(lineRemTabNHTML) \n```", "```scala\nscala> def deleteExcessLFCRWS(instr: String): String = { \n     |       val pattern1 = new Regex(\"[\\n\\r]+\") \n     |       val str1 = pattern1.replaceAllIn(instr, \"\\n\") \n     |       val pattern2 = new Regex(\"[\\t]+\") \n     |       val str2 = pattern2.replaceAllIn(str1, \" \") \n     |       val pattern3 = new Regex(\"\\\\s+\") \n     |       val str3 = pattern3.replaceAllIn(str2, \" \") \n     |       str3 \n     | } \n\nscala> val lineRemExcessLFCRWS = deleteExcessLFCRWS(lineRemExtChrst) \n```", "```scala\nscala> def deleteStrings(str: String): String = { \n     |       val strings = Array(\"IDEA: XBRL DOCUMENT\", \"\\\\/\\\\* Do Not Remove This Comment \\\\*\\\\/\", \"v2.4.0.8\") \n     |       //println(\"str=\"+ str.length()) \n     |       var str1 = str \n     |       for(myString <- strings) { \n     |          var pattern1 = new Regex(myString) \n     |          str1 = pattern1.replaceAllIn(str1, \" \") \n     |       } \n     |       str1 \n     | } \n\nscala> val lineRemStrings = deleteStrings(lineRemExcessLFCRWS) \n```", "```scala\nscala> def deleteAllURLsFileNamesDigitsPunctuationExceptPeriod(instr: String): String = { \n     |       val pattern1 = new Regex(\"\\\\b(https?|ftp|file)://[-a-zA-Z0-9+&@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&@#/%=~_|]\") \n     |       val str1 = pattern1.replaceAllIn(instr, \"\") \n     |       val pattern2 = new Regex(\"[_a-zA-Z0-9\\\\-\\\\.]+.(txt|sgml|xml|xsd|htm|html)\") \n     |       val str2 = pattern2.replaceAllIn(str1, \" \") \n     |       val pattern3 = new Regex(\"[^a-zA-Z|^.]\") \n     |       val str3 = pattern3.replaceAllIn(str2, \" \") \n     |       str3 \n     | } \n\nscala> val lineRemAllUrlsFileNamesDigitsPuncXPeriod = deleteAllURLsFileNamesDigitsPunctuationExceptPeriod(lineRemStrings) \n```", "```scala\nscala> val countPeriods = lineRemAllUrlsFileNamesDigitsPuncXPeriod.count(_ == '.')   \ncountPeriods: Int = 2538 \n```", "```scala\nscala> def keepOnlyAlphas(instr: String): String = { \n     |       val pattern1 = new Regex(\"[^a-zA-Z|]\") \n     |       val str1 = pattern1.replaceAllIn(instr, \" \") \n     |       val str2 = str1.replaceAll(\"[\\\\s]+\", \" \") \n     |       str2 \n     | } \n\nscala> val lineWords = keepOnlyAlphas(lineRemAllUrlsFileNamesDigitsPuncXPeriod) \n```", "```scala\nscala> val wordsStringDF = sc.parallelize(List(lineWords)).toDF() \n\nscala> val wordsDF = wordsStringDF.withColumn(\"words10k\", explode(split($\"value\", \"[\\\\s]\"))).drop(\"value\") \n```", "```scala\nscala> val dictDF = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"file:///Users/aurobindosarkar/Downloads/edgardata/LoughranMcDonald_MasterDictionary_2014.csv\") \n```", "```scala\nscala> val joinWordsDict = wordsDF.join(dictDF, lower(wordsDF(\"words10k\")) === lower(dictDF(\"Word\"))) \n\nscala> val numWords = joinWordsDict.count() \nnumWords: Long = 54701 \n\n```", "```scala\nscala> val avgWordsPerSentence = numWords / countPeriods \navgWordsPerSentence: Long = 21 \n```", "```scala\nscala> val numPolySylb = joinWordsDict.select(\"words10k\", \"Syllables\").where(joinWordsDict(\"Syllables\") > 2) \n\nscala> val polySCount = numPolySylb.count() \npolySCount: Long = 14093 \n\n```", "```scala\nscala> val fogIndex = 0.4*(avgWordsPerSentence+((polySCount/numWords)*100)) \nfogIndex: Double = 8.4 \n```", "```scala\nscala> def calcFileSize(rdd: RDD[String]): Long = { \n     |   rdd.map(_.getBytes(\"UTF-8\").length.toLong) \n     |      .reduce(_+_) //add the sizes together \n     | } \n\nscala> val lines = sc.textFile(\"file:///Users/aurobindosarkar/Downloads/edgardata/0001193125-14-383437.txt\") \n```", "```scala\nscala> val fileSize = calcFileSize(lines)/1000000.0 \nfileSize: Double = 11.91724 \n\nscala> math.log(fileSize) \nres1: Double = 2.477986091202679 \n```", "```scala\nscala> val negWordCount = joinWordsDict.select(\"words10k\", \"negative\").where(joinWordsDict(\"negative\") > 0).count() \nnegWordCount: Long = 1004 \n\nscala> val sentiment = negWordCount / (numWords.toDouble) \nsentiment: Double = 0.01835432624632091 \n```", "```scala\nscala> val modalWordCount = joinWordsDict.select(\"words10k\", \"modal\").where(joinWordsDict(\"modal\") > 0).groupBy(\"modal\").count() \n```", "```scala\nscala> modalWordCount.show() \n+-----+-----+ \n|modal|count| \n+-----+-----+ \n|    3|  386| \n|    1|  115| \n|    2|  221| \n+-----+-----+ \n\n```", "```scala\npackage org.chap9.edgar10k\nimport org.apache.spark.ml.UnaryTransformer\nimport org.apache.spark.sql.types.{DataType, DataTypes, StringType}\nimport scala.util.matching.Regex\nimport org.apache.spark.ml.util.Identifiable\n\nclass TablesNHTMLElemCleaner(override val uid: String) extends UnaryTransformer[String, String, TablesNHTMLElemCleaner] {\n   def this() = this(Identifiable.randomUID(\"cleaner\"))\n   def deleteTablesNHTMLElem(instr: String): String = {\n      val pattern1 = new Regex(\"(?s)(?i)<Table.*?</Table>\")\n      val str1 = pattern1.replaceAllIn(instr, \" \")\n      val pattern2 = new Regex(\"(?s)<[^>]*>\")\n      val str2 = pattern2.replaceAllIn(str1, \" \")\n      str2\n   }\n\noverride protected def createTransformFunc: String => String = {\n   deleteTablesNHTMLElem _\n}\n\noverride protected def validateInputType(inputType: DataType): Unit = {\n   require(inputType == StringType)\n}\n\noverride protected def outputDataType: DataType = DataTypes.StringType\n}\n```", "```scala\npackage org.chap9.edgar10k\nimport org.apache.spark.ml.UnaryTransformer\nimport org.apache.spark.sql.types.{DataType, DataTypes, StringType}\nimport scala.util.matching.Regex\nimport org.apache.spark.ml.util.Identifiable\n\nclass AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner(override val uid: String) extends UnaryTransformer[String, String, AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner] {\n   def this() = this(Identifiable.randomUID(\"cleaner\"))\n   def deleteAllURLsFileNamesDigitsPunctuationExceptPeriod(instr: String): String = {\n      val pattern1 = new Regex(\"\\\\b(https?|ftp|file)://[-a-zA-Z0-9+&@#/%?=~_|!:,.;]*[-a-zA-Z0-9+&@#/%=~_|]\")\n      val str1 = pattern1.replaceAllIn(instr, \"\")\n      val pattern2 = new Regex(\"[_a-zA-Z0-9\\\\-\\\\.]+.(txt|sgml|xml|xsd|htm|html)\")\n      val str2 = pattern2.replaceAllIn(str1, \" \")\n      val pattern3 = new Regex(\"[^a-zA-Z|^.]\")\n      val str3 = pattern3.replaceAllIn(str2, \" \")\n      str3\n}\n\noverride protected def createTransformFunc: String => String = {\n   deleteAllURLsFileNamesDigitsPunctuationExceptPeriod _\n}\n\noverride protected def validateInputType(inputType: DataType): Unit = {\n   require(inputType == StringType)\n}\n\noverride protected def outputDataType: DataType = DataTypes.StringType\n}\n```", "```scala\npackage org.chap9.edgar10k\nimport org.apache.spark.ml.UnaryTransformer\nimport org.apache.spark.sql.types.{DataType, DataTypes, StringType}\nimport scala.util.matching.Regex\nimport org.apache.spark.ml.util.Identifiable\nclass OnlyAlphasCleaner(override val uid: String) extends UnaryTransformer[String, String, OnlyAlphasCleaner] {\n   def this() = this(Identifiable.randomUID(\"cleaner\"))\n   def keepOnlyAlphas(instr: String): String = {\n      val pattern1 = new Regex(\"[^a-zA-Z|]\")\n      val str1 = pattern1.replaceAllIn(instr, \" \")\n      val str2 = str1.replaceAll(\"[\\\\s]+\", \" \")\n      str2\n   }\noverride protected def createTransformFunc: String => String = {\n   keepOnlyAlphas _\n}\n\noverride protected def validateInputType(inputType: DataType): Unit = {\nrequire(inputType == StringType)\n}\n\noverride protected def outputDataType: DataType = DataTypes.StringType\n}\n```", "```scala\npackage org.chap9.edgar10k\nimport org.apache.spark.ml.UnaryTransformer\nimport org.apache.spark.sql.types.{DataType, DataTypes, StringType}\nimport scala.util.matching.Regex\nimport org.apache.spark.ml.util.Identifiable\n\nclass ExcessLFCRWSCleaner(override val uid: String) extends UnaryTransformer[String, String, ExcessLFCRWSCleaner] {\n   def this() = this(Identifiable.randomUID(\"cleaner\"))\n   def deleteExcessLFCRWS(instr: String): String = {\n   val pattern1 = new Regex(\"[\\n\\r]+\")\n   val str1 = pattern1.replaceAllIn(instr, \"\\n\")\n   val pattern2 = new Regex(\"[\\t]+\")\n   val str2 = pattern2.replaceAllIn(str1, \" \")\n   val pattern3 = new Regex(\"\\\\s+\")\n   val str3 = pattern3.replaceAllIn(str2, \" \")\n   str3\n}\n\noverride protected def createTransformFunc: String => String = {\n   deleteExcessLFCRWS _\n}\n\noverride protected def validateInputType(inputType: DataType): Unit = {\n   require(inputType == StringType)\n}\n\noverride protected def outputDataType: DataType = DataTypes.StringType\n}\n```", "```scala\nname := \"Chapter9\"\nversion := \"2.0\"\nscalaVersion := \"2.11.8\"\nlibraryDependencies ++= Seq(\n(\"org.apache.spark\" % \"spark-core_2.11\" % \"2.2.0\" % \"provided\"),\n(\"org.apache.spark\" % \"spark-sql_2.11\" % \"2.2.0\" % \"provided\"),\n(\"org.apache.spark\" % \"spark-mllib_2.11\" % \"2.2.0\" % \"provided\")\n)\nlibraryDependencies += \"com.github.scopt\" %% \"scopt\" % \"3.4.0\"\nlibraryDependencies += \"com.typesafe\" % \"config\" % \"1.3.0\"\nlibraryDependencies += \"com.typesafe.scala-logging\" %% \"scala-logging-api\" % \"2.1.2\"\nlibraryDependencies += \"com.typesafe.scala-logging\" %% \"scala-logging-slf4j\" % \"2.1.2\"\nlibraryDependencies += \"org.scalatest\" % \"scalatest_2.11\" % \"3.0.1\" % \"test\"\n```", "```scala\nAurobindos-MacBook-Pro-2:Chapter9 aurobindosarkar$ sbt package\n```", "```scala\nAurobindos-MacBook-Pro-2:spark-2.2.1-SNAPSHOT-bin-hadoop2.7 aurobindosarkar$ bin/spark-shell --driver-memory 12g --conf spark.driver.maxResultSize=12g --conf spark.sql.shuffle.partitions=800 --jars /Users/aurobindosarkar/Downloads/Chapter9/target/scala-2.11/chapter9_2.11-2.0.jar\n```", "```scala\nscala> val linesDF1 = sc.textFile(\"file:///Users/aurobindosarkar/Downloads/reuters21578/reut2-020-1.sgm\").toDF()\n```", "```scala\nscala> val tablesNHTMLElemCleaner = new TablesNHTMLElemCleaner().setInputCol(\"value\").setOutputCol(\"tablesNHTMLElemCleaned\")\n\nscala> val allURLsFileNamesDigitsPunctuationExceptPeriodCleaner = new AllURLsFileNamesDigitsPunctuationExceptPeriodCleaner().setInputCol(\"tablesNHTMLElemCleaned\").setOutputCol(\"allURLsFileNamesDigitsPunctuationExceptPeriodCleaned\")\n\nscala> val onlyAlphasCleaner = new OnlyAlphasCleaner().setInputCol(\"allURLsFileNamesDigitsPunctuationExceptPeriodCleaned\").setOutputCol(\"text\")\n\nscala> val excessLFCRWSCleaner = new ExcessLFCRWSCleaner().setInputCol(\"text\").setOutputCol(\"cleaned\")\n```", "```scala\nscala> val tokenizer = new RegexTokenizer().setInputCol(\"cleaned\").setOutputCol(\"words\").setPattern(\"\\\\W\")\n\nscala> val stopwords: Array[String] = sc.textFile(\"file:///Users/aurobindosarkar/Downloads/StopWords_GenericLong.txt\").flatMap(_.stripMargin.split(\"\\\\s+\")).collect\n```", "```scala\nscala> val remover = new StopWordsRemover().setStopWords(stopwords).setCaseSensitive(false).setInputCol(\"words\").setOutputCol(\"filtered\")\n```", "```scala\nscala> val pipeline = new Pipeline().setStages(Array(tablesNHTMLElemCleaner, allURLsFileNamesDigitsPunctuationExceptPeriodCleaner, onlyAlphasCleaner, excessLFCRWSCleaner, tokenizer, remover))\n```", "```scala\nscala> val model = pipeline.fit(linesDF1)\n```", "```scala\nscala> val cleanedDF = model.transform(linesDF1).drop(\"value\").drop(\"tablesNHTMLElemCleaned\").drop(\"excessLFCRWSCleaned\").drop(\"allURLsFileNamesDigitsPunctuationExceptPeriodCleaned\").drop(\"text\").drop(\"word\")\n```", "```scala\nscala> val finalDF = cleanedDF.filter(($\"cleaned\" =!= \"\") && ($\"cleaned\" =!= \" \"))\nscala> cleanedDF.count()\nres3: Long = 62\n```", "```scala\nscala> val wordsInStoryDF = finalDF.withColumn(\"wordsInStory\", explode(split($\"cleaned\", \"[\\\\s]\"))).drop(\"cleaned\")\n\nscala> val joinWordsDict = wordsInStoryDF.join(dictDF, lower(wordsInStoryDF(\"wordsInStory\")) === lower(dictDF(\"Word\")))\n\nscala> wordsInStoryDF.count()\nres4: Long = 457\n\nscala> val numWords = joinWordsDict.count().toDouble\nnumWords: Double = 334.0\n\nscala> joinWordsDict.select(\"wordsInStory\").show()\n```", "```scala\nscala> val negWordCount = joinWordsDict.select(\"wordsInStory\", \"negative\").where(joinWordsDict(\"negative\") > 0).count()\nnegWordCount: Long = 8\n\nscala> val sentiment = negWordCount / (numWords.toDouble)\nsentiment: Double = 0.023952095808383235\n\nscala> val modalWordCount = joinWordsDict.select(\"wordsInStory\", \"modal\").where(joinWordsDict(\"modal\") > 0).groupBy(\"modal\").count()\n\nscala> modalWordCount.show()\n+-----+-----+\n|modal|count|\n+-----+-----+\n|    3|    2|\n|    1|    5|\n|    2|    4|\n+-----+-----+\n```", "```scala\nscala> val linesDF2 = sc.textFile(\"file:///Users/aurobindosarkar/Downloads/reuters21578/reut2-008-1.sgm\").toDF()\n\nscala> val cleanedDF = model.transform(linesDF2).drop(\"value\").drop(\"tablesNHTMLElemCleaned\").drop(\"excessLFCRWSCleaned\").drop(\"allURLsFileNamesDigitsPunctuationExceptPeriodCleaned\").drop(\"text\").drop(\"word\")\ncleanedDF: org.apache.spark.sql.DataFrame = [cleaned: string,\n\nscala> val finalDF = cleanedDF.filter(($\"cleaned\" =!= \"\") && ($\"cleaned\" =!= \" \"))\n\nscala> cleanedDF.count()\nres7: Long = 84\n\nscala> val wordsInStoryDF = finalDF.withColumn(\"wordsInStory\", explode(split($\"cleaned\", \"[\\\\s]\"))).drop(\"cleaned\")\n\nscala> val joinWordsDict = wordsInStoryDF.join(dictDF, lower(wordsInStoryDF(\"wordsInStory\")) === lower(dictDF(\"Word\")))\n\nscala> wordsInStoryDF.count()\nres8: Long = 598\n\nscala> val numWords = joinWordsDict.count().toDouble\nnumWords: Double = 483.0\n\nscala> joinWordsDict.select(\"wordsInStory\").show()\n```", "```scala\nscala> val negWordCount = joinWordsDict.select(\"wordsInStory\", \"negative\").where(joinWordsDict(\"negative\") > 0).count()\nnegWordCount: Long = 15\n```", "```scala\nscala> val sentiment = negWordCount / (numWords.toDouble)\nsentiment: Double = 0.031055900621118012\n\nscala> val modalWordCount = joinWordsDict.select(\"wordsInStory\", \"modal\").where(joinWordsDict(\"modal\") > 0).groupBy(\"modal\").count()\n\nscala> modalWordCount.show()\n+-----+-----+\n|modal|count|\n+-----+-----+\n|    3|    1|\n|    1|    3|\n|    2|    4|\n+-----+-----+\n```", "```scala\nAurobindos-MacBook-Pro-2:spark-2.2.1-SNAPSHOT-bin-hadoop2.7 aurobindosarkar$ bin/spark-shell --driver-memory 12g --conf spark.driver.maxResultSize=12g --conf spark.sql.shuffle.partitions=800 --packages com.databricks:spark-xml_2.11:0.4.1\n```", "```scala\nscala> val numTopics: Int = 10\nscala> val maxIterations: Int = 100\nscala> val vocabSize: Int = 10000\n```", "```scala\nscala> val df = spark.read.format(\"com.databricks.spark.xml\").option(\"rowTag\", \"sentences\").option(\"mode\", \"PERMISSIVE\").load(\"file:///Users/aurobindosarkar/Downloads/corpus/fulltext/*.xml\")\n```", "```scala\nscala> val docDF = df.select(\"sentence._VALUE\").withColumn(\"docId\", monotonically_increasing_id()).withColumn(\"sentences\", concat_ws(\",\", $\"_VALUE\")).drop(\"_VALUE\")\n\nscala> // Split each document into words\nscala> val tokens = new RegexTokenizer().setGaps(false).setPattern(\"\\\\p{L}+\").setInputCol(\"sentences\").setOutputCol(\"words\").transform(docDF)\n\nscala> //Remove stop words using the default stop word list provided with the Spark distribution.\nscala> val filteredTokens = new StopWordsRemover().setCaseSensitive(false).setInputCol(\"words\").setOutputCol(\"filtered\").transform(tokens)\n```", "```scala\nscala> val cvModel = new CountVectorizer().setInputCol(\"filtered\").setOutputCol(\"features\").setVocabSize(vocabSize).fit(filteredTokens)\n\nscala> val termVectors = cvModel.transform(filteredTokens).select(\"docId\", \"features\")\n\nscala> val lda = new LDA().setK(numTopics).setMaxIter(maxIterations)\n\nscala> val ldaModel = lda.fit(termVectors)\n\nscala> println(\"Model was fit using parameters: \" + ldaModel.parent.extractParamMap)\n\nModel was fit using parameters: {\nlda_8b00356ca964-checkpointInterval: 10,\nlda_8b00356ca964-featuresCol: features,\nlda_8b00356ca964-k: 10,\nlda_8b00356ca964-keepLastCheckpoint: true,\nlda_8b00356ca964-learningDecay: 0.51,\nlda_8b00356ca964-learningOffset: 1024.0,\nlda_8b00356ca964-maxIter: 100,\nlda_8b00356ca964-optimizeDocConcentration: true,\nlda_8b00356ca964-optimizer: online,\nlda_8b00356ca964-seed: 1435876747,\nlda_8b00356ca964-subsamplingRate: 0.05,\nlda_8b00356ca964-topicDistributionCol: topicDistribution\n}\n```", "```scala\nscala> val ll = ldaModel.logLikelihood(termVectors)\nll: Double = -6.912755229181568E7\n\nscala> val lp = ldaModel.logPerplexity(termVectors)\nlp: Double = 7.558777992719632\n\nscala> println(s\"The lower bound on the log likelihood of the entire corpus: $ll\")\nThe lower bound on the log likelihood of the entire corpus: -6.912755229181568E7\n\nscala> println(s\"The upper bound on perplexity: $lp\")\nThe upper bound on perplexity: 7.558777992719632\n```", "```scala\nscala> val topicsDF = ldaModel.describeTopics(3)\n\nscala> println(\"The topics described by their top-weighted terms:\")\nThe topics described by their top-weighted terms are the following:\n\nscala> topicsDF.show(false)\n```", "```scala\nscala> val transformed = ldaModel.transform(termVectors)\n\nscala> transformed.select(\"docId\", \"topicDistribution\").take(3).foreach(println)\n\n[0,[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0]]\n[8589934592,[8.963966883240337E-5,7.477786237947913E-5,1.1695214724007773E-4,7.651092413869693E-5,5.878144972523343E-5,1.533289774455994E-4,8.250794034920294E-5,6.472049126896475E-5,7.008103300313653E-5,0.9992126995056172]]\n[17179869184,[9.665344356333612E-6,8.06287932260242E-6,0.13933607311582796,8.249745717562721E-6,6.338075472527743E-6,1.6528598250017008E-5,8.89637068587104E-6,6.978449157294409E-6,0.029630980885952427,0.8309682265352574]]\n\nscala> val vocab = cvModel.vocabulary\n```", "```scala\nscala> for ((row) <- topicsDF) {\n| var i = 0\n| var termsString = \"\"\n| var topicTermIndicesString = \"\"\n| val topicNumber = row.get(0)\n| val topicTerms:WrappedArray[Int] = row.get(1).asInstanceOf[WrappedArray[Int]]\n|\n| for (i <- 0 to topicTerms.length-1){\n| topicTermIndicesString += topicTerms(i) +\", \"\n| termsString += vocab(topicTerms(i)) +\", \"\n| }\n|\n| println (\"Topic: \"+ topicNumber+ \"|[\"+topicTermIndicesString + \"]|[\" + termsString +\"]\")\n| }\n\nTopic: 1|[3, 231, 292, ]|[act, title, native, ]\nTopic: 5|[4, 12, 1, ]|[tribunal, appellant, applicant, ]\nTopic: 6|[0, 6, 13, ]|[mr, evidence, said, ]\nTopic: 0|[40, 168, 0, ]|[company, scheme, mr, ]\nTopic: 2|[0, 32, 3, ]|[mr, agreement, act, ]\nTopic: 7|[124, 198, 218, ]|[commissioner, income, tax, ]\nTopic: 3|[3, 44, 211, ]|[act, conduct, price, ]\nTopic: 8|[197, 6, 447, ]|[trade, evidence, mark, ]\nTopic: 4|[559, 130, 678, ]|[patent, dr, university, ]\nTopic: 9|[2, 1, 9, ]|[court, applicant, respondent, ]Using\n```", "```scala\nscala> val inDF = spark.read.json(\"file:///Users/aurobindosarkar/Downloads/reviews_Electronics_5.json\")\n\nscala> inDF.show()\n```", "```scala\nscala> inDF.printSchema()\nroot\n|-- asin: string (nullable = true)\n|-- helpful: array (nullable = true)\n| |-- element: long (containsNull = true)\n|-- overall: double (nullable = true)\n|-- reviewText: string (nullable = true)\n|-- reviewTime: string (nullable = true)\n|-- reviewerID: string (nullable = true)\n|-- reviewerName: string (nullable = true)\n|-- summary: string (nullable = true)\n|-- unixReviewTime: long (nullable = true)\n```", "```scala\nscala> inDF.groupBy(\"overall\").count().orderBy(\"overall\").show()\n+-------+-------+\n|overall| count|\n+-------+-------+\n|    1.0| 108725|\n|    2.0|  82139|\n|    3.0| 142257|\n|    4.0| 347041|\n|    5.0|1009026|\n+-------+-------+\n```", "```scala\nscala> inDF.createOrReplaceTempView(\"reviewsTable\")\n\nscala> val reviewsDF = spark.sql(\n| \"\"\"\n| SELECT text, label, rowNumber FROM (\n| SELECT\n| overall AS label, reviewText AS text, row_number() OVER (PARTITION BY overall ORDER BY rand()) AS rowNumber FROM reviewsTable\n| ) reviewsTable\n| WHERE rowNumber <= 60000\n| \"\"\"\n| )\n\nscala> reviewsDF.groupBy(\"label\").count().orderBy(\"label\").show()\n+-----+-----+\n|label|count|\n+-----+-----+\n|  1.0|60000|\n|  2.0|60000|\n|  3.0|60000|\n|  4.0|60000|\n| 5.0|60000|\n+-----+-----+\n```", "```scala\nscala> val trainingData = reviewsDF.filter(reviewsDF(\"rowNumber\") <= 50000).select(\"text\",\"label\")\n\nscala> val testData = reviewsDF.filter(reviewsDF(\"rowNumber\") > 10000).select(\"text\",\"label\")\n```", "```scala\nscala> val regexTokenizer = new RegexTokenizer().setPattern(\"[a-zA-Z']+\").setGaps(false).setInputCol(\"text\")\n\nscala> val remover = new StopWordsRemover().setInputCol(regexTokenizer.getOutputCol)\n\nscala> val bigrams = new NGram().setN(2).setInputCol(remover.getOutputCol)\n\nscala> val trigrams = new NGram().setN(3).setInputCol(remover.getOutputCol)\n```", "```scala\nscala> val removerHashingTF = new HashingTF().setInputCol(remover.getOutputCol)\n\nscala> val ngram2HashingTF = new HashingTF().setInputCol(bigrams.getOutputCol)\n\nscala> val ngram3HashingTF = new HashingTF().setInputCol(trigrams.getOutputCol)\n\nscala> val assembler = new VectorAssembler().setInputCols(Array(removerHashingTF.getOutputCol, ngram2HashingTF.getOutputCol, ngram3HashingTF.getOutputCol))\n\nscala> val labelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(reviewsDF)\n\nscala> val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n```", "```scala\nscala> val nb = new NaiveBayes().setLabelCol(labelIndexer.getOutputCol).setFeaturesCol(assembler.getOutputCol).setPredictionCol(\"prediction\").setModelType(\"multinomial\")\n```", "```scala\nscala> val pipeline = new Pipeline().setStages(Array(regexTokenizer, remover, bigrams, trigrams, removerHashingTF, ngram2HashingTF, ngram3HashingTF, assembler, labelIndexer, nb, labelConverter))\n```", "```scala\nscala> val paramGrid = new ParamGridBuilder().addGrid(removerHashingTF.numFeatures, Array(1000,10000)).addGrid(ngram2HashingTF.numFeatures, Array(1000,10000)).addGrid(ngram3HashingTF.numFeatures, Array(1000,10000)).build()\n\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\nhashingTF_4b2023cfcec8-numFeatures: 1000,\nhashingTF_7bd4dd537583-numFeatures: 1000,\nhashingTF_7cd2d166ac2c-numFeatures: 1000\n}, {\nhashingTF_4b2023cfcec8-numFeatures: 10000,\nhashingTF_7bd4dd537583-numFeatures: 1000,\nhashingTF_7cd2d166ac2c-numFeatures: 1000\n}, {\nhashingTF_4b2023cfcec8-numFeatures: 1000,\nhashingTF_7bd4dd537583-numFeatures: 10000,\nhashingTF_7cd2d166ac2c-numFeatures: 1000\n}, {\nhashingTF_4b2023cfcec8-numFeatures: 10000,\nhashingTF_7bd4dd537583-numFeatures: 10000,\nhashingTF_7cd2d166ac2c-numFeatures: 1000\n}, {\nhashingTF_4b2023cfcec8-numFeatures: 1000,\nhashingTF_7bd4dd537583-numFeatures: 1000,\nhashingTF_7cd2d166ac2c-numFeatures: 10000\n}, {\nhashingTF_4b2023cfcec8-numFeatures: 10000,\nhashingTF_7bd4dd537...\n```", "```scala\nscala> val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")).setEstimatorParamMaps(paramGrid).setNumFolds(5)\n\nscala> val cvModel = cv.fit(trainingData)\n\nscala> val predictions = cvModel.transform(testData)\n\nscala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n\nscala> val accuracy = evaluator.evaluate(predictions)\naccuracy: Double = 0.481472\n\nscala> println(\"Test Error = \" + (1.0 - accuracy))\nTest Error = 0.518528\n```", "```scala\nscala> def udfReviewBins() = udf[Double, Double] { a => val x = a match { case 1.0 => 1.0; case 2.0 => 1.0; case 3.0 => 2.0; case 4.0 => 3.0; case 5.0 => 3.0;}; x;}\n\nscala> val modifiedInDF = inDF.withColumn(\"rating\", udfReviewBins()($\"overall\")).drop(\"overall\")\n\nscala> modifiedInDF.show()\n```", "```scala\nscala> modifiedInDF.groupBy(\"rating\").count().orderBy(\"rating\").show()\n+------+-------+\n|rating| count|\n+------+-------+\n|   1.0| 190864|\n|   2.0| 142257|\n|   3.0|1356067|\n+------+-------+\n\nscala> modifiedInDF.createOrReplaceTempView(\"modReviewsTable\")\n\nscala> val reviewsDF = spark.sql(\n| \"\"\"\n| SELECT text, label, rowNumber FROM (\n| SELECT\n| rating AS label, reviewText AS text, row_number() OVER (PARTITION BY rating ORDER BY rand()) AS rowNumber FROM modReviewsTable\n| ) modReviewsTable\n| WHERE rowNumber <= 120000\n| \"\"\"\n| )\nreviewsDF: org.apache.spark.sql.DataFrame = [text: string,\n\nscala> reviewsDF.groupBy(\"label\").count().orderBy(\"label\").show()\n+-----+------+\n|label| count|\n+-----+------+\n|  1.0|120000|\n|  2.0|120000|\n|  3.0|120000|\n+-----+------+\n\nscala> val trainingData = reviewsDF.filter(reviewsDF(\"rowNumber\") <= 100000).select(\"text\",\"label\")\n\nscala> val testData = reviewsDF.filter(reviewsDF(\"rowNumber\") > 20000).select(\"text\",\"label\")\n\nscala> val regexTokenizer = new RegexTokenizer().setPattern(\"[a-zA-Z']+\").setGaps(false).setInputCol(\"text\")\n\nscala> val remover = new StopWordsRemover().setInputCol(regexTokenizer.getOutputCol)\n\nscala> val bigrams = new NGram().setN(2).setInputCol(remover.getOutputCol)\n\nscala> val trigrams = new NGram().setN(3).setInputCol(remover.getOutputCol)\n\nscala> val removerHashingTF = new HashingTF().setInputCol(remover.getOutputCol)\n\nscala> val ngram2HashingTF = new HashingTF().setInputCol(bigrams.getOutputCol)\n\nscala> val ngram3HashingTF = new HashingTF().setInputCol(trigrams.getOutputCol)\n\nscala> val assembler = new VectorAssembler().setInputCols(Array(removerHashingTF.getOutputCol, ngram2HashingTF.getOutputCol, ngram3HashingTF.getOutputCol))\n\nscala> val labelIndexer = new StringIndexer().setInputCol(\"label\").setOutputCol(\"indexedLabel\").fit(reviewsDF)\n```", "```scala\nscala> val labelConverter = new IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n```", "```scala\nscala> val nb = new NaiveBayes().setLabelCol(labelIndexer.getOutputCol).setFeaturesCol(assembler.getOutputCol).setPredictionCol(\"prediction\").setModelType(\"multinomial\")\n```", "```scala\nscala> val pipeline = new Pipeline().setStages(Array(regexTokenizer, remover, bigrams, trigrams, removerHashingTF, ngram2HashingTF, ngram3HashingTF, assembler, labelIndexer, nb, labelConverter))\n```", "```scala\nscala> val paramGrid = new ParamGridBuilder().addGrid(removerHashingTF.numFeatures, Array(1000,10000)).addGrid(ngram2HashingTF.numFeatures, Array(1000,10000)).addGrid(ngram3HashingTF.numFeatures, Array(1000,10000)).build()\n\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\nhashingTF_2f3a479f07ef-numFeatures: 1000,\nhashingTF_0dc7c74af716-numFeatures: 1000,\nhashingTF_17632a08c82c-numFeatures: 1000\n}, {\nhashingTF_2f3a479f07ef-numFeatures: 10000,\nhashingTF_0dc7c74af716-numFeatures: 1000,\nhashingTF_17632a08c82c-numFeatures: 1000\n}, {\nhashingTF_2f3a479f07ef-numFeatures: 1000,\nhashingTF_0dc7c74af716-numFeatures: 10000,\nhashingTF_17632a08c82c-numFeatures: 1000\n}, {\nhashingTF_2f3a479f07ef-numFeatures: 10000,\nhashingTF_0dc7c74af716-numFeatures: 10000,\nhashingTF_17632a08c82c-numFeatures: 1000\n}, {\nhashingTF_2f3a479f07ef-numFeatures: 1000,\nhashingTF_0dc7c74af716-numFeatures: 1000,\nhashingTF_17632a08c82c-numFeatures: 10000\n}, {\nhashingTF_2f3a479f07ef-numFeatures: 10000,\nhashingTF_0dc7c74af...\n\nscala> val cv = new CrossValidator().setEstimator(pipeline).setEvaluator(new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")).setEstimatorParamMaps(paramGrid).setNumFolds(5)\n\nscala> val cvModel = cv.fit(trainingData)\n\nscala> val predictions = cvModel.transform(testData)\n\nscala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"indexedLabel\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n\nscala> val accuracy = evaluator.evaluate(predictions)\naccuracy: Double = 0.63663\n\nscala> println(\"Test Error = \" + (1.0 - accuracy))\nTest Error = 0.36336999999999997\n```", "```scala\nscala> val inRDD = spark.sparkContext.textFile(\"file:///Users/aurobindosarkar/Downloads/CNAE-9.data\")\n\nscala> val rowRDD = inRDD.map(_.split(\",\")).map(attributes => Row(attributes(0).toDouble, attributes(1).toDouble, attributes(2).toDouble, attributes(3).toDouble, attributes(4).toDouble, attributes(5).toDouble,\n.\n.\n.\nattributes(852).toDouble, attributes(853).toDouble, attributes(854).toDouble, attributes(855).toDouble, attributes(856).toDouble))\n```", "```scala\nscala> val schemaString = \"label _c715 _c195 _c480 _c856 _c136 _c53 _c429 _c732 _c271 _c742 _c172 _c45 _c374 _c233 _c720\n.\n.\n.\n_c408 _c604 _c766 _c676 _c52 _c755 _c728 _c693 _c119 _c160 _c141 _c516 _c419 _c69 _c621 _c423 _c137 _c549 _c636 _c772 _c799 _c336 _c841 _c82 _c123 _c474 _c470 _c286 _c555 _c36 _c299 _c829 _c361 _c263 _c522 _c495 _c135\"\n\nscala> val fields = schemaString.split(\" \").map(fieldName => StructField(fieldName, DoubleType, nullable = false))\n\nscala> val schema = StructType(fields)\n```", "```scala\nscala> val inDF = spark.createDataFrame(rowRDD, schema)\n\nscala> inDF.take(1).foreach(println)\n```", "```scala\nscala> val indexedDF= inDF.withColumn(\"id\", monotonically_increasing_id())\n\nscala> indexedDF.select(\"label\", \"id\").show()\n```", "```scala\nscala> val columnNames = Array(\"_c715\",\"_c195\",\"_c480\",\"_c856\",\"_c136\",\"_c53\",\"_c429\",\"_c732\",\"_c271\",\"_c742\",\"_c172\",\"_c45\",\"_c374\",\"_c233\",\"_c720\",\"_c294\",\"_c461\",\"_c87\",\"_c599\",\"_c84\",\"_c28\",\"_c79\",\"_c615\",\"_c243\",\"_c603\",\"_c531\",\"_c503\",\"_c630\",\"_c33\",\"_c428\",\"_c385\",\"_c751\",\"_c664\",\"_c540\",\"_c626\",\"_c730\",\"_c9\",\"_c699\",\"_c117\",\"\n.\n.\n.\nc693\",\"_c119\",\"_c160\",\"_c141\",\"_c516\",\"_c419\",\"_c69\",\"_c621\",\"_c423\",\"_c137\",\"_c549\",\"_c636\",\"_c772\",\"_c799\",\"_c336\",\"_c841\",\"_c82\",\"_c123\",\"id\",\"_c474\",\"_c470\",\"_c286\",\"_c555\",\"_c36\",\"_c299\",\"_c829\",\"_c361\",\"_c263\",\"_c522\",\"_c495\",\"_c135\")\n\nscala> val assembler = new VectorAssembler().setInputCols(columnNames).setOutputCol(\"features\")\n\nscala> val output = assembler.transform(indexedDF)\n\nscala> output.select(\"id\", \"label\", \"features\").take(5).foreach(println)\n[0,1.0,(857,[333,606,829],[1.0,1.0,1.0])]\n[1,2.0,(857,[725,730,740,844],[1.0,1.0,1.0,1.0])]\n[2,3.0,(857,[72,277,844],[1.0,1.0,2.0])]\n[3,4.0,(857,[72,606,813,822,844],[1.0,1.0,1.0,1.0,3.0])]\n[4,5.0,(857,[215,275,339,386,475,489,630,844],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,4.0])]\n```", "```scala\nscala> val Array(trainingData, testData) = output.randomSplit(Array(0.9, 0.1), seed = 12345)\n\nscala> val copyTestData = testData.drop(\"id\").drop(\"features\")\ncopyTestData: org.apache.spark.sql.DataFrame = [label: double, _c715: double ... 855 more fields]\n\nscala> copyTestData.coalesce(1).write.format(\"csv\").option(\"header\", \"false\").mode(\"overwrite\").save(\"file:///Users/aurobindosarkar/Downloads/CNAE-9/input\")\n```", "```scala\nscala> val lr = new LogisticRegression().setMaxIter(20).setRegParam(0.3).setElasticNetParam(0.8)\n\nscala> // Fit the model\n\nscala> val lrModel = lr.fit(trainingData)\n```", "```scala\nscala> println(\"Model was fit using parameters: \" + lrModel.parent.extractParamMap)\n\nModel was fit using parameters: {\nlogreg_801d78ffc37d-aggregationDepth: 2,\nlogreg_801d78ffc37d-elasticNetParam: 0.8,\nlogreg_801d78ffc37d-family: auto,\nlogreg_801d78ffc37d-featuresCol: features,\nlogreg_801d78ffc37d-fitIntercept: true,\nlogreg_801d78ffc37d-labelCol: label,\nlogreg_801d78ffc37d-maxIter: 20,\nlogreg_801d78ffc37d-predictionCol: prediction,\nlogreg_801d78ffc37d-probabilityCol: probability,\nlogreg_801d78ffc37d-rawPredictionCol: rawPrediction,\nlogreg_801d78ffc37d-regParam: 0.3,\nlogreg_801d78ffc37d-standardization: true,\nlogreg_801d78ffc37d-threshold: 0.5,\nlogreg_801d78ffc37d-tol: 1.0E-6\n}\n```", "```scala\nscala> println(s\"Coefficients: \\n${lrModel.coefficientMatrix}\")\nCoefficients:\n10 x 857 CSCMatrix\n(1,206) 0.33562831098750884\n(5,386) 0.2803498729889301\n(7,545) 0.525713129850472\n\nscala> println(s\"Intercepts: ${lrModel.interceptVector}\")\nIntercepts: [-5.399655915806082,0.6130722758222028,0.6011509547631415,0.6381333836655702,0.6011509547630515,0.5542027670693254,0.6325214445680327,0.5332703316733128,0.6325214445681948,0.5936323589132501]\n```", "```scala\nscala> val predictions = lrModel.transform(testData)\n```", "```scala\nscala> predictions.select(\"prediction\", \"label\", \"features\").show(5)\n```", "```scala\nscala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\nscala> val accuracy = evaluator.evaluate(predictions)\naccuracy: Double = 0.2807017543859649\nscala> println(\"Test Error = \" + (1.0 - accuracy))\nTest Error = 0.7192982456140351\n```", "```scala\nscala> val lr = new LogisticRegression()\nlr: org.apache.spark.ml.classification.LogisticRegression = logreg_3fa32a4b5c6d\n\nscala> val paramGrid = new ParamGridBuilder().addGrid(lr.regParam, Array(0.1, 0.01)).addGrid(lr.elasticNetParam, Array(0.0, 0.5, 1.0)).build()\n\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\nlogreg_3fa32a4b5c6d-elasticNetParam: 0.0,\nlogreg_3fa32a4b5c6d-regParam: 0.1\n}, {\nlogreg_3fa32a4b5c6d-elasticNetParam: 0.0,\nlogreg_3fa32a4b5c6d-regParam: 0.01\n}, {\nlogreg_3fa32a4b5c6d-elasticNetParam: 0.5,\nlogreg_3fa32a4b5c6d-regParam: 0.1\n}, {\nlogreg_3fa32a4b5c6d-elasticNetParam: 0.5,\nlogreg_3fa32a4b5c6d-regParam: 0.01\n}, {\nlogreg_3fa32a4b5c6d-elasticNetParam: 1.0,\nlogreg_3fa32a4b5c6d-regParam: 0.1\n}, {\nlogreg_3fa32a4b5c6d-elasticNetParam: 1.0,\nlogreg_3fa32a4b5c6d-regParam: 0.01\n})\n```", "```scala\nscala> val cv = new CrossValidator().setEstimator(lr).setEvaluator(new MulticlassClassificationEvaluator).setEstimatorParamMaps(paramGrid).setNumFolds(5)\n```", "```scala\nscala> val cvModel = cv.fit(trainingData)\n\nscala> println(\"Model was fit using parameters: \" + cvModel.parent.extractParamMap)\n\nModel was fit using parameters: {\ncv_00543dadc091-estimator: logreg_41377555b425,\ncv_00543dadc091-estimatorParamMaps: [Lorg.apache.spark.ml.param.ParamMap;@14ca1e23,\ncv_00543dadc091-evaluator: mcEval_0435b4f19e2a,\ncv_00543dadc091-numFolds: 5,\ncv_00543dadc091-seed: -1191137437\n}\n```", "```scala\nscala> cvModel.transform(testData).select(\"id\", \"label\", \"probability\", \"prediction\").collect().foreach { case Row(id: Long, label: Double, prob: Vector, prediction: Double) =>\n| println(s\"($id, $label) --> prob=$prob, prediction=$prediction\")\n| }\n```", "```scala\nscala> val cvPredictions = cvModel.transform(testData)\ncvPredictions: org.apache.spark.sql.DataFrame = [label: double, _c715: double ... 860 more fields]\n\nscala> cvPredictions.select(\"prediction\", \"label\", \"features\").show(5)\n```", "```scala\nscala> val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n```", "```scala\nscala> val accuracy = evaluator.evaluate(cvPredictions)\naccuracy: Double = 0.9736842105263158\n\nscala> println(\"Test Error = \" + (1.0 - accuracy))\nTest Error = 0.02631578947368418\n```", "```scala\nscala> cvModel.write.overwrite.save(\"file:///Users/aurobindosarkar/Downloads/CNAE-9/model\")\n```", "```scala\npackage org.chap9.ml\nimport scala.collection.mutable\nimport scopt.OptionParser\nimport org.apache.spark.sql._\nimport org.apache.spark.sql.types._\nimport org.apache.spark.ml._\nimport org.apache.spark.examples.mllib.AbstractParams\nimport org.apache.spark.ml.feature.{VectorAssembler}\nimport org.apache.spark.ml.classification.{LogisticRegression, LogisticRegressionModel}\nimport org.apache.spark.ml.tuning.{CrossValidator, CrossValidatorModel}\nimport org.apache.spark.ml.evaluation.{MulticlassClassificationEvaluator}\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n\nobject LRExample {\n   case class Params(\n   inputModelPath: String = null,\n   testInput: String = \"\"\n) extends AbstractParams[Params]\n\ndef main(args: Array[String]) {\n   val defaultParams = Params()\n   val parser = new OptionParser[Params](\"LRExample\") {\n      head(\"LRExample: an example Logistic Regression.\")\n      arg[String](\"inputModelPath\")\n      .text(s\"input path to saved model.\")\n      .required()\n      .action((x, c) => c.copy(inputModelPath = x))\n      arg[String](\"<testInput>\")\n      .text(\"test input path to new data\")\n      .required()\n      .action((x, c) => c.copy(testInput = x))\n   checkConfig { params =>\n      if ((params.testInput == null) || (params.inputModelPath == null)) {\n         failure(s\"Both Test Input File && input model path values need to be provided.\")\n      } else {\n         success\n      }\n   }\n}\nparser.parse(args, defaultParams) match {\n   case Some(params) => run(params)\n   case _ => sys.exit(1)\n  }\n}\ndef run(params: Params): Unit = {\n   val spark = SparkSession\n      .builder\n      .appName(s\"LogisticRegressionExample with $params\")\n      .getOrCreate()\n   println(s\"LogisticRegressionExample with parameters:\\n$params\")\n   val inRDD = spark.sparkContext.textFile(\"file://\" + params.testInput)\n\n   val rowRDD = inRDD.map(_.split(\",\")).map(attributes => Row(attributes(0).toDouble, attributes(1).toDouble, attributes(2).toDouble, attributes(3).toDouble, attributes(4).toDouble, attributes(5).toDouble, attributes(6).toDouble,\n.\n.\n.\nattributes(850).toDouble, attributes(851).toDouble, attributes(852).toDouble, attributes(853).toDouble, attributes(854).toDouble, attributes(855).toDouble, attributes(856).toDouble))\nval schemaString = \"label _c715 _c195 _c480 _c856 _c136 _c53 _c429 _c732 _c271 _c742 _c172 _c45 _c374 _c233 _c720 _c294 _c461 _c87 _c599 _c84 _c28 _c79 _c615 _c243\n.\n.\n.\n_c336 _c841 _c82 _c123 _c474 _c470 _c286 _c555 _c36 _c299 _c829 _c361 _c263 _c522 _c495 _c135\"\nval fields = schemaString.split(\" \").map(fieldName => StructField(fieldName, DoubleType, nullable = false))\nval schema = StructType(fields)\nval inDF = spark.createDataFrame(rowRDD, schema)\nval indexedDF= inDF.withColumn(\"id\",org.apache.spark.sql.functions.monotonically_increasing_id())\nval columnNames = Array(\"_c715\",\"_c195\",\"_c480\",\"_c856\",\"_c136\",\"_c53\",\"_c429\",\"_c732\",\"_c271\",\"_c742\",\"_c172\",\"_c45\",\"_c374\",\"_c233\",\"_c720\",\"_c294\",\"_c461\",\"_c87\",\"_c599\",\"_c84\",\"_c28\",\"_c\n.\n.\n.\n141\",\"_c516\",\"_c419\",\"_c69\",\"_c621\",\"_c423\",\"_c137\",\"_c549\",\"_c636\",\"_c772\",\"_c799\",\"_c336\",\"_c841\",\"_c82\",\"_c123\",\"id\",\"_c474\",\"_c470\",\"_c286\",\"_c555\",\"_c36\",\"_c299\",\"_c829\",\"_c361\",\"_c263\",\"_c522\",\"_c495\",\"_c135\")\n   val assembler = new VectorAssembler().setInputCols(columnNames).setOutputCol(\"features\")\n   val output = assembler.transform(indexedDF)\n   val cvModel = CrossValidatorModel.load(\"file://\"+ params.inputModelPath)\n   val cvPredictions = cvModel.transform(output)\n   val evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"label\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n   val accuracy = evaluator.evaluate(cvPredictions)\n   println(\"Test Error = \" + (1.0 - accuracy))\n   spark.stop()\n  }\n}\n```", "```scala\nAurobindos-MacBook-Pro-2:Chapter9 aurobindosarkar$ sbt package\n```", "```scala\nAurobindos-MacBook-Pro-2:scala-2.11 aurobindosarkar$ /Users/aurobindosarkar/Downloads/spark-2.2.1-SNAPSHOT-bin-hadoop2.7/bin/spark-submit --jars /Users/aurobindosarkar/Downloads/Chapter9/lib/spark-examples_2.11-2.2.1-SNAPSHOT.jar,/Users/aurobindosarkar/Downloads/Chapter9/lib/scopt_2.11-3.3.0.jar --class org.chap9.ml.LRExample --master local[*] chapter9_2.11-2.0.jar /Users/aurobindosarkar/Downloads/CNAE-9/model /Users/aurobindosarkar/Downloads/CNAE-9/input/part-00000-61f03111-53bb-4404-bef7-0dd4ac1be950-c000.csv\n\nLogisticRegressionExample with parameters:\n{\ninputModelPath: /Users/aurobindosarkar/Downloads/CNAE-9/model,\ntestInput: /Users/aurobindosarkar/Downloads/CNAE-9/input/part-00000-61f03111-53bb-4404-bef7-0dd4ac1be950-c000.csv\n}\nTest Error = 0.02631578947368418\n```"]