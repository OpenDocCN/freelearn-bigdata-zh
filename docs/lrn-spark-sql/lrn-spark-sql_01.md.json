["```scala\n    scala> spark.conf.set(\"spark.executor.cores\", \"2\")\n\n    scala> spark.conf.set(\"spark.executor.memory\", \"4g\")\n```", "```scala\nscala> import org.apache.spark.sql.types._\n\nscala> val recordSchema = new StructType().add(\"sample\", \"long\").add(\"cThick\", \"integer\").add(\"uCSize\", \"integer\").add(\"uCShape\", \"integer\").add(\"mAdhes\", \"integer\").add(\"sECSize\", \"integer\").add(\"bNuc\", \"integer\").add(\"bChrom\", \"integer\").add(\"nNuc\", \"integer\").add(\"mitosis\", \"integer\").add(\"clas\", \"integer\")\n\n```", "```scala\nval df = spark.read.format(\"csv\").option(\"header\", false).schema(recordSchema).load(\"file:///Users/aurobindosarkar/Downloads/breast-cancer-wisconsin.data\")\n```", "```scala\nscala> df.createOrReplaceTempView(\"cancerTable\") \n\nscala> val sqlDF = spark.sql(\"SELECT sample, bNuc from cancerTable\") \n```", "```scala\ncase class and the toDS() method. Then, we define a UDF to convert the clas\u00a0column, currently containing\u00a02's\u00a0and\u00a04's\u00a0to \u00a00's and\u00a01's\u00a0respectively. We register the UDF using the SparkSession object and use it in a SQL statement:\n```", "```scala\nscala> case class CancerClass(sample: Long, cThick: Int, uCSize: Int, uCShape: Int, mAdhes: Int, sECSize: Int, bNuc: Int, bChrom: Int, nNuc: Int, mitosis: Int, clas: Int)\n\nscala> val cancerDS = spark.sparkContext.textFile(\"file:///Users/aurobindosarkar/Documents/SparkBook/data/breast-cancer-wisconsin.data\").map(_.split(\",\")).map(attributes => CancerClass(attributes(0).trim.toLong, attributes(1).trim.toInt, attributes(2).trim.toInt, attributes(3).trim.toInt, attributes(4).trim.toInt, attributes(5).trim.toInt, attributes(6).trim.toInt, attributes(7).trim.toInt, attributes(8).trim.toInt, attributes(9).trim.toInt, attributes(10).trim.toInt)).toDS()\n\nscala> def binarize(s: Int): Int = s match {case 2 => 0 case 4 => 1 }\n\nscala> spark.udf.register(\"udfValueToCategory\", (arg: Int) => binarize(arg))\n\nscala> val sqlUDF = spark.sql(\"SELECT *, udfValueToCategory(clas) from cancerTable\")\n\nscala> sqlUDF.show()\n```", "```scala\nscala> spark.catalog.currentDatabase\n\nres5: String = default\n\nscala> spark.catalog.isCached(\"cancerTable\") \n\nres6: Boolean = false \n\nscala> spark.catalog.cacheTable(\"cancerTable\") \n\nscala> spark.catalog.isCached(\"cancerTable\") \n\nres8: Boolean = true \n\nscala> spark.catalog.clearCache \n\nscala> spark.catalog.isCached(\"cancerTable\") \n\nres10: Boolean = false \n\nscala> spark.catalog.listDatabases.show()\n```", "```scala\nscala> spark.catalog.listDatabases.take(1)\nres13: Array[org.apache.spark.sql.catalog.Database] = Array(Database[name='default', description='Default Hive database', path='file:/Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7/spark-warehouse'])\n\nscala> spark.catalog.listTables.show()\n```", "```scala\nscala> spark.catalog.dropTempView(\"cancerTable\")\n\nscala> spark.catalog.listTables.show()\n```", "```scala\nscala> val cancerRDD = sc.textFile(\"file:///Users/aurobindosarkar/Downloads/breast-cancer-wisconsin.data\", 4)\n\nscala> cancerRDD.partitions.size\nres37: Int = 4\n```", "```scala\nscala> import spark.implicits._scala> \nval cancerDF = cancerRDD.toDF()\n```", "```scala\ndef row(line: List[String]): Row = { Row(line(0).toLong, line(1).toInt, line(2).toInt, line(3).toInt, line(4).toInt, line(5).toInt, line(6).toInt, line(7).toInt, line(8).toInt, line(9).toInt, line(10).toInt) }\nval data = cancerRDD.map(_.split(\",\").to[List]).map(row)\nval cancerDF = spark.createDataFrame(data, recordSchema)\n```", "```scala\nscala> val cancerDS = cancerDF.as[CancerClass]\n```", "```scala\nscala> case class RestClass(name: String, street: String, city: String, phone: String, cuisine: String)\n```", "```scala\nscala> val rest1DS = spark.sparkContext.textFile(\"file:///Users/aurobindosarkar/Documents/SparkBook/data/zagats.csv\").map(_.split(\",\")).map(attributes => RestClass(attributes(0).trim, attributes(1).trim, attributes(2).trim, attributes(3).trim, attributes(4).trim)).toDS()\n\nscala> val rest2DS = spark.sparkContext.textFile(\"file:///Users/aurobindosarkar/Documents/SparkBook/data/fodors.csv\").map(_.split(\",\")).map(attributes => RestClass(attributes(0).trim, attributes(1).trim, attributes(2).trim, attributes(3).trim, attributes(4).trim)).toDS()\n```", "```scala\nscala> def formatPhoneNo(s: String): String = s match {case s if s.contains(\"/\") => s.replaceAll(\"/\", \"-\").replaceAll(\"- \", \"-\").replaceAll(\"--\", \"-\") case _ => s } \n\nscala> val udfStandardizePhoneNos = udf[String, String]( x => formatPhoneNo(x) ) \n\nscala> val rest2DSM1 = rest2DS.withColumn(\"stdphone\", udfStandardizePhoneNos(rest2DS.col(\"phone\")))\n```", "```scala\nscala> rest1DS.createOrReplaceTempView(\"rest1Table\") \n\nscala> rest2DSM1.createOrReplaceTempView(\"rest2Table\")\n```", "```scala\nscala> spark.sql(\"SELECT count(*) from rest1Table, rest2Table where rest1Table.phone = rest2Table.stdphone\").show()\n```", "```scala\nscala> val sqlDF = spark.sql(\"SELECT a.name, b.name, a.phone, b.stdphone from rest1Table a, rest2Table b where a.phone = b.stdphone\")\n```", "```scala\nscala> case class PinTrans(bidid: String, timestamp: String, ipinyouid: String, useragent: String, IP: String, region: String, city: String, adexchange: String, domain: String, url:String, urlid: String, slotid: String, slotwidth: String, slotheight: String, slotvisibility: String, slotformat: String, slotprice: String, creative: String, bidprice: String) \n\nscala> case class PinRegion(region: String, regionName: String)\n```", "```scala\nscala> val pintransDF = spark.sparkContext.textFile(\"file:///Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/training1st/bid.20130314.txt\").map(_.split(\"\\t\")).map(attributes => PinTrans(attributes(0).trim, attributes(1).trim, attributes(2).trim, attributes(3).trim, attributes(4).trim, attributes(5).trim, attributes(6).trim, attributes(7).trim, attributes(8).trim, attributes(9).trim, attributes(10).trim, attributes(11).trim, attributes(12).trim, attributes(13).trim, attributes(14).trim, attributes(15).trim, attributes(16).trim, attributes(17).trim, attributes(18).trim)).toDF() \n\nscala> val pinregionDF = spark.sparkContext.textFile(\"file:///Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/region.en.txt\").map(_.split(\"\\t\")).map(attributes => PinRegion(attributes(0).trim, attributes(1).trim)).toDF()\n```", "```scala\nscala> def benchmark(name: String)(f: => Unit) { \n val startTime = System.nanoTime \n f \n val endTime = System.nanoTime \n println(s\"Time taken in $name: \" + (endTime - startTime).toDouble / 1000000000 + \" seconds\") \n}\n```", "```scala\nscala> spark.conf.set(\"spark.sql.codegen.wholeStage\", false) \nscala> benchmark(\"Spark 1.6\") {  \n|  pintransDF.join(pinregionDF, \"region\").count()  \n| }\nTime taken in Spark 1.6: 3.742190552 seconds \n```", "```scala\nscala> spark.conf.set(\"spark.sql.codegen.wholeStage\", true) \nscala> benchmark(\"Spark 2.2\") {  \n|  pintransDF.join(pinregionDF, \"region\").count()  \n| }\nTime taken in Spark 2.2: 1.881881579 seconds    \n```", "```scala\nscala> pintransDF.join(pinregionDF, \"region\").selectExpr(\"count(*)\").explain(true) \n```", "```scala\nscala> pintransDF.join(pinregionDF, \"region\").selectExpr(\"count(*)\").explain() \n```", "```scala\nscala> import org.apache.spark.sql.types._ \nscala> import org.apache.spark.sql.functions._ \nscala> import scala.concurrent.duration._ \nscala> import org.apache.spark.sql.streaming.ProcessingTime \nscala> import org.apache.spark.sql.streaming.OutputMode.Complete \n\nscala> val bidSchema = new StructType().add(\"bidid\", StringType).add(\"timestamp\", StringType).add(\"ipinyouid\", StringType).add(\"useragent\", StringType).add(\"IP\", StringType).add(\"region\", IntegerType).add(\"city\", IntegerType).add(\"adexchange\", StringType).add(\"domain\", StringType).add(\"url:String\", StringType).add(\"urlid: String\", StringType).add(\"slotid: String\", StringType).add(\"slotwidth\", StringType).add(\"slotheight\", StringType).add(\"slotvisibility\", StringType).add(\"slotformat\", StringType).add(\"slotprice\", StringType).add(\"creative\", StringType).add(\"bidprice\", StringType) \n\nscala> val streamingInputDF = spark.readStream.format(\"csv\").schema(bidSchema).option(\"header\", false).option(\"inferSchema\", true).option(\"sep\", \"\\t\").option(\"maxFilesPerTrigger\", 1).load(\"file:///Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/bidfiles\")\n```", "```scala\nscala> val streamingCountsDF = streamingInputDF.groupBy($\"city\").count() \n\nscala> val query = streamingCountsDF.writeStream.format(\"console\").trigger(ProcessingTime(20.seconds)).queryName(\"counts\").outputMode(Complete).start()\n```", "```scala\nscala> spark.streams.active.foreach(println) \nStreaming Query - counts [state = ACTIVE]\n```", "```scala\n//Execute the stop() function after you have finished executing the code in the next section.\nscala> query.stop()\n```", "```scala\nscala> spark.streams.active(0).explain \n```"]