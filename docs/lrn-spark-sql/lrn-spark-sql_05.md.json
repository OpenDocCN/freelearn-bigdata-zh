["```scala\nscala> import org.apache.spark.sql.types._\nscala> import org.apache.spark.sql.functions._\nscala> import scala.concurrent.duration._\nscala> import org.apache.spark.sql.streaming.ProcessingTime\nscala> import org.apache.spark.sql.streaming.OutputMode.Complete\nscala> import spark.implicits._\n```", "```scala\nscala> val bidSchema = new StructType().add(\"bidid\", StringType).add(\"timestamp\", StringType).add(\"ipinyouid\", StringType).add(\"useragent\", StringType).add(\"IP\", StringType).add(\"region\", IntegerType).add(\"cityID\", IntegerType).add(\"adexchange\", StringType).add(\"domain\", StringType).add(\"turl\", StringType).add(\"urlid\", StringType).add(\"slotid\", StringType).add(\"slotwidth\", StringType).add(\"slotheight\", StringType).add(\"slotvisibility\", StringType).add(\"slotformat\", StringType).add(\"slotprice\", StringType).add(\"creative\", StringType).add(\"bidprice\", StringType)\n```", "```scala\nscala> val streamingInputDF = spark.readStream.format(\"csv\").schema(bidSchema).option(\"header\", false).option(\"inferSchema\", true).option(\"sep\", \"\\t\").option(\"maxFilesPerTrigger\", 1).load(\"file:///Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/bidfiles\")\n```", "```scala\nscala> streamingInputDF.printSchema()\nroot\n|-- bidid: string (nullable = true)\n|-- timestamp: string (nullable = true)\n|-- ipinyouid: string (nullable = true)\n|-- useragent: string (nullable = true)\n|-- IP: string (nullable = true)\n|-- region: integer (nullable = true)\n|-- cityID: integer (nullable = true)\n|-- adexchange: string (nullable = true)\n|-- domain: string (nullable = true)\n|-- turl: string (nullable = true)\n|-- urlid: string (nullable = true)\n|-- slotid: string (nullable = true)\n|-- slotwidth: string (nullable = true)\n|-- slotheight: string (nullable = true)\n|-- slotvisibility: string (nullable = true)\n|-- slotformat: string (nullable = true)\n|-- slotprice: string (nullable = true)\n|-- creative: string (nullable = true)\n|-- bidprice: string (nullable = true)\n```", "```scala\nscala> val ts = unix_timestamp($\"timestamp\", \"yyyyMMddHHmmssSSS\").cast(\"timestamp\")\n\nscala> val streamingCityTimeDF = streamingInputDF.withColumn(\"ts\", ts).select($\"cityID\", $\"ts\")\n```", "```scala\nscala> val windowedCounts = streamingCityTimeDF.groupBy(window($\"ts\", \"10 minutes\", \"5 minutes\"), $\"cityID\").count().writeStream.outputMode(\"complete\").format(\"console\").start()\n```", "```scala\nscala> val citySchema = new StructType().add(\"cityID\", StringType).add(\"cityName\", StringType)\n\nscala> val staticDF = spark.read.format(\"csv\").schema(citySchema).option(\"header\", false).option(\"inferSchema\", true).option(\"sep\", \"\\t\").load(\"file:///Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/city.en.txt\")\n```", "```scala\nscala> val joinedDF = streamingCityTimeDF.join(staticDF, \"cityID\")\n```", "```scala\nscala> val windowedCityCounts = joinedDF.groupBy(window($\"ts\", \"10 minutes\", \"5 minutes\"), $\"cityName\").count().writeStream.outputMode(\"complete\").format(\"console\").start()\n```", "```scala\nscala> val streamingCityNameBidsTimeDF = streamingInputDF.withColumn(\"ts\", ts).select($\"ts\", $\"bidid\", $\"cityID\", $\"bidprice\", $\"slotprice\").join(staticDF, \"cityID\") \n```", "```scala\nscala> val cityBids = streamingCityNameBidsTimeDF.select($\"ts\", $\"bidid\", $\"bidprice\", $\"slotprice\", $\"cityName\").writeStream.outputMode(\"append\").format(\"console\").start()\n```", "```scala\nscala> case class Bid(bidid: String, timestamp: String, ipinyouid: String, useragent: String, IP: String, region: Integer, cityID: Integer, adexchange: String, domain: String, turl: String, urlid: String, slotid: String, slotwidth: String, slotheight: String, slotvisibility: String, slotformat: String, slotprice: String, creative: String, bidprice: String)\n```", "```scala\nscala> val ds = streamingInputDF.as[Bid]\n```", "```scala\nimport org.apache.spark.sql.ForeachWriter\n\nval writer = new ForeachWriter[String] {\n   override def open(partitionId: Long, version: Long) = true\n   override def process(value: String) = println(value)\n   override def close(errorOrNull: Throwable) = {}\n}\n```", "```scala\nscala> val dsForeach = ds.filter(_.adexchange == \"3\").map(_.useragent).writeStream.foreach(writer).start()\n```", "```scala\nscala> val aggAdexchangeDF = streamingInputDF.groupBy($\"adexchange\").count()\n\nscala> val aggQuery = aggAdexchangeDF.writeStream.queryName(\"aggregateTable\").outputMode(\"complete\").format(\"memory\").start()\n\nscala> spark.sql(\"select * from aggregateTable\").show()   \n```", "```scala\nscala> val cityBidsParquet = streamingCityNameBidsTimeDF.select($\"bidid\", $\"bidprice\", $\"slotprice\", $\"cityName\").writeStream.outputMode(\"append\").format(\"parquet\").option(\"path\", \"hdfs://localhost:9000/pout\").option(\"checkpointLocation\", \"hdfs://localhost:9000/poutcp\").start()\n```", "```scala\nAurobindos-MacBook-Pro-2:~ aurobindosarkar$ hdfs dfs -ls /pout\n```", "```scala\nAurobindos-MacBook-Pro-2:~ aurobindosarkar$ hdfs dfs -ls /poutcp\n```", "```scala\nscala> spark.streams.active.foreach(x => println(\"ID:\"+ x.id + \"             Run ID:\"+ x.runId + \"               Status: \"+ x.status))\n\nID:0ebe31f5-6b76-46ea-a328-cd0c637be49c             \nRun ID:6f203d14-2a3a-4c9f-9ea0-8a6783d97873               \nStatus: {\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\nID:519cac9a-9d2f-4a01-9d67-afc15a6b03d2             \nRun ID:558590a7-cbd3-42b8-886b-cdc32bb4f6d7               \nStatus: {\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\nID:1068bc38-8ba9-4d5e-8762-bbd2abffdd51             \nRun ID:bf875a27-c4d8-4631-9ea2-d51a0e7cb232               \nStatus: {\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\nID:d69c4005-21f1-487a-9fe5-d804ca86f0ff             \nRun ID:a6969c1b-51da-4986-b5f3-a10cd2397784               \nStatus: {\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\nID:1fa9e48d-091a-4888-9e69-126a2f1c081a             \nRun ID:34dc2c60-eebc-4ed6-bf25-decd6b0ad6c3               \nStatus: {\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,  \"isTriggerActive\" : false\n}\nID:a7ff2807-dc23-4a14-9a9c-9f8f1fa6a6b0             \nRun ID:6c8f1a83-bb1c-4dd7-8974\n83042a286bae               \nStatus: {\n  \"message\" : \"Waiting for data to arrive\",\n  \"isDataAvailable\" : false,\n  \"isTriggerActive\" : false\n}\n```", "```scala\nscala> // get the unique identifier of the running query that persists across restarts from checkpoint data\nscala> windowedCounts.id          \nres6: java.util.UUID = 0ebe31f5-6b76-46ea-a328-cd0c637be49c\n\nscala> // get the unique id of this run of the query, which will be generated at every start/restart\nscala> windowedCounts.runId       \nres7: java.util.UUID = 6f203d14-2a3a-4c9f-9ea0-8a6783d97873\n\nscala> // the exception if the query has been terminated with error\nscala> windowedCounts.exception       \nres8: Option[org.apache.spark.sql.streaming.StreamingQueryException] = None\n\nscala> // the most recent progress update of this streaming query\nscala> windowedCounts.lastProgress \nres9: org.apache.spark.sql.streaming.StreamingQueryProgress =\n```", "```scala\nscala> windowedCounts.stop()\n```", "```scala\nbin/zookeeper-server-start.sh config/zookeeper.properties\n```", "```scala\nbin/kafka-server-start.sh config/server.properties\n```", "```scala\nbin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test\n```", "```scala\nbin/kafka-topics.sh --list --zookeeper localhost:2181\n```", "```scala\nbin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\nThis is the first message.\nThis is another message.\n```", "```scala\nAurobindos-MacBook-Pro-2:spark-2.1.0-bin-hadoop2.7 aurobindosarkar$ ./bin/spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.1.0,org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.0\n```", "```scala\nscala> val ds1 = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"test\").load().selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n```", "```scala\nscala> val query = ds1.writeStream.outputMode(\"append\").format(\"console\").start()\n```", "```scala\nAurobindos-MacBook-Pro-2:kafka_2.11-0.10.1.0 aurobindosarkar$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic connect-test < /Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/bidfiles/bid.20130311.txt\n```", "```scala\nscala> val ds2 = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"connect-test\").load().selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\").as[(String, String)]\n\nscala> val query = ds2.writeStream.outputMode(\"append\").format(\"console\").start()\n```", "```scala\nimport org.apache.spark.storage.StorageLevel\nimport org.apache.spark.streaming.receiver.Receiver\nimport org.jfarcand.wcs.{TextListener, WebSocket}\nimport scala.util.parsing.json.JSON\nimport scalaj.http.Http\nimport java.io.BufferedReader;\nimport java.io.IOException;\nimport java.io.InputStreamReader;\nimport org.apache.http.HttpResponse;\nimport org.apache.http.client.ClientProtocolException;\nimport org.apache.http.client.methods.HttpGet;\nimport org.apache.http.impl.client.DefaultHttpClient;\n/**\n* Spark Streaming Example TfL Receiver\n*/\nclass TFLArrivalPredictionsByLine() extends Receiver[String](StorageLevel.MEMORY_ONLY) with Runnable {\n//Replace the app_key parameter with your own key\nprivate val tflUrl = \"https://api.tfl.gov.uk/Line/circle/Arrivals?stopPointId=940GZZLUERC&app_id=a73727f3&app_key=xxx\"\n@transient\nprivate var thread: Thread = _\noverride def onStart(): Unit = {\n   thread = new Thread(this)\n   thread.start()\n}\noverride def onStop(): Unit = {\n   thread.interrupt()\n}\noverride def run(): Unit = {\n   while (true){\n     receive();\n     Thread.sleep(60*1000);\n   }\n}\nprivate def receive(): Unit = {\n   val httpClient = new DefaultHttpClient();\n   val getRequest = new HttpGet(tflUrl);\n   getRequest.addHeader(\"accept\", \"application/json\");\n   val response = httpClient.execute(getRequest);\n   if (response.getStatusLine().getStatusCode() != 200) {\n      throw new RuntimeException(\"Failed : HTTP error code : \"\n         + response.getStatusLine().getStatusCode());\n   }\n   val br = new BufferedReader(\n      new InputStreamReader((response.getEntity().getContent())));\n   var output=br.readLine();\n   while(output!=null){        \n      println(output)\n      output=br.readLine()\n   } \n}\n}\n```", "```scala\nimport org.apache.spark.SparkConf\nimport org.apache.spark.streaming.{Seconds, StreamingContext}\n/**\n* Spark Streaming Example App\n*/\nobject TFLStreamingApp {\ndef main(args: Array[String]) {\n   val conf = new SparkConf().setAppName(\"TFLStreaming\")\n   val ssc = new StreamingContext(conf, Seconds(300))\n   val stream = ssc.receiverStream(new TFLArrivalPredictionsByLine())\n   stream.print()\n   if (args.length > 2) {\n      stream.saveAsTextFiles(args(2))\n   }\n   ssc.start()\n   ssc.awaitTermination()\n   }\n}\n```", "```scala\nname := \"spark-streaming-example\"\nversion := \"1.0\"\nscalaVersion := \"2.11.7\"\nresolvers += \"jitpack\" at \"https://jitpack.io\"\nlibraryDependencies ++= Seq(\"org.apache.spark\" %% \"spark-core\" % \"2.0.0\",       \"org.apache.spark\" %% \"spark-streaming\" % \"2.0.0\",\n\"org.apache.httpcomponents\" % \"httpclient\" % \"4.5.2\",\n\"org.scalaj\" %% \"scalaj-http\" % \"2.2.1\",\n\"org.jfarcand\" % \"wcs\" % \"1.5\")\n```", "```scala\nAurobindos-MacBook-Pro-2:scala-2.11 aurobindosarkar$ /Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7/bin/spark-submit --class TFLStreamingApp --master local[*] spark-streaming-example_2.11-1.0.jar\n```"]