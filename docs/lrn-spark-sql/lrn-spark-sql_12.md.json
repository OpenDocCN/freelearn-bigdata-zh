["```scala\nspark.read.json(\"/source/path\") //Extract\n.filter(...) //Transform\n.agg(...) //Transform\n.write.mode(\"append\") .parquet(\"/output/path\") //Load\n```", "```scala\nscala> val jsonDF = spark.read.json(\"file:///Users/aurobindosarkar/Downloads/cache-0-json\")\n\nscala> jsonDF.printSchema()\n\nscala> val rawTweetsSchema = jsonDF.schema\n\nscala> val jsonString = rawTweetsSchema.json\n\nscala> val schema = DataType.fromJson(jsonString).asInstanceOf[StructType]\n```", "```scala\nscala> val rawTweets = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"tweetsa\").load()\n\nscala> val parsedTweets = rawTweets.selectExpr(\"cast (value as string) as json\").select(from_json($\"json\", schema).as(\"data\")).select(\"data.*\")\n```", "```scala\nAurobindos-MacBook-Pro-2:kafka_2.11-0.10.2.1 aurobindosarkar$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic tweetsa < /Users/aurobindosarkar/Downloads/cache-0-json\n```", "```scala\nscala> val selectFields = parsedTweets.select(\"place.country\").where($\"place.country\".isNotNull)\n```", "```scala\nscala> val s5 = selectFields.writeStream.outputMode(\"append\").format(\"console\").start()\n```", "```scala\nscala> val selectFields = parsedTweets.select(\"place.*\").where($\"place.country\".isNotNull)\n```", "```scala\nscala> val selectFields = parsedTweets.select(struct(\"place.country_code\", \"place.name\") as 'locationInfo).where($\"locationInfo.country_code\".isNotNull)\n```", "```scala\nscala> val selectFields = parsedTweets.select($\"entities.hashtags\" as 'tags).select('tags.getItem(0) as 'x).select($\"x.indices\" as 'y).select($\"y\".getItem(0) as 'z).where($\"z\".isNotNull)\n```", "```scala\nscala> val selectFields = parsedTweets.select($\"entities.hashtags\" as 'tags).select('tags.getItem(0) as 'x).select($\"x.text\" as 'y).where($\"y\".isNotNull)\n```", "```scala\nscala> val selectFields = parsedTweets.select($\"entities.hashtags.indices\" as 'tags).select(explode('tags))\n```", "```scala\nscala> val selectFields = parsedTweets.select($\"entities.hashtags.indices\".getItem(0) as 'tags).select(explode('tags))\n```", "```scala\nscala> val selectFields = parsedTweets.select(struct($\"entities.media.type\" as 'x, $\"entities.media.url\" as 'y) as 'z).where($\"z.x\".isNotNull).select(to_json('z) as 'c)\n```", "```scala\n{\"a\":1, \"b\":2, \"c\":3}\n{\"a\":2, \"d\":5, \"e\":3}\n{\"d\":1, \"c\":4, \"f\":6}\n{\"a\":7, \"b\":8}\n{\"c\":5, \"e\":4, \"d\":3}\n{\"f\":3, \"e\":3, \"d\":4}\n{\"a\":1, \"b\":2, \"c\":3, \"f\":3, \"e\":3, \"d\":4}\n```", "```scala\nscala> spark.read.json(\"file:///Users/aurobindosarkar/Downloads/test1.json\").printSchema()\nroot\n|-- a: long (nullable = true)\n|-- b: long (nullable = true)\n|-- c: long (nullable = true)\n|-- d: long (nullable = true)\n|-- e: long (nullable = true)\n|-- f: long (nullable = true)\n```", "```scala\n{\"a\":1, \"b\":2, \"c\":3}\n{\"a\":2, \"d\":5, \"e\":3}\n{\"d\":1, \"c\":4, \"f\":6}\n{\"a\":7, \"b\":8}\n{\"c\":5, \"e\":4.5, \"d\":3}\n{\"f\":\"3\", \"e\":3, \"d\":4}\n{\"a\":1, \"b\":2.1, \"c\":3, \"f\":3, \"e\":3, \"d\":4}\n\nscala> spark.read.json(\"file:///Users/aurobindosarkar/Downloads/test1.json\").printSchema()\nroot\n|-- a: long (nullable = true)\n|-- b: double (nullable = true)\n|-- c: long (nullable = true)\n|-- d: long (nullable = true)\n|-- e: double (nullable = true)\n|-- f: string (nullable = true)\n```", "```scala\na,b,c,d,e,f\n1,2,3,,,\n2,,,5,3,\n,,4,1,,,6\n7,8,,,,f\n,,5,3,4.5,\n,,,4,3,\"3\"\n1,2.1,3,3,3,4\n\nscala> val schema = new StructType().add(\"a\", \"int\").add(\"b\", \"double\")\n\nscala> spark.read.option(\"header\", true).schema(schema).csv(\"file:///Users/aurobindosarkar/Downloads/test1.csv\").show()\n```", "```scala\n{\"a\":1, \"b\":2, \"c\":3}\n{\"a\":2, \"d\":5, \"e\":3}\n{\"d\":1, \"c\":4, \"f\":6}\n{\"a\":7, \"b\":{}\n{\"c\":5, \"e\":4.5, \"d\":3}\n{\"f\":\"3\", \"e\":3, \"d\":4}\n{\"a\":1, \"b\":2.1, \"c\":3, \"f\":3, \"e\":3, \"d\":4}\n\nscala> spark.read.option(\"mode\", \"PERMISSIVE\").option(\"columnNameOfCorruptRecord\", \"_corrupt_record\").json(\"file:///Users/aurobindosarkar/Downloads/test1.json\").show()\n```", "```scala\nscala> spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"file:///Users/aurobindosarkar/Downloads/test1.json\").show()\n```", "```scala\n{\"a\":1, \"b\":2, \"c\":3}\n{\"a\":2, \"d\":5, \"e\":3}\n{\"d\":1, \"c\":4, \"f\":6}\n{\"a\":7, \"b\":$}\n{\"c\":5, \"e\":4.5, \"d\":3}\n{\"f\":\"3\", \"e\":3, \"d\":4}\n{\"a\":1, \"b\":2.1, \"c\":3, \"f\":3, \"e\":3, \"d\":4}\n\nscala> spark.read.option(\"mode\", \"FAILFAST\").json(\"file:///Users/aurobindosarkar/Downloads/test1.json\").show()\n```", "```scala\n{\"a\":{\"a1\":2, \"a2\":8},\n\"b\":5, \"c\":3}\n\nscala> spark.read.option(\"wholeFile\",true).option(\"mode\", \"PERMISSIVE\").option(\"columnNameOfCorruptRecord\", \"_corrupt_record\").json(\"file:///Users/aurobindosarkar/Downloads/testMultiLine.json\").show()\n+-----+---+---+\n|    a|  b|  c|\n+-----+---+---+\n|[2,8]|  5|  3|\n+-----+---+---+\n```", "```scala\nAurobindos-MacBook-Pro-2:spark-2.2.0-bin-hadoop2.7 aurobindosarkar$ ./bin/spark-shell --packages org.apache.spark:spark-streaming-kafka-0-10_2.11:2.1.1,org.apache.spark:spark-sql-kafka-0-10_2.11:2.1.1 --driver-memory 12g\n```", "```scala\nscala> import org.apache.spark.sql.types._\nscala> import org.apache.spark.sql.functions._\nscala> import spark.implicits._\nscala> import org.apache.spark.sql.streaming._\n```", "```scala\nscala> val schema = new StructType().add(\"clientIpAddress\", \"string\").add(\"rfc1413ClientIdentity\", \"string\").add(\"remoteUser\", \"string\").add(\"dateTime\", \"string\").add(\"zone\", \"string\").add(\"request\",\"string\").add(\"httpStatusCode\", \"string\").add(\"bytesSent\", \"string\").add(\"referer\", \"string\").add(\"userAgent\", \"string\")\n```", "```scala\nscala> val rawRecords = spark.readStream.option(\"header\", false).schema(schema).option(\"sep\", \" \").format(\"csv\").load(\"file:///Users/aurobindosarkar/Downloads/NASA\")\n\nscala> val ts = unix_timestamp(concat($\"dateTime\", lit(\" \"), $\"zone\"), \"[dd/MMM/yyyy:HH:mm:ss Z]\").cast(\"timestamp\")\n```", "```scala\nscala> val logEvents = rawRecords.withColumn(\"ts\", ts).withColumn(\"date\", ts.cast(DateType)).select($\"ts\", $\"date\", $\"clientIpAddress\", concat($\"dateTime\", lit(\" \"), $\"zone\").as(\"original_dateTime\"), $\"request\", $\"httpStatusCode\", $\"bytesSent\")\n```", "```scala\nscala> val query = logEvents.writeStream.outputMode(\"append\").format(\"console\").start()\n```", "```scala\nscala> val streamingETLQuery = logEvents.writeStream.trigger(Trigger.ProcessingTime(\"2 minutes\")).format(\"parquet\").partitionBy(\"date\").option(\"path\", \"file:///Users/aurobindosarkar/Downloads/NASALogs\").option(\"checkpointLocation\", \"file:///Users/aurobindosarkar/Downloads/NASALogs/checkpoint/\").start()\n```", "```scala\nval rawCSV = spark.readStream.schema(schema).option(\"latestFirst\", \"true\").option(\"maxFilesPerTrigger\", \"5\").option(\"header\", false).option(\"sep\", \" \").format(\"csv\").load(\"file:///Users/aurobindosarkar/Downloads/NASA\")\n```", "```scala\nval streamingETLQuery = logEvents.writeStream.trigger(Trigger.ProcessingTime(\"2 minutes\")).format(\"json\").partitionBy(\"date\").option(\"path\", \"file:///Users/aurobindosarkar/Downloads/NASALogs\").option(\"checkpointLocation\", \"file:///Users/aurobindosarkar/Downloads/NASALogs/checkpoint/\").start()\n```", "```scala\nscala> val kafkaQuery = logEvents.selectExpr(\"CAST(ts AS STRING) AS key\", \"to_json(struct(*)) AS value\").writeStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"topic\", \"topica\").option(\"checkpointLocation\", \"file:///Users/aurobindosarkar/Downloads/NASALogs/kafkacheckpoint/\").start()\n```", "```scala\nscala> val kafkaDF = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"topica\").option(\"startingOffsets\", \"earliest\").load()\n```", "```scala\nscala> kafkaDF.printSchema()\nroot\n|-- key: binary (nullable = true)\n|-- value: binary (nullable = true)\n|-- topic: string (nullable = true)\n|-- partition: integer (nullable = true)\n|-- offset: long (nullable = true)\n|-- timestamp: timestamp (nullable = true)\n|-- timestampType: integer (nullable = true)\n```", "```scala\nscala> val kafkaSchema = new StructType().add(\"ts\", \"timestamp\").add(\"date\", \"string\").add(\"clientIpAddress\", \"string\").add(\"rfc1413ClientIdentity\", \"string\").add(\"remoteUser\", \"string\").add(\"original_dateTime\", \"string\").add(\"request\", \"string\").add(\"httpStatusCode\", \"string\").add(\"bytesSent\", \"string\")\n```", "```scala\nscala> val kafkaDF1 = kafkaDF.select(col(\"key\").cast(\"string\"), from_json(col(\"value\").cast(\"string\"), kafkaSchema).as(\"data\")).select(\"data.*\")\n```", "```scala\nscala> val kafkaQuery1 = kafkaDF1.select($\"ts\", $\"date\", $\"clientIpAddress\", $\"original_dateTime\", $\"request\", $\"httpStatusCode\", $\"bytesSent\").writeStream.outputMode(\"append\").format(\"console\").start()\n```", "```scala\nscala> val kafkaDF2 = spark.read.format(\"kafka\").option(\"kafka.bootstrap.servers\",\"localhost:9092\").option(\"subscribe\", \"topica\").load().selectExpr(\"CAST(value AS STRING) as myvalue\")\n```", "```scala\nscala> kafkaDF2.registerTempTable(\"topicData3\")\n\nscala> spark.sql(\"select myvalue from topicData3\").take(3).foreach(println)\n```", "```scala\nscala> val parsed = spark.readStream.format(\"kafka\").option(\"kafka.bootstrap.servers\", \"localhost:9092\").option(\"subscribe\", \"topica\").option(\"startingOffsets\", \"earliest\").load().select(from_json(col(\"value\").cast(\"string\"), kafkaSchema).alias(\"parsed_value\"))\n```", "```scala\nscala> val query = parsed.writeStream.outputMode(\"append\").format(\"console\").start()\n```", "```scala\nscala> val selectAllParsed = parsed.select(\"parsed_value.*\")\n```", "```scala\nscala> val selectFieldsParsed = selectAllParsed.select(\"ts\", \"clientIpAddress\", \"request\", \"httpStatusCode\")\n```", "```scala\nscala> val s1 = selectFieldsParsed.groupBy(window($\"ts\", \"10 minutes\", \"5 minutes\"), $\"httpStatusCode\").count().writeStream.outputMode(\"complete\").format(\"console\").start()\n```", "```scala\nscala> val s2 = selectFieldsParsed.groupBy(window($\"ts\", \"10 minutes\", \"5 minutes\"), $\"request\").count().writeStream.outputMode(\"complete\").format(\"console\").start()\n```", "```scala\nscala> val s4 = selectFieldsParsed.withWatermark(\"ts\", \"10 minutes\").groupBy(window($\"ts\", \"10 minutes\", \"5 minutes\"), $\"request\").count().writeStream.outputMode(\"complete\").format(\"console\").start()\n```", "```scala\n>>> from pyspark.ml import Pipeline\n>>> from pyspark.ml.classification import LogisticRegression\n>>> from pyspark.ml.feature import HashingTF, Tokenizer\n>>> training = spark.createDataFrame([\n... (0, \"a b c d e spark\", 1.0),\n... (1, \"b d\", 0.0),\n... (2, \"spark f g h\", 1.0),\n... (3, \"hadoop mapreduce\", 0.0)\n... ], [\"id\", \"text\", \"label\"])\n>>> tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n>>> hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n>>> lr = LogisticRegression(maxIter=10, regParam=0.001)\n>>> pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n>>> model = pipeline.fit(training)\n>>> model.save(\"file:///Users/aurobindosarkar/Downloads/spark-logistic-regression-model\")\n>>> quit()\n```", "```scala\nscala> import org.apache.spark.ml.{Pipeline, PipelineModel}\nscala> import org.apache.spark.ml.classification.LogisticRegression\nscala> import org.apache.spark.ml.feature.{HashingTF, Tokenizer}\nscala> import org.apache.spark.ml.linalg.Vector\nscala> import org.apache.spark.sql.Row\n\nscala> val sameModel = PipelineModel.load(\"file:///Users/aurobindosarkar/Downloads/spark-logistic-regression-model\")\n```", "```scala\nscala> val test = spark.createDataFrame(Seq(\n| (4L, \"spark i j k\"),\n| (5L, \"l m n\"),\n| (6L, \"spark hadoop spark\"),\n| (7L, \"apache hadoop\")\n| )).toDF(\"id\", \"text\")\n```", "```scala\nscala> sameModel.transform(test).select(\"id\", \"text\", \"probability\", \"prediction\").collect().foreach { case Row(id: Long, text: String, prob: Vector, prediction: Double) => println(s\"($id, $text) --> prob=$prob, prediction=$prediction\")}\n\n(4, spark i j k) --> prob=[0.15554371384424398,0.844456286155756], prediction=1.0\n(5, l m n) --> prob=[0.8307077352111738,0.16929226478882617], prediction=0.0\n(6, spark hadoop spark) --> prob=[0.06962184061952888,0.9303781593804711], prediction=1.0\n(7, apache hadoop) --> prob=[0.9815183503510166,0.018481649648983405], prediction=0.0\n```", "```scala\nscala> val df = spark.read.parquet(\"file:///Users/aurobindosarkar/Downloads/spark-logistic-regression-model/stages/2_LogisticRegression_4abda37bdde1ddf65ea0/data/part-00000-415bf215-207a-4a49-985e-190eaf7253a7-c000.snappy.parquet\")\n\nscala> df.show()\n```", "```scala\nscala> df.collect.foreach(println)\n```", "```scala\nscala> spark.read.parquet(\"file:///Users/aurobindosarkar/Downloads/spark-logistic-regression-model/stages/2_LogisticRegression_4abda37bdde1ddf65ea0/data/part-00000-415bf215-207a-4a49-985e-190eaf7253a7-c000.snappy.parquet\").write.mode(\"overwrite\").json(\"file:///Users/aurobindosarkar/Downloads/lr-model-json\")\n```", "```scala\nAurobindos-MacBook-Pro-2:lr-model-json aurobindosarkar$ more part-00000-e2b14eb8-724d-4262-8ea5-7c23f846fed0-c000.json\n```"]