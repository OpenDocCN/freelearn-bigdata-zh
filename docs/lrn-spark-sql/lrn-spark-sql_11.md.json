["```scala\nscala> import org.apache.spark.sql._ \nscala> import org.apache.spark.sql.types._ \nscala> import org.apache.spark.sql.functions._ \nscala> import org.apache.spark.sql.streaming._ \nscala> import spark.implicits._ \nscala> import spark.sessionState.conf \nscala> import org.apache.spark.sql.internal.SQLConf.SHUFFLE_PARTITIONS \nscala> import org.apache.spark.sql.Encoders \nscala> import org.apache.spark.sql.catalyst.encoders.ExpressionEncoder \n```", "```scala\nscala> case class Bid(bidid: String, timestamp: String, ipinyouid: String, useragent: String, IP: String, region: Integer, cityID: Integer, adexchange: String, domain: String, turl: String, urlid: String, slotid: String, slotwidth: String, slotheight: String, slotvisibility: String, slotformat: String, slotprice: String, creative: String, bidprice: String) \n```", "```scala\nscala> val bidEncoder = Encoders.product[Bid] \n```", "```scala\nscala> bidEncoder.schema\n```", "```scala\nscala> val bidExprEncoder = bidEncoder.asInstanceOf[ExpressionEncoder[Bid]] \n```", "```scala\nscala> bidExprEncoder.serializer \n\nscala> bidExprEncoder.namedExpressions \n```", "```scala\nscala> val bidsDF = spark.read.format(\"csv\").schema(bidEncoder.schema).option(\"sep\", \"\\t\").option(\"header\", false).load(\"file:///Users/aurobindosarkar/Downloads/make-ipinyou-data-master/original-data/ipinyou.contest.dataset/bidfiles\") \n```", "```scala\nscala> bidsDF.take(1).foreach(println) \n\n[e3d962536ef3ac7096b31fdd1c1c24b0,20130311172101557,37a6259cc0c1dae299a7866489dff0bd,Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; QQDownload 734; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; eSobiSubscriber 2.0.4.16; MAAR),gzip(gfe),gzip(gfe),219.232.120.*,1,1,2,DF9blS9bQqsIFYB4uA5R,b6c5272dfc63032f659be9b786c5f8da,null,2006366309,728,90,1,0,5,5aca4c5f29e59e425c7ea657fdaac91e,300] \n```", "```scala\nscala> val bid = Bid(\"e3d962536ef3ac7096b31fdd1c1c24b0\",\"20130311172101557\",\"37a6259cc0c1dae299a7866489dff0bd\",\"Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 6.1; Trident/4.0; QQDownload 734; SLCC2; .NET CLR 2.0.50727; .NET CLR 3.5.30729; .NET CLR 3.0.30729; eSobiSubscriber 2.0.4.16; MAAR),gzip(gfe),gzip(gfe)\",\"219.232.120.*\",1,1,\"2\",\"\",\"DF9blS9bQqsIFYB4uA5R,b6c5272dfc63032f659be9b786c5f8da\",null,\"2006366309\",\"728\",\"90\",\"1\",\"0\",\"5\",\"5aca4c5f29e59e425c7ea657fdaac91e\",\"300\") \n```", "```scala\nscala> val row = bidExprEncoder.toRow(bid)  \n```", "```scala\nscala> import org.apache.spark.sql.catalyst.dsl.expressions._ \n\nscala> val attrs = Seq(DslSymbol('bidid).string, DslSymbol('timestamp).string, DslSymbol('ipinyouid).string, DslSymbol('useragent).string, DslSymbol('IP).string, DslSymbol('region).int, DslSymbol('cityID).int, DslSymbol('adexchange).string, DslSymbol('domain).string, DslSymbol('turl).string, DslSymbol('urlid).string, DslSymbol('slotid).string, DslSymbol('slotwidth).string, DslSymbol('slotheight).string, DslSymbol('slotvisibility).string, DslSymbol('slotformat).string, DslSymbol('slotprice).string, DslSymbol('creative).string, DslSymbol('bidprice).string) \n```", "```scala\nscala> val getBackBid = bidExprEncoder.resolveAndBind(attrs).fromRow(row) \n```", "```scala\nscala> bid == getBackBid \nres30: Boolean = true \n```", "```scala\nscala> val t1 = spark.range(7) \nscala> val t2 = spark.range(13) \nscala> val t3 = spark.range(19) \n\nscala> t1.explain() \n== Physical Plan == \n*Range (0, 7, step=1, splits=8) \n\nscala> t1.explain(extended=true) \n== Parsed Logical Plan == \nRange (0, 7, step=1, splits=Some(8)) \n\n== Analyzed Logical Plan == \nid: bigint \nRange (0, 7, step=1, splits=Some(8)) \n\n== Optimized Logical Plan == \nRange (0, 7, step=1, splits=Some(8)) \n\n== Physical Plan == \n*Range (0, 7, step=1, splits=8) \n\nscala> t1.filter(\"id != 0\").filter(\"id != 2\").explain(true) \n== Parsed Logical Plan == \n'Filter NOT ('id = 2) \n+- Filter NOT (id#0L = cast(0 as bigint)) \n   +- Range (0, 7, step=1, splits=Some(8)) \n\n== Analyzed Logical Plan == \nid: bigint \nFilter NOT (id#0L = cast(2 as bigint)) \n+- Filter NOT (id#0L = cast(0 as bigint)) \n   +- Range (0, 7, step=1, splits=Some(8)) \n\n== Optimized Logical Plan == \nFilter (NOT (id#0L = 0) && NOT (id#0L = 2)) \n+- Range (0, 7, step=1, splits=Some(8)) \n\n== Physical Plan == \n*Filter (NOT (id#0L = 0) && NOT (id#0L = 2)) \n+- *Range (0, 7, step=1, splits=8) \n```", "```scala\nscala> spark.sessionState.analyzer \nres30: org.apache.spark.sql.catalyst.analysis.Analyzer = org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1@21358f6c \n```", "```scala\nlog4j.logger.org.apache.spark.sql.hive.HiveSessionStateBuilder$$anon$1=DEBUG scala> val t1 = spark.range(7) \n17/07/13 10:25:38 DEBUG HiveSessionStateBuilder$$anon$1:  \n=== Result of Batch Resolution === \n!'DeserializeToObject unresolveddeserializer(staticinvoke(class java.lang.Long, ObjectType(class java.lang.Long), valueOf, upcast(getcolumnbyordinal(0, LongType), LongType, - root class: \"java.lang.Long\"), true)), obj#2: java.lang.Long   DeserializeToObject staticinvoke(class java.lang.Long, ObjectType(class java.lang.Long), valueOf, cast(id#0L as bigint), true), obj#2: java.lang.Long \n +- LocalRelation <empty>, [id#0L]                                                                                                                                                                                                            +- LocalRelation <empty>, [id#0L] \n\nt1: org.apache.spark.sql.Dataset[Long] = [id: bigint] \n```", "```scala\nscala> t1.filter(\"id != 0\").filter(\"id != 2\") \n17/07/13 10:43:17 DEBUG HiveSessionStateBuilder$$anon$1:  \n=== Result of Batch Resolution === \n!'Filter NOT ('id = 0)                      \nFilter NOT (id#0L = cast(0 as bigint)) \n +- Range (0, 7, step=1, splits=Some(8))    \n+- Range (0, 7, step=1, splits=Some(8)) \n... \n\n17/07/13 10:43:17 DEBUG HiveSessionStateBuilder$$anon$1:  \n=== Result of Batch Resolution === \n!'Filter NOT ('id = 2)                         \nFilter NOT (id#0L = cast(2 as bigint)) \n +- Filter NOT (id#0L = cast(0 as bigint))     \n   +- Filter NOT (id#0L = cast(0 as bigint)) \n    +- Range (0, 7, step=1, splits=Some(8))       \n   +- Range (0, 7, step=1, splits=Some(8)) \n```", "```scala\nscala> val t0 = spark.range(0, 10000000) \nscala> val df1 = t0.withColumn(\"uniform\", rand(seed=10)) \nscala> val df2 = t0.withColumn(\"normal\", randn(seed=27)) \nscala> df1.createOrReplaceTempView(\"t1\") \nscala> df2.createOrReplaceTempView(\"t2\") \n\nscala> spark.sql(\"SELECT sum(v) FROM (SELECT t1.id, 1 + t1.normal AS v FROM t1 JOIN t2 WHERE t1.id = t2.id AND t2.id > 5000000) tmp\").explain(true) \n```", "```scala\n== Parsed Logical Plan == \n'Project [unresolvedalias('sum('v), None)] ------------------> SELECT sum(v) \n+- 'SubqueryAlias tmp \n   +- 'Project ['t1.id, (1 + 't1.normal) AS v#79] ----------->       SELECT t1.id,  \n                                                               1 + t1.normal as v \n      +- 'Filter (('t1.id = 't2.id) && ('t2.id > 5000000))---> WHERE t1.id = t2.id,  \n                                                                    t2.id > 5000000 \n         +- 'Join Inner -------------------------------------> t1 JOIN t2 \n            :- 'UnresolvedRelation `t1` \n            +- 'UnresolvedRelation `t2` \n\n== Analyzed Logical Plan == \nsum(v): double \nAggregate [sum(v#79) AS sum(v)#86] \n+- SubqueryAlias tmp \n   +- Project [id#10L, (cast(1 as double) + normal#13) AS v#79] \n      +- Filter ((id#10L = id#51L) && (id#51L > cast(5000000 as bigint))) \n         +- Join Inner \n            :- SubqueryAlias t1 \n            :  +- Project [id#10L, randn(27) AS normal#13] \n            :     +- Range (0, 10000000, step=1, splits=Some(8)) \n            +- SubqueryAlias t2 \n               +- Project [id#51L, rand(10) AS uniform#54] \n                  +- Range (0, 10000000, step=1, splits=Some(8)) \n\n== Optimized Logical Plan == \nAggregate [sum(v#79) AS sum(v)#86] \n+- Project [(1.0 + normal#13) AS v#79] \n   +- Join Inner, (id#10L = id#51L) \n      :- Filter (id#10L > 5000000) \n      :  +- Project [id#10L, randn(27) AS normal#13] \n      :     +- Range (0, 10000000, step=1, splits=Some(8)) \n      +- Filter (id#51L > 5000000) \n         +- Range (0, 10000000, step=1, splits=Some(8)) \n\n== Physical Plan == \n*HashAggregate(keys=[], functions=[sum(v#79)], output=[sum(v)#86]) \n+- Exchange SinglePartition \n   +- *HashAggregate(keys=[], functions=[partial_sum(v#79)], output=[sum#88]) \n      +- *Project [(1.0 + normal#13) AS v#79] \n         +- *SortMergeJoin [id#10L], [id#51L], Inner \n            :- *Sort [id#10L ASC NULLS FIRST], false, 0 \n            :  +- Exchange hashpartitioning(id#10L, 200) \n            :     +- *Filter (id#10L > 5000000) \n            :        +- *Project [id#10L, randn(27) AS normal#13] \n            :           +- *Range (0, 10000000, step=1, splits=8) \n            +- *Sort [id#51L ASC NULLS FIRST], false, 0 \n               +- Exchange hashpartitioning(id#51L, 200) \n                  +- *Filter (id#51L > 5000000) \n                     +- *Range (0, 10000000, step=1, splits=8) \n```", "```scala\nscala> val t1 = spark.range(7) \nscala> val t2 = spark.range(13) \nscala> val t3 = spark.range(19) \nscala> val t4 = spark.range(1e8.toLong) \nscala> val t5 = spark.range(1e8.toLong) \nscala> val t6 = spark.range(1e3.toLong)  \n```", "```scala\nscala> val query = t1.join(t2).where(t1(\"id\") === t2(\"id\")).join(t3).where(t3(\"id\") === t1(\"id\")).explain() \n== Physical Plan == \n*BroadcastHashJoin [id#6L], [id#12L], Inner, BuildRight \n:- *BroadcastHashJoin [id#6L], [id#9L], Inner, BuildRight \n:  :- *Range (0, 7, step=1, splits=8) \n:  +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false])) \n:     +- *Range (0, 13, step=1, splits=8) \n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false])) \n   +- *Range (0, 19, step=1, splits=8) \nquery: Unit = () \n\nscala> val query = t1.join(t2).where(t1(\"id\") === t2(\"id\")).join(t3).where(t3(\"id\") === t1(\"id\")).count() \nquery: Long = 7 \n```", "```scala\nscala> val query = t4.join(t5).where(t4(\"id\") === t5(\"id\")).join(t6).where(t4(\"id\") === t6(\"id\")).explain() \n== Physical Plan == \n*BroadcastHashJoin [id#72L], [id#78L], Inner, BuildRight \n:- *SortMergeJoin [id#72L], [id#75L], Inner \n:  :- *Sort [id#72L ASC NULLS FIRST], false, 0 \n:  :  +- Exchange hashpartitioning(id#72L, 200) \n:  :     +- *Range (0, 100000000, step=1, splits=8) \n:  +- *Sort [id#75L ASC NULLS FIRST], false, 0 \n:     +- ReusedExchange [id#75L], Exchange hashpartitioning(id#72L, 200) \n+- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false])) \n   +- *Range (0, 1000, step=1, splits=8) \nquery: Unit = () \n```", "```scala\nANALYZE TABLE table_name COMPUTE STATISTICS \n```", "```scala\nANALYZE TABLE table_name COMPUTE STATISTICS FOR COLUMNS column-name1, column-name2, .... \n```", "```scala\nDESCRIBE EXTENDED table_name \n```", "```scala\nscala> sql(\"DESCRIBE EXTENDED customers\").collect.foreach(println) \n[# col_name,data_type,comment] \n[id,bigint,null] \n[name,string,null] \n[,,] \n[# Detailed Table Information,,] \n[Database,default,] \n[Table,customers,] \n[Owner,aurobindosarkar,] \n[Created,Sun Jul 09 23:16:38 IST 2017,] \n[Last Access,Thu Jan 01 05:30:00 IST 1970,] \n[Type,MANAGED,] \n[Provider,parquet,] \n[Properties,[serialization.format=1],] \n[Statistics,1728063103 bytes, 200000000 rows,] \n[Location,file:/Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7/spark-warehouse/customers,] \n[Serde Library,org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe,] \n[InputFormat,org.apache.hadoop.hive.ql.io.parquet.MapredParquetInputFormat,] \n[OutputFormat,org.apache.hadoop.hive.ql.io.parquet.MapredParquetOutputFormat,] \n```", "```scala\nEXPLAIN COST SELECT * FROM table_name WHERE condition \n```", "```scala\nscala> spark.sql(\"DROP TABLE IF EXISTS t1\") \nscala> spark.sql(\"DROP TABLE IF EXISTS t2\") \nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS t1(id long, value long) USING parquet\") \nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS t2(id long, value string) USING parquet\") \n\nscala> spark.range(5E8.toLong).select('id, (rand(17) * 1E6) cast \"long\").write.mode(\"overwrite\").insertInto(\"t1\") \nscala> spark.range(1E8.toLong).select('id, 'id cast \"string\").write.mode(\"overwrite\").insertInto(\"t2\") \n\nscala> sql(\"SELECT t1.id FROM t1, t2 WHERE t1.id = t2.id AND t1.value = 100\").explain() \n== Physical Plan == \n*Project [id#79L] \n+- *SortMergeJoin [id#79L], [id#81L], Inner \n   :- *Sort [id#79L ASC NULLS FIRST], false, 0 \n   :  +- Exchange hashpartitioning(id#79L, 200) \n   :     +- *Project [id#79L] \n   :        +- *Filter ((isnotnull(value#80L) && (value#80L = 100)) && isnotnull(id#79L)) \n   :           +- *FileScan parquet default.t1[id#79L,value#80L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7/spark-warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(value), EqualTo(value,100), IsNotNull(id)], ReadSchema: struct<id:bigint,value:bigint> \n   +- *Sort [id#81L ASC NULLS FIRST], false, 0 \n      +- Exchange hashpartitioning(id#81L, 200) \n         +- *Project [id#81L] \n            +- *Filter isnotnull(id#81L) \n               +- *FileScan parquet default.t2[id#81L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7/spark-warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint> \n```", "```scala\nscala> sql(\"CREATE TABLE IF NOT EXISTS customers(id long, name string) USING parquet\") \nscala> sql(\"CREATE TABLE IF NOT EXISTS goods(id long, price long) USING parquet\") \nscala> sql(\"CREATE TABLE IF NOT EXISTS orders(customer_id long, good_id long) USING parquet\") \n\nscala> import org.apache.spark.sql.functions.rand \n\nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS customers(id long, name string) USING parquet\") \nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS goods(id long, price long) USING parquet\") \nscala> spark.sql(\"CREATE TABLE IF NOT EXISTS orders(customer_id long, good_id long) USING parquet\") \n\nscala> spark.range(2E8.toLong).select('id, 'id cast \"string\").write.mode(\"overwrite\").insertInto(\"customers\") \n\nscala> spark.range(1E8.toLong).select('id, (rand(17) * 1E6 + 2) cast \"long\").write.mode(\"overwrite\").insertInto(\"goods\") \nspark.range(1E7.toLong).select(rand(3) * 2E8 cast \"long\", (rand(5) * 1E8) cast \"long\").write.mode(\"overwrite\").insertInto(\"orders\") \n```", "```scala\nscala> def benchmark(name: String)(f: => Unit) { \n     |      val startTime = System.nanoTime \n     |      f \n     |      val endTime = System.nanoTime \n     |      println(s\"Time taken with $name: \" + (endTime - \n                    startTime).toDouble / 1000000000 + \" seconds\") \n     | } \n\n```", "```scala\n\nscala> val conf = spark.sessionState.conf \n\nscala> spark.conf.set(\"spark.sql.cbo.enabled\", false) \n\nscala> conf.cboEnabled \nres1: Boolean = false \n\nscala> conf.joinReorderEnabled \nres2: Boolean = false \n\nscala> benchmark(\"CBO OFF & JOIN REORDER DISABLED\"){ sql(\"SELECT name FROM customers, orders, goods WHERE customers.id = orders.customer_id AND orders.good_id = goods.id AND goods.price > 1000000\").show() } \n```", "```scala\nscala> spark.conf.set(\"spark.sql.cbo.enabled\", true) \nscala> conf.cboEnabled \nres11: Boolean = true \nscala> conf.joinReorderEnabled \nres12: Boolean = false \n\nscala> benchmark(\"CBO ON & JOIN REORDER DIABLED\"){ sql(\"SELECT name FROM customers, orders, goods WHERE customers.id = orders.customer_id AND orders.good_id = goods.id AND goods.price > 1000000\").show()} \n```", "```scala\nscala> spark.conf.set(\"spark.sql.cbo.enabled\", true) \nscala> spark.conf.set(\"spark.sql.cbo.joinReorder.enabled\", true) \nscala> conf.cboEnabled \nres2: Boolean = true \nscala> conf.joinReorderEnabled \nres3: Boolean = true \n\nscala> benchmark(\"CBO ON & JOIN REORDER ENABLED\"){ sql(\"SELECT name FROM customers, orders, goods WHERE customers.id = orders.customer_id AND orders.good_id = goods.id AND goods.price > 1000000\").show()} \n```", "```scala\nscala> sql(\"select count(*) from orders where customer_id = 26333955\").explain() \n\n== Optimized Logical Plan == \nAggregate [count(1) AS count(1)#45L] \n+- Project \n   +- Filter (isnotnull(customer_id#42L) && (customer_id#42L = \n              26333955)) \n      +- Relation[customer_id#42L,good_id#43L] parquet \n```", "```scala\nlong count = 0; \nfor (customer_id in orders) {  \n   if (customer_id == 26333955) { \n         count += 1; \n   } \n} \n```", "```scala\nscala> sql(\"EXPLAIN CODEGEN SELECT name FROM customers, orders, goods WHERE customers.id = orders.customer_id AND orders.good_id = goods.id AND goods.price > 1000000\").take(1).foreach(println) \n[Found 6 WholeStageCodegen subtrees.                                             \n== Subtree 1 / 6 == \n*Project [id#11738L] \n+- *Filter ((isnotnull(price#11739L) && (price#11739L > 1000000)) && isnotnull(id#11738L)) \n   +- *FileScan parquet default.goods[id#11738L,price#11739L] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7/spark-warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(price), GreaterThan(price,1000000), IsNotNull(id)], ReadSchema: struct<id:bigint,price:bigint> \n\nGenerated code: \n/* 001 */ public Object generate(Object[] references) { \n/* 002 */   return new GeneratedIterator(references); \n/* 003 */ } \n... \n== Subtree 6 / 6 == \n*Sort [id#11734L ASC NULLS FIRST], false, 0 \n+- Exchange hashpartitioning(id#11734L, 200) \n   +- *Project [id#11734L, name#11735] \n      +- *Filter isnotnull(id#11734L) \n         +- *FileScan parquet default.customers[id#11734L,name#11735] Batched: true, Format: Parquet, Location: InMemoryFileIndex[file:/Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7/spark-warehouse..., PartitionFilters: [], PushedFilters: [IsNotNull(id)], ReadSchema: struct<id:bigint,name:string> \n\nGenerated code: \n/* 001 */ public Object generate(Object[] references) { \n/* 002 */   return new GeneratedIterator(references); \n/* 003 */ } \n... \n] \n```", "```scala\nscala> spark.conf.set(\"spark.sql.codegen.wholeStage\", false) \n\nscala> conf.wholeStageEnabled \nres77: Boolean = false \n\nscala> val N = 20 << 20 \nN: Int = 20971520 \n\nscala> val M = 1 << 16 \nM: Int = 65536 \n\nscala> val dim = broadcast(spark.range(M).selectExpr(\"id as k\", \"cast(id as string) as v\")) \n\nscala> benchmark(\"Join w long\") { \n     |   spark.range(N).join(dim, (col(\"id\") % M) === col(\"k\")).count() \n     | } \nTime taken in Join w long: 2.612163207 seconds                                   \n\nscala> spark.conf.set(\"spark.sql.codegen.wholeStage\", true) \n\nscala> conf.wholeStageEnabled \nres80: Boolean = true \n\nscala> val dim = broadcast(spark.range(M).selectExpr(\"id as k\", \"cast(id as string) as v\")) \n\nscala> benchmark(\"Join w long\") { \n     |   spark.range(N).join(dim, (col(\"id\") % M) === col(\"k\")).count() \n     | } \nTime taken in Join w long: 0.777796256 seconds \n```", "```scala\nscala> val dim = broadcast(spark.range(M).selectExpr(\"id as k\", \"cast(id as string) as v\")) \nscala> benchmark(\"Join w long duplicated\") { \n     |     val dim = broadcast(spark.range(M).selectExpr(\"cast(id/10 as long) as k\")) \n     |     spark.range(N).join(dim, (col(\"id\") % M) === col(\"k\")).count() \n     | } \nTime taken in Join w long duplicated: 1.514799811 seconds           \nTime taken in Join w long duplicated: 0.278705816 seconds \n\nscala> val dim3 = broadcast(spark.range(M).selectExpr(\"id as k1\", \"id as k2\", \"cast(id as string) as v\")) \nscala> benchmark(\"Join w 2 longs\") { \n     |     spark.range(N).join(dim3, (col(\"id\") % M) === col(\"k1\") && (col(\"id\") % M) === col(\"k2\")).count() \n     | } \nTime taken in Join w 2 longs: 2.048950962 seconds       \nTime taken in Join w 2 longs: 0.681936701 seconds \n\nscala> val dim4 = broadcast(spark.range(M).selectExpr(\"cast(id/10 as long) as k1\", \"cast(id/10 as long) as k2\")) \nscala> benchmark(\"Join w 2 longs duplicated\") { \n     |     spark.range(N).join(dim4, (col(\"id\") bitwiseAND M) === col(\"k1\") && (col(\"id\") bitwiseAND M) === col(\"k2\")).count() \n     | } \nTime taken in Join w 2 longs duplicated: 4.924196601 seconds      \nTime taken in Join w 2 longs duplicated: 0.818748429 seconds      \n\nscala> val dim = broadcast(spark.range(M).selectExpr(\"id as k\", \"cast(id as string) as v\")) \nscala> benchmark(\"outer join w long\") { \n     |     spark.range(N).join(dim, (col(\"id\") % M) === col(\"k\"), \"left\").count() \n     | } \nTime taken in outer join w long: 1.580664228 seconds        \nTime taken in outer join w long: 0.280608235 seconds \n\nscala> val dim = broadcast(spark.range(M).selectExpr(\"id as k\", \"cast(id as string) as v\")) \nscala> benchmark(\"semi join w long\") { \n     |     spark.range(N).join(dim, (col(\"id\") % M) === col(\"k\"), \"leftsemi\").count() \n     | } \nTime taken in semi join w long: 1.027175143 seconds             \nTime taken in semi join w long: 0.180771478 seconds \n\nscala> val N = 2 << 20 \nN: Int = 2097152 \nscala> benchmark(\"merge join\") { \n     |     val df1 = spark.range(N).selectExpr(s\"id * 2 as k1\") \n     |     val df2 = spark.range(N).selectExpr(s\"id * 3 as k2\") \n     |     df1.join(df2, col(\"k1\") === col(\"k2\")).count() \n     | } \nTime taken in merge join: 2.260524298 seconds          \nTime taken in merge join: 2.053497825 seconds             \n\nscala> val N = 2 << 20 \nN: Int = 2097152 \nscala> benchmark(\"sort merge join\") { \n     |     val df1 = spark.range(N).selectExpr(s\"(id * 15485863) % ${N*10} as k1\") \n     |     val df2 = spark.range(N).selectExpr(s\"(id * 15485867) % ${N*10} as k2\") \n     |     df1.join(df2, col(\"k1\") === col(\"k2\")).count() \n     | } \nTime taken in sort merge join: 2.481585466 seconds                \nTime taken in sort merge join: 1.992168281 seconds                \n```", "```scala\nscala> conf.getAllConfs.foreach(println) \n(spark.driver.host,192.168.1.103) \n(spark.sql.autoBroadcastJoinThreshold,1000000) \n(spark.driver.port,57085) \n(spark.repl.class.uri,spark://192.168.1.103:57085/classes) \n(spark.jars,) \n(spark.repl.class.outputDir,/private/var/folders/tj/prwqrjj16jn4k5jh6g91rwtc0000gn/T/spark-9f8b5ba4-e8f4-4c60-b01b-30c4b71a06e1/repl-ae75dedc-703a-41b8-b949-b91ed3b362f1) \n(spark.app.name,Spark shell) \n(spark.driver.memory,14g) \n(spark.sql.codegen.wholeStage,true) \n(spark.executor.id,driver) \n(spark.sql.cbo.enabled,true) \n(spark.sql.join.preferSortMergeJoin,false) \n(spark.submit.deployMode,client) \n(spark.master,local[*]) \n(spark.home,/Users/aurobindosarkar/Downloads/spark-2.2.0-bin-hadoop2.7) \n(spark.sql.catalogImplementation,hive) \n(spark.app.id,local-1499953390374) \n(spark.sql.shuffle.partitions,2) \n```", "```scala\nscala> conf.getAllDefinedConfs.foreach(println) \n```"]