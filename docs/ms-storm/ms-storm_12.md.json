["```scala\n   <dependencies> \n         <dependency> \n               <groupId>org.apache.kafka</groupId> \n               <artifactId>kafka_2.10</artifactId> \n               <version>0.9.0.1</version> \n               <exclusions> \n                     <exclusion> \n                           <groupId>com.sun.jdmk</groupId> \n                           <artifactId>jmxtools</artifactId> \n                     </exclusion> \n                     <exclusion> \n                           <groupId>com.sun.jmx</groupId> \n                           <artifactId>jmxri</artifactId> \n                     </exclusion> \n               </exclusions> \n         </dependency> \n         <dependency> \n               <groupId>org.apache.logging.log4j</groupId> \n               <artifactId>log4j-slf4j-impl</artifactId> \n               <version>2.0-beta9</version> \n         </dependency> \n         <dependency> \n               <groupId>org.apache.logging.log4j</groupId> \n               <artifactId>log4j-1.2-api</artifactId> \n               <version>2.0-beta9</version> \n         </dependency> \n\n         <!-- https://mvnrepository.com/artifact/org.twitter4j/twitter4j-stream --> \n         <dependency> \n               <groupId>org.twitter4j</groupId> \n               <artifactId>twitter4j-stream</artifactId> \n               <version>4.0.6</version> \n         </dependency> \n\n   </dependencies> \n```", "```scala\npublic class TwitterData { \n\n   /** The actual Twitter stream. It's set up to collect raw JSON data */ \n   private TwitterStream twitterStream; \n   static String consumerKeyStr = \"r1wFskT3q\"; \n   static String consumerSecretStr = \"fBbmp71HKbqalpizIwwwkBpKC\"; \n   static String accessTokenStr = \"298FPfE16frABXMcRIn7aUSSnNneMEPrUuZ\"; \n   static String accessTokenSecretStr = \"1LMNZZIfrAimpD004QilV1pH3PYTvM\"; \n\n   public void start() { \n         ConfigurationBuilder cb = new ConfigurationBuilder(); \n         cb.setOAuthConsumerKey(consumerKeyStr); \n         cb.setOAuthConsumerSecret(consumerSecretStr); \n         cb.setOAuthAccessToken(accessTokenStr); \n         cb.setOAuthAccessTokenSecret(accessTokenSecretStr); \n         cb.setJSONStoreEnabled(true); \n         cb.setIncludeEntitiesEnabled(true); \n         // instance of TwitterStreamFactory \n         twitterStream = new TwitterStreamFactory(cb.build()).getInstance(); \n\n         final Producer<String, String> producer = new KafkaProducer<String, String>( \n                     getProducerConfig()); \n         // topicDetails \n         // new CreateTopic(\"127.0.0.1:2181\").createTopic(\"twitterData\", 2, 1); \n\n         /** Twitter listener **/ \n         StatusListener listener = new StatusListener() { \n               public void onStatus(Status status) { \n                     ProducerRecord<String, String> data = new ProducerRecord<String, String>( \n                                 \"twitterData\", DataObjectFactory.getRawJSON(status)); \n                     // send the data to kafka \n                     producer.send(data); \n               } \n\n               public void onException(Exception arg0) { \n                     System.out.println(arg0); \n               } \n\n               public void onDeletionNotice(StatusDeletionNotice arg0) { \n               } \n\n               public void onScrubGeo(long arg0, long arg1) { \n               } \n\n               public void onStallWarning(StallWarning arg0) { \n               } \n\n               public void onTrackLimitationNotice(int arg0) { \n               } \n         }; \n\n         /** Bind the listener **/ \n         twitterStream.addListener(listener); \n\n         /** GOGOGO **/ \n         twitterStream.sample(); \n   } \n\n   private Properties getProducerConfig() { \n\n         Properties props = new Properties(); \n\n         // List of kafka borkers. Complete list of brokers is not required as \n         // the producer will auto discover the rest of the brokers. \n         props.put(\"bootstrap.servers\", \"localhost:9092\"); \n         props.put(\"batch.size\", 1); \n         // Serializer used for sending data to kafka. Since we are sending \n         // string, \n         // we are using StringSerializer. \n         props.put(\"key.serializer\", \n                     \"org.apache.kafka.common.serialization.StringSerializer\"); \n         props.put(\"value.serializer\", \n                     \"org.apache.kafka.common.serialization.StringSerializer\"); \n\n         props.put(\"producer.type\", \"sync\"); \n\n         return props; \n\n   } \n\n   public static void main(String[] args) throws InterruptedException { \n         new TwitterData().start(); \n   } \n```", "```scala\n   <dependencies> \n         <dependency> \n               <groupId>org.codehaus.jackson</groupId> \n               <artifactId>jackson-mapper-asl</artifactId> \n               <version>1.9.13</version> \n         </dependency> \n\n         <dependency> \n               <groupId>org.apache.hadoop</groupId> \n               <artifactId>hadoop-client</artifactId> \n               <version>2.2.0</version> \n               <exclusions> \n                     <exclusion> \n                           <groupId>org.slf4j</groupId> \n                           <artifactId>slf4j-log4j12</artifactId> \n                     </exclusion> \n               </exclusions> \n         </dependency> \n         <dependency> \n               <groupId>org.apache.hadoop</groupId> \n               <artifactId>hadoop-hdfs</artifactId> \n               <version>2.2.0</version> \n               <exclusions> \n                     <exclusion> \n                           <groupId>org.slf4j</groupId> \n                           <artifactId>slf4j-log4j12</artifactId> \n                     </exclusion> \n               </exclusions> \n         </dependency> \n         <!-- Dependency for Storm-Kafka spout --> \n         <dependency> \n               <groupId>org.apache.storm</groupId> \n               <artifactId>storm-kafka</artifactId> \n               <version>1.0.2</version> \n               <exclusions> \n                     <exclusion> \n                           <groupId>org.apache.kafka</groupId> \n                           <artifactId>kafka-clients</artifactId> \n                     </exclusion> \n               </exclusions> \n         </dependency> \n\n         <dependency> \n               <groupId>org.apache.kafka</groupId> \n               <artifactId>kafka_2.10</artifactId> \n               <version>0.9.0.1</version> \n               <exclusions> \n                     <exclusion> \n                           <groupId>com.sun.jdmk</groupId> \n                           <artifactId>jmxtools</artifactId> \n                     </exclusion> \n                     <exclusion> \n                           <groupId>com.sun.jmx</groupId> \n                           <artifactId>jmxri</artifactId> \n                     </exclusion> \n               </exclusions> \n         </dependency> \n\n         <dependency> \n               <groupId>org.apache.storm</groupId> \n               <artifactId>storm-core</artifactId> \n               <version>1.0.2</version> \n               <scope>provided</scope> \n         </dependency> \n   </dependencies> \n   <repositories> \n         <repository> \n               <id>clojars.org</id> \n               <url>http://clojars.org/repo</url> \n         </repository> \n   </repositories> \n```", "```scala\nBrokerHosts zkHosts = new ZkHosts(\"localhost:2181\"); \n\n       // Create the KafkaSpout configuartion \n       // Second argument is the topic name \n       // Third argument is the zookeeper root for Kafka \n       // Fourth argument is consumer group id \n       SpoutConfig kafkaConfig = new SpoutConfig(zkHosts, \"twitterData\", \"\", \n                   \"id7\"); \n\n       // Specify that the kafka messages are String \n       kafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme()); \n\n       // We want to consume all the first messages in the topic everytime \n       // we run the topology to help in debugging. In production, this \n       // property should be false \n       kafkaConfig.startOffsetTime = kafka.api.OffsetRequest \n                   .EarliestTime(); \n\n       // Now we create the topology \n       TopologyBuilder builder = new TopologyBuilder(); \n\n       // set the kafka spout class \n       builder.setSpout(\"KafkaSpout\", new KafkaSpout(kafkaConfig), 1); \n```", "```scala\npublic class JSONParsingBolt extends BaseRichBolt implements Serializable{ \n\n   private OutputCollector collector; \n\n   public void prepare(Map stormConf, TopologyContext context, \n               OutputCollector collector) { \n         this.collector = collector; \n\n   } \n\n   public void execute(Tuple input) { \n         try { \n               String tweet = input.getString(0); \n               Map<String, Object> map = new ObjectMapper().readValue(tweet, Map.class); \n               collector.emit(\"stream1\",new Values(tweet)); \n               collector.emit(\"stream2\",new Values(map.get(\"text\"))); \n               this.collector.ack(input); \n         } catch (Exception exception) { \n               exception.printStackTrace(); \n               this.collector.fail(input); \n         } \n   } \n\n   public void declareOutputFields(OutputFieldsDeclarer declarer) { \n         declarer.declareStream(\"stream1\",new Fields(\"tweet\")); \n         declarer.declareStream(\"stream2\",new Fields(\"text\")); \n   } \n\n} \n```", "```scala\npublic final class SentimentBolt extends BaseRichBolt { \n   private static final Logger LOGGER = LoggerFactory \n               .getLogger(SentimentBolt.class); \n   private static final long serialVersionUID = -5094673458112825122L; \n   private OutputCollector collector; \n   private String path; \n   public SentimentBolt(String path) { \n         this.path = path; \n   } \n   private Map<String, Integer> afinnSentimentMap = new HashMap<String, Integer>(); \n\n   public final void prepare(final Map map, \n               final TopologyContext topologyContext, \n               final OutputCollector collector) { \n         this.collector = collector; \n         // Bolt will read the AFINN Sentiment file [which is in the classpath] \n         // and stores the key, value pairs to a Map. \n         try { \n               BufferedReader br = new BufferedReader(new FileReader(path)); \n               String line; \n               while ((line = br.readLine()) != null) { \n                     String[] tabSplit = line.split(\"\\t\"); \n                     afinnSentimentMap.put(tabSplit[0], \n                                 Integer.parseInt(tabSplit[1])); \n               } \n               br.close(); \n\n         } catch (final IOException ioException) { \n               LOGGER.error(ioException.getMessage(), ioException); \n               ioException.printStackTrace(); \n               System.exit(1); \n         } \n\n   }\n```", "```scala\n   public final void declareOutputFields( \n               final OutputFieldsDeclarer outputFieldsDeclarer) { \n         outputFieldsDeclarer.declare(new Fields(\"tweet\",\"sentiment\")); \n   } \n\n   public final void execute(final Tuple input) { \n         try { \n         final String tweet = (String) input.getValueByField(\"text\"); \n         final int sentimentCurrentTweet = getSentimentOfTweet(tweet); \n         collector.emit(new Values(tweet,sentimentCurrentTweet)); \n         this.collector.ack(input); \n         }catch(Exception exception) { \n               exception.printStackTrace(); \n               this.collector.fail(input); \n         } \n   } \n\n   /** \n    * Gets the sentiment of the current tweet. \n    * \n    * @param status \n    *            -- Status Object. \n    * @return sentiment of the current tweet. \n    */ \n   private final int getSentimentOfTweet(final String text) { \n         // Remove all punctuation and new line chars in the tweet. \n         final String tweet = text.replaceAll(\"\\\\p{Punct}|\\\\n\", \" \") \n                     .toLowerCase(); \n         // Splitting the tweet on empty space. \n         final Iterable<String> words = Splitter.on(' ').trimResults() \n                     .omitEmptyStrings().split(tweet); \n         int sentimentOfCurrentTweet = 0; \n         // Loop thru all the wordsd and find the sentiment of this tweet. \n         for (final String word : words) { \n               if (afinnSentimentMap.containsKey(word)) { \n                     sentimentOfCurrentTweet += afinnSentimentMap.get(word); \n               } \n         } \n         LOGGER.debug(\"Tweet : Sentiment {} ==> {}\", tweet, \n                     sentimentOfCurrentTweet); \n         return sentimentOfCurrentTweet; \n   } \n\n} \n```", "```scala\n// use \"|\" instead of \",\" for field delimiter \n         RecordFormat format = new DelimitedRecordFormat() \n                     .withFieldDelimiter(\",\"); \n\n         // sync the filesystem after every 1k tuples \n         SyncPolicy syncPolicy = new CountSyncPolicy(1000); \n\n         // rotate files when they reach 5MB \n         FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, \n                     Units.MB); \n\n         FileNameFormat fileNameFormatSentiment = new DefaultFileNameFormat() \n         .withPath(\"/sentiment-tweet/\"); \n\n         HdfsBolt hdfsBolt2 = new HdfsBolt().withFsUrl(\"hdfs://127.0.0.1:8020\") \n                     .withFileNameFormat(fileNameFormatSentiment).withRecordFormat(format) \n                     .withRotationPolicy(rotationPolicy).withSyncPolicy(syncPolicy); \n\n         //builder.setBolt(\"HDFSBolt\", hdfsBolt).shuffleGrouping(\"KafkaSpout\"); \n         builder.setBolt(\"json\", new JSONParsingBolt()).shuffleGrouping(\"KafkaSpout\"); \n\n         // \n         builder.setBolt(\"sentiment\", new SentimentBolt(\"/home/centos/Desktop/workspace/storm_twitter/src/main/resources/AFINN-111.txt\")).shuffleGrouping(\"json\",\"stream2\"); \n\n         // \n         builder.setBolt(\"HDFS2\", hdfsBolt2).shuffleGrouping(\"sentiment\"); \n```", "```scala\npublic class StormHDFSTopology { \n\n   public static void main(String[] args) { \n         // zookeeper hosts for the Kafka cluster \n         BrokerHosts zkHosts = new ZkHosts(\"localhost:2181\"); \n\n         // Create the KafkaSpout configuartion \n         // Second argument is the topic name \n         // Third argument is the zookeeper root for Kafka \n         // Fourth argument is consumer group id \n         SpoutConfig kafkaConfig = new SpoutConfig(zkHosts, \"twitterData\", \"\", \n                     \"id7\"); \n\n         // Specify that the kafka messages are String \n         kafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme()); \n\n         // We want to consume all the first messages in the topic everytime \n         // we run the topology to help in debugging. In production, this \n         // property should be false \n         kafkaConfig.startOffsetTime = kafka.api.OffsetRequest \n                     .EarliestTime(); \n\n         // Now we create the topology \n         TopologyBuilder builder = new TopologyBuilder(); \n\n         // set the kafka spout class \n         builder.setSpout(\"KafkaSpout\", new KafkaSpout(kafkaConfig), 1); \n\n         // use \"|\" instead of \",\" for field delimiter \n         RecordFormat format = new DelimitedRecordFormat() \n                     .withFieldDelimiter(\",\"); \n\n         // sync the filesystem after every 1k tuples \n         SyncPolicy syncPolicy = new CountSyncPolicy(1000); \n\n         // rotate files when they reach 5MB \n         FileRotationPolicy rotationPolicy = new FileSizeRotationPolicy(5.0f, \n                     Units.MB); \n\n         FileNameFormat fileNameFormatSentiment = new DefaultFileNameFormat() \n         .withPath(\"/sentiment-tweet/\"); \n\n         HdfsBolt hdfsBolt2 = new HdfsBolt().withFsUrl(\"hdfs://127.0.0.1:8020\") \n                     .withFileNameFormat(fileNameFormatSentiment).withRecordFormat(format) \n                     .withRotationPolicy(rotationPolicy).withSyncPolicy(syncPolicy); \n\n         //builder.setBolt(\"HDFSBolt\", hdfsBolt).shuffleGrouping(\"KafkaSpout\"); \n         builder.setBolt(\"json\", new JSONParsingBolt()).shuffleGrouping(\"KafkaSpout\"); \n\n         // \n         builder.setBolt(\"sentiment\", new SentimentBolt(\"/home/centos/Desktop/workspace/storm_twitter/src/main/resources/AFINN-111.txt\")).shuffleGrouping(\"json\",\"stream2\"); \n\n         // \n         builder.setBolt(\"HDFS2\", hdfsBolt2).shuffleGrouping(\"sentiment\"); \n\n         // create an instance of LocalCluster class for executing topology in \n         // local mode. \n         LocalCluster cluster = new LocalCluster(); \n         Config conf = new Config(); \n\n         // Submit topology for execution \n         cluster.submitTopology(\"KafkaToplogy\", conf, builder.createTopology()); \n\n         try { \n               // Wait for some time before exiting \n               System.out.println(\"Waiting to consume from kafka\"); \n               Thread.sleep(6000000); \n         } catch (Exception exception) { \n               System.out.println(\"Thread interrupted exception : \" + exception); \n         } \n\n         // kill the KafkaTopology \n         cluster.killTopology(\"KafkaToplogy\"); \n\n         // shut down the storm test cluster \n         cluster.shutdown(); \n\n   } \n} \n```"]