["```scala\n    <repositories> \n        <repository> \n            <id>clojars.org</id> \n            <url>http://clojars.org/repo</url> \n        </repository> \n    </repositories> \n    <dependencies> \n        <dependency> \n            <groupId>org.apache.storm</groupId> \n            <artifactId>storm-core</artifactId> \n            <version>1.0.2</version> \n            <scope>provided</scope> \n        </dependency> \n        <dependency> \n            <groupId>org.apache.hadoop</groupId> \n            <artifactId>hadoop-core</artifactId> \n            <version>1.1.1</version> \n        </dependency> \n        <dependency> \n            <groupId>org.slf4j</groupId> \n            <artifactId>slf4j-api</artifactId> \n            <version>1.7.7</version> \n        </dependency> \n\n        <dependency> \n            <groupId>org.apache.hbase</groupId> \n            <artifactId>hbase</artifactId> \n            <version>0.94.5</version> \n            <exclusions> \n                <exclusion> \n                    <artifactId>zookeeper</artifactId> \n                    <groupId>org.apache.zookeeper</groupId> \n                </exclusion> \n\n            </exclusions> \n        </dependency> \n\n        <dependency> \n            <groupId>junit</groupId> \n            <artifactId>junit</artifactId> \n            <version>4.10</version> \n        </dependency> \n    </dependencies> \n    <build> \n        <plugins> \n            <plugin> \n                <groupId>org.apache.maven.plugins</groupId> \n                <artifactId>maven-compiler-plugin</artifactId> \n                <version>2.5.1</version> \n                <configuration> \n                    <source>1.6</source> \n                    <target>1.6</target> \n                </configuration> \n            </plugin> \n            <plugin> \n                <artifactId>maven-assembly-plugin</artifactId> \n                <version>2.2.1</version> \n                <configuration> \n                    <descriptorRefs> \n                        <descriptorRef>jar-\n                        with-dependencies</descriptorRef> \n                    </descriptorRefs> \n                    <archive> \n                        <manifest> \n                            <mainClass /> \n                        </manifest> \n                    </archive> \n                </configuration> \n                <executions> \n                    <execution> \n                        <id>make-assembly</id> \n                        <phase>package</phase> \n                        <goals> \n                            <goal>single</goal> \n                        </goals> \n                    </execution> \n                </executions> \n            </plugin> \n        </plugins> \n    </build> \n```", "```scala\n{  \n\n  \"columnfamily1\":  \n  {  \n    \"column1\":\"abc\",  \n    \"column2\":\"pqr\"  \n  },  \n  \"columnfamily2\":  \n  {  \n    \"column3\":\"bc\",  \n    \"column4\":\"jkl\"  \n  }  \n}  \n```", "```scala\npublic class HBaseOperations implements Serializable{ \n\n    private static final long serialVersionUID = 1L; \n\n    // Instance of Hadoop Cofiguration class \n    Configuration conf = new Configuration(); \n\n    HTable hTable = null; \n\n    public HBaseOperations(String tableName, List<String> ColumnFamilies, \n            List<String> zookeeperIPs, int zkPort) { \n        conf = HBaseConfiguration.create(); \n        StringBuffer zookeeperIP = new StringBuffer(); \n        // Set the zookeeper nodes \n        for (String zookeeper : zookeeperIPs) { \n            zookeeperIP.append(zookeeper).append(\",\"); \n        } \n        zookeeperIP.deleteCharAt(zookeeperIP.length() - 1); \n\n        conf.set(\"hbase.zookeeper.quorum\", zookeeperIP.toString()); \n\n        // Set the zookeeper client port \n        conf.setInt(\"hbase.zookeeper.property.clientPort\", zkPort); \n        // call the createTable method to create a table into HBase. \n        createTable(tableName, ColumnFamilies); \n        try { \n            // initilaize the HTable.  \n            hTable = new HTable(conf, tableName); \n        } catch (IOException e) { \n            throw new RuntimeException(\"Error occure while creating instance of HTable class : \" + e); \n        } \n    } \n\n    /** \n     * This method create a table into HBase \n     *  \n     * @param tableName \n     *            Name of the HBase table \n     * @param ColumnFamilies \n     *            List of column famallies \n     *  \n     */ \n    public void createTable(String tableName, List<String> ColumnFamilies) { \n        HBaseAdmin admin = null; \n        try { \n            admin = new HBaseAdmin(conf); \n            // Set the input table in HTableDescriptor \n            HTableDescriptor tableDescriptor = new HTableDescriptor( \n                    Bytes.toBytes(tableName)); \n            for (String columnFamaliy : ColumnFamilies) { \n                HColumnDescriptor columnDescriptor = new HColumnDescriptor( \n                        columnFamaliy); \n                // add all the HColumnDescriptor into HTableDescriptor \n                tableDescriptor.addFamily(columnDescriptor); \n            } \n            /* execute the creaetTable(HTableDescriptor tableDescriptor) of HBaseAdmin \n             * class to createTable into HBase. \n            */  \n            admin.createTable(tableDescriptor); \n            admin.close(); \n\n        }catch (TableExistsException tableExistsException) { \n            System.out.println(\"Table already exist : \" + tableName); \n            if(admin != null) { \n                try { \n                admin.close();  \n                } catch (IOException ioException) { \n                    System.out.println(\"Error occure while closing the HBaseAdmin connection : \" + ioException); \n                } \n            } \n\n        }catch (MasterNotRunningException e) { \n            throw new RuntimeException(\"HBase master not running, table creation failed : \"); \n        } catch (ZooKeeperConnectionException e) { \n            throw new RuntimeException(\"Zookeeper not running, table creation failed : \"); \n        } catch (IOException e) { \n            throw new RuntimeException(\"IO error, table creation failed : \"); \n        } \n    } \n\n    /** \n     * This method insert the input record into HBase. \n     *  \n     * @param record \n     *            input record \n     * @param rowId \n     *            unique id to identify each record uniquely. \n     */ \n    public void insert(Map<String, Map<String, Object>> record, String rowId) { \n        try { \n        Put put = new Put(Bytes.toBytes(rowId));         \n        for (String cf : record.keySet()) { \n            for (String column: record.get(cf).keySet()) { \n                put.add(Bytes.toBytes(cf), Bytes.toBytes(column), Bytes.toBytes(record.get(cf).get(column).toString())); \n            }  \n        } \n        hTable.put(put); \n        }catch (Exception e) { \n            throw new RuntimeException(\"Error occure while storing record into HBase\"); \n        } \n\n    } \n\n    public static void main(String[] args) { \n        List<String> cFs = new ArrayList<String>(); \n        cFs.add(\"cf1\"); \n        cFs.add(\"cf2\"); \n\n        List<String> zks = new ArrayList<String>(); \n        zks.add(\"192.168.41.122\"); \n        Map<String, Map<String, Object>> record = new HashMap<String, Map<String,Object>>(); \n\n        Map<String, Object> cf1 = new HashMap<String, Object>(); \n        cf1.put(\"aa\", \"1\"); \n\n        Map<String, Object> cf2 = new HashMap<String, Object>(); \n        cf2.put(\"bb\", \"1\"); \n\n        record.put(\"cf1\", cf1); \n        record.put(\"cf2\", cf2); \n\n        HBaseOperations hbaseOperations = new HBaseOperations(\"tableName\", cFs, zks, 2181); \n        hbaseOperations.insert(record, UUID.randomUUID().toString()); \n\n    } \n} \n```", "```scala\n[\"john\",\"watson\",\"abc\"]  \n```", "```scala\npublic class SampleSpout extends BaseRichSpout { \n    private static final long serialVersionUID = 1L; \n    private SpoutOutputCollector spoutOutputCollector; \n\n    private static final Map<Integer, String> FIRSTNAMEMAP = new HashMap<Integer, String>(); \n    static { \n        FIRSTNAMEMAP.put(0, \"john\"); \n        FIRSTNAMEMAP.put(1, \"nick\"); \n        FIRSTNAMEMAP.put(2, \"mick\"); \n        FIRSTNAMEMAP.put(3, \"tom\"); \n        FIRSTNAMEMAP.put(4, \"jerry\"); \n    } \n\n    private static final Map<Integer, String> LASTNAME = new HashMap<Integer, String>(); \n    static { \n        LASTNAME.put(0, \"anderson\"); \n        LASTNAME.put(1, \"watson\"); \n        LASTNAME.put(2, \"ponting\"); \n        LASTNAME.put(3, \"dravid\"); \n        LASTNAME.put(4, \"lara\"); \n    } \n\n    private static final Map<Integer, String> COMPANYNAME = new HashMap<Integer, String>(); \n    static { \n        COMPANYNAME.put(0, \"abc\"); \n        COMPANYNAME.put(1, \"dfg\"); \n        COMPANYNAME.put(2, \"pqr\"); \n        COMPANYNAME.put(3, \"ecd\"); \n        COMPANYNAME.put(4, \"awe\"); \n    } \n\n    public void open(Map conf, TopologyContext context, \n            SpoutOutputCollector spoutOutputCollector) { \n        // Open the spout \n        this.spoutOutputCollector = spoutOutputCollector; \n    } \n\n    public void nextTuple() { \n        // Storm cluster repeatedly call this method to emit the continuous // \n        // stream of tuples. \n        final Random rand = new Random(); \n        // generate the random number from 0 to 4\\. \n        int randomNumber = rand.nextInt(5); \n        spoutOutputCollector.emit (new Values(FIRSTNAMEMAP.get(randomNumber),LASTNAME.get(randomNumber),COMPANYNAME.get(randomNumber))); \n    } \n\n    public void declareOutputFields(OutputFieldsDeclarer declarer) { \n        // emits the field  firstName , lastName and companyName. \n        declarer.declare(new Fields(\"firstName\",\"lastName\",\"companyName\")); \n    } \n} \n\n```", "```scala\npublic class StormHBaseBolt implements IBasicBolt { \n\n    private static final long serialVersionUID = 2L; \n    private HBaseOperations hbaseOperations; \n    private String tableName; \n    private List<String> columnFamilies; \n    private List<String> zookeeperIPs; \n    private int zkPort; \n    /** \n     * Constructor of StormHBaseBolt class \n     *  \n     * @param tableName \n     *            HBaseTableNam \n     * @param columnFamilies \n     *            List of column families \n     * @param zookeeperIPs \n     *            List of zookeeper nodes \n     * @param zkPort \n     *            Zookeeper client port \n     */ \n    public StormHBaseBolt(String tableName, List<String> columnFamilies, \n            List<String> zookeeperIPs, int zkPort) { \n        this.tableName =tableName; \n        this.columnFamilies = columnFamilies; \n        this.zookeeperIPs = zookeeperIPs; \n        this.zkPort = zkPort; \n\n    } \n\n    public void execute(Tuple input, BasicOutputCollector collector) { \n        Map<String, Map<String, Object>> record = new HashMap<String, Map<String, Object>>(); \n        Map<String, Object> personalMap = new HashMap<String, Object>(); \n        // \"firstName\",\"lastName\",\"companyName\") \n        personalMap.put(\"firstName\", input.getValueByField(\"firstName\")); \n        personalMap.put(\"lastName\", input.getValueByField(\"lastName\")); \n\n        Map<String, Object> companyMap = new HashMap<String, Object>(); \n        companyMap.put(\"companyName\", input.getValueByField(\"companyName\")); \n\n        record.put(\"personal\", personalMap); \n        record.put(\"company\", companyMap); \n        // call the inset method of HBaseOperations class to insert record into \n        // HBase \n        hbaseOperations.insert(record, UUID.randomUUID().toString()); \n    } \n\n    public void declareOutputFields(OutputFieldsDeclarer declarer) { \n\n    } \n\n    public Map<String, Object> getComponentConfiguration() { \n        // TODO Auto-generated method stub \n        return null; \n    } \n\n    public void prepare(Map stormConf, TopologyContext context) { \n        // create the instance of HBaseOperations class \n        hbaseOperations = new HBaseOperations(tableName, columnFamilies, \n                zookeeperIPs, zkPort); \n    } \n\n    public void cleanup() { \n        // TODO Auto-generated method stub \n\n    } \n\n} \n```", "```scala\npublic class Topology {\n    public static void main(String[] args) throws AlreadyAliveException, \n            InvalidTopologyException { \n        TopologyBuilder builder = new TopologyBuilder(); \n\n        List<String> zks = new ArrayList<String>(); \n        zks.add(\"127.0.0.1\"); \n\n        List<String> cFs = new ArrayList<String>(); \n        cFs.add(\"personal\"); \n        cFs.add(\"company\"); \n\n        // set the spout class \n        builder.setSpout(\"spout\", new SampleSpout(), 2); \n        // set the bolt class \n        builder.setBolt(\"bolt\", new StormHBaseBolt(\"user\", cFs, zks, 2181), 2) \n                .shuffleGrouping(\"spout\"); \n        Config conf = new Config(); \n        conf.setDebug(true); \n        // create an instance of LocalCluster class for \n        // executing topology in local mode. \n        LocalCluster cluster = new LocalCluster(); \n\n        // LearningStormTopolgy is the name of submitted topology. \n        cluster.submitTopology(\"StormHBaseTopology\", conf, \n                builder.createTopology()); \n        try { \n            Thread.sleep(60000); \n        } catch (Exception exception) { \n            System.out.println(\"Thread interrupted exception : \" + exception); \n        } \n        System.out.println(\"Stopped Called : \"); \n        // kill the LearningStormTopology \n        cluster.killTopology(\"StormHBaseTopology\"); \n        // shutdown the storm test cluster \n        cluster.shutdown(); \n\n    } \n} \n\n```", "```scala\n    sudo yum -y install make gcc cc\n```", "```scala\n    cd /home/$USER \n    Here, $USER is the name of the Linux user. \n    http://download.redis.io/releases/redis-2.6.16.tar.gz \n    tar -xvf redis-2.6.16.tar.gz \n    cd redis-2.6.16 \n    make \n    sudo cp src/redis-server /usr/local/bin \n    sudo cp src/redis-cli /usr/local/bin\n```", "```scala\n    sudo mkdir -p /etc/redis \n    sudo mkdir -p /var/redis \n    cd /home/$USER/redis-2.6.16/ \n    sudo cp utils/redis_init_script /etc/init.d/redis \n    wget https://bitbucket.org/ptylr/public-stuff/raw/41d5c8e87ce6adb3 \n    4aa16cd571c3f04fb4d5e7ac/etc/init.d/redis \n    sudo cp redis /etc/init.d/redis \n    cd /home/$USER/redis-2.6.16/ \n    sudo cp redis.conf /etc/redis/redis.conf\n```", "```scala\n    chkconfig --add redis \n    chkconfig redis on \n    service redis start\n```", "```scala\n    redis-cli ping\n```", "```scala\n<repositories> \n        <repository> \n            <id>central</id> \n            <name>Maven Central</name> \n            <url>http://repo1.maven.org/maven2/</url> \n        </repository> \n        <repository> \n            <id>cloudera-repo</id> \n            <name>Cloudera CDH</name> \n            <url>https://repository.cloudera.com/artifactory/cloudera-\n            repos/</url> \n        </repository> \n        <repository> \n            <id>clojars.org</id> \n            <url>http://clojars.org/repo</url> \n        </repository> \n    </repositories> \n    <dependencies> \n        <dependency> \n            <groupId>storm</groupId> \n            <artifactId>storm</artifactId> \n            <version>0.9.0.1</version> \n        </dependency> \n                <dependency> \n            <groupId>com.fasterxml.jackson.core</groupId> \n            <artifactId>jackson-core</artifactId> \n            <version>2.1.1</version> \n        </dependency> \n\n        <dependency> \n            <groupId>com.fasterxml.jackson.core</groupId> \n            <artifactId>jackson-databind</artifactId> \n            <version>2.1.1</version> \n        </dependency> \n        <dependency> \n            <groupId>junit</groupId> \n            <artifactId>junit</artifactId> \n            <version>3.8.1</version> \n            <scope>test</scope> \n        </dependency> \n        <dependency> \n            <groupId>redis.clients</groupId> \n            <artifactId>jedis</artifactId> \n            <version>2.4.2</version> \n        </dependency> \n    </dependencies> \n```", "```scala\npublic class RedisOperations implements Serializable { \n\n    private static final long serialVersionUID = 1L; \n    Jedis jedis = null; \n\n    public RedisOperations(String redisIP, int port) { \n        // Connecting to Redis on localhost \n        jedis = new Jedis(redisIP, port); \n    } \n\n    public void insert(Map<String, Object> record, String id) { \n        try { \n            jedis.set(id, new ObjectMapper().writeValueAsString(record)); \n        } catch (Exception e) { \n            System.out.println(\"Record not persist into datastore : \"); \n        } \n    } \n} \n```", "```scala\n    public class StormRedisBolt implements IBasicBolt{ \n\n    private static final long serialVersionUID = 2L; \n    private RedisOperations redisOperations = null; \n    private String redisIP = null; \n    private int port; \n    public StormRedisBolt(String redisIP, int port) { \n        this.redisIP = redisIP; \n        this.port = port; \n    } \n\n    public void execute(Tuple input, BasicOutputCollector collector) { \n        Map<String, Object> record = new HashMap<String, Object>(); \n        //\"firstName\",\"lastName\",\"companyName\") \n        record.put(\"firstName\", input.getValueByField(\"firstName\")); \n        record.put(\"lastName\", input.getValueByField(\"lastName\")); \n        record.put(\"companyName\", input.getValueByField(\"companyName\")); \n        redisOperations.insert(record, UUID.randomUUID().toString()); \n    } \n\n    public void declareOutputFields(OutputFieldsDeclarer declarer) { \n\n    } \n\n    public Map<String, Object> getComponentConfiguration() { \n        return null; \n    } \n\n    public void prepare(Map stormConf, TopologyContext context) { \n        redisOperations = new RedisOperations(this.redisIP, this.port); \n    } \n\n    public void cleanup() { \n\n    } \n\n} \n\n```", "```scala\npublic class Topology { \n    public static void main(String[] args) throws AlreadyAliveException, \n            InvalidTopologyException { \n        TopologyBuilder builder = new TopologyBuilder(); \n\n        List<String> zks = new ArrayList<String>(); \n        zks.add(\"192.168.41.122\"); \n\n        List<String> cFs = new ArrayList<String>(); \n        cFs.add(\"personal\"); \n        cFs.add(\"company\"); \n\n        // set the spout class \n        builder.setSpout(\"spout\", new SampleSpout(), 2); \n        // set the bolt class \n        builder.setBolt(\"bolt\", new StormRedisBolt(\"192.168.41.122\",2181), 2).shuffleGrouping(\"spout\"); \n\n        Config conf = new Config(); \n        conf.setDebug(true); \n        // create an instance of LocalCluster class for \n        // executing topology in local mode. \n        LocalCluster cluster = new LocalCluster(); \n\n        // LearningStormTopolgy is the name of submitted topology. \n        cluster.submitTopology(\"StormRedisTopology\", conf, \n                builder.createTopology()); \n        try { \n            Thread.sleep(10000); \n        } catch (Exception exception) { \n            System.out.println(\"Thread interrupted exception : \" + exception); \n        } \n        // kill the LearningStormTopology \n        cluster.killTopology(\"StormRedisTopology\"); \n        // shutdown the storm test cluster \n        cluster.shutdown(); \n} \n} \n```", "```scala\n<dependencies> \n        <dependency> \n            <groupId>org.elasticsearch</groupId> \n            <artifactId>elasticsearch</artifactId> \n            <version>2.4.4</version> \n        </dependency> \n        <dependency> \n            <groupId>junit</groupId> \n            <artifactId>junit</artifactId> \n            <version>3.8.1</version> \n            <scope>test</scope> \n        </dependency> \n        <dependency> \n            <groupId>org.apache.storm</groupId> \n            <artifactId>storm-core</artifactId> \n            <version>1.0.2</version> \n            <scope>provided</scope> \n        </dependency> \n    </dependencies> \n```", "```scala\npublic class ElasticSearchOperation { \n\n    private TransportClient client; \n\n    public ElasticSearchOperation(List<String> esNodes) throws Exception { \n        try { \n            Settings settings = Settings.settingsBuilder() \n                    .put(\"cluster.name\", \"elasticsearch\").build(); \n            client = TransportClient.builder().settings(settings).build(); \n            for (String esNode : esNodes) { \n                client.addTransportAddress(new InetSocketTransportAddress( \n                        InetAddress.getByName(esNode), 9300)); \n            } \n\n        } catch (Exception e) { \n            throw e; \n        } \n\n    } \n\n    public void insert(Map<String, Object> data, String indexName, String indexMapping, String indexId) { \n        client.prepareIndex(indexName, indexMapping, indexId) \n                .setSource(data).get(); \n    } \n\n    public static void main(String[] s){ \n        try{ \n            List<String> esNodes = new ArrayList<String>(); \n            esNodes.add(\"127.0.0.1\"); \n            ElasticSearchOperation elasticSearchOperation  = new ElasticSearchOperation(esNodes); \n            Map<String, Object> data = new HashMap<String, Object>(); \n            data.put(\"name\", \"name\"); \n            data.put(\"add\", \"add\"); \n            elasticSearchOperation.insert(data,\"indexName\",\"indexMapping\",UUID.randomUUID().toString()); \n        }catch(Exception e) { \n            e.printStackTrace(); \n            //System.out.println(e); \n        } \n    } \n\n} \n```", "```scala\npublic class ESBolt implements IBasicBolt { \n\n    private static final long serialVersionUID = 2L; \n    private ElasticSearchOperation elasticSearchOperation; \n    private List<String> esNodes; \n\n    /** \n     *  \n     * @param esNodes \n     */ \n    public ESBolt(List<String> esNodes) { \n        this.esNodes = esNodes; \n\n    } \n\n    public void execute(Tuple input, BasicOutputCollector collector) { \n        Map<String, Object> personalMap = new HashMap<String, Object>(); \n        // \"firstName\",\"lastName\",\"companyName\") \n        personalMap.put(\"firstName\", input.getValueByField(\"firstName\")); \n        personalMap.put(\"lastName\", input.getValueByField(\"lastName\")); \n\n        personalMap.put(\"companyName\", input.getValueByField(\"companyName\")); \n        elasticSearchOperation.insert(personalMap,\"person\",\"personmapping\",UUID.randomUUID().toString()); \n    } \n\n    public void declareOutputFields(OutputFieldsDeclarer declarer) { \n\n    } \n\n    public Map<String, Object> getComponentConfiguration() { \n        // TODO Auto-generated method stub \n        return null; \n    } \n\n    public void prepare(Map stormConf, TopologyContext context) { \n        try { \n            // create the instance of ESOperations class \n            elasticSearchOperation = new ElasticSearchOperation(esNodes); \n        } catch (Exception e) { \n            throw new RuntimeException(); \n        } \n    } \n\n    public void cleanup() { \n\n    } \n\n} \n```", "```scala\npublic class ESTopology {\n    public static void main(String[] args) throws AlreadyAliveException, \n            InvalidTopologyException { \n        TopologyBuilder builder = new TopologyBuilder(); \n\n        //ES Node list \n        List<String> esNodes = new ArrayList<String>(); \n        esNodes.add(\"10.191.209.14\"); \n\n        // set the spout class \n        builder.setSpout(\"spout\", new SampleSpout(), 2); \n        // set the ES bolt class \n        builder.setBolt(\"bolt\", new ESBolt(esNodes), 2) \n                .shuffleGrouping(\"spout\"); \n        Config conf = new Config(); \n        conf.setDebug(true); \n        // create an instance of LocalCluster class for \n        // executing topology in local mode. \n        LocalCluster cluster = new LocalCluster(); \n\n        // ESTopology is the name of submitted topology. \n        cluster.submitTopology(\"ESTopology\", conf, \n                builder.createTopology()); \n        try { \n            Thread.sleep(60000); \n        } catch (Exception exception) { \n            System.out.println(\"Thread interrupted exception : \" + exception); \n        } \n        System.out.println(\"Stopped Called : \"); \n        // kill the LearningStormTopology \n        cluster.killTopology(\"StormHBaseTopology\"); \n        // shutdown the storm test cluster \n        cluster.shutdown(); \n\n    } \n} \n```", "```scala\n    <dependencies>\n        <dependency>\n            <groupId>com.espertech</groupId>\n            <artifactId>esper</artifactId>\n            <version>5.3.0</version>\n        </dependency>\n        <dependency>\n            <groupId>junit</groupId>\n            <artifactId>junit</artifactId>\n            <version>3.8.1</version>\n            <scope>test</scope>\n        </dependency>\n        <dependency>\n            <groupId>org.apache.storm</groupId>\n            <artifactId>storm-core</artifactId>\n            <version>1.0.2</version>\n            <scope>provided</scope>\n        </dependency>\n    </dependencies>\n```", "```scala\npublic class EsperOperation { \n\n    private EPRuntime cepRT = null; \n\n    public EsperOperation() { \n        Configuration cepConfig = new Configuration(); \n        cepConfig.addEventType(\"StockTick\", Stock.class.getName()); \n        EPServiceProvider cep = EPServiceProviderManager.getProvider( \n                \"myCEPEngine\", cepConfig); \n        cepRT = cep.getEPRuntime(); \n\n        EPAdministrator cepAdm = cep.getEPAdministrator(); \n        EPStatement cepStatement = cepAdm \n                .createEPL(\"select sum(price),product from \" \n                        + \"StockTick.win:time_batch(5 sec) \" \n                        + \"group by product\"); \n\n        cepStatement.addListener(new CEPListener()); \n    } \n\n    public static class CEPListener implements UpdateListener { \n\n        public void update(EventBean[] newData, EventBean[] oldData) { \n            try { \n                System.out.println(\"#################### Event received: \n                \"+newData); \n                for (EventBean eventBean : newData) { \n                    System.out.println(\"************************ Event \n                     received 1: \" + eventBean.getUnderlying()); \n                } \n\n            } catch (Exception e) { \n                e.printStackTrace(); \n                System.out.println(e); \n            } \n        } \n    } \n\n    public void esperPut(Stock stock) { \n        cepRT.sendEvent(stock); \n    } \n\n    private static Random generator = new Random(); \n\n    public static void main(String[] s) throws InterruptedException { \n        EsperOperation esperOperation = new EsperOperation(); \n        // We generate a few ticks... \n        for (int i = 0; i < 5; i++) { \n            double price = (double) generator.nextInt(10); \n            long timeStamp = System.currentTimeMillis(); \n            String product = \"AAPL\"; \n            Stock stock = new Stock(product, price, timeStamp); \n            System.out.println(\"Sending tick:\" + stock); \n            esperOperation.esperPut(stock); \n        } \n        Thread.sleep(200000); \n    } \n\n} \n```", "```scala\n    [\"product type\",\"price\",\"sale date\"] \n```", "```scala\npublic class SampleSpout extends BaseRichSpout { \n    private static final long serialVersionUID = 1L; \n    private SpoutOutputCollector spoutOutputCollector; \n\n    private static final Map<Integer, String> PRODUCT = new \n    HashMap<Integer, String>(); \n    static { \n        PRODUCT.put(0, \"A\"); \n        PRODUCT.put(1, \"B\"); \n        PRODUCT.put(2, \"C\"); \n        PRODUCT.put(3, \"D\"); \n        PRODUCT.put(4, \"E\"); \n    } \n\n    private static final Map<Integer, Double> price = new \n    HashMap<Integer, Double>(); \n    static { \n        price.put(0, 500.0); \n        price.put(1, 100.0); \n        price.put(2, 300.0); \n        price.put(3, 900.0); \n        price.put(4, 1000.0); \n    } \n\n    public void open(Map conf, TopologyContext context, \n            SpoutOutputCollector spoutOutputCollector) { \n        // Open the spout \n        this.spoutOutputCollector = spoutOutputCollector; \n    } \n\n    public void nextTuple() { \n        // Storm cluster repeatedly call this method to emit the \n        continuous // \n        // stream of tuples. \n        final Random rand = new Random(); \n        // generate the random number from 0 to 4\\. \n        int randomNumber = rand.nextInt(5); \n\n        spoutOutputCollector.emit (new \n        Values(PRODUCT.get(randomNumber),price.get(randomNumber), \n        System.currentTimeMillis())); \n        try { \n            Thread.sleep(1000); \n        } catch (InterruptedException e) { \n            // TODO Auto-generated catch block \n            e.printStackTrace(); \n        } \n    } \n\n    public void declareOutputFields(OutputFieldsDeclarer declarer) { \n        // emits the field  firstName , lastName and companyName. \n        declarer.declare(new Fields(\"product\",\"price\",\"timestamp\")); \n    } \n} \n```", "```scala\npublic class EsperBolt implements IBasicBolt { \n\n    private static final long serialVersionUID = 2L; \n    private EsperOperation esperOperation; \n\n    public EsperBolt() { \n\n    } \n\n    public void execute(Tuple input, BasicOutputCollector collector) { \n\n        double price = input.getDoubleByField(\"price\"); \n        long timeStamp = input.getLongByField(\"timestamp\"); \n        //long timeStamp = System.currentTimeMillis(); \n        String product = input.getStringByField(\"product\"); \n        Stock stock = new Stock(product, price, timeStamp); \n        esperOperation.esperPut(stock); \n    } \n\n    public void declareOutputFields(OutputFieldsDeclarer declarer) { \n\n    } \n\n    public Map<String, Object> getComponentConfiguration() { \n        // TODO Auto-generated method stub \n        return null; \n    } \n\n    public void prepare(Map stormConf, TopologyContext context) { \n        try { \n            // create the instance of ESOperations class \n            esperOperation = new EsperOperation(); \n        } catch (Exception e) { \n            throw new RuntimeException(); \n        } \n    } \n\n    public void cleanup() { \n\n    } \n} \n```", "```scala\npublic class EsperTopology { \n    public static void main(String[] args) throws AlreadyAliveException, \n            InvalidTopologyException { \n        TopologyBuilder builder = new TopologyBuilder(); \n\n        // set the spout class \n        builder.setSpout(\"spout\", new SampleSpout(), 2); \n        // set the ES bolt class \n        builder.setBolt(\"bolt\", new EsperBolt(), 2) \n                .shuffleGrouping(\"spout\"); \n        Config conf = new Config(); \n        conf.setDebug(true); \n        // create an instance of LocalCluster class for \n        // executing topology in local mode. \n        LocalCluster cluster = new LocalCluster(); \n\n        // EsperTopology is the name of submitted topology. \n        cluster.submitTopology(\"EsperTopology\", conf, \n                builder.createTopology()); \n        try { \n            Thread.sleep(60000); \n        } catch (Exception exception) { \n            System.out.println(\"Thread interrupted exception : \" + exception); \n        } \n        System.out.println(\"Stopped Called : \"); \n        // kill the LearningStormTopology \n        cluster.killTopology(\"EsperTopology\"); \n        // shutdown the storm test cluster \n        cluster.shutdown(); \n\n    } \n} \n```"]