["```scala\n> unzip logstash-5.4.1.zip\n```", "```scala\ninput {\n  file {\n    path => \"PATH_TO_APACHE_LOG\"\n    start_position => \"beginning\"\n  }\n}\noutput {\n  kafka {\n    topic_id => \"TOPIC_NAME\"\n    bootstrap_servers => \"KAFKA_IP:KAFKA_PORT\"\n  }\n}\n```", "```scala\n$ bin/logstash agent -f logstash.conf\n```", "```scala\n       <dependency> \n             <groupId>org.apache.storm</groupId> \n             <artifactId>storm-core</artifactId> \n             <version>1.0.2</version> \n             <scope>provided</scope> \n       </dependency> \n\n       <!-- Utilities --> \n       <dependency> \n             <groupId>commons-collections</groupId> \n             <artifactId>commons-collections</artifactId> \n             <version>3.2.1</version> \n       </dependency> \n       <dependency> \n             <groupId>com.google.guava</groupId> \n             <artifactId>guava</artifactId> \n             <version>15.0</version> \n       </dependency> \n```", "```scala\n/** \n * This class contains logic to Parse an Apache log file with Regular \n * Expressions \n */ \npublic class ApacheLogSplitter { \n\n public Map<String,Object> logSplitter(String apacheLog) { \n\n       String logEntryLine = apacheLog; \n       // Regex pattern to split fetch the different properties from log lines. \n       String logEntryPattern = \"^([\\\\d.]+) (\\\\S+) (\\\\S+) \\\\[([\\\\w-:/]+\\\\s[+\\\\-]\\\\d{4})\\\\] \\\"(.+?)\\\" (\\\\d{3}) (\\\\d+) \\\"([^\\\"]+)\\\" \\\"([^\\\"]+)\\\"\"; \n\n       Pattern p = Pattern.compile(logEntryPattern); \n       Matcher matcher = p.matcher(logEntryLine); \n       Map<String,Object> logMap = new HashMap<String, Object>(); \n       if (!matcher.matches() || 9 != matcher.groupCount()) { \n             System.err.println(\"Bad log entry (or problem with RE?):\"); \n             System.err.println(logEntryLine); \n             return logMap; \n       } \n       // set the ip, dateTime, request, etc into map. \n       logMap.put(\"ip\", matcher.group(1)); \n       logMap.put(\"dateTime\", matcher.group(4)); \n       logMap.put(\"request\", matcher.group(5)); \n       logMap.put(\"response\", matcher.group(6)); \n       logMap.put(\"bytesSent\", matcher.group(7)); \n       logMap.put(\"referrer\", matcher.group(8)); \n       logMap.put(\"useragent\", matcher.group(9)); \n       return logMap; \n } \n```", "```scala\n98.83.179.51 - - [18/May/2011:19:35:08 -0700] \\\"GET /css/main.css HTTP/1.1\\\" 200 1837 \\\"http://www.safesand.com/information.htm\\\" \\\"Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1\\\" \n```", "```scala\n{response=200, referrer=http://www.safesand.com/information.htm, bytesSent=1837, useragent=Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1, dateTime=18/May/2011:19:35:08 -0700, request=GET /css/main.css HTTP/1.1, ip=98.83.179.51}  \n```", "```scala\n/** \n *  \n * This class call the ApacheLogSplitter class and pass the set of fields (ip, \n * referrer, user-agent, etc) to next bolt in Topology. \n */ \n\npublic class ApacheLogSplitterBolt extends BaseBasicBolt { \n\n private static final long serialVersionUID = 1L; \n // Create the instance of ApacheLogSplitter class. \n private static final ApacheLogSplitter apacheLogSplitter = new ApacheLogSplitter(); \n private static final List<String> LOG_ELEMENTS = new ArrayList<String>(); \n static { \n       LOG_ELEMENTS.add(\"ip\"); \n       LOG_ELEMENTS.add(\"dateTime\"); \n       LOG_ELEMENTS.add(\"request\"); \n       LOG_ELEMENTS.add(\"response\"); \n       LOG_ELEMENTS.add(\"bytesSent\"); \n       LOG_ELEMENTS.add(\"referrer\"); \n       LOG_ELEMENTS.add(\"useragent\"); \n } \n\n public void execute(Tuple input, BasicOutputCollector collector) { \n       // Get the Apache log from the tuple \n       String log = input.getString(0); \n\n       if (StringUtils.isBlank(log)) { \n             // ignore blank lines \n             return; \n       } \n       // call the logSplitter(String apachelog) method of ApacheLogSplitter \n       // class. \n       Map<String, Object> logMap = apacheLogSplitter.logSplitter(log); \n       List<Object> logdata = new ArrayList<Object>(); \n       for (String element : LOG_ELEMENTS) { \n             logdata.add(logMap.get(element)); \n       } \n       // emits set of fields (ip, referrer, user-agent, bytesSent, etc) \n       collector.emit(logdata); \n\n } \n\n public void declareOutputFields(OutputFieldsDeclarer declarer) { \n       // specify the name of output fields. \n       declarer.declare(new Fields(\"ip\", \"dateTime\", \"request\", \"response\", \n                   \"bytesSent\", \"referrer\", \"useragent\")); \n } \n} \n\n```", "```scala\n       <dependency> \n             <groupId>org.geomind</groupId> \n             <artifactId>geoip</artifactId> \n             <version>1.2.8</version> \n       </dependency> \n```", "```scala\n        <repository> \n             <id>geoip</id> \n             <url>http://snambi.github.com/maven/</url> \n       </repository> \n```", "```scala\n/** \n * This class contains logic to calculate the country name from IP address \n *  \n */ \npublic class IpToCountryConverter { \n\n private static LookupService cl = null; \n\n /** \n  * An parameterised constructor which would take the location of \n  * GeoLiteCity.dat file as input. \n  *  \n  * @param pathTOGeoLiteCityFile \n  */ \n public IpToCountryConverter(String pathTOGeoLiteCityFile) { \n       try { \n             cl = new LookupService(\"pathTOGeoLiteCityFile\", \n                         LookupService.GEOIP_MEMORY_CACHE); \n       } catch (Exception exception) { \n             throw new RuntimeException( \n                         \"Error occurs while initializing IpToCountryConverter class : \"); \n       } \n } \n\n /** \n  * This method takes ip address an input and convert it into country name. \n  *  \n  * @param ip \n  * @return \n  */ \n public String ipToCountry (String ip) { \n       Location location = cl.getLocation(ip); \n       if (location == null) { \n             return \"NA\"; \n       } \n       if (location.countryName == null) { \n             return \"NA\"; \n       } \n       return location.countryName; \n } \n} \n```", "```scala\n /** \n * This class use the IpToCountryConverter and UserAgentTools class to calculate \n * the country, os and browser from log line. \n *  \n */ \npublic class UserInformationGetterBolt extends BaseRichBolt { \n\n private static final long serialVersionUID = 1L; \n private IpToCountryConverter ipToCountryConverter = null; \n private UserAgentTools userAgentTools = null; \n public OutputCollector collector; \n private String pathTOGeoLiteCityFile; \n\n public UserInformationGetterBolt(String pathTOGeoLiteCityFile) { \n       // set the path of GeoLiteCity.dat file. \n       this.pathTOGeoLiteCityFile = pathTOGeoLiteCityFile; \n } \n\n public void declareOutputFields(OutputFieldsDeclarer declarer) { \n       declarer.declare(new Fields(\"ip\", \"dateTime\", \"request\", \"response\", \n                   \"bytesSent\", \"referrer\", \"useragent\", \"country\", \"browser\", \n                   \"os\")); \n } \n\n public void prepare(Map stormConf, TopologyContext context, \n             OutputCollector collector) { \n       this.collector = collector; \n       this.ipToCountryConverter = new IpToCountryConverter( \n                   this.pathTOGeoLiteCityFile); \n       this.userAgentTools = new UserAgentTools(); \n\n } \n\n public void execute(Tuple input) { \n\n       String ip = input.getStringByField(\"ip\").toString(); \n\n       // calculate the country from ip \n       Object country = ipToCountryConverter.ipToCountry(ip); \n       // calculate the browser from useragent. \n       Object browser = userAgentTools.getBrowser(input.getStringByField( \n                   \"useragent\").toString())[1]; \n       // calculate the os from useragent. \n       Object os = userAgentTools.getOS(input.getStringByField(\"useragent\") \n                   .toString())[1]; \n       collector.emit(new Values(input.getString(0), input.getString(1), input \n                   .getString(2), input.getString(3), input.getString(4), input \n                   .getString(5), input.getString(6), country, browser, os)); \n\n } \n} \n```", "```scala\n/** \n * This class takes referrer URL as input, analyze the URL and return search \n * keyword as output. \n *  \n */ \npublic class KeywordGenerator { \n public String getKeyword(String referer) { \n\n       String[] temp; \n       Pattern pat = Pattern.compile(\"[?&#]q=([^&]+)\"); \n       Matcher m = pat.matcher(referer); \n       if (m.find()) { \n             String searchTerm = null; \n             searchTerm = m.group(1); \n             temp = searchTerm.split(\"\\\\+\"); \n             searchTerm = temp[0]; \n             for (int i = 1; i < temp.length; i++) { \n                   searchTerm = searchTerm + \" \" + temp[i]; \n             } \n             return searchTerm; \n       } else { \n             pat = Pattern.compile(\"[?&#]p=([^&]+)\"); \n             m = pat.matcher(referer); \n             if (m.find()) { \n                   String searchTerm = null; \n                   searchTerm = m.group(1); \n                   temp = searchTerm.split(\"\\\\+\"); \n                   searchTerm = temp[0]; \n                   for (int i = 1; i < temp.length; i++) { \n                         searchTerm = searchTerm + \" \" + temp[i]; \n                   } \n                   return searchTerm; \n             } else { \n                   // \n                   pat = Pattern.compile(\"[?&#]query=([^&]+)\"); \n                   m = pat.matcher(referer); \n                   if (m.find()) { \n                         String searchTerm = null; \n                         searchTerm = m.group(1); \n                         temp = searchTerm.split(\"\\\\+\"); \n                         searchTerm = temp[0]; \n                         for (int i = 1; i < temp.length; i++) { \n                               searchTerm = searchTerm + \" \" + temp[i]; \n                         } \n                         return searchTerm; \n                   }  else { \n                               return \"NA\"; \n                         } \n                   } \n       } \n } \n\n} \n```", "```scala\nindia live score\n```", "```scala\n/** \n * This class use the KeywordGenerator class to generate the search keyword from \n * referrer URL. \n *  \n */ \npublic class KeyWordIdentifierBolt extends BaseRichBolt { \n\n private static final long serialVersionUID = 1L; \n private KeywordGenerator keywordGenerator = null; \n public OutputCollector collector; \n\n public KeyWordIdentifierBolt() { \n\n } \n\n public void declareOutputFields(OutputFieldsDeclarer declarer) { \n       declarer.declare(new Fields(\"ip\", \"dateTime\", \"request\", \"response\", \n                   \"bytesSent\", \"referrer\", \"useragent\", \"country\", \"browser\", \n                   \"os\", \"keyword\")); \n } \n\n public void prepare(Map stormConf, TopologyContext context, \n             OutputCollector collector) { \n       this.collector = collector; \n       this.keywordGenerator = new KeywordGenerator(); \n\n } \n\n public void execute(Tuple input) { \n\n       String referrer = input.getStringByField(\"referrer\").toString(); \n       // call the getKeyword(String referrer) method KeywordGenerator class to \n       // generate the search keyword. \n       Object keyword = keywordGenerator.getKeyword(referrer); \n       // emits all the field emitted by previous bolt + keyword \n       collector.emit(new Values(input.getString(0), input.getString(1), input \n                   .getString(2), input.getString(3), input.getString(4), input \n                   .getString(5), input.getString(6), input.getString(7), input \n                   .getString(8), input.getString(9), keyword)); \n\n } \n} \n```", "```scala\n\n       <dependency> \n             <groupId>mysql</groupId> \n             <artifactId>mysql-connector-java</artifactId> \n             <version>5.1.6</version> \n       </dependency> \n```", "```scala\n/** \n *  \n * This class return the MySQL connection. \n */ \npublic class MySQLConnection { \n\n private static Connection connect = null; \n\n /** \n  * This method return the MySQL connection. \n  *  \n  * @param ip \n  *            ip of MySQL server \n  * @param database \n  *            name of database \n  * @param user \n  *            name of user \n  * @param password \n  *            password of given user \n  * @return MySQL connection \n  */ \n public static Connection getMySQLConnection(String ip, String database, String user, String password) { \n       try { \n             // this will load the MySQL driver, each DB has its own driver \n             Class.forName(\"com.mysql.jdbc.Driver\"); \n             // setup the connection with the DB. \n             connect = DriverManager \n                         .getConnection(\"jdbc:mysql://\"+ip+\"/\"+database+\"?\" \n                                     + \"user=\"+user+\"&password=\"+password+\"\"); \n             return connect; \n       } catch (Exception e) { \n             throw new RuntimeException(\"Error occurs while get mysql connection : \"); \n       } \n } \n} \n```", "```scala\n/** \n * This class contains logic to persist record into MySQL database. \n *  \n */ \npublic class MySQLDump { \n /** \n  * Name of database you want to connect \n  */ \n private String database; \n /** \n  * Name of MySQL user \n  */ \n private String user; \n /** \n  * IP of MySQL server \n  */ \n private String ip; \n /** \n  * Password of MySQL server \n  */ \n private String password; \n\n public MySQLDump(String ip, String database, String user, String password) { \n       this.ip = ip; \n       this.database = database; \n       this.user = user; \n       this.password = password; \n } \n\n /** \n  * Get the MySQL connection \n  */ \n private Connection connect = MySQLConnection.getMySQLConnection(ip,database,user,password); \n\n private PreparedStatement preparedStatement = null; \n\n /** \n  * Persist input tuple. \n  * @param tuple \n  */ \n public void persistRecord(Tuple tuple) { \n       try { \n\n             // preparedStatements can use variables and are more efficient \n             preparedStatement = connect \n                         .prepareStatement(\"insert into  apachelog values (default, ?, ?, ?,?, ?, ?, ?, ? , ?, ?, ?)\"); \n\n             preparedStatement.setString(1, tuple.getStringByField(\"ip\")); \n             preparedStatement.setString(2, tuple.getStringByField(\"dateTime\")); \n             preparedStatement.setString(3, tuple.getStringByField(\"request\")); \n             preparedStatement.setString(4, tuple.getStringByField(\"response\")); \n             preparedStatement.setString(5, tuple.getStringByField(\"bytesSent\")); \n             preparedStatement.setString(6, tuple.getStringByField(\"referrer\")); \n             preparedStatement.setString(7, tuple.getStringByField(\"useragent\")); \n             preparedStatement.setString(8, tuple.getStringByField(\"country\")); \n             preparedStatement.setString(9, tuple.getStringByField(\"browser\")); \n             preparedStatement.setString(10, tuple.getStringByField(\"os\")); \n             preparedStatement.setString(11, tuple.getStringByField(\"keyword\")); \n\n             // Insert record \n             preparedStatement.executeUpdate(); \n\n       } catch (Exception e) { \n             throw new RuntimeException( \n                         \"Error occurs while persisting records in mysql : \"); \n       } finally { \n             // close prepared statement \n             if (preparedStatement != null) { \n                   try { \n                         preparedStatement.close(); \n                   } catch (Exception exception) { \n                         System.out \n                                     .println(\"Error occurs while closing PreparedStatement : \"); \n                   } \n             } \n       } \n\n } \n public void close() { \n       try { \n       connect.close(); \n       }catch(Exception exception) { \n             System.out.println(\"Error occurs while clossing the connection\"); \n       } \n } \n} \n```", "```scala\n/** \n * This Bolt call the getConnectionn(....) method of MySQLDump class to persist \n * the record into MySQL database. \n *  \n * @author Admin \n *  \n */ \npublic class PersistenceBolt implements IBasicBolt { \n\n private MySQLDump mySQLDump = null; \n private static final long serialVersionUID = 1L; \n /** \n  * Name of database you want to connect \n  */ \n private String database; \n /** \n  * Name of MySQL user \n  */ \n private String user; \n /** \n  * IP of MySQL server \n  */ \n private String ip; \n /** \n  * Password of MySQL server \n  */ \n private String password; \n\n public PersistenceBolt(String ip, String database, String user, \n             String password) { \n       this.ip = ip; \n       this.database = database; \n       this.user = user; \n       this.password = password; \n } \n\n public void declareOutputFields(OutputFieldsDeclarer declarer) { \n } \n\n public Map<String, Object> getComponentConfiguration() { \n       return null; \n } \n\n public void prepare(Map stormConf, TopologyContext context) { \n\n       // create the instance of MySQLDump(....) class. \n       mySQLDump = new MySQLDump(ip, database, user, password); \n } \n\n /** \n  * This method call the persistRecord(input) method of MySQLDump class to \n  * persist record into MySQL. \n  */ \n public void execute(Tuple input, BasicOutputCollector collector) { \n       System.out.println(\"Input tuple : \" + input); \n       mySQLDump.persistRecord(input); \n } \n\n public void cleanup() { \n       // Close the connection \n       mySQLDump.close(); \n } \n\n} \n```", "```scala\n       <dependency> \n             <groupId>org.apache.storm</groupId> \n             <artifactId>storm-kafka</artifactId> \n             <version>1.0.2</version> \n             <exclusions> \n                   <exclusion> \n                         <groupId>org.apache.kafka</groupId> \n                         <artifactId>kafka-clients</artifactId> \n                   </exclusion> \n             </exclusions> \n       </dependency> \n\n       <dependency> \n             <groupId>org.apache.kafka</groupId> \n             <artifactId>kafka_2.10</artifactId> \n             <version>0.9.0.1</version> \n             <exclusions> \n                   <exclusion> \n                         <groupId>com.sun.jdmk</groupId> \n                         <artifactId>jmxtools</artifactId> \n                   </exclusion> \n                   <exclusion> \n                         <groupId>com.sun.jmx</groupId> \n                         <artifactId>jmxri</artifactId> \n                   </exclusion> \n             </exclusions> \n       </dependency> \n```", "```scala\n       <build> \n       <plugins> \n             <plugin> \n                   <artifactId>maven-assembly-plugin</artifactId> \n                   <configuration> \n                         <descriptorRefs> \n                               <descriptorRef>jar-with-\n                               dependencies</descriptorRef> \n                         </descriptorRefs> \n                         <archive> \n                               <manifest> \n                                     <mainClass></mainClass> \n                               </manifest> \n                         </archive> \n                   </configuration> \n                   <executions> \n                         <execution> \n                               <id>make-assembly</id> \n                               <phase>package</phase> \n                               <goals> \n                                     <goal>single</goal> \n                               </goals> \n                         </execution> \n                   </executions> \n             </plugin> \n\n             <plugin> \n                   <groupId>org.codehaus.mojo</groupId> \n                   <artifactId>exec-maven-plugin</artifactId> \n                   <version>1.2.1</version> \n                   <executions> \n                         <execution> \n                               <goals> \n                                     <goal>exec</goal> \n                               </goals> \n                         </execution> \n                   </executions> \n                   <configuration> \n                         <executable>java</executable> \n                    <includeProjectDependencies>true</includeProjectDependencies> \n                    <includePluginDependencies>false</includePluginDependencies> \n                         <classpathScope>compile</classpathScope> \n                         <mainClass>${main.class}</mainClass> \n                   </configuration> \n             </plugin> \n\n             <plugin> \n                   <groupId>org.apache.maven.plugins</groupId> \n                   <artifactId>maven-compiler-plugin</artifactId> \n             </plugin> \n\n       </plugins> \n </build> \n```", "```scala\npublic class LogProcessingTopology { \n public static void main(String[] args) throws Exception { \n\n       // zookeeper hosts for the Kafka cluster \n       BrokerHosts zkHosts = new ZkHosts(\"ZK:2183\"); \n\n       // Create the KafkaSpout configuartion \n       // Second argument is the topic name \n       // Third argument is the zookeepr root for Kafka \n       // Fourth argument is consumer group id \n       SpoutConfig kafkaConfig = new SpoutConfig(zkHosts, \"apache_log\", \"\", \n                   \"id2\"); \n\n       // Specify that the Kafka messages are String \n       kafkaConfig.scheme = new SchemeAsMultiScheme(new StringScheme()); \n\n       // We want to consume all the first messages in the topic everytime \n       // we run the topology to help in debugging. In production, this \n       // property should be false \n\n       kafkaConfig.startOffsetTime = kafka.api.OffsetRequest \n                   .EarliestTime(); \n\n       // Now we create the topology \n       TopologyBuilder builder = new TopologyBuilder(); \n\n       // set the Kafka spout class \n       builder.setSpout(\"KafkaSpout\", new KafkaSpout(kafkaConfig), 2); \n\n       // set the LogSplitter, IpToCountry, Keyword and PersistenceBolt bolts \n       // class. \n       builder.setBolt(\"LogSplitter\", new ApacheLogSplitterBolt(), 1) \n                   .globalGrouping(\"KafkaSpout\"); \n\n       builder.setBolt( \n                   \"IpToCountry\", \n                   new UserInformationGetterBolt( \n                               args[0]), 1) \n                   .globalGrouping(\"LogSplitter\"); \n       builder.setBolt(\"Keyword\", new KeyWordIdentifierBolt(), 1) \n                   .globalGrouping(\"IpToCountry\"); \n       builder.setBolt(\"PersistenceBolt\", \n                   new PersistenceBolt(args[1], args[2], args[3], args[4]), \n                   1).globalGrouping(\"Keyword\"); \n\n       if (args.length == 6) { \n             // Run the topology on remote cluster. \n             Config conf = new Config(); \n             conf.setNumWorkers(4); \n             try { \n                   StormSubmitter.submitTopology(args[4], conf, \n                               builder.createTopology()); \n             } catch (AlreadyAliveException alreadyAliveException) { \n                   System.out.println(alreadyAliveException); \n             } catch (InvalidTopologyException invalidTopologyException) { \n                   System.out.println(invalidTopologyException); \n             } \n       } else { \n             // create an instance of LocalCluster class for executing topology \n             // in local mode. \n             LocalCluster cluster = new LocalCluster(); \n             Config conf = new Config(); \n             conf.setDebug(true); \n             // Submit topology for execution \n             cluster.submitTopology(\"KafkaToplogy1\", conf, \n                         builder.createTopology()); \n\n             try { \n                   // Wait for sometime before exiting \n                   System.out \n                               .println(\"**********************Waiting to consume from kafka\"); \n                   Thread.sleep(100000); \n                   System.out.println(\"Stopping the sleep thread\"); \n\n             } catch (Exception exception) { \n                   System.out \n                               .println(\"******************Thread interrupted exception : \" \n                                           + exception); \n             } \n\n             // kill the KafkaTopology \n             cluster.killTopology(\"KafkaToplogy1\"); \n\n             // shutdown the storm test cluster \n             cluster.shutdown(); \n\n       } \n\n } \n} \n```", "```scala\nmysql> create database apachelog; \nmysql> use apachelog; \nmysql> create table apachelog( \n       id INT NOT NULL AUTO_INCREMENT, \n       ip VARCHAR(100) NOT NULL, \n       dateTime VARCHAR(200) NOT NULL, \n       request VARCHAR(100) NOT NULL, \n       response VARCHAR(200) NOT NULL, \n       bytesSent VARCHAR(200) NOT NULL, \n        referrer VARCHAR(500) NOT NULL, \n       useragent VARCHAR(500) NOT NULL, \n       country VARCHAR(200) NOT NULL, \n       browser VARCHAR(200) NOT NULL, \n       os VARCHAR(200) NOT NULL, \n       keyword VARCHAR(200) NOT NULL, \n       PRIMARY KEY (id) \n ); \n```", "```scala\n> mvn clean install -DskipTests \n```", "```scala\n> java -cp target/logprocessing-0.0.1-SNAPSHOT-jar-with-dependencies.jar:$STORM_HOME/storm-core-0.9.0.1.jar:$STORM_HOME/lib/* com.stormadvance.logprocessing.LogProcessingTopology path/to/GeoLiteCity.dat localhost apachelog root root \n```", "```scala\nmysql> select * from apachelog limit 2 \n    -> ; \n+----+----------------+--------------------------+----------------+----------+-----------+-----------------------------------------+-----------------------------------------------------------------------------------------+---------------+----------------+-------+---------+ \n| id | ip             | dateTime                 | request        | response | bytesSent | referrer                                | useragent                                                                               | country       | browser        | os    | keyword | \n+----+----------------+--------------------------+----------------+----------+-----------+-----------------------------------------+-----------------------------------------------------------------------------------------+---------------+----------------+-------+---------+ \n|  1 | 24.25.135.19   | 1-01-2011:06:20:31 -0500 | GET / HTTP/1.1 | 200      | 864       | http://www.adeveloper.com/resource.html | Mozilla/5.0 (Windows; U; Windows NT 5.1; hu-HU; rv:1.7.12) Gecko/20050919 Firefox/1.0.7 | United States | Gecko(Firefox) | WinXP | NA      | \n|  2 | 180.183.50.208 | 1-01-2011:06:20:31 -0500 | GET / HTTP/1.1 | 200      | 864       | http://www.adeveloper.com/resource.html | Mozilla/5.0 (Windows; U; Windows NT 5.1; hu-HU; rv:1.7.12) Gecko/20050919 Firefox/1.0.7 | Thailand      | Gecko(Firefox) | WinXP | NA      | \n+----+----------------+--------------------------+----------------+----------+-----------+-----------------------------------------+-----------------------------------------------------------------------------------------+---------------+----------------+-------+---------+ \n```", "```scala\nmysql> select country, count(*) from apachelog group by country; \n+---------------------------+----------+ \n| country                   | count(*) | \n+---------------------------+----------+ \n| Asia/Pacific Region       |        9 | \n| Belarus                   |       12 | \n| Belgium                   |       12 | \n| Bosnia and Herzegovina    |       12 | \n| Brazil                    |       36 | \n| Bulgaria                  |       12 | \n| Canada                    |      218 | \n| Europe                    |       24 | \n| France                    |       44 | \n| Germany                   |       48 | \n| Greece                    |       12 | \n| Hungary                   |       12 | \n| India                     |      144 | \n| Indonesia                 |       60 | \n| Iran, Islamic Republic of |       12 | \n| Italy                     |       24 | \n| Japan                     |       12 | \n| Malaysia                  |       12 | \n| Mexico                    |       36 | \n| NA                        |       10 | \n| Nepal                     |       24 | \n| Netherlands               |      164 | \n| Nigeria                   |       24 | \n| Puerto Rico               |       72 | \n| Russian Federation        |       60 | \n| Singapore                 |      165 | \n| Spain                     |       48 | \n| Sri Lanka                 |       12 | \n| Switzerland               |        7 | \n| Taiwan                    |       12 | \n| Thailand                  |       12 | \n| Ukraine                   |       12 | \n| United Kingdom            |       48 | \n| United States             |     5367 | \n| Vietnam                   |       12 | \n| Virgin Islands, U.S.      |      129 | \n+---------------------------+----------+ \n36 rows in set (0.08 sec) \n```", "```scala\nmysql> select browser, count(*) from apachelog group by browser; \n+----------------+----------+ \n| browser        | count(*) | \n+----------------+----------+ \n| Gecko(Firefox) |     6929 | \n+----------------+----------+ \n1 row in set (0.00 sec)  \n```", "```scala\nmysql> select os,count(*) from apachelog group by os; \n+-------+----------+ \n| os    | count(*) | \n+-------+----------+ \n| WinXP |     6929 | \n+-------+----------+ \n1 row in set (0.00 sec) \n```"]