["```scala\nfrom pyspark.sql import SparkSession\nspark = SparkSession.builder \\\n                    .master(\"local\") \\\n                    .appName(\"Predicting Fire Dept Calls\") \\\n                    .config(\"spark.executor.memory\", \"6gb\") \\\n                    .getOrCreate()\n\ndf = spark.read.format('com.databricks.spark.csv')\\\n                    .options(header='true', inferschema='true')\\\n                    .load('Fire_Department_Calls_for_Service.csv')\ndf.show(2)\n```", "```scala\ndf.select('Call Type Group').distinct().show()\n```", "```scala\ndf.groupBy('Call Type Group').count().show()\n```", "```scala\ndf2 = df.groupBy('Call Type Group').count()\ngraphDF = df2.toPandas()\ngraphDF = graphDF.sort_values('count', ascending=False)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ngraphDF.plot(x='Call Type Group', y = 'count', kind='bar')\nplt.title('Call Type Group by Count')\nplt.show()\n```", "```scala\ndf.groupBy('Call Type').count().orderBy('count', ascending=False).show(100)\n```", "```scala\nfrom pyspark.sql import functions as F\nfireIndicator = df.select(df[\"Call Type\"],F.when(df[\"Call Type\"].like(\"%Fire%\"),1)\\\n                          .otherwise(0).alias('Fire Indicator'))\nfireIndicator.show()\n```", "```scala\nfireIndicator.groupBy('Fire Indicator').count().show()\n```", "```scala\ndf = df.withColumn(\"fireIndicator\",\\ \nF.when(df[\"Call Type\"].like(\"%Fire%\"),1).otherwise(0))\n```", "```scala\ndf.printSchema()\n```", "```scala\nCASE WHEN Call Type LIKE %Fire% THEN 1 ELSE 0 END\n```", "```scala\ndf = df.select('fireIndicator', \n    'Zipcode of Incident',\n    'Battalion',\n    'Station Area',\n    'Box', \n    'Number of Alarms',\n    'Unit sequence in call dispatch',\n    'Neighborhooods - Analysis Boundaries',\n    'Fire Prevention District',\n    'Supervisor District')\ndf.show(5)\n```", "```scala\nprint('Total Rows')\ndf.count()\nprint('Rows without Null values')\ndf.dropna().count()\nprint('Row with Null Values')\ndf.count()-df.dropna().count()\n```", "```scala\ndf = df.dropna()\n```", "```scala\ndf.groupBy('fireIndicator').count().orderBy('count', ascending = False).show()\n```", "```scala\nfrom pyspark.ml.feature import StringIndexer\n```", "```scala\ncolumn_names = df.columns[1:]\n```", "```scala\ncategoricalColumns = column_names\nindexers = []\nfor categoricalCol in categoricalColumns:\n    stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"_Index\")\n    indexers += [stringIndexer]\n```", "```scala\nmodels = []\nfor model in indexers:\n    indexer_model = model.fit(df)\n    models+=[indexer_model]\n\nfor i in models:\n    df = i.transform(df)\n```", "```scala\ndf = df.select(\n          'fireIndicator',\n          'Zipcode of Incident_Index',\n          'Battalion_Index',\n          'Station Area_Index',\n          'Box_Index',\n          'Number of Alarms_Index',\n          'Unit sequence in call dispatch_Index',\n          'Neighborhooods - Analysis Boundaries_Index',\n          'Fire Prevention District_Index',\n          'Supervisor District_Index')\n```", "```scala\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.evaluation import BinaryClassificationEvaluator\nfrom pyspark.ml.classification import LogisticRegression\n```", "```scala\nfeatures = df.columns[1:]\n```", "```scala\nfrom pyspark.ml.feature import VectorAssembler\nfeature_vectors = VectorAssembler(\n    inputCols = features,\n    outputCol = \"features\")\n```", "```scala\ndf = feature_vectors.transform(df)\n```", "```scala\ndf = df.drop( 'Zipcode of Incident_Index',\n              'Battalion_Index',\n              'Station Area_Index',\n              'Box_Index',\n              'Number of Alarms_Index',\n              'Unit sequence in call dispatch_Index',\n              'Neighborhooods - Analysis Boundaries_Index',\n              'Fire Prevention District_Index',\n              'Supervisor District_Index')\n```", "```scala\ndf = df.withColumnRenamed('fireIndicator', 'label')\n```", "```scala\n(trainDF, testDF) = df.randomSplit([0.75, 0.25], seed = 12345)\n```", "```scala\nfrom pyspark.ml.classification import LogisticRegression\nlogreg = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\nLogisticRegressionModel = logreg.fit(trainDF)\n```", "```scala\ndf_predicted = LogisticRegressionModel.transform(testDF)\n```", "```scala\ndf_predicted.crosstab('label', 'prediction').show()\n```", "```scala\nfrom sklearn import metrics\n```", "```scala\nactual = df_predicted.select('label').toPandas()\npredicted = df_predicted.select('prediction').toPandas()\n```", "```scala\nmetrics.accuracy_score(actual, predicted)\n```"]