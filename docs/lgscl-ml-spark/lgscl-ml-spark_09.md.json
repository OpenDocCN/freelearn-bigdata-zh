["```scala\nstatic SparkSession spark = SparkSession \n      .builder() \n      .appName(\"JavaLDAExample\") \n         .master(\"local[*]\") \n         .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\"). \n          getOrCreate(); \n\n```", "```scala\nString ratigsFile = \"input/ratings.csv\"; \nDataset<Row> df1 = spark.read().format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(ratigsFile);     \nDataset<Row> ratingsDF = df1.select(df1.col(\"userId\"), df1.col(\"movieId\"), df1.col(\"rating\"),  df1.col(\"timestamp\")); \nratingsDF.show(false); \n\n```", "```scala\nString moviesFile = \"input/movies.csv\"; \nDataset<Row> df2 = spark.read().format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(moviesFile); \nDataset<Row> moviesDF = df2.select(df2.col(\"movieId\"), df2.col(\"title\"), df2.col(\"genres\"));  \nmoviesDF.show(false); \n\n```", "```scala\nratingsDF.createOrReplaceTempView(\"ratings\"); \nmoviesDF.createOrReplaceTempView(\"movies\"); \n\n```", "```scala\nlong numRatings = ratingsDF.count(); \nlong numUsers = ratingsDF.select(ratingsDF.col(\"userId\")).distinct().count(); \nlong numMovies = ratingsDF.select(ratingsDF.col(\"movieId\")).distinct().count(); \nSystem.out.println(\"Got \" + numRatings + \" ratings from \" + numUsers + \" users on \" + numMovies + \" movies.\"); \n\n```", "```scala\nDataset<Row> results = spark.sql(\"select movies.title, movierates.maxr, movierates.minr, movierates.cntu \" + \"from(SELECT ratings.movieId,max(ratings.rating) as maxr,\"  + \"min(ratings.rating) as minr,count(distinct userId) as cntu \"  + \"FROM ratings group by ratings.movieId) movierates \" + \"join movies on movierates.movieId=movies.movieId \" + \"order by movierates.cntu desc\"); \nresults.show(false); \n\n```", "```scala\nDataset<Row> mostActiveUsersSchemaRDD = spark.sql(\"SELECT ratings.userId, count(*) as ct from ratings \" + \"group by ratings.userId order by ct desc limit 10\"); \nmostActiveUsersSchemaRDD.show(false); \n\n```", "```scala\nDataset<Row> results2 = spark.sql(\"SELECT ratings.userId, ratings.movieId,\" + \"ratings.rating, movies.title FROM ratings JOIN movies \"+ \"ON movies.movieId=ratings.movieId \" + \"where ratings.userId=668 and ratings.rating > 4\"); \nresults2.show(false); \n\n```", "```scala\nDataset<Row> [] splits = ratingsDF.randomSplit(new double[] { 0.8, 0.2 }); \nDataset<Row> trainingData = splits[0]; \nDataset<Row> testData = splits[1]; \nlong numTraining = trainingData.count(); \nlong numTest = testData.count(); \nSystem.out.println(\"Training: \" + numTraining + \" test: \" + numTest); \n\n```", "```scala\nJavaRDD<Rating> ratingsRDD = trainingData.toJavaRDD().map(new Function<Row, Rating>() { \n      @Override \n      public Rating call(Row r) throws Exception { \n        // TODO Auto-generated method stub \n        int userId = Integer.parseInt(r.getString(0)); \n        int movieId = Integer.parseInt(r.getString(1)); \n        double ratings = Double.parseDouble(r.getString(2)); \n        return new Rating(userId, movieId, ratings); \n      } \n    }); \n\n```", "```scala\nJavaRDD<Rating> testRDD = testData.toJavaRDD().map(new Function<Row, Rating>() { \n      @Override \n      public Rating call(Row r) throws Exception { \n        int userId = Integer.parseInt(r.getString(0)); \n        int movieId = Integer.parseInt(r.getString(1)); \n        double ratings = Double.parseDouble(r.getString(2)); \n        return new Rating(userId, movieId, ratings); \n      } \n    }); \n\n```", "```scala\nint rank = 20; \nint numIterations = 10; \ndouble lambda = 0.01; \nMatrixFactorizationModel model = ALS.train(JavaRDD.toRDD(ratingsRDD), rank, numIterations, 0.01); \n\n```", "```scala\nSystem.out.println(\"Rating:(UserID, MovieId, Rating)\"); \nRating[] topRecsForUser = model.recommendProducts(668, 6); \nfor (Rating rating : topRecsForUser) \nSystem.out.println(rating.toString()); \n\n```", "```scala\nJavaRDD<Tuple2<Object, Object>> testUserProductRDD = testData.toJavaRDD() \n        .map(new Function<Row, Tuple2<Object, Object>>() { \n          @Override \n          public Tuple2<Object, Object> call(Row r) throws Exception { \n\n            int userId = Integer.parseInt(r.getString(0)); \n            int movieId = Integer.parseInt(r.getString(1)); \n            double ratings = Double.parseDouble(r.getString(2)); \n            return new Tuple2<Object, Object>(userId, movieId); \n          } \n        }); \nJavaRDD<Rating> predictionsForTestRDD = model.predict(JavaRDD.toRDD(testUserProductRDD)).toJavaRDD(); \n\n```", "```scala\nSystem.out.println(predictionsForTestRDD.take(10).toString()); \n\n```", "```scala\nJavaPairRDD<Tuple2<Integer, Integer>, Double> predictionsKeyedByUserProductRDD = JavaPairRDD.fromJavaRDD( \n        predictionsForTestRDD.map(new Function<Rating, Tuple2<Tuple2<Integer, Integer>, Double>>() { \n          @Override \n          public Tuple2<Tuple2<Integer, Integer>, Double> call(Rating r) throws Exception { \n            return new Tuple2<Tuple2<Integer, Integer>, Double>( \n                new Tuple2<Integer, Integer>(r.user(), r.product()), r.rating()); \n          } \n        })); \n\n```", "```scala\nJavaPairRDD<Tuple2<Integer, Integer>, Double> testKeyedByUserProductRDD = JavaPairRDD  .fromJavaRDD(testRDD.map(new Function<Rating, Tuple2<Tuple2<Integer, Integer>, Double>>() { \n          @Override \n          public Tuple2<Tuple2<Integer, Integer>, Double> call(Rating r) throws Exception { \n            return new Tuple2<Tuple2<Integer, Integer>, Double>( \n                new Tuple2<Integer, Integer>(r.user(), r.product()), r.rating()); \n          } \n        })); \n\n```", "```scala\nJavaPairRDD<Tuple2<Integer, Integer>, Tuple2<Double, Double>> testAndPredictionsJoinedRDD = testKeyedByUserProductRDD \n        .join(predictionsKeyedByUserProductRDD); \nSystem.out.println(\"(UserID, MovieId) => (Test rating, Predicted rating)\"); \nSystem.out.println(\"----------------------------------\"); \nfor (Tuple2<Tuple2<Integer, Integer>, Tuple2<Double, Double>> t : testAndPredictionsJoinedRDD.take(6)) { \n      Tuple2<Integer, Integer> userProduct = t._1; \n      Tuple2<Double, Double> testAndPredictedRating = t._2; \n      System.out.println(\"(\" + userProduct._1() + \",\" + userProduct._2() + \") => (\" + testAndPredictedRating._1() \n          + \",\" + testAndPredictedRating._2() + \")\"); \n    } \n\n```", "```scala\nJavaPairRDD<Tuple2<Integer, Integer>, Tuple2<Double, Double>> truePositives = testAndPredictionsJoinedRDD \n        .filter(new Function<Tuple2<Tuple2<Integer, Integer>, Tuple2<Double, Double>>, Boolean>() { \n          @Override \n          public Boolean call(Tuple2<Tuple2<Integer, Integer>, Tuple2<Double, Double>> r) throws Exception { \n            return (r._2._1() <= 1 && r._2._2() < 5); \n          } \n        }); \n\n```", "```scala\nfor (Tuple2<Tuple2<Integer, Integer>, Tuple2<Double, Double>> t : truePositives.take(2)) { \n      Tuple2<Integer, Integer> userProduct = t._1; \n      Tuple2<Double, Double> testAndPredictedRating = t._2; \n    } \nSystem.out.println(\"Number of true positive prediction is: \"+ truePositives.count()); \n\n```", "```scala\nJavaPairRDD<Tuple2<Integer, Integer>, Tuple2<Double, Double>> falsePositives = testAndPredictionsJoinedRDD \n        .filter(new Function<Tuple2<Tuple2<Integer, Integer>, Tuple2<Double, Double>>, Boolean>() { \n          @Override \n          public Boolean call(Tuple2<Tuple2<Integer, Integer>, Tuple2<Double, Double>> r) throws Exception { \n            return (r._2._1() <= 1 && r._2._2() >= 5); \n          } \n        }); \nfor (Tuple2<Tuple2<Integer, Integer>, Tuple2<Double, Double>> t : falsePositives.take(2)) { \n      Tuple2<Integer, Integer> userProduct = t._1; \n      Tuple2<Double, Double> testAndPredictedRating = t._2; \n    } \nSystem.out.println(\"Number of false positive prediction is: \"+ falsePositives.count()); \n\n```", "```scala\ndouble meanAbsoluteError = JavaDoubleRDD        .fromRDD(testAndPredictionsJoinedRDD.values().map(new Function<Tuple2<Double, Double>, Object>() { \n          public Object call(Tuple2<Double, Double> pair) { \n            Double err = pair._1() - pair._2(); \n            return err * err; \n          } \n        }).rdd()).mean(); \n    System.out.printing(\"Mean Absolute Error: \"+meanAbsoluteError); \n\n```", "```scala\nMean Absolute Error: 1.5800601618477566 \n\n```", "```scala\nimport org.apache.log4j.Level; \nimport org.apache.log4j.Logger; \nimport org.apache.spark.SparkConf; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.streaming.Duration; \nimport org.apache.spark.streaming.api.java.JavaDStream; \nimport org.apache.spark.streaming.api.java.JavaStreamingContext; \nimport org.apache.spark.streaming.twitter.TwitterUtils; \nimport twitter4j.Status;  \n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.OFF); \nLogger.getLogger(\"akka\").setLevel(Level.OFF); \nLogger.getLogger(\"org.apache.spark\").setLevel(Level.WARN); \nLogger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF);  \n\n```", "```scala\nSparkConf conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"TwitterExample\"); \nJavaStreamingContext jssc = new JavaStreamingContext(conf, new Duration(1000)); \n\n```", "```scala\nString consumerKey = \"VQINrM6ZcNqaCAawA6IN4xRTP\"; \nString consumerSecret = \"F2OsVEuJypOZSAoNDFrWgoCHyNJNXbTr8T3yEbp9cWEYjTctye\"; \nString accessToken = \"475468363-IfRcZnbkEVPRw6bwXovMnw1FsbxuetvEF2JvbAvD\"; \nString accessTokenSecret = \"vU7VtzZVyugUHO7ddeTvucu1wRrCZqFTPJUW8VAe6xgyf\"; \n\n```", "```scala\nSystem.setProperty(\"twitter4j.oauth.consumerKey\", consumerKey); \nSystem.setProperty(\"twitter4j.oauth.consumerSecret\", consumerSecret); \nSystem.setProperty(\"twitter4j.oauth.accessToken\", accessToken); \nSystem.setProperty(\"twitter4j.oauth.accessTokenSecret\", accessTokenSecret); \n\n```", "```scala\njssc.checkpoint(\"src/main/resources/twitterdata/\"); \n\n```", "```scala\nJavaDStream<Status> tweets = TwitterUtils.createStream(jssc); \nfinal String outputDirectory=\"src/main/resources/twitterdata/\"; \nfinal long numTweetsToCollect = 100; \n\n```", "```scala\ntweets.foreachRDD(new Function<JavaRDD<Status>, Void>() { \n      public long numTweetsCollected = 0; \n      @Override \n      public Void call(JavaRDD<Status> status) throws Exception {         \n        long count = status.count(); \n        if (count > 0) { \n          status.saveAsTextFile(outputDirectory + \"/tweets_\" + System.currentTimeMillis()); \n          numTweetsCollected += count; \n          if (numTweetsCollected >= numTweetsToCollect) { \n               System.exit(0); \n          } \n        } \n        return null; \n      } \n    }); \n\n```", "```scala\njssc.start(); \njssc.awaitTermination();  \n\n```", "```scala\nimport java.io.Serializable; \nimport java.util.ArrayList; \nimport java.util.HashMap; \nimport java.util.Map; \nimport org.apache.log4j.Level; \nimport org.apache.log4j.Logger; \nimport org.apache.spark.SparkConf; \nimport org.apache.spark.api.java.JavaPairRDD; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.JavaSparkContext; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.api.java.function.Function2; \nimport org.apache.spark.api.java.function.PairFlatMapFunction; \nimport org.apache.spark.api.java.function.PairFunction; \nimport org.apache.spark.ml.feature.StopWordsRemover; \nimport org.apache.spark.mllib.clustering.LDA; \nimport org.apache.spark.mllib.clustering.LDAModel; \nimport org.apache.spark.mllib.linalg.Vector; \nimport org.apache.spark.mllib.linalg.Vectors; \nimport org.apache.spark.sql.SQLContext; \nimport scala.Tuple2; \n\n```", "```scala\nprivate transient static SparkConf sparkConf = new SparkConf().setMaster(\"local[*]\").setAppName(\"TopicModelingLDA\"); \nprivate transient static JavaSparkContext jsc = new JavaSparkContext(sparkConf); \nprivate transient static SQLContext sqlContext = new org.apache.spark.sql.SQLContext(jsc); \n\n```", "```scala\nLogger.getLogger(\"org\").setLevel(Level.OFF); \nLogger.getLogger(\"akka\").setLevel(Level.OFF); \nLogger.getLogger(\"org.apache.spark\").setLevel(Level.WARN); \nLogger.getLogger(\"org.eclipse.jetty.server\").setLevel(Level.OFF); \n\n```", "```scala\nJavaRDD<String> data = jsc.wholeTextFiles(\"src/main/resources/test/*.txt\") \n        .map(new Function<Tuple2<String, String>, String>() { \n          @Override \n          public String call(Tuple2<String, String> v1) throws Exception {     \n            return v1._2; \n          } \n        }).cache(); \n\n```", "```scala\npublic static String[] stopwords = new StopWordsRemover().getStopWords(); \nJavaRDD<String[]> tokenized = data.map(new Function<String, String[]>() { \nlist.toArray(new String[0]); \n      }      @Override \n      public String[] call(String v1) throws Exception { \n        ArrayList<String> list = new ArrayList<>(); \n        for (String s : v1.split(\"\\\\s\")) { \n          if (s.length() > 3 && !isStopWord(s) && isOnlyLetter(s)) \n            list.add(s.toLowerCase()); \n        } \n        return \n    }); \n\n```", "```scala\nJavaPairRDD<String, Integer> termCounts = data \n        .flatMapToPair(new PairFlatMapFunction<String, String, Integer>() { \n          @Override \n          public Iterable<Tuple2<String, Integer>> call(String t) throws Exception {     \n            ArrayList<Tuple2<String, Integer>> tc = new ArrayList<>();   \n            for (String s : t.split(\"\\\\s\")) { \n\n              if (s.length() > 3 && !isStopWord(s) && isOnlyLetter(s)) \n                tc.add(new Tuple2<String, Integer>(s.toLowerCase(), 1)); \n            } \n            return tc; \n          } \n        }).reduceByKey(new Function2<Integer, Integer, Integer>() { \n          @Override \n          public Integer call(Integer v1, Integer v2) throws Exception {     \n            return v1 + v2; \n          } \n        }); \n\n```", "```scala\nJavaPairRDD<String, Integer> termCountsSorted = termCounts \n        .mapToPair(new PairFunction<Tuple2<String, Integer>, Integer, String>() { \n          @Override \n          public Tuple2<Integer, String> call(Tuple2<String, Integer> t) throws Exception { \n            return t.swap(); \n          } \n        }).sortByKey().mapToPair(new PairFunction<Tuple2<Integer, String>, String, Integer>() { \n          @Override \n          public Tuple2<String, Integer> call(Tuple2<Integer, String> t) throws Exception { \n            return t.swap(); \n          } \n        }); \n\n```", "```scala\nJavaRDD<String> vocabArray = termCountsSorted.map(new Function<Tuple2<String, Integer>, String>() { \n      @Override \n      public String call(Tuple2<String, Integer> v1) throws Exception { \n        return v1._1; \n      } \n    }); \nfinal Map<String, Long> vocab = vocabArray.zipWithIndex().collectAsMap(); \n    for (Map.Entry<String, Long> entry : vocab.entrySet()) { \n      System.out.println(entry.getKey() + \"/\" + entry.getValue()); \n    } \n\n```", "```scala\nJavaPairRDD<Long, Vector> documents = JavaPairRDD \n        .fromJavaRDD(tokenized.zipWithIndex().map(new Function<Tuple2<String[], Long>, Tuple2<Long, Vector>>() { \n          @Override \n          public Tuple2<Long, Vector> call(Tuple2<String[], Long> v1) throws Exception { \n            String[] tokens = v1._1; \n            Map<Integer, Double> counts = new HashMap(); \n\n            for (String s : tokens) { \n              if (vocab.containsKey(s)) { \n                long idx = vocab.get(s); \n                int a = (int) idx; \n                if (counts.containsKey(a)) { \n                  counts.put(a, counts.get(a) + 1.0); \n                } else \n                  counts.put(a, 0.0); \n              } \n            } \n            ArrayList<Tuple2<Integer, Double>> ll = new ArrayList<>(); \n            ArrayList<Double> dd = new ArrayList<>(); \n\n            for (Map.Entry<Integer, Double> entry : counts.entrySet()) { \n              ll.add(new Tuple2<Integer, Double>(entry.getKey(), entry.getValue())); \n              dd.add(entry.getValue()); \n            } \n\n            return new Tuple2<Long, Vector>(v1._2, Vectors.sparse(vocab.size(), ll)); \n          } \n        })).cache(); \n\n```", "```scala\nLDAModel ldaModel = new LDA().setK(4).setMaxIterations(10).run(documents); \nTuple2<int[], double[]>[] topicDesces = ldaModel.describeTopics(10); \nint topicCount = topicDesces.length; \n\n```", "```scala\nfor (int t = 0; t < topicCount; t++) { \n      Tuple2<int[], double[]> topic = topicDesces[t]; \n      System.out.println(\"      Topic: \" + t); \n      int[] indices = topic._1(); \n      double[] values = topic._2(); \n      double sum = 0.0d; \n      int wordCount = indices.length; \n      System.out.println(\"Terms |\\tIndex |\\tWeight\"); \n      System.out.println(\"------------------------\"); \n      for (int w = 0; w < wordCount; w++) { \n        double prob = values[w]; \n        int vocabIndex = indices[w]; \n        String vocabKey = \"\"; \n        for (Map.Entry<String, Long> entry : vocab.entrySet()) { \n          if (entry.getValue() == vocabIndex) { \n            vocabKey = entry.getKey(); \n            break; \n          } } \nSystem.out.format(\"%s \\t %d \\t %f \\n\", vocabKey, vocabIndex, prob); \n        sum += prob; \n      } \n      System.out.println(\"--------------------\"); \n      System.out.println(\"Sum:= \" + sum); \n      System.out.println(); \n    }  } \n\n```", "```scala\npublic static boolean isStopWord(String word) { \n    for (String s : stopwords) { \n      if (word.equals(s))   \n        return true; \n    } \n    return false; \n  } \n\n```", "```scala\npublic static boolean isOnlyLetter(String word) { \n    for (Character ch : word.toCharArray()) { \n      if (!Character.isLetter(ch)) \n        return false; \n    } \n    return true; \n  } \n\n```", "```scala\n$ cd home/spark-2.0.0-bin-hadoop2.7/bin\n$./spark-shell\n\n```", "```scala\npackage com.examples.graphs \nimport org.apache.spark._ \nimport org.apache.spark.graphx._ \nimport org.apache.spark.rdd.RDD  \n\n```", "```scala\nval conf = new SparkConf().setAppName(\"GraphXDemo\").setMaster(\"local[*]\") \nval sc = new SparkContext(conf) \n\n```", "```scala\nval corpus: RDD[String] = sc.wholeTextFiles(\"home/ /topics/*.txt\").map(_._2) \nval tokenized: RDD[Seq[String]] = corpus.map(_.toLowerCase.split(\"\\\\s\")) \ntokenized.foreach { x => println(x) } \n\n```", "```scala\nval nodes: RDD[(VertexId, (String, Long))] = tokenized.zipWithIndex().map{  \n      case (tokens, id) => \n        val nodeName=\"Topic_\"+id; \n        (id, (nodeName, tokens.size)) \n\n    } \n    nodes.collect().foreach{ \n      x =>  \n        println(x._1+\": (\"+x._2._1+\",\"+x._2._2+\")\")        \n    } \n\n```", "```scala\nval wordPairs: RDD[(String, Long)] = \n      tokenized.zipWithIndex.flatMap { \n        case (tokens, id) => \n          val list = new Array[(String, Long)](tokens.size) \n\n          for (i <- 0 to tokens.length - 1) { \n            //tokens.foreach { term => \n            list(i) = (tokens(i), id) \n          } \n          list.toSeq \n      } \n    wordPairs.collect().foreach(x => println(x)) \n    println(wordPairs.count()) \n\n```", "```scala\nval relationships: RDD[Edge[String]] = wordPairs.groupByKey().flatMap{ \n      case(edge, nodes)=> \n        val nodesList = nodes.toArray \n        val list = new Array[Edge[String]](nodesList.length * nodesList.length) \n        if (nodesList.length>1){                   \n          var count:Int=0; \n          for (i <- 0 to nodesList.length-2) { \n            for(j<-i+1 to nodesList.length-1){ \n         list(count) = new Edge(nodesList(i), nodesList(j), edge)  \n         //list(count+1) = new Edge(nodesList(j), nodesList(i), edge) \n              count += 1; \n              //count += 2; \n            } \n          } \n        } \n        list.toSeq \n    }.filter { x => x!=null } \n    relationships.collect().foreach { x => println(x) } \n\n```", "```scala\nlist(count+1) = new Edge(nodesList(j), nodesList(i), edge) \n\n```", "```scala\nlist(count) = new Edge(nodesList(i), nodesList(j), edge)  \n\n```", "```scala\nval graph = Graph(nodes, relationships) \nprintln(graph.edges.count) \n\n```", "```scala\nval facts: RDD[String] = \n      graph.triplets.map(triplet => \n        triplet.srcAttr._1 + \" contains the terms \"\" + triplet.attr + \"\" like as \" + triplet.dstAttr._1) \n    facts.collect.foreach(println(_)) \n\n```"]