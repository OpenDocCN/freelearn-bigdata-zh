["```scala\ninstall.packages(\"moments\")  \n\n```", "```scala\nlibrary(moments) \n\n```", "```scala\ntime_taken <- c (15, 16, 18, 17.16, 16.5, 18.6, 19.0, 20.4, 20.6, 25.15, 27.27, 25.24, 21.05, 21.65, 20.92, 22.61, 23.71, 35, 39, 50) \n\n```", "```scala\ndf<- data.frame(time_taken) \n\n```", "```scala\nskewness(df) \n[1]1.769592  \n\n```", "```scala\n> kurtosis(df) \n[1]5.650427  \n\n```", "```scala\ninstall.packages(\"ggplot2\") \n\n```", "```scala\nlibrary(ggplot2)\n\n```", "```scala\nggplot(df, aes(x = time_taken)) + stat_density(geom=\"line\", col=  \n\"green\", size = 1, bw = 4) + theme_bw() \n\n```", "```scala\n  static SparkSession spark = SparkSession \n      .builder().appName(\"JavaLDAExample\") \n      .master(\"local[*]\") \n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n      .getOrCreate(); \n\n```", "```scala\npublic static void main(String[] args) { \n // Prepare training documents, which are labelled. \n Dataset<Row> smsspamdataset = spark.createDataFrame(Arrays.asList( \n      new SMSSpamHamLabelDocument(0.0, \"What you doing how are you\"), \n      new SMSSpamHamLabelDocument(0.0, \"Ok lar Joking wif u oni\"), \n      new SMSSpamHamLabelDocument(1.0, \"FreeMsg Txt CALL to No 86888 & claim your reward of 3 hours talk time to use from your phone now ubscribe6GBP mnth inc 3hrs 16 stop txtStop\"), \n      new SMSSpamHamLabelDocument(0.0, \"dun say so early hor U c already then say\"), \n      new SMSSpamHamLabelDocument(0.0, \"MY NO IN LUTON 0125698789 RING ME IF UR AROUND H\"), \n      new SMSSpamHamLabelDocument(1.0, \"Sunshine Quiz Win a super Sony DVD recorder if you canname the capital of Australia Text MQUIZ to 82277 B\") \n    ), SMSSpamHamLabelDocument.class); \n\n```", "```scala\nSmsspamdataset.show(); \n\n```", "```scala\npublic class SMSSpamHamLabelDocument implements Serializable { \n    private double label; \n    private String wordText; \n    public SMSSpamHamLabelDocument(double label, String wordText) { \n      this.label = label; \n      this.wordText = wordText; \n    } \n    public double getLabel() { return this.label; } \n    public void setLabel(double id) { this.label = label; } \n    public String getWordText() { return this.wordText; }    public void setWordText(String wordText) { this.wordText = wordText; } \n}  } \n\n```", "```scala\nDataset<Row>[] splits = smsspamdataset.randomSplit(new double[] { 0.6, 0.4 }); \nDataset<Row> trainingData = splits[0]; \nDataset<Row> testData = splits[1]; \n\n```", "```scala\nTokenizer tokenizer = new Tokenizer() \n      .setInputCol(\"wordText\") \n      .setOutputCol(\"words\"); \nHashingTF hashingTF = new HashingTF() \n      .setNumFeatures(100) \n      .setInputCol(tokenizer.getOutputCol()) \n      .setOutputCol(\"features\"); \nLogisticRegression logisticRegression = new LogisticRegression() \n      .setMaxIter(10) \n      .setRegParam(0.01); \nPipeline pipeline = new Pipeline().setStages(new PipelineStage[] {tokenizer, hashingTF, logisticRegression}); \n    // Fit the pipeline to training documents. \nPipelineModel model = pipeline.fit(trainingData); \nDataset<Row> predictions = model.transform(testData); \nfor (Row r: predictions.select(\"label\", \"wordText\", \"prediction\").collectAsList()) { \n  System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prediction=\" + r.get(2)); \n    } } \n\n```", "```scala\n(0.0, What you doing how are you)  \n--> prediction=0.0 \n(0.0, MY NO IN LUTON 0125698789 RING ME IF UR AROUND H)  \n--> prediction=0.0 \n(1.0, Sunshine Quiz Win a super Sony DVD recorder if you canname the capital of Australia Text MQUIZ to 82277 B)  \n--> prediction=0.0 \n\n```", "```scala\nDataset<Row> df = spark.read().text(\"input/SMSSpamCollection.txt\"); \ndf.show();  \n\n```", "```scala\n// Feature Transformers (RegexTokenizer) \nRegexTokenizer regexTokenizer1 = new RegexTokenizer() \n        .setInputCol(\"value\") \n        .setOutputCol(\"labelText\") \n        .setPattern(\"\\\\t.*$\");     \nDataset<Row> labelTextDataFrame = regexTokenizer1.transform(df); \nRegexTokenizer regexTokenizer2 = new RegexTokenizer() \n        .setInputCol(\"value\").setOutputCol(\"text\").setPattern(\"\\\\W\"); \nDataset<Row> labelFeatureDataFrame = regexTokenizer2 \n        .transform(labelTextDataFrame); \nfor (Row r : labelFeatureDataFrame.select(\"text\", \"labelText\").collectAsList()) { \n      System.out.println( r.getAs(1) + \": \" + r.getAs(0)); \n    } \n\n```", "```scala\nWrappedArray(ham): WrappedArray(ham, what, you, doing, how, are, you) \nWrappedArray(ham): WrappedArray(ham, ok, lar, joking, wif, u, oni) \nWrappedArray(ham): WrappedArray(ham, dun, say, so, early, hor, u, c, already, then, say) \nWrappedArray(ham): WrappedArray(ham, my, no, in, luton, 0125698789, ring, me, if, ur, around, h) \nWrappedArray(spam): WrappedArray(spam, freemsg, txt, call, to, no, 86888, claim, your, reward, of, 3, hours, talk, time, to, use, from, your, phone, now, ubscribe6gbp, mnth, inc, 3hrs, 16, stop, txtstop) \nWrappedArray(ham): WrappedArray(ham, siva, is, in, hostel, aha) \nWrappedArray(ham): WrappedArray(ham, cos, i, was, out, shopping, wif, darren, jus, now, n, i, called, him, 2, ask, wat, present, he, wan, lor, then, he, started, guessing, who, i, was, wif, n, he, finally, guessed, darren, lor) \nWrappedArray(spam): WrappedArray(spam, sunshine, quiz, win, a, super, sony, dvd, recorder, if, you, canname, the, capital, of, australia, text, mquiz, to, 82277, b) \n\n```", "```scala\nDataset<Row> newDF = labelFeatureDataFrame.withColumn(\"labelTextTemp\",        labelFeatureDataFrame.col(\"labelText\").cast(DataTypes.StringType))        .drop(labelFeatureDataFrame.col(\"labelText\"))        .withColumnRenamed(\"labelTextTemp\", \"labelText\"); \n\n```", "```scala\n// Feature Transformer (StringIndexer) \nStringIndexer indexer = new StringIndexer().setInputCol(\"labelText\") \n        .setOutputCol(\"label\"); \nDataset<Row> indexed = indexer.fit(newDF).transform(newDF); \n    indexed.select(indexed.col(\"labelText\"), indexed.col(\"label\"), indexed.col(\"text\")).show();  \n\n```", "```scala\n// Feature Transformers (StopWordsRemover) \nStopWordsRemover remover = new StopWordsRemover(); \nString[] stopwords = remover.getStopWords(); \nString[] newStopworks = new String[stopwords.length+2]; \nnewStopworks[0]=\"spam\"; \nnewStopworks[1]=\"ham\"; \nfor(int i=2;i<stopwords.length;i++){ \n      newStopworks[i]=stopwords[i];}   \nremover.setStopWords(newStopworks).setInputCol(\"text\").setOutputCol(\"filteredWords\"); \nDataset<Row> filteredDF = remover.transform(indexed); \nfilteredDF.select(filteredDF.col(\"label\"), filteredDF.col(\"filteredWords\")).show();  \n\n```", "```scala\n// Feature Extractors (HashingTF transformer) \nint numFeatures = 100; \nHashingTF hashingTF = new HashingTF().setInputCol(\"filteredWords\") \n        .setOutputCol(\"rawFeatures\").setNumFeatures(numFeatures); \nDataset<Row> featurizedData = hashingTF.transform(filteredDF); \n    for (Row r : featurizedData.select(\"rawFeatures\", \"label\").collectAsList()) { \nVector features = r.getAs(0); ////Problematic line \nDouble label = r.getDouble(1); \nSystem.out.println(label + \",\" + features); \n    }  \n\n```", "```scala\n0.0,(100,[19],[1.0]) \n0.0,(100,[9,16,17,48,86,96],[1.0,1.0,1.0,1.0,1.0,1.0]) \n0.0,(100,[17,37,43,71,99],[1.0,1.0,2.0,1.0,2.0]) \n0.0,(100,[4,41,42,47,92],[1.0,1.0,1.0,1.0,1.0]) \n1.0,(100,[3,12,19,26,28,29,34,41,46,51,71,73,88,93,94,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]) \n0.0,(100,[19,25,38],[1.0,1.0,1.0]) \n0.0,(100,[8,10,16,30,37,43,48,49,50,55,76,82,89,95,99],[1.0,4.0,2.0,1.0,1.0,2.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0]) \n1.0,(100,[0,24,36,39,42,48,53,58,67,86,95,97,98],[1.0,1.0,1.0,1.0,1.0,1.0,1.0,1.0,2.0,1.0,1.0,2.0,1.0]) \n\n```", "```scala\n// Feature Extractors (IDF Estimator) \nIDF idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\"); \nIDFModel idfModel = idf.fit(featurizedData); \nDataset<Row> rescaledData = idfModel.transform(featurizedData); \nfor (Row r : rescaledData.select(\"features\", \"label\").collectAsList()) { \nVector features = r.getAs(0); \nDouble label = r.getDouble(1); \nSystem.out.println(label + \",\" + features); \n    }  \n\n```", "```scala\n0.0,(100,[19],[0.8109302162163288]) \n0.0,(100,[9,16,17,48,86,96],[1.5040773967762742,1.0986122886681098,1.0986122886681098,0.8109302162163288,1.0986122886681098,1.5040773967762742]) \n0.0,(100,[17,37,43,71,99],[1.0986122886681098,1.0986122886681098,2.1972245773362196,1.0986122886681098,2.1972245773362196]) \n0.0,(100,[4,41,42,47,92],[1.5040773967762742,1.0986122886681098,1.0986122886681098,1.5040773967762742,1.5040773967762742]) \n1.0,(100,[3,12,19,26,28,29,34,41,46,51,71,73,88,93,94,98],[1.5040773967762742,1.5040773967762742,0.8109302162163288,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.0986122886681098,1.5040773967762742,1.5040773967762742,1.0986122886681098,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,2.1972245773362196]) \n0.0,(100,[19,25,38],[0.8109302162163288,1.5040773967762742,1.5040773967762742]) \n0.0,(100,[8,10,16,30,37,43,48,49,50,55,76,82,89,95,99],[1.5040773967762742,6.016309587105097,2.1972245773362196,1.5040773967762742,1.0986122886681098,2.1972245773362196,0.8109302162163288,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.0986122886681098,2.1972245773362196]) \n1.0,(100,[0,24,36,39,42,48,53,58,67,86,95,97,98],[1.5040773967762742,1.5040773967762742,1.5040773967762742,1.5040773967762742,1.0986122886681098,0.8109302162163288,1.5040773967762742,1.5040773967762742,3.0081547935525483,1.0986122886681098,1.0986122886681098,3.0081547935525483,1.0986122886681098]) \n\n```", "```scala\norg.apache.spark.ml.feature.ChiSqSelector selector = new org.apache.spark.ml.feature.ChiSqSelector(); \nselector.setNumTopFeatures(3).setFeaturesCol(\"features\") \n        .setLabelCol(\"label\").setOutputCol(\"selectedFeatures\"); \nDataset<Row> result = selector.fit(rescaledData).transform(rescaledData); \n    for (Row r : result.select(\"selectedFeatures\", \"label\").collectAsList()) { \n  Vector features = r.getAs(0); \n  Double label = r.getDouble(1); \n  System.out.println(label + \",\" + features); \n    } \n\n```", "```scala\n0.0,(3,[],[]) \n0.0,(3,[],[]) \n0.0,(3,[],[]) \n0.0,(3,[],[]) \n1.0,(3,[1,2],[1.5040773967762742,2.1972245773362196]) \n0.0,(3,[],[]) \n0.0,(3,[],[]) \n1.0,(3,[0,2],[1.5040773967762742,1.0986122886681098]) \n\n```"]