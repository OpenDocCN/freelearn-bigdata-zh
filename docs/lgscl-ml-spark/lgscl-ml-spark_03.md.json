["```scala\nstatic SparkSession spark = SparkSession \n      .builder() \n      .appName(\"JavaLDAExample\") \n      .master(\"local[*]\") \n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n      .getOrCreate(); \n\n```", "```scala\nString csvFile = \"input/test/1.txt\"; \nRDD<String> distFile = spark.sparkContext().textFile(csvFile, 2); \n\n```", "```scala\nJavaRDD<String> distFile2 = distFile.toJavaRDD(); \n\n```", "```scala\nRDD<Tuple2<String, String>> distFile = spark.sparkContext().wholeTextFiles(\"csvFiles/*.txt\", 2); \nJavaRDD<Tuple2<String, String>> distFile2 = distFile.toJavaRDD(); \n\n```", "```scala\nSparkConf conf = new SparkConf().setAppName(\"SampleAppliation\").setMaster(\"local[*]\"); \nJavaSparkContext sc = new JavaSparkContext(conf); \n\n```", "```scala\n          List<Integer> list = Arrays.asList(1,2,3,4,5); \n          JavaRDD<Integer> rdd = sc.parallelize(list); \n\n    ```", "```scala\n          List<Tuple2<Integer, String>> pairs = Arrays.asList( \n                    new Tuple2<>(1, \"Hello\"), \n                    new Tuple2<>(2, \"World\"), \n                    new Tuple2<>(3, \"How are you?\")); \n          JavaPairRDD<Integer, String> pairRDD = sc.parallelizePairs(pairs); \n\n    ```", "```scala\n          String filePath = \"input/SMSSpamCollection.txt\"; \n          JavaRDD<String> linesRDD = sc.textFile(filePath); \n\n    ```", "```scala\n          JavaRDD<String> spamRDD = linesRDD.filter(new Function<String,   \n            Boolean>() { \n            @Override \n            public Boolean call(String line) throws Exception { \n              return line.split(\"\\t\")[0].equals(\"spam\");}}); \n\n    ```", "```scala\n          JavaRDD<String> hamRDD = linesRDD.filter(new Function<String,  \n           Boolean>() { \n           @Override \n           public Boolean call(String line) throws Exception { \n             return line.split(\"\\t\")[0].equals(\"ham\"); \n            } \n         }); \n\n    ```", "```scala\n          JavaRDD<String> spam = spamRDD.map(new Function<String, String>() { \n            @Override \n            public String call(String line) throws Exception {       \n             return line.split(\"\\t\")[1]; \n            } \n         }); \n\n    ```", "```scala\n      JavaRDD<String> ham = hamRDD.map(new Function<String, String>() { \n        @Override \n        public String call(String line) throws Exception {     \n          return line.split(\"\\t\")[1]; \n      } \n      }); \n\n```", "```scala\n      JavaRDD<ArrayList<String>> spamWordList = spam.map(new  \n        Function<String, ArrayList<String>>() { \n          @Override \n      public ArrayList<String> call(String line) throws Exception{ \n            ArrayList<String> words = new ArrayList<>(); \n            words.addAll(Arrays.asList(line.split(\" \"))); \n            return words; \n          }}); \n      JavaRDD<ArrayList<String>> hamWordList = ham.map(new Function<String,  \n        ArrayList<String>>() { \n          @Override \n      public ArrayList<String> call(String line) throws Exception{   \n            ArrayList<String> words = new ArrayList<>(); \n            words.addAll(Arrays.asList(line.split(\" \"))); \n            return words;} \n      }); \n\n```", "```scala\n          JavaRDD<Tuple2<Double, ArrayList<String>>> spamWordsLabelPair =  \n          spamWordList.map(new Function<ArrayList<String>, Tuple2<Double,  \n              ArrayList<String>>>() { \n              @Override \n                public Tuple2<Double, ArrayList<String>> call( \n                ArrayList<String> v1) throws Exception {     \n                return new Tuple2<Double, ArrayList<String>>(1.0, v1); \n              }}); \n          JavaRDD<Tuple2<Double, ArrayList<String>>> hamWordsLabelPair =  \n          hamWordList.map(new Function<ArrayList<String>, Tuple2<Double,  \n              ArrayList<String>>>() { \n              @Override \n              public Tuple2<Double, ArrayList<String>> call( \n                ArrayList<String> v1) throws Exception {     \n                return new Tuple2<Double, ArrayList<String>>(0.0, v1); \n              }}); \n\n          [Output: print spamWordsLabelPair2.collect() using for loop] \n          1.0: [FreeMsg:, Txt:, CALL, to, No:, 86888, &, claim, your, reward, \n          of, 3, hours, talk, time, to, use, from, your, phone, now!, \n          ubscribe6GBP/, mnth, inc, 3hrs, 16, stop?txtStop] \n          1.0: [Sunshine, Quiz!, Win, a, super, Sony, DVD, recorder,\n          if, you, canname, the, capital, of, Australia?, Text, MQUIZ, \n          to, 82277., B] \n\n    ```", "```scala\n          JavaRDD<Tuple2<Double, ArrayList<String>>> train_set =  \n          spamWordsLabelPair.union(hamWordsLabelPair); \n\n    ```", "```scala\n          System.out.println(train_set.count()); \n          The following output is 8\n\n    ```", "```scala\n          for (Tuple2<Double, ArrayList<String>> tt : train_set.collect()) { \n              System.out.println(tt._1 + \": \" + tt._2.toString()); } \n\n    ```", "```scala\n          1.0: [FreeMsg:, Txt:, CALL, to, No:, 86888,\n           &, claim, your, reward, of, 3, hours, talk, time, to, use, from,\n           your, phone, now!, ubscribe6GBP/, mnth, inc, 3hrs, 16, stop?txtStop] \n          1.0: [Sunshine, Quiz!, Win, a, super, Sony, DVD, recorder, if, \n          you, canname, the, capital, of, Australia?, Text, MQUIZ, \n          to, 82277., B] \n          0.0: [What, you, doing?, how, are, you?] \n          0.0: [Ok, lar..., Joking, wif, u, oni...] \n          0.0: [dun, say, so, early, hor..., U, c, already, then, say...] \n          0.0: [MY, NO., IN, LUTON, 0125698789, RING, ME, IF, UR, AROUND!, H*] \n          0.0: [Siva, is, in, hostel, aha:-.] \n          0.0: [Cos, i, was, out, shopping, wif, darren, jus, now,\n           n, i, called, \n          him, 2, ask, wat, present, he, wan, lor., Then, he, \n          started, guessing, \n          who, i, was, wif, n, he, finally, guessed, darren, lor.] \n\n    ```", "```scala\n          train_set.saveAsTextFile(\"output.txt\"); \n\n    ```", "```scala\nString path = \"input/SMSSpamCollection.txt\";     \nRDD<String> lines = spark.sparkContext().textFile(path, 2); \nSystem.out.println(lines.take(10)); \n\nJavaRDD<Row> rowRDD = lines.toJavaRDD().map( new Function<String, Row>() { \n    public Row call(String line) throws Exception { \n      return RowFactory.create(line); \n      }}); \nSystem.out.println(rowRDD.collect()); \nList<StructField> fields = new ArrayList<StructField>(); \nfields.add(DataTypes.createStructField(\"line\", DataTypes.StringType, true)); \norg.apache.spark.sql.types.StructType schema = DataTypes.createStructType(fields); \nDataset<Row> df = spark.sqlContext().createDataFrame(rowRDD, schema); \ndf.select(\"line\").show(); \nDataset<Row> spam = df.filter(df.col(\"line\").like(\"%spam%\")); \nDataset<Row> ham = df.filter(df.col(\"line\").like(\"%ham%\")); \nSystem.out.println(spam.count()); \nSystem.out.println(ham.count()); \nspam.show(); \n\n```", "```scala\n747 \n4831 \n\n```", "```scala\nJavaRDD<Integer> rdd = sc.parallelize(Arrays.asList(1,2,1,3,4,5)); \nJavaPairRDD<Integer, Integer> pairs = rdd.mapToPair( \n  new PairFunction<Integer, Integer, Integer>() { \n    @Override \n    public Tuple2<Integer, Integer> call(Integer x) { \n      return new Tuple2<>(x, 1); \n    } \n}).cache(); \n\n[Output: pairs.collect()] \n[(1,1), (2,1), (1,1), (3,1), (4,1), (5,1)] \n\n```", "```scala\nJavaRDD<String> rdd = sc.parallelize(Arrays.asList(\"Hello World!\", \n  \"How are you.\")); \nJavaRDD<String> words = rdd \n  .flatMap(new FlatMapFunction<String, String>() { \n    @Override \n    public Iterable<String> call(String t) throws Exception { \n      return Arrays.asList(t.split(\" \")); \n    }}); \n\n[output: words.collect()] \n[Hello, World!, How, are, you.] \n\n```", "```scala\nJavaPairRDD<Integer, Integer> rdd_from_integer = sc \n.parallelizePairs(Arrays.asList( new Tuple2<>(1, 1),  \nnew Tuple2<>(1, 1), new Tuple2<>(3, 2), \nnew Tuple2<>(5, 1), new Tuple2<>(5, 3)), 2);  \n\n```", "```scala\n[Output: pairs.groupByKey(2).collect() ] \n\n```", "```scala\n          JavaPairRDD<Integer, Integer> counts = rdd \n            .reduceByKey(new Function2<Integer, Integer, Integer>() { \n              @Override \n              public Integer call(Integer a, Integer b) { \n                return a + b;}}); \n\n    ```", "```scala\n          JavaPairRDD<Integer, Integer> counts = pairs.aggregateByKey(0, \n              new Function2<Integer, Integer, Integer>() { \n                @Override \n                public Integer call(Integer v1, Integer v2) { \n                  return v1 + v2; \n                } \n                }, new Function2<Integer, Integer, Integer>() { \n                @Override \n                public Integer call(Integer v1, Integer v2) { \n                      return v1 + v2; \n                    } \n                  }); \n\n    ```", "```scala\nList<Tuple2<Integer, Integer>> pairs = new ArrayList<>(); \npairs.add(new Tuple2<>(1, 5)); \npairs.add(new Tuple2<>(4, 2)); \npairs.add(new Tuple2<>(-1, 1)); \npairs.add(new Tuple2<>(1, 1)); \n\n```", "```scala\n      JavaPairRDD<Integer, Integer> rdd = sc.parallelizePairs(pairs); \n      JavaPairRDD<Integer, Integer> sortedRDD=rdd.sortByKey(Collections.\n      <Integer> reverseOrder(), false); \n      [Output: sortedRDD.collect()] \n\n```", "```scala\n      JavaRDD<Tuple2<Integer, Integer>> rdd_new = sc.parallelize(pairs); \n      JavaRDD<Tuple2<Integer, Integer>> sortedRDD=rdd.sortBy( \n      new Function<Tuple2<Integer, Integer>, Integer>() { \n      @Override \n          public Integer call(Tuple2<Integer, Integer> t) { \n              return t._2(); \n          } \n      ,} true, 2);\n```", "```scala\nJavaSparkContext sc = new JavaSparkContext(\"local\",\"DFDemo\"); \nSQLContext sqlContext = new org.apache.spark.sql.SQLContext(sc); \n\n```", "```scala\n[Input File] \n{\"name\":\"Michael\"} \n{\"name\":\"Andy\", \"age\":30} \n{\"name\":\"Justin\", \"age\":19} \n[Code]    \nDataset<Row> df = sqlContext.read().json(\"people.json\");    \n\n[Output: df.show()] \n+----+-------+ \n| age|   name| \n+----+-------+ \n|null|Michael| \n|  30|   Andy| \n|  19| Justin| \n+----+-------+ \n\n```", "```scala\nThe code is as follows:]hiveContext.sql(\"CREATE TEMPORARY TABLE people USING  \norg.apache.spark.sql.json OPTIONS ( path \"people.json\" )\"); \nDataset<Row> results = hiveContext.sql(\"SELECT * FROM people \"); \nresults.show(); \n\n```", "```scala\n          Dataset<Row> df = spark.read().load(\"input/SMSSpamCollection.txt\"); \n          df.show(); \n\n    ```", "```scala\n             JavaRDD<Row> rowRDD = df.toJavaRDD(); \n\n    ```", "```scala\n          JavaRDD<Row> splitedRDD = rowRDD.map(new Function<Row, Row>() { \n               @Override \n              public Row call(Row r) throws Exception { \n                String[] split = r.getString(0).split(\"\\t\"); \n                return RowFactory.create(split[0],split[1]); \n              }}); \n\n    ```", "```scala\n              List<StructField> fields  = new ArrayList<>(); \n              fields.add(DataTypes.createStructField(\"labelString\", \n              DataTypes.StringType, true)); \n              fields.add(DataTypes.createStructField(\"featureString\",  \n              DataTypes.StringType, true)); \n              org.apache.spark.sql.types.StructType schema = DataTypes \n              .createStructType(fields); \n              Dataset<Row> schemaSMSSpamCollection = sqlContext \n              .createDataFrame(splitedRDD, schema); \n              schemaSMSSpamCollection.printSchema(); \n              [Output: schemaSMSSpamCollection.printSchema()] \n\n        ```", "```scala\n          fields.add(DataTypes.createStructField(\"labelDouble\",  \n          DataTypes.DoubleType, true)); \n          fields.add(DataTypes.createStructField(\"featureTokens\",  \n          DataTypes.StringType, true)); \n          org.apache.spark.sql.types.StructType schemaUpdated =  \n          DataTypes.createStructType(fields); \n          Dataset Row> newColumnsaddedDF = sqlContext \n          .createDataFrame(schemaSMSSpamCollection.javaRDD().map( \n          new Function<Row, Row>() { \n              @Override \n              public Row call(Row row) throws Exception { \n                double label; \n                if(row.getString(0).equalsIgnoreCase(\"spam\")) \n                  label = 1.0; \n                else \n                  label = 0.0; \n                String[] split = row.getString(1).split(\" \"); \n                ArrayList<String> tokens = new ArrayList<>(); \n                for(String s:split) \n                  tokens.add(s.trim()); \n                return RowFactory.create(row.getString(0), \n           row.getString(1),label, tokens.toString()); \n              }}), schemaUpdated);   \n          [Output: newColumnsaddedDF.show()] \n\n    ```", "```scala\n          newColumnsaddedDF.select(newColumnsaddedDF.col(\"labelDouble\"),\n          newColumnsaddedDF.col(\"featureTokens\")).show(); \n\n    ```", "```scala\n          newColumnsaddedDF.filter(newColumnsaddedDF.col\n          (\"labelDouble\").gt(0.0)).show(); \n\n    ```", "```scala\n          newColumnsaddedDF.groupBy(\"labelDouble\").count().show(); \n\n    ```", "```scala\n          Dataset<Row> spam = spark.sqlContext().sql(\"SELECT * FROM \n          SMSSpamCollection\n          WHERE labelDouble=1.0\"); \n          spam.show();  \n\n    ```", "```scala\n          Dataset<Row> counts = sqlContext.sql(\"SELECT labelDouble, COUNT(*)  \n          AS count FROM SMSSpamCollection GROUP BY labelDouble\"); \n          counts.show(); \n\n    ```", "```scala\npublic class SMSSpamBean implements Serializable { \n  private String labelString; \n  private String featureString; \npublic SMSSpamBean(String labelString, String featureString) { \n    super(); \n    this.labelString = labelString; \n    this.featureString = featureString; \n  } \n  public String getLabelString() { \n    return labelString; \n  } \n  public void setLabelString(String labelString) { \n    this.labelString = labelString; \n  } \n  public String getFeatureString() { \n    return featureString; \n  }  public void setFeatureString(String featureString) {    this.featureString = featureString; \n  }}  \n\n```", "```scala\nJavaRDD<SMSSpamBean> smsSpamBeanRDD =  rowRDD.map(new Function<Row, SMSSpamBean>() { \n      @Override \n    public SMSSpamBean call(Row r) throws Exception { \n        String[] split = r.getString(0).split(\"\\t\"); \n        return new SMSSpamBean(split[0],split[1]); \n      }});   \nDataset<Row> SMSSpamDF = spark.sqlContext().createDataFrame(smsSpamBeanRDD, SMSSpamBean.class); \nSMSSpamDF.show();   \n\n```", "```scala\nstatic SparkSession spark = SparkSession.builder() \n      .appName(\"DatasetDemo\") \n      .master(\"local[*]\") \n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n      .getOrCreate(); \n\n```", "```scala\nDataset<String> ds = spark.read().text(\"input/SMSSpamCollection.txt\").as(org.apache.spark.sql.Encoders.STRING()); \nds.show(); \n\n```", "```scala\nDataset<SMSSpamTokenizedBean> dsSMSSpam = ds.map( \nnew MapFunction<String, SMSSpamTokenizedBean>() { \n          @Override \npublic SMSSpamTokenizedBean call(String value) throws Exception { \n      String[] split = value.split(\"\\t\"); \n      double label; \n      if(split[0].equalsIgnoreCase(\"spam\")) \n          label = 1.0; \n      else \n          label=0.0; \nArrayList<String> tokens = new ArrayList<>(); \n  for(String s:split) \n    tokens.add(s.trim());           \n      return new SMSSpamTokenizedBean(label, tokens.toString()); \n         } \n}, org.apache.spark.sql.Encoders.bean(SMSSpamTokenizedBean.class)); \n\n```", "```scala\ndsSMSSpam.show(); \ndsSMSSpam.printSchema(); \n\n```", "```scala\nDataset<Row> df = dsSMSSpam.toDF(); \ndf.createOrReplaceTempView(\"SMSSpamCollection\");      \n\n```", "```scala\ndf.show(); \n\n```", "```scala\npublic class SMSSpamTokenizedBean implements Serializable { \nprivate Double labelDouble; \nprivate String tokens;     \npublic SMSSpamTokenizedBean(Double labelDouble, String tokens) { \n  super(); \n  this.labelDouble = labelDouble; \n  this.tokens = tokens; \n  } \n  public Double getLabelDouble() { \n    return labelDouble; \n  } \n  public void setLabelDouble(Double labelDouble) { \n    this.labelDouble = labelDouble; \n  } \n  public String getTokens() { \n    return tokens; \n  } \n  public void setTokens(String tokens) { \n    this.tokens = tokens; \n  }} \n\n```", "```scala\nBroadcast<int[]> broadcastVariable=sc.broadcast(new int[] {2,3,4}); \nint[] values = broadcastVariable.value(); \nfor(int i:values){ \n  System.out.println(i);} \n\n```", "```scala\nSparkContext. accumulator(val) \n\n```", "```scala\nAccumulator<Integer> accumulator = sc.accumulator(0); \nsc.parallelize(Arrays.asList(1, 5, 3, 4)) \n.foreach(x -> accumulator.add(x));   \nSystem.out.println(accumulator.value()); \n\n```"]