["```scala\nimport java.util.Arrays; \nimport org.apache.spark.ml.regression.GeneralizedLinearRegression; \nimport org.apache.spark.ml.regression.GeneralizedLinearRegressionModel; \nimport org.apache.spark.ml.regression.GeneralizedLinearRegressionTrainingSummary; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \n\n```", "```scala\nSparkSession spark = SparkSession \n    .builder() \n    .appName(\"JavaGeneralizedLinearRegressionExample\") \n    .master(\"local[*]\") \n    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n    .getOrCreate();  \n\n```", "```scala\nDataset<Row>dataset = spark.read().format(\"libsvm\").load(\"input/Letterdata_libsvm.data\"); \n\n```", "```scala\ndouble[] weights = {0.8, 0.2}; \nlong seed = 12345L; \nDataset<Row>[] split = dataset.randomSplit(weights, seed); \nDataset<Row> training = split[0]; \nDataset<Row> test = split[1]; \n\n```", "```scala\nGeneralizedLinearRegression glr = new GeneralizedLinearRegression() \n.setFamily(\"gaussian\") \n.setLink(\"identity\") \n.setMaxIter(10) \n.setRegParam(0.3); \n\n```", "```scala\nGeneralizedLinearRegressionModel model = glr.fit(training); \n\n```", "```scala\nSystem.out.println(\"Coefficients: \" + model.coefficients()); \nSystem.out.println(\"Intercept: \" + model.intercept()); \n\n```", "```scala\nCoefficients: [-0.0022864381796305487,-0.002728958263362158,0.001582003618682323,-0.0027708788253722914,0.0021962329827476565,-0.014769839282003813,0.027752802299957722,0.005757124632688538,0.013869444611365267,-0.010555326094498824,-0.006062727351948948,-0.01618167221020619,0.02894330366681715,-0.006180003317929849,-0.0025768386348180294,0.015161831324693125,0.8125261496082304] \nIntercept: 1.2140016821111255  \n\n```", "```scala\nGeneralizedLinearRegressionTrainingSummary summary = model.summary(); \n\n```", "```scala\nSystem.out.println(\"Coefficient Standard Errors: \" \n      + Arrays.toString(summary.coefficientStandardErrors())); \nSystem.out.println(\"T Values: \" + Arrays.toString(summary.tValues())); \nSystem.out.println(\"P Values: \" + Arrays.toString(summary.pValues())); \nSystem.out.println(\"Dispersion: \" + summary.dispersion()); \nSystem.out.println(\"Null Deviance: \" + summary.nullDeviance()); \nSystem.out.println(\"Residual Degree Of Freedom Null: \" + summary.residualDegreeOfFreedomNull()); \nSystem.out.println(\"Deviance: \" + summary.deviance()); \nSystem.out.println(\"Residual Degree Of Freedom: \" + summary.residualDegreeOfFreedom()); \n    System.out.println(\"AIC: \" + summary.aic()); \n\n```", "```scala\nCoefficient Standard Errors:[2.877963555951775E-4, 0.0016618949921257992, 9.147115254397696E-4, 0.001633197607413805, 0.0013194682048354774, 0.001427648472211677, 0.0010797461071614422, 0.001092731825368789, 7.922778963434026E-4, 9.413717346009722E-4, 8.746375698587989E-4, 9.768068714323967E-4, 0.0010276211138097238, 0.0011457739746946476, 0.0015025626835648176, 9.048329671989396E-4, 0.0013145697411570455, 0.02274018067510297] \nT Values:[-7.944639100457261, -1.6420762300218703, 1.729510971146599, -1.6965974067032972, 1.6644834446931607, -10.345571455081481, 25.703081600282317, 5.2685613240426585, 17.50578259898057, -11.212707697212734, -6.931702411237277, -16.56588695621814, 28.165345454527458, -5.3937368577226055, -1.714962485760994, 16.756497468951743, 618.0928437414578, 53.385753589911985] \nP Values:[1.9984014443252818E-15, 0.10059394323065063, 0.08373705354670546, 0.0897923347927514, 0.09603552109755675, 0.0, 0.0, 1.3928712139232857E-7, 0.0, 0.0, 4.317657342767234E-12, 0.0, 0.0, 6.999167956323049E-8, 0.08637155105770145, 0.0, 0.0, 0.0] \nDispersion: 0.07102433332236015  \nNull Deviance: 41357.85510971454 \nResidual Degree Of Freedom Null: 15949 \nDeviance: 1131.5596784918419 \nResidual Degree Of Freedom: 15932 \nAIC: 3100.6418768238423  \n\n```", "```scala\nsummary.residuals().show(); \n\n```", "```scala\nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.mllib.classification.SVMModel; \nimport org.apache.spark.mllib.classification.SVMWithSGD; \nimport org.apache.spark.mllib.evaluation.BinaryClassificationMetrics; \nimport org.apache.spark.mllib.evaluation.MulticlassMetrics; \nimport org.apache.spark.mllib.optimization.L1Updater; \nimport org.apache.spark.mllib.regression.LabeledPoint; \nimport org.apache.spark.mllib.util.MLUtils; \nimport org.apache.spark.sql.SparkSession; \n\n```", "```scala\nSparkSession spark = SparkSession \n    .builder() \n    .appName(\"JavaLDAExample\") \n    .master(\"local[*]\") \n    .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n    .getOrCreate(); \n\n```", "```scala\nString path = \"input/colon-cancer.data\"; \nJavaRDD<LabeledPoint>data = MLUtils.loadLibSVMFile(spark.sparkContext(), path).toJavaRDD(); \n\n```", "```scala\n    JavaRDD<LabeledPoint>training = data.sample(false, 0.8, 11L); \ntraining.cache(); \n    JavaRDD<LabeledPoint>test = data.subtract(training); \n\n```", "```scala\nintnumIterations = 500; \nfinal SVMModel model = SVMWithSGD.train(training.rdd(), numIterations); \n\n```", "```scala\nJavaRDD<Tuple2<Object, Object>>scoreAndLabels = test.map( \nnewFunction<LabeledPoint, Tuple2<Object, Object>>() { \npublic Tuple2<Object, Object> call(LabeledPoint p) { \n          Double score = model.predict(p.features()); \nreturnnew Tuple2<Object, Object>(score, p.label()); \n        }}); \n\n```", "```scala\nBinaryClassificationMetrics metrics = new BinaryClassificationMetrics(JavaRDD.toRDD(scoreAndLabels)); \nSystem.out.println(\"Area Under PR = \" + metrics.areaUnderPR()); \nSystem.out.println(\"Area Under ROC = \" + metrics.areaUnderROC()); \nArea Under PR = 0.6266666666666666 \nArea Under ROC = 0.875  \n\n```", "```scala\nSVMWithSGD svmAlg = new SVMWithSGD(); \nsvmAlg.optimizer() \n      .setNumIterations(500) \n      .setRegParam(0.1) \n      .setUpdater(new L1Updater()); \nfinal SVMModel model = svmAlg.run(training.rdd()); \n\n```", "```scala\nArea Under PR = 0.9380952380952381 \nArea Under ROC = 0.95 \n\n```", "```scala\nimport org.apache.spark.ml.classification.MultilayerPerceptronClassificationModel; \nimport org.apache.spark.ml.classification.MultilayerPerceptronClassifier; \nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \nimport com.example.SparkSession.UtilityForSparkSession; \n\n```", "```scala\nSparkSession spark = UtilityForSparkSession.mySession(); \n\n```", "```scala\npublic static SparkSession mySession() { \nSparkSession spark = SparkSession.builder() \n.appName(\"MultilayerPerceptronClassificationModel\") \n.master(\"local[*]\") \n.config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n.getOrCreate(); \n    return spark; \n  } \n\n```", "```scala\nString path = \"input/iris.data\"; \nDataset<Row> dataFrame = spark.read().format(\"libsvm\").load(path); \n\n```", "```scala\nDataset<Row>[] splits = dataFrame.randomSplit(new double[] { 0.7, 0.3 }, 12345L); \nDataset<Row> train = splits[0]; \nDataset<Row> test = splits[1]; \n\n```", "```scala\nint[] layers = newint[] { 4, 4, 3, 3 }; \n\n```", "```scala\nMultilayerPerceptronClassifier trainer = new MultilayerPerceptronClassifier() \n        .setLayers(layers)        \n        .setTol(1E-4)         \n        .setBlockSize(128)         \n        .setSeed(12345L)  \n        .setMaxIter(100); \n\n```", "```scala\nMultilayerPerceptronClassificationModel model = trainer.fit(train); \n\n```", "```scala\nDataset<Row> result = model.transform(test); \nDataset<Row> predictionAndLabels = result.select(\"prediction\", \"label\"); \n\n```", "```scala\nMulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator().setMetricName(\"accuracy\"); \nMulticlassClassificationEvaluator evaluator2 = new MulticlassClassificationEvaluator().setMetricName(\"weightedPrecision\"); \nMulticlassClassificationEvaluator evaluator3 = new MulticlassClassificationEvaluator().setMetricName(\"weightedRecall\"); \nSystem.out.println(\"Accuracy = \" + evaluator.evaluate(predictionAndLabels)); \nSystem.out.println(\"Precision = \" + evaluator2.evaluate(predictionAndLabels)); \nSystem.out.println(\"Recall = \" + evaluator3.evaluate(predictionAndLabels)); \n\n```", "```scala\nAccuracy = 0.9545454545454546  \nPrecision = 0.9595959595959596 \nRecall = 0.9545454545454546  \n\n```", "```scala\nspark.stop(); \n\n```", "```scala\nimport org.apache.spark.api.java.JavaPairRDD; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.api.java.function.PairFunction; \nimport org.apache.spark.mllib.classification.NaiveBayes; \nimport org.apache.spark.mllib.classification.NaiveBayesModel; \nimport org.apache.spark.mllib.regression.LabeledPoint; \nimport org.apache.spark.mllib.util.MLUtils; \nimport org.apache.spark.sql.SparkSession; \nimportscala.Tuple2; \n\n```", "```scala\nstatic SparkSession spark = SparkSession \n      .builder() \n      .appName(\"JavaLDAExample\").master(\"local[*]\") \n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n      .getOrCreate();  \n\n```", "```scala\nstatic String path = \"input/rcv1_train.multiclass.data\"; \nJavaRDD<LabeledPoint> inputData = MLUtils.loadLibSVMFile(spark.sparkContext(), path).toJavaRDD();  \n\n```", "```scala\nJavaRDD<LabeledPoint>[] split = inputData.randomSplit(new double[]{0.8, 0.2}, 12345L); \nJavaRDD<LabeledPoint> training = split[0];  \nJavaRDD<LabeledPoint> test = split[1]; \n\n```", "```scala\nfinal NaiveBayesModel model = NaiveBayes.train(training.rdd(), 1.0, \"multinomial\"); \n\n```", "```scala\nJavaPairRDD<Double,Double> predictionAndLabel = \ntest.mapToPair(new PairFunction<LabeledPoint, Double, Double>() { \n@Override \npublic Tuple2<Double, Double> call(LabeledPoint p) { \nreturn new Tuple2<>(model.predict(p.features()), p.label()); \n          } \n        }); \n\n```", "```scala\ndouble accuracy = predictionAndLabel.filter(new Function<Tuple2<Double, Double>, Boolean>() { \n@Override \npublic Boolean call(Tuple2<Double, Double>pl) { \nreturnpl._1().equals(pl._2()); \n        } \n      }).count() / (double) test.count(); \n\n```", "```scala\nSystem.out.println(\"Accuracy of the classification: \"+accuracy); \n\n```", "```scala\nAccuracy of the classification: 0.5941753719531497  \n\n```", "```scala\nimport java.util.HashMap; \nimport java.util.List; \nimport org.apache.spark.api.java.JavaPairRDD; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.api.java.function.PairFunction; \nimport org.apache.spark.ml.classification.LogisticRegression; \nimport org.apache.spark.mllib.classification.LogisticRegressionModel; \nimport org.apache.spark.mllib.classification.NaiveBayes; \nimport org.apache.spark.mllib.classification.NaiveBayesModel; \nimport org.apache.spark.mllib.linalg.DenseVector; \nimport org.apache.spark.mllib.linalg.Vector; \nimport org.apache.spark.mllib.regression.LabeledPoint; \nimport org.apache.spark.mllib.regression.LinearRegressionModel; \nimport org.apache.spark.mllib.regression.LinearRegressionWithSGD; \nimport org.apache.spark.mllib.tree.DecisionTree; \nimport org.apache.spark.mllib.tree.RandomForest; \nimport org.apache.spark.mllib.tree.model.DecisionTreeModel; \nimport org.apache.spark.mllib.tree.model.RandomForestModel; \nimport org.apache.spark.rdd.RDD; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \nimport com.example.SparkSession.UtilityForSparkSession; \nimport javassist.bytecode.Descriptor.Iterator; \nimport scala.Tuple2; \n\n```", "```scala\nSparkSession spark = UtilityForSparkSession.mySession(); \n\n```", "```scala\nimport org.apache.spark.sql.SparkSession; \npublic class UtilityForSparkSession { \n  public static SparkSession mySession() { \n    SparkSession spark = SparkSession \n                          .builder() \n                          .appName(\"UtilityForSparkSession\") \n                          .master(\"local[*]\") \n                          .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n                          .getOrCreate(); \n    return spark; \n  } \n} \n\n```", "```scala\nString input = \"heart_diseases/processed_cleveland.data\"; \nDataset<Row> my_data = spark.read().format(\"com.databricks.spark.csv\").load(input); \nmy_data.show(false); \nRDD<String> linesRDD = spark.sparkContext().textFile(input, 2); \n\n```", "```scala\nJavaRDD<LabeledPoint> data = linesRDD.toJavaRDD().map(new Function<String, LabeledPoint>() { \n      @Override \n  public LabeledPoint call(String row) throws Exception { \n      String line = row.replaceAll(\"\\\\?\", \"999999.0\"); \n      String[] tokens = line.split(\",\"); \n      Integer last = Integer.parseInt(tokens[13]); \n      double[] features = new double[13]; \n      for (int i = 0; i < 13; i++) { \n      features[i] = Double.parseDouble(tokens[i]); \n      } \n      Vector v = new DenseVector(features); \n      Double value = 0.0; \n      if (last.intValue() > 0) \n        value = 1.0; \n      LabeledPoint lp = new LabeledPoint(value, v); \n    return lp; \n      } \n    }); \n\n```", "```scala\ndouble[] weights = {0.7, 0.3}; \nlong split_seed = 12345L; \nJavaRDD<LabeledPoint>[] split = data.randomSplit(weights, split_seed); \nJavaRDD<LabeledPoint> training = split[0]; \nJavaRDD<LabeledPoint> test = split[1]; \n\n```", "```scala\nfinal double stepSize = 0.0000000009; \nfinal int numberOfIterations = 40;  \nLinearRegressionModel model = LinearRegressionWithSGD.train(JavaRDD.toRDD(training), numberOfIterations, stepSize); \n\n```", "```scala\nString model_storage_loc = \"models/heartModel\";   \nmodel.save(spark.sparkContext(), model_storage_loc); \n\n```", "```scala\nJavaPairRDD<Double,Double> predictionAndLabel = \n  test.mapToPair(new PairFunction<LabeledPoint, Double, Double>() { \n            @Override \n    public Tuple2<Double, Double> call(LabeledPoint p) { \n       return new Tuple2<>(model.predict(p.features()), p.label()); \n            } \n          });   \n\n```", "```scala\ndouble accuracy = predictionAndLabel.filter(new Function<Tuple2<Double, Double>, Boolean>() { \n          @Override \n          public Boolean call(Tuple2<Double, Double> pl) { \n            return pl._1().equals(pl._2()); \n          } \n        }).count() / (double) test.count(); \nSystem.out.println(\"Accuracy of the classification: \"+accuracy);   \n\n```", "```scala\nAccuracy of the classification: 0.0 \n\n```", "```scala\nInteger numClasses = 26; //Number of classes \n\n```", "```scala\nHashMap<Integer, Integer> categoricalFeaturesInfo = new HashMap<Integer, Integer>(); \n\n```", "```scala\nInteger numTrees = 5; // Use more in practice \nString featureSubsetStrategy = \"auto\"; // Let algorithm choose the best \nString impurity = \"gini\"; // info. gain & variance also available \nInteger maxDepth = 20; // set the value of maximum depth accordingly \nInteger maxBins = 40; // set the value of bin accordingly \nInteger seed = 12345; //Setting a long seed value is recommended       \nfinal RandomForestModel model = RandomForest.trainClassifier(training, numClasses,categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed); \n\n```", "```scala\nAccuracy of the classification: 0.7843137254901961  \n\n```", "```scala\nString new_data = \"heart_diseases/processed_hungarian.data\"; \nRDD<String> linesRDD = spark.sparkContext().textFile(new_data, 2); \nJavaRDD<LabeledPoint> data = linesRDD.toJavaRDD().map(new Function<String, LabeledPoint>() { \n      @Override \n  public LabeledPoint call(String row) throws Exception { \n  String line = row.replaceAll(\"\\\\?\", \"999999.0\"); \n  String[] tokens = line.split(\",\"); \n  Integer last = Integer.parseInt(tokens[13]); \n    double[] features = new double[13]; \n             for (int i = 0; i < 13; i++) { \n          features[i] = Double.parseDouble(tokens[i]); \n                } \n      Vector v = new DenseVector(features); \n      Double value = 0.0; \n      if (last.intValue() > 0) \n        value = 1.0; \n      LabeledPoint p = new LabeledPoint(value, v); \n      return p; \n      } }); \n\n```", "```scala\nRandomForestModel model2 =  \nRandomForestModel.load(spark.sparkContext(), model_storage_loc); \n\n```", "```scala\nJavaPairRDD<Double, Double> predictionAndLabel = \n  data.mapToPair(new PairFunction<LabeledPoint, Double, Double>() { \n          @Override \n          public Tuple2<Double, Double> call(LabeledPoint p) { \n      return new Tuple2<>(model2.predict(p.features()), p.label()); \n            } \n          }); \n\n```", "```scala\ndouble accuracy = predictionAndLabel.filter(new Function<Tuple2<Double, Double>, Boolean>() { \n          @Override \n          public Boolean call(Tuple2<Double, Double> pl) { \n            return pl._1().equals(pl._2()); \n          } \n        }).count() / (double) data.count(); \nSystem.out.println(\"Accuracy of the classification: \"+accuracy);   \n\n```", "```scala\nAccuracy of the classification: 0.9108910891089109 \n\n```"]