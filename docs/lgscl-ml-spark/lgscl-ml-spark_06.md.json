["```scala\nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.ml.Pipeline; \nimport org.apache.spark.ml.PipelineModel; \nimport org.apache.spark.ml.PipelineStage; \nimport org.apache.spark.ml.classification.LogisticRegression; \nimport org.apache.spark.ml.feature.LabeledPoint; \nimport org.apache.spark.ml.linalg.DenseVector; \nimport org.apache.spark.ml.linalg.Vector; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \n\n```", "```scala\nstatic SparkSession spark = SparkSession \n        .builder() \n        .appName(\"BreastCancerDetectionDiagnosis\") \n       .master(\"local[*]\") \n       .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n       .getOrCreate();\n```", "```scala\n  String path = \"input/wdbc.data\"; \n  JavaRDD<String> lines = spark.sparkContext().textFile(path, 3).toJavaRDD();\n```", "```scala\nJavaRDD<LabeledPoint> linesRDD = lines \n        .map(new Function<String, LabeledPoint>() { \n          public LabeledPoint call(String lines) { \n            String[] tokens = lines.split(\",\"); \n            double[] features = new double[30]; \n            for (int i = 2; i < features.length; i++) { \n              features[i - 2] = Double.parseDouble(tokens[i]); \n            } \n            Vector v = new DenseVector(features); \n            if (tokens[1].equals(\"B\")) { \n              return new LabeledPoint(1.0, v); // benign \n            } else { \n              return new LabeledPoint(0.0, v); // malignant \n            } \n          } \n        }); \n\n```", "```scala\nDataset<Row> data = spark.createDataFrame(linesRDD,LabeledPoint.class); \ndata.show(); \n\n```", "```scala\nDataset<Row>[] splits = data.randomSplit(new double[] { 0.6, 0.4 }, 12345L); \nDataset<Row> trainingData = splits[0]; \nDataset<Row> testData = splits[1]; \n\n```", "```scala\nLogisticRegression logisticRegression = new LogisticRegression() \n                          .setMaxIter(100) \n                             .setRegParam(0.01) \n                             .setElasticNetParam(0.4); \n\n```", "```scala\n      LogisticRegression lr = new \n      LogisticRegression().setMaxIter(100)\n      .setRegParam(0.01).setElasticNetParam(0.4); \n\n```", "```scala\nPipeline pipeline = new Pipeline().setStages(new PipelineStage[] {logisticRegression}); \nPipelineModel model = pipeline.fit(trainingData); \n\n```", "```scala\nDataset<Row> predictions=model.transform(testData); \n\n```", "```scala\npredictions.show(); \nlong count = 0; \nfor (Row r : predictions.select(\"features\", \"label\", \"prediction\").collectAsList()) { \n    System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + r.get(2) + \", prediction=\" + r.get(2)); \n      count++; \n    } \n\n```", "```scala\nSystem.out.println(\"precision: \" + (double) (count * 100) / predictions.count()); \nPrecision - 100.0 \n\n```", "```scala\nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.ml.Pipeline; \nimport org.apache.spark.ml.PipelineModel; \nimport org.apache.spark.ml.PipelineStage; \nimport org.apache.spark.ml.classification.LogisticRegression; \nimport org.apache.spark.ml.feature.LabeledPoint; \nimport org.apache.spark.ml.linalg.DenseVector; \nimport org.apache.spark.ml.linalg.Vector; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \n\n```", "```scala\nstatic SparkSession spark = SparkSession \n        .builder() \n        .appName(\"BreastCancerDetectionPrognosis\") \n       .master(\"local[*]\") \n       .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n       .getOrCreate(); \n\n```", "```scala\nString path = \"input/wpbc.data\"; \nJavaRDD<String> lines = spark.sparkContext().textFile(path, 3).toJavaRDD(); \n\n```", "```scala\nJavaRDD<LabeledPoint> linesRDD = lines.map(new Function<String, LabeledPoint>() { \n      public LabeledPoint call(String lines) { \n        String[] tokens = lines.split(\",\"); \n        double[] features = new double[30]; \n        for (int i = 2; i < features.length; i++) { \n          features[i - 2] = Double.parseDouble(tokens[i]); \n        } \n        Vector v = new DenseVector(features); \n        if (tokens[1].equals(\"N\")) { \n          return new LabeledPoint(1.0, v); // recurrent \n        } else { \n          return new LabeledPoint(0.0, v); // non-recurrent \n        } \n      } \n    });  \n\n```", "```scala\nDataset<Row> data = spark.createDataFrame(linesRDD,LabeledPoint.class); \ndata.show(); \n\n```", "```scala\nDataset<Row>[] splits = data.randomSplit(new double[] { 0.6, 0.4 }, 12345L); \nDataset<Row> trainingData = splits[0];   \nDataset<Row> testData = splits[1]; \n\n```", "```scala\nLogisticRegression logisticRegression = new LogisticRegression() \n.setMaxIter(100) \n.setRegParam(0.01) \n.setElasticNetParam(0.4); \n\n```", "```scala\nPipeline pipeline = new Pipeline().setStages(new PipelineStage[]{logisticRegression}); \nPipelineModel model=pipeline.fit(trainingData); \n\n```", "```scala\nDataset<Row> predictions=model.transform(testData); \n\n```", "```scala\npredictions.show(); \n\n```", "```scala\nlong count = 0; \nfor (Row r : predictions.select(\"features\", \"label\", \"prediction\").collectAsList()) { \n      System.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + r.get(2) + \", prediction=\" + r.get(2)); \n      count++; \n    } \n\n```", "```scala\nSystem.out.println(\"precision: \" + (double) (count * 100) / predictions.count());  \nPrecision: 100.0  \n\n```", "```scala\nimport java.util.ArrayList; \nimport java.util.Iterator; \nimport java.util.List; \nimport org.apache.spark.api.java.JavaPairRDD; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.api.java.function.Function2; \nimport org.apache.spark.api.java.function.PairFlatMapFunction; \nimport org.apache.spark.rdd.RDD; \nimport org.apache.spark.sql.SparkSession; \nimport scala.Tuple2;  \nimport scala.Tuple4; \n\n```", "```scala\nSparkSession spark = SparkSession \n.builder() \n.appName(\"MarketBasketAnalysis\") \n.master(\"local[*]\") \n.config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n.getOrCreate(); \n\n```", "```scala\nString transactionsFileName = \"Input/groceries.data\"; \nRDD<String> transactions = spark.sparkContext().textFile(transactionsFileName, 1); \ntransactions.saveAsTextFile(\"output/transactions\"); \n\n```", "```scala\n  static List<String> toList(String transaction) { \n    String[] items = transaction.trim().split(\",\"); \n    List<String>list = new ArrayList<String>(); \n    for (String item :items) { \n      list.add(item); \n    } \n    returnlist; \n  } \n\n```", "```scala\nstatic List<String> removeOneItemAndNullTransactions(List<String>list, int i) { \n    if ((list == null) || (list.isEmpty())) { \n      returnlist; \n    } \n    if ((i< 0) || (i> (list.size() - 1))) { \n      returnlist; \n    } \n    List<String>cloned = new ArrayList<String>(list); \n    cloned.remove(i); \n    return cloned; \n  } \n\n```", "```scala\nJavaPairRDD<List<String>, Integer> patterns = transactions.toJavaRDD() \n        .flatMapToPair(new PairFlatMapFunction<String, List<String>, Integer>() { \n          @Override \n  public Iterator<Tuple2<List<String>, Integer>> call(String transaction) { \n  List<String> list = toList(transaction); \n  List<List<String>> combinations = Combination.findSortedCombinations(list); \n  List<Tuple2<List<String>, Integer>> result = new ArrayList<Tuple2<List<String>, Integer>>(); \nfor (List<String> combList : combinations) { \n  if (combList.size() > 0) { \n  result.add(new Tuple2<List<String>, Integer>(combList, 1)); \n              } \n            } \n    return result.iterator(); \n          } \n        }); \n    patterns.saveAsTextFile(\"output/1itemsets\"); \n\n```", "```scala\nJavaPairRDD<List<String>, Integer> combined = patterns.reduceByKey(new Function2<Integer, Integer, Integer>() { \n      public Integer call(Integer i1, Integer i2) { \n        int support = 0; \n        if (i1 + i2 >= 2) { \n          support = i1 + i2; \n        } \n        // if(support >= 2) \n        return support; \n      } \n    }); \n  combined.saveAsTextFile(\"output/frequent_patterns\"); \n\n```", "```scala\nJavaPairRDD<List<String>, Tuple2<List<String>, Integer>> candidate-patterns = combined.flatMapToPair( \nnew PairFlatMapFunction<Tuple2<List<String>, Integer>, List<String>, Tuple2<List<String>, Integer>>() { \n          @Override \npublic Iterator<Tuple2<List<String>, Tuple2<List<String>, Integer>>> call( \nTuple2<List<String>, Integer> pattern) { \nList<Tuple2<List<String>, Tuple2<List<String>, Integer>>> result = new ArrayList<Tuple2<List<String>, Tuple2<List<String>, Integer>>>(); \n  List<String> list = pattern._1; \n  frequency = pattern._2; \n  result.add(new Tuple2(list, new Tuple2(null, frequency))); \n            if (list.size() == 1) { \n              return result.iterator(); \n            } \n\n  // pattern has more than one item \n  // result.add(new Tuple2(list, new Tuple2(null,size))); \n    for (int i = 0; i < list.size(); i++) { \n    List<String> sublist = removeOneItem(list, i); \n              result.add(new Tuple2<List<String>, Tuple2<List<String>, Integer>>(sublist, \n                  new Tuple2(list, frequency))); \n            } \n            return result.iterator(); \n          } \n        }); \ncandidate-patterns.saveAsTextFile(\"output/sub_patterns\"); \n\n```", "```scala\nJavaPairRDD<List<String>, Iterable<Tuple2<List<String>, Integer>>>rules = candidate_patterns.groupByKey(); \nrules.saveAsTextFile(\"Output/combined_subpatterns\"); \n\n```", "```scala\nJavaRDD<List<Tuple4<List<String>, List<String>, Double, Double>>> assocRules = rules.map( \n        new Function<Tuple2<List<String>, Iterable<Tuple2<List<String>, Integer>>>, List<Tuple4<List<String>, List<String>, Double, Double>>>() { \n          @Override \npublic List<Tuple4<List<String>, List<String>, Double, Double>> call( \nTuple2<List<String>, Iterable<Tuple2<List<String>, Integer>>> in) throws Exception { \n\nList<Tuple4<List<String>, List<String>, Double, Double>> result = new ArrayList<Tuple4<List<String>, List<String>, Double, Double>>(); \n  List<String> fromList = in._1; \n  Iterable<Tuple2<List<String>, Integer>> to = in._2; \n  List<Tuple2<List<String>, Integer>> toList = new ArrayList<Tuple2<List<String>, Integer>>(); \nTuple2<List<String>, Integer> fromCount = null; \n      for (Tuple2<List<String>, Integer> t2 : to) { \n        // find the \"count\" object \n      if (t2._1 == null) { \n                fromCount = t2; \n              } else { \n                toList.add(t2); \n              } \n            } \n            if (toList.isEmpty()) { \n              return result; \n            } \nfor (Tuple2<List<String>, Integer> t2 : toList) { \n  double confidence = (double) t2._2 / (double) fromCount._2; \ndouble lift = confidence / (double) t2._2; \ndouble support = (double) fromCount._2; \nList<String> t2List = new ArrayList<String>(t2._1); \nt2List.removeAll(fromList); \nif (support >= 2.0 && fromList != null && t2List != null) { \n  result.add(new Tuple4(fromList, t2List, support, confidence)); \nSystem.out.println(fromList + \"=>\" + t2List + \",\" + support + \",\" + confidence + \",\" + lift); \n              } \n            } \n            return result; \n          } \n        }); \nassocRules.saveAsTextFile(\"output/association_rules_with_conf_lift\"); \n\n```", "```scala\nimport java.util.HashMap; \nimport java.util.Map; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.mllib.classification.LogisticRegressionWithLBFGS; \nimport org.apache.spark.mllib.evaluation.MulticlassMetrics; \nimport org.apache.spark.mllib.evaluation.MultilabelMetrics; \nimport org.apache.spark.mllib.linalg.DenseVector; \nimport org.apache.spark.mllib.linalg.Vector; \nimport org.apache.spark.mllib.regression.LabeledPoint; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \nimport scala.Tuple2; \n\n```", "```scala\n  static SparkSession spark = SparkSession \n        .builder() \n        .appName(\"OCRPrediction\") \n            .master(\"local[*]\") \n            .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\"). \n            getOrCreate(); \n\n```", "```scala\nString input = \"input/letterdata.data\"; \nDataset<Row> df = spark.read().format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(input);  \n  df.show();  \n\n```", "```scala\nfinal Map<String, Integer>alpha = newHashMap(); \n    intcount = 0; \n    for(chari = 'A'; i<= 'Z'; i++){ \n      alpha.put(i + \"\", count++); \n      System.out.println(alpha); \n    } \n\n```", "```scala\nJavaRDD<LabeledPoint> dataRDD = df.toJavaRDD().map(new Function<Row, LabeledPoint>() { \n      @Override \n      public LabeledPoint call(Row row) throws Exception { \n\n        String letter = row.getString(0); \n        double label = alpha.get(letter); \n        double[] features= new double [row.size()]; \n        for(int i = 1; i < row.size(); i++){ \n          features[i-1] = Double.parseDouble(row.getString(i)); \n        } \n        Vector v = new DenseVector(features);         \n        return new LabeledPoint(label, v); \n      } \n    }); \n\ndataRDD.saveAsTextFile(\"Output/dataRDD\"); \nSystem.out.println(dataRDD.collect()); \n\n```", "```scala\nJavaRDD<LabeledPoint>[] splits = dataRDD.randomSplit(new double[] {0.7, 0.3}, 12345L); \nJavaRDD<LabeledPoint> training = splits[0]; \nJavaRDD<LabeledPoint> test = splits[1];  \n\n```", "```scala\ntraining.saveAsTextFile(\"Output/training\"); \ntest.saveAsTextFile(\"Output/test\"); \n\n```", "```scala\nBoolean useFeatureScaling= true; \nfinal LogisticRegressionModel model = new LogisticRegressionWithLBFGS() \n  .setNumClasses(26).setFeatureScaling(useFeatureScaling) \n  .run(training.rdd()); \n\n```", "```scala\nJavaRDD<Tuple2<Object, Object>> predictionAndLabels = test.map( \n    new Function<LabeledPoint, Tuple2<Object, Object>>() { \n    public Tuple2<Object, Object> call(LabeledPoint p) { \n    Double prediction = model.predict(p.features()); \n    return new Tuple2<Object, Object>(prediction, p.label()); \n          } \n        } \n      );  \npredictionAndLabels.saveAsTextFile(\"output/prd2\");  \n\n```", "```scala\nMulticlassMetrics metrics = new MulticlassMetrics(predictionAndLabels.rdd()); \nMultilabelMetrics(predictionAndLabels.rdd()); \nSystem.out.println(metrics.confusionMatrix()); \ndouble precision = metrics.precision(metrics.labels()[0]); \ndouble recall = metrics.recall(metrics.labels()[0]); \ndouble tp = 8.0; \ndouble TP = metrics.truePositiveRate(tp); \ndouble FP = metrics.falsePositiveRate(tp); \ndouble WTP = metrics.weightedTruePositiveRate(); \ndouble WFP =  metrics.weightedFalsePositiveRate(); \nSystem.out.println(\"Precision = \" + precision); \nSystem.out.println(\"Recall = \" + recall); \nSystem.out.println(\"True Positive Rate = \" + TP); \nSystem.out.println(\"False Positive Rate = \" + FP); \nSystem.out.println(\"Weighted True Positive Rate = \" + WTP); \nSystem.out.println(\"Weighted False Positive Rate = \" + WFP); \n\n```", "```scala\nimport java.io.File; \nimport java.io.FileNotFoundException; \nimport java.io.Serializable; \nimport java.util.ArrayList; \nimport java.util.List; \nimport java.util.Scanner; \nimport org.apache.spark.ml.clustering.LDA; \nimport org.apache.spark.ml.clustering.LDAModel; \nimport org.apache.spark.ml.feature.ChiSqSelector; \nimport org.apache.spark.ml.feature.HashingTF; \nimport org.apache.spark.ml.feature.IDF; \nimport org.apache.spark.ml.feature.IDFModel; \nimport org.apache.spark.ml.feature.RegexTokenizer; \nimport org.apache.spark.ml.feature.StopWordsRemover; \nimport org.apache.spark.ml.feature.StringIndexer; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \nimport org.apache.spark.sql.types.DataTypes; \n\n```", "```scala\nstatic SparkSession spark = SparkSession \n        .builder() \n        .appName(\"JavaLDAExample\") \n        .master(\"local[*]\") \n        .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n        .getOrCreate(); \n\n```", "```scala\nDataset<Row> df = spark.read().text(\"input/test/*.txt\"); \n\n```", "```scala\ndf.show(); \n\n```", "```scala\nRegexTokenizer regexTokenizer1 = new RegexTokenizer().setInputCol(\"value\").setOutputCol(\"labelText\").setPattern(\"\\\\t.*$\"); \n\n```", "```scala\nDataset<Row> labelTextDataFrame = regexTokenizer1.transform(df); \n\n```", "```scala\nlabelTextDataFrame.show(); \n\n```", "```scala\nRegexTokenizer regexTokenizer2 = new RegexTokenizer().setInputCol(\"value\").setOutputCol(\"text\").setPattern(\"\\\\W\"); \n\n```", "```scala\nDataset<Row> labelFeatureDataFrame = regexTokenizer2.transform(labelTextDataFrame); \nlabelFeaturedDataFrame.show(); \n\n```", "```scala\nDataset<Row> newDF = labelFeatureDataFrame \n        .withColumn(\"labelTextTemp\",          labelFeatureDataFrame.col(\"labelText\") \n          .cast(DataTypes.StringType))        .drop(labelFeatureDataFrame.col(\"labelText\")).withColumnRenamed(\"labelTextTemp\", \"labelText\"); \nnewDF.show(); \n\n```", "```scala\nStringIndexer indexer = new StringIndexer().setInputCol(\"labelText\").setOutputCol(\"label\"); \n\n```", "```scala\nDataset<Row> indexed = indexer.fit(newDF).transform(newDF); \nindexed.select(indexed.col(\"labelText\"), indexed.col(\"label\"), indexed.col(\"text\")).show(); \nIndexed.show(); \n\n```", "```scala\nStopWordsRemover remover = new StopWordsRemover(); \nString[] stopwords = remover.getStopWords(); \nremover.setStopWords(stopwords).setInputCol(\"text\").setOutputCol(\"filteredWords\"); \n\n```", "```scala\nDataset<Row> filteredDF = remover.transform(indexed); \nfilteredDF.show(); \nfilteredDF.select(filteredDF.col(\"label\"), filteredDF.col(\"filteredWords\")).show(); \n\n```", "```scala\nDataset<Row> featurizedData = hashingTF.transform(filteredDF); \nfeaturizedData.show(); \n\n```", "```scala\nint numFeatures = 5; \nHashingTF hashingTF = new HashingTF().setInputCol(\"filteredWords\").setOutputCol(\"rawFeatures\").setNumFeatures(numFeatures); \n\n```", "```scala\nDataset<Row> featurizedData = hashingTF.transform(filteredDF); \n       featurizedData.show();   \n\n```", "```scala\nIDF idf = new IDF().setInputCol(\"rawFeatures\").setOutputCol(\"features\"); \nIDFModel idfModel = idf.fit(featurizedData); \n\n```", "```scala\nDataset<Row> rescaledData = idfModel.transform(featurizedData); \nrescaledData.show(). \n\n```", "```scala\nChiSqSelector selector = new org.apache.spark.ml.feature.ChiSqSelector(); \nselector.setNumTopFeatures(5).setFeaturesCol(\"features\").setLabelCol(\"label\").setOutputCol(\"selectedFeatures\"); \n\n```", "```scala\nDataset<Row> result = selector.fit(rescaledData).transform(rescaledData); \nresult.show(); \n\n```", "```scala\nlong value = 5;     \nLDA lda = new LDA().setK(10).setMaxIter(10).setSeed(value); \nLDAModel model = lda.fit(result); \n\n```", "```scala\nSystem.out.println(model.vocabSize()); \nDataset<Row> topics = model.describeTopics(5); \norg.apache.spark.ml.linalg.Matrix metric = model.topicsMatrix(); \n\n```", "```scala\nSystem.out.println(metric); \ntopics.show(false); \n\n```", "```scala\nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.ml.classification.RandomForestClassificationModel; \nimport org.apache.spark.ml.classification.RandomForestClassifier; \nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator; \nimport org.apache.spark.ml.feature.StringIndexer; \nimport org.apache.spark.ml.feature.VectorAssembler; \nimport org.apache.spark.mllib.evaluation.RegressionMetrics; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \n\n```", "```scala\n  static SparkSession spark = SparkSession.builder() \n      .appName(\"CreditRiskAnalysis\") \n      .master(\"local[*]\") \n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n      .getOrCreate();  \n\n```", "```scala\nString csvFile = \"input/german_credit.data\"; \nDataset<Row> df = spark.read().format(\"com.databricks.spark.csv\").option(\"header\", \"false\").load(csvFile); \n\n```", "```scala\ndf.show(); \n\n```", "```scala\nJavaRDD<Credit> creditRDD = df.toJavaRDD().map(new Function<Row, Credit>() { \n      @Override \n      public Credit call(Row r) throws Exception { \n        return new Credit(parseDouble(r.getString(0)), parseDouble(r.getString(1)) - 1, \n            parseDouble(r.getString(2)), parseDouble(r.getString(3)), parseDouble(r.getString(4)), \n            parseDouble(r.getString(5)), parseDouble(r.getString(6)) - 1, parseDouble(r.getString(7)) - 1, \n            parseDouble(r.getString(8)), parseDouble(r.getString(9)) - 1, parseDouble(r.getString(10)) - 1, \n            parseDouble(r.getString(11)) - 1, parseDouble(r.getString(12)) - 1, \n            parseDouble(r.getString(13)), parseDouble(r.getString(14)) - 1, \n            parseDouble(r.getString(15)) - 1, parseDouble(r.getString(16)) - 1, \n            parseDouble(r.getString(17)) - 1, parseDouble(r.getString(18)) - 1, \n            parseDouble(r.getString(19)) - 1, parseDouble(r.getString(20)) - 1); \n      } \n    }); \n\n```", "```scala\n  public static double parseDouble(String str) { \n    return Double.parseDouble(str); \n  } \n\n```", "```scala\npublic class Credit { \n  private double creditability; \n  private double balance; \n  private double duration; \n  private double history; \n  private double purpose; \n  private double amount; \n  private double savings; \n  private double employment; \n  private double instPercent; \n  private double sexMarried; \n  private double guarantors; \n  private double residenceDuration; \n  private double assets; \n  private double age; \n  private double concCredit; \n  private double apartment; \n  private double credits; \n  private double occupation; \n  private double dependents; \n  private double hasPhone; \n  private double foreign; \n\n  public Credit(double creditability, double balance, double duration, \n  double history, double purpose, double amount, \n      double savings, double employment, double instPercent, \n      double sexMarried, double guarantors, \n      double residenceDuration, double assets, double age, \n      double concCredit, double apartment, double credits, \n      double occupation, double dependents, double hasPhone, double foreign) { \n    super(); \n    this.creditability = creditability; \n    this.balance = balance; \n    this.duration = duration; \n    this.history = history; \n    this.purpose = purpose; \n    this.amount = amount; \n    this.savings = savings; \n    this.employment = employment; \n    this.instPercent = instPercent; \n    this.sexMarried = sexMarried; \n    this.guarantors = guarantors; \n    this.residenceDuration = residenceDuration; \n    this.assets = assets; \n    this.age = age; \n    this.concCredit = concCredit; \n    this.apartment = apartment; \n    this.credits = credits; \n    this.occupation = occupation; \n    this.dependents = dependents; \n    this.hasPhone = hasPhone; \n    this.foreign = foreign; \n  } \n\n  public double getCreditability() { \n    return creditability; \n  } \n\n  public void setCreditability(double creditability) { \n    this.creditability = creditability; \n  } \n\n  public double getBalance() { \n    return balance; \n  } \n\n  public void setBalance(double balance) { \n    this.balance = balance; \n  } \n\n  public double getDuration() { \n    return duration; \n  } \n\n  public void setDuration(double duration) { \n    this.duration = duration; \n  } \n\n  public double getHistory() { \n    return history; \n  } \n\n  public void setHistory(double history) { \n    this.history = history; \n  } \n\n  public double getPurpose() { \n    return purpose; \n  } \n\n  public void setPurpose(double purpose) { \n    this.purpose = purpose; \n  } \n\n  public double getAmount() { \n    return amount; \n  } \n\n  public void setAmount(double amount) { \n    this.amount = amount; \n  } \n\n  public double getSavings() { \n    return savings; \n  } \n\n  public void setSavings(double savings) { \n    this.savings = savings; \n  } \n\n  public double getEmployment() { \n    return employment; \n  } \n\n  public void setEmployment(double employment) { \n    this.employment = employment; \n  } \n\n  public double getInstPercent() { \n    return instPercent; \n  } \n\n  public void setInstPercent(double instPercent) { \n    this.instPercent = instPercent; \n  } \n\n  public double getSexMarried() { \n    return sexMarried; \n  } \n\n  public void setSexMarried(double sexMarried) { \n    this.sexMarried = sexMarried; \n  } \n\n  public double getGuarantors() { \n    return guarantors; \n  } \n\n  public void setGuarantors(double guarantors) { \n    this.guarantors = guarantors; \n  } \n\n  public double getResidenceDuration() { \n    return residenceDuration; \n  } \n\n  public void setResidenceDuration(double residenceDuration) { \n    this.residenceDuration = residenceDuration; \n  } \n\n  public double getAssets() { \n    return assets; \n  } \n\n  public void setAssets(double assets) { \n    this.assets = assets; \n  } \n\n  public double getAge() { \n    return age; \n  } \n\n  public void setAge(double age) { \n    this.age = age; \n  } \n\n  public double getConcCredit() { \n    return concCredit; \n  } \n\n  public void setConcCredit(double concCredit) { \n    this.concCredit = concCredit; \n  } \n\n  public double getApartment() { \n    return apartment; \n  } \n\n  public void setApartment(double apartment) { \n    this.apartment = apartment; \n  } \n\n  public double getCredits() { \n    return credits; \n  } \n\n  public void setCredits(double credits) { \n    this.credits = credits; \n  } \n\n  public double getOccupation() { \n    return occupation; \n  } \n\n  public void setOccupation(double occupation) { \n    this.occupation = occupation; \n  } \n\n  public double getDependents() { \n    return dependents; \n  } \n\n  public void setDependents(double dependents) { \n    this.dependents = dependents; \n  } \n\n  public double getHasPhone() { \n    return hasPhone; \n  } \n\n  public void setHasPhone(double hasPhone) { \n    this.hasPhone = hasPhone; \n  } \n\n  public double getForeign() { \n    return foreign; \n  } \n\n  public void setForeign(double foreign) { \n    this.foreign = foreign; \n  } \n} \n\n```", "```scala\nDataset<Row> creditData = spark.sqlContext().createDataFrame(creditRDD, Credit.class); \n\n```", "```scala\ncreditData.createOrReplaceTempView(\"credit\"); \n\n```", "```scala\ncreditData.printSchema(); \n\n```", "```scala\nVectorAssembler assembler = new VectorAssembler() \n        .setInputCols(new String[] { \"balance\", \"duration\", \"history\", \"purpose\", \"amount\", \"savings\", \n            \"employment\", \"instPercent\", \"sexMarried\", \"guarantors\", \"residenceDuration\", \"assets\", \"age\", \n            \"concCredit\", \"apartment\", \"credits\", \"occupation\", \"dependents\", \"hasPhone\", \"foreign\" }) \n        .setOutputCol(\"features\"); \n\n```", "```scala\nDataset<Row> assembledFeatures = assembler.transform(creditData); \nassembledFeatures.show(); \n\n```", "```scala\nStringIndexer creditabilityIndexer = new StringIndexer().setInputCol(\"creditability\").setOutputCol(\"label\"); \nDataset<Row> creditabilityIndexed = creditabilityIndexer.fit(assembledFeatures).transform(assembledFeatures); \n\n```", "```scala\ncreditabilityIndexed.show(); \n\n```", "```scala\nlong splitSeed = 12345L; \nDataset<Row>[] splits = creditabilityIndexed.randomSplit(new double[] { 0.7, 0.3 }, splitSeed); \nDataset<Row> trainingData = splits[0]; \nDataset<Row> testData = splits[1]; \n\n```", "```scala\nRandomForestClassifier classifier = new RandomForestClassifier() \n        .setImpurity(\"gini\") \n        .setMaxDepth(3) \n        .setNumTrees(20) \n        .setFeatureSubsetStrategy(\"auto\") \n        .setSeed(splitSeed); \n\n```", "```scala\nRandomForestClassificationModel model = classifier.fit(trainingData); \nBinaryClassificationEvaluator evaluator = new BinaryClassificationEvaluator().setLabelCol(\"label\"); \n\n```", "```scala\nDataset<Row> predictions = model.transform(testData); \nmodel.toDebugString(); \n\n```", "```scala\ndouble accuracy = evaluator.evaluate(predictions); \nSystem.out.println(\"Accuracy after pipeline fitting: \" + accuracy); \nRegressionMetrics rm = new RegressionMetrics(predictions); \nSystem.out.println(\"MSE: \" + rm.meanSquaredError()); \nSystem.out.println(\"MAE: \" + rm.meanAbsoluteError()); \nSystem.out.println(\"RMSE Squared: \" + rm.rootMeanSquaredError()); \nSystem.out.println(\"R Squared: \" + rm.r2()); \nSystem.out.println(\"Explained Variance: \" + rm.explainedVariance() + \"\\n\"); \n\n```", "```scala\nAccuracy after pipeline fitting: 0.7622000403307129 \nMSE: 1.926235109206349E7 \nMAE: 3338.3492063492063 \nRMSE Squared: 4388.8895055655585 \nR Squared: -1.372326447615067 \nExplained Variance: 1.1144695981899707E7 \n\n```", "```scala\nInteger numClasses = 26; \nHashMap<Integer, Integer>categoricalFeaturesInfo = new HashMap<Integer, Integer>(); \nInteger numTrees = 5; // Use more in practice. \nString featureSubsetStrategy = \"auto\"; // Let the algorithm choose. \nString impurity = \"gini\"; \nInteger maxDepth = 20; \nInteger maxBins = 40; \nInteger seed = 12345; \n\n```", "```scala\nfinal RandomForestModelmodel = RandomForest.trainClassifier(training, numClasses, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins, seed); \n\n```"]