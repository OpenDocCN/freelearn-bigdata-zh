["```scala\nimport org.apache.log4j.Level; \nimport org.apache.log4j.Logger; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.ml.Pipeline; \nimport org.apache.spark.ml.PipelineModel; \nimport org.apache.spark.ml.PipelineStage; \nimport org.apache.spark.ml.classification.DecisionTreeClassificationModel; \nimport org.apache.spark.ml.classification.DecisionTreeClassifier; \nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator; \nimport org.apache.spark.ml.feature.IndexToString; \nimport org.apache.spark.ml.feature.LabeledPoint; \nimport org.apache.spark.ml.feature.StringIndexer; \nimport org.apache.spark.ml.feature.StringIndexerModel; \nimport org.apache.spark.ml.feature.VectorAssembler; \nimport org.apache.spark.ml.feature.VectorIndexer; \nimport org.apache.spark.ml.feature.VectorIndexerModel; \nimport org.apache.spark.ml.linalg.Vector; \nimport org.apache.spark.rdd.RDD; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \nimport scala.Tuple2; \n\n```", "```scala\n  static SparkSession spark = SparkSession \n      .builder() \n      .appName(\"JavaLDAExample\") \n      .master(\"local[*]\") \n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n      .getOrCreate(); \n\n```", "```scala\nString csvFile = \"input/On_Time_On_Time_Performance_2016_1.csv\"; \nDataset<Row> df = spark.read().format(\"com.databricks.spark.csv\").option(\"header\", \"true\").load(csvFile);  \nRDD<Tuple2<String, String>> distFile = spark.sparkContext().wholeTextFiles(\"input/test/*.txt\", 2); \nJavaRDD<Tuple2<String, String>> distFile2 = distFile.toJavaRDD(); \nJavaRDD<Row> rowRDD = df.toJavaRDD(); \nDataset<Row> newDF = df.select(df.col(\"ArrDelayMinutes\"), \ndf.col(\"DayofMonth\"), df.col(\"DayOfWeek\"), \ndf.col(\"CRSDepTime\"), df.col(\"CRSArrTime\"), df.col(\"Carrier\"), \ndf.col(\"CRSElapsedTime\"), df.col(\"Origin\"), df.col(\"Dest\")); \nnewDF.show(5); \n\n```", "```scala\npublic class Flight implements Serializable { \n  double label; \n  double monthDay; \n  double weekDay; \n  double crsdeptime; \n  double crsarrtime; \n  String carrier; \n  double crselapsedtime; \n  String origin; \n  String dest; \n\npublic Flight(double label, double monthDay, double weekDay, double crsdeptime, double crsarrtime, String carrier, \n      double crselapsedtime, String origin, String dest) { \n    super(); \n    this.label = label; \n    this.monthDay = monthDay; \n    this.weekDay = weekDay; \n    this.crsdeptime = crsdeptime; \n    this.crsarrtime = crsarrtime; \n    this.carrier = carrier; \n    this.crselapsedtime = crselapsedtime; \n    this.origin = origin; \n    this.dest = dest; \n  } \n  public double getLabel() { \n    return label; \n  } \n  public void setLabel(double label) { \n    this.label = label; \n  } \n  public double getMonthDay() { \n    return monthDay; \n  } \n  public void setMonthDay(double monthDay) { \n    this.monthDay = monthDay; \n  } \n  public double getWeekDay() { \n    return weekDay; \n  } \n  public void setWeekDay(double weekDay) { \n    this.weekDay = weekDay; \n  } \n  public double getCrsdeptime() { \n    return crsdeptime; \n  } \n  public void setCrsdeptime(double crsdeptime) { \n    this.crsdeptime = crsdeptime; \n  } \n  public double getCrsarrtime() { \n    return crsarrtime; \n  } \n  public void setCrsarrtime(double crsarrtime) { \n    this.crsarrtime = crsarrtime; \n  } \n  public String getCarrier() { \n    return carrier; \n  } \n  public void setCarrier(String carrier) { \n    this.carrier = carrier; \n  } \n  public double getCrselapsedtime() { \n    return crselapsedtime; \n  } \n  public void setCrselapsedtime(double crselapsedtime) { \n    this.crselapsedtime = crselapsedtime; \n  } \n  public String getOrigin() { \n    return origin; \n  } \n  public void setOrigin(String origin) { \n    this.origin = origin; \n  } \n  public String getDest() { \n    return dest; \n  } \n  public void setDest(String dest) { \n    this.dest = dest; \n  } \n  @Override \n  public String toString() { \n    return \"Flight [label=\" + label + \", monthDay=\" + monthDay + \", weekDay=\"\n       + weekDay + \", crsdeptime=\" \n        + crsdeptime + \", crsarrtime=\" + crsarrtime + \", carrier=\" + \n      carrier + \", crselapsedtime=\" \n        + crselapsedtime + \", origin=\" + origin + \", dest=\" +\n       dest + \"]\"; \n  } \n\n```", "```scala\nJavaRDD<Flight> flightsRDD = newDF.toJavaRDD().filter(new Function<Row, Boolean>() { \n          @Override \n          public Boolean call(Row v1) throws Exception { \n            return !v1.getString(0).isEmpty(); \n          } \n        }).map(new Function<Row, Flight>() { \n          @Override \n          public Flight call(Row r) throws Exception { \n            double label; \n            double delay = Double.parseDouble(r.getString(0)); \n            if (delay > 60) \n              label = 1.0; \nelse \n      label = 0.0; \ndouble monthday = Double.parseDouble(r.getString(1)) - 1; \ndouble weekday = Double.parseDouble(r.getString(2)) - 1; \ndouble crsdeptime = Double.parseDouble(r.getString(3)); \ndouble crsarrtime = Double.parseDouble(r.getString(4)); \nString carrier = r.getString(5); \ndouble crselapsedtime1 = Double.parseDouble(r.getString(6)); \nString origin = r.getString(7); \nString dest = r.getString(8); \nFlight flight = new Flight(label, monthday, weekday,crsdeptime, crsarrtime, carrier,crselapsedtime1, origin, dest); \n        return flight; \n    }}); \n\n```", "```scala\nDataset<Row> flightDelayData = spark.sqlContext().createDataFrame(flightsRDD,Flight.class); \nflightDelayData.printSchema(); \n\n```", "```scala\nflightDelayData.show(5); \n\n```", "```scala\nStringIndexer carrierIndexer = new StringIndexer().setInputCol(\"carrier\").setOutputCol(\"carrierIndex\"); \nDataset<Row> carrierIndexed = carrierIndexer.fit(flightDelayData).transform(flightDelayData); \nStringIndexer originIndexer = new StringIndexer().setInputCol(\"origin\").setOutputCol(\"originIndex\"); \nDataset<Row> originIndexed = originIndexer.fit(carrierIndexed).transform(carrierIndexed); \nStringIndexer destIndexer = new StringIndexer().setInputCol(\"dest\").setOutputCol(\"destIndex\"); \nDataset<Row> destIndexed = destIndexer.fit(originIndexed).transform(originIndexed); \ndestIndexed.show(5); \n\n```", "```scala\nVectorAssembler assembler = new VectorAssembler().setInputCols( \n        new String[] { \"monthDay\", \"weekDay\", \"crsdeptime\", \n            \"crsarrtime\", \"carrierIndex\", \"crselapsedtime\", \n            \"originIndex\", \"destIndex\" }).setOutputCol( \n        \"assembeledVector\"); \n\n```", "```scala\nDataset<Row> assembledFeatures = assembler.transform(destIndexed); \n\n```", "```scala\nJavaRDD<Row> rescaledRDD = assembledFeatures.select(\"label\",\"assembeledVector\").toJavaRDD(); \n\n```", "```scala\nJavaRDD<LabeledPoint> mlData = rescaledRDD.map(new Function<Row, LabeledPoint>() { \n          @Override \n          public LabeledPoint call(Row row) throws Exception { \n            double label = row.getDouble(0); \n            Vector v = row.getAs(1); \n            return new LabeledPoint(label, v); \n          } \n        }); \n\n```", "```scala\nSystem.out.println(mlData.take(5));  \n\n```", "```scala\nJavaRDD<LabeledPoint> splitedData0 = mlData.filter(new Function<LabeledPoint, Boolean>() { \n          @Override \n          public Boolean call(LabeledPoint r) throws Exception { \n              return r.label() == 0; \n          } \n        }).randomSplit(new double[] { 0.85, 0.15 })[1]; \n\n    JavaRDD<LabeledPoint> splitedData1 = mlData.filter(new Function<LabeledPoint, Boolean>() { \n          @Override \n          public Boolean call(LabeledPoint r) throws Exception { \n            return r.label() == 1; \n          } \n        }); \n\n    JavaRDD<LabeledPoint> splitedData2 = splitedData1.union(splitedData0); \n    System.out.println(splitedData2.take(1)); \n\n```", "```scala\nJavaRDD<LabeledPoint> splitedData2 = splitedData1.union(splitedData0); \nSystem.out.println(splitedData2.take(1)); \n\n```", "```scala\nDataset<Row> data = spark.sqlContext().createDataFrame(splitedData2, LabeledPoint.class); \ndata.show(100); \n\n```", "```scala\nVectorIndexerModel featureIndexer = new VectorIndexer() \n          .setInputCol(\"features\") \n          .setOutputCol(\"indexedFeatures\") \n          .setMaxCategories(4) \n          .fit(data); \n\n```", "```scala\nStringIndexerModel labelIndexer = new StringIndexer() \n          .setInputCol(\"label\") \n          .setOutputCol(\"indexedLabel\") \n          .fit(data); \n\n```", "```scala\nDataset<Row>[] splits = data.randomSplit(new double[]{0.7, 0.3}); \nDataset<Row> trainingData = splits[0]; \nDataset<Row> testData = splits[1]; \n\n```", "```scala\nDecisionTreeClassifier dt = new DecisionTreeClassifier() \n      .setLabelCol(\"indexedLabel\") \n      .setFeaturesCol(\"indexedFeatures\"); \n\n```", "```scala\nIndexToString labelConverter = new IndexToString() \n      .setInputCol(\"prediction\") \n      .setOutputCol(\"predictedLabel\")         \n        .setLabels(labelIndexer.labels());  \n\n```", "```scala\nPipeline pipeline = new Pipeline() \n      .setStages(new PipelineStage[]{labelIndexer,  \n        featureIndexer, dt, labelConverter}); \n\n```", "```scala\nPipelineModel model = pipeline.fit(trainingData); \n\n```", "```scala\nDataset<Row> predictions = model.transform(testData); \npredictions.select(\"predictedLabel\", \"label\", \"features\").show(5); \n\n```", "```scala\nMulticlassClassificationEvaluator evaluator = new MulticlassClassificationEvaluator() \n      .setLabelCol(\"indexedLabel\") \n      .setPredictionCol(\"prediction\") \n      .setMetricName(\"accuracy\"); \n    double accuracy = evaluator.evaluate(predictions); \n    System.out.println(\"accuracy: \"+accuracy); \n    System.out.println(\"Test Error = \" + (1.0 - accuracy)); \n\n```", "```scala\nAccuracy: 0.7540472721385786 \nTest Error = 0.24595272786142142 \n\n```", "```scala\nDecisionTreeClassificationModel treeModel = \n      (DecisionTreeClassificationModel) (model.stages()[2]); \nSystem.out.println(\"Learned classification tree model:\\n\" + treeModel.toDebugString()); \n\n```", "```scala\nspark.stop();\n```", "```scala\nimport java.io.Serializable; \nimport java.util.List; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.mllib.clustering.KMeans; \nimport org.apache.spark.mllib.clustering.KMeansModel; \nimport org.apache.spark.mllib.linalg.Vector; \nimport org.apache.spark.mllib.linalg.Vectors; \nimport org.apache.spark.rdd.RDD; \nimport org.apache.spark.sql.SparkSession;  \n\n```", "```scala\n  static SparkSession spark = SparkSession \n      .builder().appName(\"JavaLDAExample\") \n      .master(\"local[*]\") \n      .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n      .getOrCreate(); \n\n```", "```scala\nRDD<String> data = spark.sparkContext().textFile(\"input/Saratoga_ NY_Homes.txt\", 2); \n\n```", "```scala\nJavaRDD<Vector> parsedData = data.toJavaRDD().map(new Function<String, Vector>() { \n      @Override \n      public Vector call(String s) throws Exception { \n        String[] sarray = s.split(\",\"); \n        double[] values = new double[sarray.length]; \n        for (int i = 0; i < sarray.length; i++) \n          values[i] = Double.parseDouble(sarray[i]); \n        return Vectors.dense(values); \n      } \n    });  \n\n```", "```scala\nint numClusters = 4; \nint numIterations = 10; \nint runs = 2; \nKMeansModel clusters = KMeans.train(parsedData.rdd(), numClusters, numIterations, runs , KMeans.K_MEANS_PARALLEL());  \nNow estimate the cost to compute the clsuters as follows: \ndouble cost = clusters.computeCost(parsedData.rdd()); \nSystem.out.println(\"Cost: \" + cost);  \n\n```", "```scala\nCost: 3.60148995801542E12   \n\n```", "```scala\nVector[] centers = clusters.clusterCenters(); \nSystem.out.println(\"Cluster Centers: \"); \nfor (Vector center : centers)  \n{ \n  System.out.println(center); \n} \n\n```", "```scala\n[545360.4081632652,0.9008163265306122,0.1020408163265306,21.73469387755102,111630.61224489794,0.061224489795918366,0.7551020408163265,2.3061224489795915,2.1632653061224487,2.714285714285714,2860.755102040816,59.346938775510196,3.510204081632653,1.1020408163265305,2.714285714285714,10.061224489795917] \n[134073.06845637583,0.3820000000000002,0.0026845637583892616,33.72617449664429,19230.76510067114,0.012080536912751677,0.22818791946308722,2.621476510067114,2.7234899328859057,2.6630872483221477,1332.9234899328858,52.86040268456375,2.7395973154362414,0.38120805369127514,1.4946308724832214,5.806711409395973] \n[218726.0625,0.5419711538461538,0.0,25.495192307692307,32579.647435897434,0.041666666666666664,0.3830128205128205,2.3205128205128203,2.4615384615384617,2.692307692307692,1862.3076923076922,57.4599358974359,3.3894230769230766,0.7019230769230769,2.032852564102564,7.44551282051282] \n[332859.0580645161,0.6369354838709671,0.025806451612903226,19.803225806451614,63188.06451612903,0.13870967741935483,0.6096774193548387,2.2225806451612904,2.2483870967741937,2.774193548387097,2378.4290322580646,57.66774193548387,3.6225806451612903,0.8516129032258064,2.479032258064516,8.719354838709677] \n\n```", "```scala\ndouble WSSSE = clusters.computeCost(parsedData.rdd()); \nSystem.out.println(\"Within Set Sum of Squared Errors = \" + WSSSE); \n\n```", "```scala\nWithin Set Sum of Squared Errors = 3.60148995801542E12 \n\n```", "```scala\nList<Vector> houses = parsedData.collect(); \nint prediction  = clusters.predict(houses.get(18)); \nSystem.out.println(\"Prediction: \"+prediction);  \n\n```", "```scala\nspark.stop(); \n\n```", "```scala\nint prediction  = clusters.predict(houses.get(18)); \n    System.out.println(\"Prediction: \"+prediction); \n\n```"]