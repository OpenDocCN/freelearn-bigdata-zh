["```scala\ndef adder(x): \n    return lambda y: x + y \nadd6 = adder(6) \nadd4(4) \n\n```", "```scala\nSubject<Integer> sub = x -> x % 2 = 0; // Tests if the parameter is even. \nboolean result = sub.test(8); \n\ntrue since 8 is divisible by 2.\n```", "```scala\n$ java -version \n\n```", "```scala\njava version \"1.7.0_80\"\nJava(TM) SE Runtime Environment (build 1.7.0_80-b15)\nJava HotSpot(TM) 64-Bit Server VM (build 24.80-b11, mixed mode)\n\n```", "```scala\n$ sudo apt-add-repository ppa:webupd8team/java\n$ sudo apt-get update\n$ sudo apt-get install oracle-java8-installer\n\n```", "```scala\n$ echo \"export JAVA_HOME=/usr/lib/jvm/java-8-oracle\" >> ~/.bashrc \n$ echo \"export PATH=$PATH:$JAVA_HOME/bin\" >> ~/.bashrc\n\n```", "```scala\n$ scala -version\n\n```", "```scala\nScala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL\n\n```", "```scala\n     $ tar -xvzf scala-2.11.8.tgz \n\n    ```", "```scala\n     $ cd /home/Downloads/ \n     $ mv scala-2.11.8 /usr/local/scala \n\n    ```", "```scala\n    $ echo \"export SCALA_HOME=/usr/local/scala/scala-2.11.8\" >> \n            ~/.bashrc \n    $ echo \"export PATH=$PATH:$SCALA_HOME/bin\" >> ~/.bashrc\n\n    ```", "```scala\n     $ scala -version\n\n    ```", "```scala\n    Scala code runner version 2.11.8 -- Copyright 2002-2016, LAMP/EPFL\n\n    ```", "```scala\n     $ tar -xvzf spark-2.0.0-bin-hadoop2.7.tgz \n\n    ```", "```scala\n     $ cd /home/Downloads/ \n     $ mv spark-2.0.0-bin-hadoop2.7 /usr/local/spark \n\n    ```", "```scala\n    $ echo \"export SPARK_HOME=/usr/local/spark/spark-2.0.0-bin-hadoop2.7\" >>\n          ~/.bashrc \n    $ echo \"export PATH=$PATH:$SPARK_HOME/bin\" >> ~/.bashrc\n\n    ```", "```scala\n$ source ~/.bashrc\n\n```", "```scala\nexport JAVA_HOME=/usr/lib/jvm/java-8-oracle\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin: /usr/lib/jvm/java-8-oracle/bin\nexport SCALA_HOME=/usr/local/scala/scala-2.11.8\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin: /bin\nexport SPARK_HOME=/usr/local/spark/spark-2.0.0-bin-hadoop2.7\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games:/usr/lib/jvm/java-8-oracle/bin:/usr/lib/jvm/java-8-oracle/db/bin:/usr/lib/jvm/java-8-oracle/jre/bin: /bin\n\n```", "```scala\n$ spark-shell\n\n```", "```scala\nimport java.util.Arrays; \nimport java.util.List; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.mllib.fpm.FPGrowth; \nimport org.apache.spark.mllib.fpm.FPGrowthModel; \nimport org.apache.spark.rdd.RDD; \nimport org.apache.spark.sql.SparkSession; \n\npublic class JavaFPGrowthExample { \n  public static void main(String[] args) { \n   //Specify the input transactional as command line argument  \n   String fileName = \"input/input.txt\";  \n   //Configure a SparkSession as spark by specifying the application name, master URL, Spark config, and Spark warehouse directory \n  SparkSession spark = SparkSession \n                  .builder() \n                  .appName(\"JavaFPGrowthExample\") \n                  .master(\"local[*]\") \n                  .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n                  .getOrCreate(); \n\n   //Create an initial RDD by reading the input database  \n   RDD<String> data = spark.sparkContext().textFile(fileName, 1); \n\n   //Read the transactions by tab delimiter & mapping RDD(data) \n   JavaRDD<List<String>> transactions = data.toJavaRDD().map( \n                   new Function<String, List<String>>(){ \n                   public List<String> call(String line) { \n                          String[] parts = line.split(\" \"); \n                          return Arrays.asList(parts); \n                                 } \n                             }); \n\n  //Create FPGrowth object by min. support & no. of partition     \n  FPGrowth fpg = new  FPGrowth() \n                       .setMinSupport(0.2) \n                       .setNumPartitions(10); \n\n  //Train and run your FPGrowth model using the transactions \n  FPGrowthModel<String> model = fpg.run(transactions); \n\n  //Convert and then collect frequent patterns as Java RDD. After that print the frequent patterns along with their support \n    for (FPGrowth.FreqItemset<String> itemset :      \n          model.freqItemsets().toJavaRDD().collect()) {   \n       System.out.println(itemset.javaItems()  \n                             + \"==> \" + itemset.freq()); \n      } \n    }   \n  }  \n\n```", "```scala\n          <dependency> \n          <groupId>org.apache.spark</groupId> \n          <artifactId>spark-core_2.11</artifactId> \n          <version>2.0.0</version> \n         </dependency> \n\n    ```", "```scala\n        <dependency> \n          <groupId>org.apache.spark</groupId> \n          <artifactId>spark-mllib_2.11</artifactId> \n          <version>2.0.0</version> \n         </dependency> \n\n    ```", "```scala\n           <plugin> \n            <groupId>org.apache.maven.plugins</groupId> \n            <artifactId>maven-eclipse-plugin</artifactId> \n            <version>2.9</version> \n            <configuration> \n              <downloadSources>true</downloadSources> \n              <downloadJavadocs>false</downloadJavadocs> \n            </configuration> \n          </plugin>  \n\n    ```", "```scala\n          <plugin> \n            <groupId>org.apache.maven.plugins</groupId> \n            <artifactId>maven-compiler-plugin</artifactId> \n            <version>2.3.2</version>         \n          </plugin> \n          <plugin> \n            <groupId>org.apache.maven.plugins</groupId> \n            <artifactId>maven-shade-plugin</artifactId> \n            <configuration> \n              <shadeTestJar>true</shadeTestJar> \n            </configuration> \n          </plugin> \n\n    ```", "```scala\n          <plugin> \n            <groupId>org.apache.maven.plugins</groupId> \n            <artifactId>maven-assembly-plugin</artifactId> \n            <version>2.4.1</version> \n            <configuration> \n              <!-- get all project dependencies --> \n              <descriptorRefs> \n                <descriptorRef>jar-with-dependencies</descriptorRef> \n              </descriptorRefs> \n              <!-- MainClass in mainfest make a executable jar --> \n              <archive> \n                <manifest>              <mainClass>com.example.SparkFPGrowth.JavaFPGrowthExample</mainClass>            </manifest> \n              </archive> \n              <property> \n                <name>oozie.launcher.mapreduce.job.user.classpath.first</name> \n                <value>true</value> \n              </property> \n              <finalName>FPGrowth-${project.version}</finalName> \n            </configuration> \n            <executions> \n              <execution> \n                <id>make-assembly</id> \n                <!-- bind to the packaging phase --> \n                <phase>package</phase> \n                <goals> \n                  <goal>single</goal> \n                </goals> \n              </execution> \n            </executions> \n          </plugin> \n\n    ```", "```scala\n$ spark-shell --master \"local[4]\" \n\n```", "```scala\nscala>import org.apache.spark.mllib.fpm.FPGrowth\nscala>import org.apache.spark.{SparkConf, SparkContext}\n\n```", "```scala\nval conf = new SparkConf().setAppName(s\"FPGrowthExample with $params\")\nval sc = new SparkContext(conf)\n\n```", "```scala\nscala> val transactions = sc.textFile(params.input).map(_.split(\" \")).cache()\n\n```", "```scala\nScala>println(s\"Number of transactions: ${transactions.count()}\")\nNumber of transactions: 22\nScala>\n\n```", "```scala\nscala>val model = new FPGrowth()\n .setMinSupport(0.2)\n .setNumPartitions(2)\n .run(transactions)\n\n```", "```scala\nscala> println(s\"Number of frequent itemsets:\n    ${model.freqItemsets.count()}\")\nNumber of frequent itemsets: 18\nScala>\n\n```", "```scala\nscala> model.freqItemsets.collect().foreach { itemset => println(itemset.items.mkString(\"[\", \",\", \"]\") + \", \" + itemset.freq)}\n\n```", "```scala\n$spark-submit [options] <app-jar | python-file> [app arguments]\n\n```", "```scala\n$./bin/spark-submit --class com.example.SparkFPGrowth.JavaFPGrowthExample --master local[4] FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar input.txt\n\n```", "```scala\n$ sudo chmod  400  /usr/local/key/my-key-pair.pem\n\n```", "```scala\n$ echo \"export AWS_ACCESS_KEY_ID=<access_key_id>\" >> ~/.bashrc \n$ echo \" export AWS_SECRET_ACCESS_KEY=<secret_access_key_id>\" >> ~/.bashrc \n\n```", "```scala\n$./spark-ec2 --key-pair=<name_of_the_key_pair> --identity-file=<path_of_the key_pair>  --instance-type=<AWS_instance_type > --region=<region> zone=<zone> --slaves=<number_of_slaves> --hadoop-major-version=<Hadoop_version> --spark-version=<spark_version> launch <cluster-name>\n\n```", "```scala\n$./spark-ec2 --key-pair=my-key-pair --identity-file=/usr/local/key/my-key-pair.pem  --instance-type=m3.2xlarge --region=eu-west-1 --zone=eu-west-1a --slaves=2 --hadoop-major-version=yarn --spark-version=1.6.0 launch ec2-spark-cluster-1\n\n```", "```scala\n$./spark-ec2 --key-pair=<name_of_the_key_pair> --identity-file=<path_of_the _key_pair> --region=<region> login <cluster-name> \n\n```", "```scala\n$./spark-ec2 --key-pair=my-key-pair --identity-file=/usr/local/key/my-key-pair.pem --region=eu-west-1 login ec2-spark-cluster-1 \n\n```", "```scala\n$ scp -i /usr/local/key/my-key-pair.pem  /usr/local/code/FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar ec2-user@ec2-52-18-252-59.eu-west-1.compute.amazonaws.com:/home/ec2-user/\n\n```", "```scala\n$ scp -i /usr/local/key/my-key-pair.pem /usr/local/data/input.txt ec2-user@ec2-52-18-252-59.eu-west-1.compute.amazonaws.com:/home/ec2-user/ \n\n```", "```scala\n$./bin/spark-submit --class com.example.SparkFPGrowth.JavaFPGrowthExample --master spark://ec2-52-48-119-121.eu-west-1.compute.amazonaws.com:7077 /home/ec2-user/FPGrowth-0.0.1-SNAPSHOT-jar-with-dependencies.jar /home/ec2-user/input.txt\n\n```", "```scala\n$./ec2/spark-ec2 --region=<ec2-region> stop <cluster-name>\n\n```", "```scala\n$./ec2/spark-ec2 --region=eu-west-1 stop ec2-spark-cluster-1\n\n```", "```scala\n$./ec2/spark-ec2 -i <key-file> --region=<ec2-region> start <cluster-name>\n\n```", "```scala\n$./ec2/spark-ec2 --identity-file=/usr/local/key/my-key-pair.pem --region=eu-west-1 start ec2-spark-cluster-1\n\n```"]