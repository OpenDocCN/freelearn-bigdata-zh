["```scala\nimport java.io.BufferedReader; \nimport java.io.FileReader; \nimport java.io.IOException; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.mllib.linalg.DenseVector; \nimport org.apache.spark.mllib.linalg.Matrices; \nimport org.apache.spark.mllib.linalg.Matrix; \nimport org.apache.spark.mllib.linalg.Vector; \nimport org.apache.spark.mllib.regression.LabeledPoint; \nimport org.apache.spark.mllib.stat.Statistics; \nimport org.apache.spark.mllib.stat.test.ChiSqTestResult; \nimport org.apache.spark.rdd.RDD; \nimport org.apache.spark.sql.SparkSession; \nimport com.example.SparkSession.UtilityForSparkSession; \n\n```", "```scala\nstatic SparkSession spark = UtilityForSparkSession.mySession(); \n\n```", "```scala\npublic class UtilityForSparkSession { \n  public static SparkSession mySession() { \n  SparkSession spark = SparkSession \n               .builder() \n               .appName(\"JavaHypothesisTestingOnBreastCancerData \") \n               .master(\"local[*]\") \n              .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n              .getOrCreate(); \n    return spark; \n  } \n} \n\n```", "```scala\nVector v = myVector(); \n\n```", "```scala\npublic static Vector myVector() throws NumberFormatException, IOException {     \nBufferedReader br = new BufferedReader(new FileReader(path)); \n    String line = nulNow let's compute the goodness of the fit. Note, if a second vector tol; \n    Vector v = null; \n    while ((line = br.readLine()) != null) { \n      String[] tokens = line.split(\",\"); \n      double[] features = new double[30]; \n      for (int i = 2; i < features.length; i++) { \n        features[i-2] =     \n                       Double.parseDouble(tokens[i]); \n      } \n      v = new DenseVector(features); \n    } \n    return v; \n  } \n\n```", "```scala\nChiSqTestResult goodnessOfFitTestResult = Statistics.chiSqTest(v); \n\n```", "```scala\nSystem.out.println(goodnessOfFitTestResult + \"\\n\"); \n\n```", "```scala\nChi squared test summary: \nmethod: pearson \ndegrees of freedom = 29  \nstatistic = 4528.611649568829  \npValue = 0.0  \n\n```", "```scala\n ((1.0, 3.0, 5.0, 2.0), (4.0, 6.0, 1.0, 3.5), (6.9, 8.9, 10.5, 12.6)) \nMatrix mat = Matrices.dense(4, 3, new double[] { 1.0, 3.0, 5.0, 2.0, 4.0, 6.0, 1.0, 3.5, 6.9, 8.9, 10.5, 12.6});     \n\n```", "```scala\nChiSqTestResult independenceTestResult = Statistics.chiSqTest(mat); \n\n```", "```scala\nSystem.out.println(independenceTestResult + \"\\n\"); \n\n```", "```scala\nChi squared test summary: \nmethod: pearson \ndegrees of freedom = 6  \nstatistic = 6.911459343085576  \npValue = 0.3291131185252161  \nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent.  \n\n```", "```scala\nstatic String path = \"breastcancer/input/wdbc.data\"; \nRDD<String> lines = spark.sparkContext().textFile(path, 2);     \nJavaRDD<LabeledPoint> linesRDD = lines.toJavaRDD().map(new Function<String, LabeledPoint>() { \n    public LabeledPoint call(String lines) { \n    String[] tokens = lines.split(\",\"); \n    double[] features = new double[30]; \n    for (int i = 2; i < features.length; i++) { \n    features[i - 2] = Double.parseDouble(tokens[i]); \n            } \n    Vector v = new DenseVector(features); \n    if (tokens[1].equals(\"B\")) { \n    return new LabeledPoint(1.0, v); // benign \n      } else { \n    return new LabeledPoint(0.0, v); // malignant \n        } \n      } \n    }); \n\n```", "```scala\nChiSqTestResult[] featureTestResults = Statistics.chiSqTest(linesRDD.rdd()); \n\n```", "```scala\nint i = 1; \nfor (ChiSqTestResult result : featureTestResults) { \nSystem.out.println(\"Column \" + i + \":\"); \nSystem.out.println(result + \"\\n\");  \ni++; \n} \n\nColumn 1: \nChi-squared test summary: \nmethod: Pearson \ndegrees of freedom = 455  \nstatistic = 513.7450859274513  \npValue = 0.02929608473276224  \nStrong presumption against null hypothesis: the occurrence of the outcomes is statistically independent. \n\nColumn 2: \nChi-squared test summary: \nmethod: Pearson \ndegrees of freedom = 478  \nstatistic = 498.41630331377735  \npValue = 0.2505929829141742  \nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent. \n\nColumn 3: \nChi-squared test summary: \nmethod: Pearson \ndegrees of freedom = 521  \nstatistic = 553.3147340697276  \npValue = 0.1582572931194156  \nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent. \n. \n. \nColumn 30: \nChi-squared test summary: \nmethod: Pearson \ndegrees of freedom = 0  \nstatistic = 0.0  \npValue = 1.0  \nNo presumption against null hypothesis: the occurrence of the outcomes is statistically independent.  \n\n```", "```scala\nimport java.io.Serializable; \nimport java.util.Arrays; \nimport java.util.logging.Level; \nimport java.util.logging.Logger; \nimport org.apache.spark.api.java.JavaPairRDD; \nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.ml.Pipeline; \nimport org.apache.spark.ml.PipelineStage; \nimport org.apache.spark.ml.classification.LogisticRegression; \nimport org.apache.spark.ml.evaluation.BinaryClassificationEvaluator; \nimport org.apache.spark.ml.feature.HashingTF; \nimport org.apache.spark.ml.feature.Tokenizer; \nimport org.apache.spark.ml.param.ParamMap; \nimport org.apache.spark.ml.tuning.CrossValidator; \nimport org.apache.spark.ml.tuning.CrossValidatorModel; \nimport org.apache.spark.ml.tuning.ParamGridBuilder; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \nimport scala.Tuple2; \n\n```", "```scala\n  static SparkSession spark = SparkSession \n        .builder() \n        .appName(\"CrossValidationforSpamFiltering\") \n        .master(\"local[*]\") \n        .config(\"spark.sql.warehouse.dir\", \"E:/Exp/\") \n        .getOrCreate(); \n\n```", "```scala\nDataset<Row> df = spark.read().text(\"input/SMSSpamCollection.txt\"); \ndf.show(); \n\n```", "```scala\nJavaPairRDD<Row, Long> rowRDD = df.toJavaRDD().zipWithIndex(); \n\n```", "```scala\nJavaRDD<LabeledDocument> splitedRDD = rowRDD.map(new Function<Tuple2<Row, Long>, LabeledDocument>() { \n@Override \npublic LabeledDocument call(Tuple2<Row, Long> v1) throws Exception {   \n  Row r = v1._1; \n  long index = v1._2; \n  String[] split = r.getString(0).split(\"\\t\"); \n  if(split[0].equals(\"ham\")) \n    return new LabeledDocument(index,split[1], 1.0); \n  else \n    return new LabeledDocument(index,split[1], 0.0); \n      } \n    });  \n\n```", "```scala\nDataset<Row> training = spark.createDataFrame(splitedRDD, LabeledDocument.class); \ntraining.show(false); \n\n```", "```scala\npublic class Document implements Serializable { \n  private long id; \n  private String text; \n  //Initialise the constructor that should take two parameters: id and text// \n  Set the id \n  Get the id \n  Set the text \n  Get the text   \n  } \n\n```", "```scala\n  public LabeledDocument (long id, String text) {   \n      this.id = id; \n           this.text = text; \n  } \n\n```", "```scala\n  public void setId(long id) { \n    this.id = id; \n  } \n  public long getId() { \n    return this.id; \n  } \n\n```", "```scala\n  public String getText() { \n    return this.text; \n  } \n\n  public void setText(String text) { \n    this.text = text; \n  } \n\n```", "```scala\nimport java.io.Serializable; \npublic class Document implements Serializable { \n  private long id; \n  private String text; \n  public Document(long id, String text) { \n    this.id = id; \n    this.text = text; \n  } \n  public long getId() { \n    return this.id; \n  } \n  public void setId(long id) { \n    this.id = id; \n  } \n  public String getText() { \n    return this.text; \n  } \n  public void setText(String text) { \n    this.text = text; \n  } \n} \n\n```", "```scala\n  public LabeledDocument(long id, String text, double label) {   \n    this.label = label; \n  } \n\n```", "```scala\n  public LabeledDocument(long id, String text, double label) { \n    super(id, text); \n    this.label = label; \n  }  \n\n```", "```scala\n  public void setLabel(double label) { \n    this.label = label; \n  } \n\n```", "```scala\n  public double getLabel() { \n    return this.label; \n  } \n\n```", "```scala\nimport java.io.Serializable; \npublic class LabeledDocument extends Document implements Serializable { \n  private double label; \n  public LabeledDocument(long id, String text, double label) { \n    super(id, text); \n    this.label = label; \n  } \n  public double getLabel() { \n    return this.label; \n  } \n  public void setLabel(double label) { \n    this.label = label; \n  } \n} \n\n```", "```scala\nTokenizer tokenizer = new Tokenizer().setInputCol(\"text\").setOutputCol(\"words\"); \nHashingTF hashingTF = new HashingTF().setNumFeatures(1000).setInputCol(tokenizer.getOutputCol()).setOutputCol(\"features\"); \nLogisticRegression lr = new LogisticRegression().setMaxIter(10).setRegParam(0.01); \nPipeline pipeline = new Pipeline().setStages(new PipelineStage[] { tokenizer, hashingTF, lr }); \n\n```", "```scala\nParamMap[] paramGrid = new ParamGridBuilder() \n.addGrid(hashingTF.numFeatures(), new int[] { 10, 100, 1000 }).addGrid(lr.regParam(), new double[] { 0.1, 0.01 }) \n .build(); \n\n```", "```scala\n    CrossValidator cv = new CrossValidator() \n        .setEstimator(pipeline) \n        .setEvaluator(new BinaryClassificationEvaluator())                  \n        .setEstimatorParamMaps(paramGrid) \n        .setNumFolds(5); // 5-fold cross validation \n\n```", "```scala\nCrossValidatorModel cvModel = cv.fit(training); \n\n```", "```scala\n    Dataset<Row> test = spark.createDataFrame(Arrays.asList( \n      new Document(4L, \"FreeMsg CALL j k\"),  \n      new Document(5L, \"Siva  hostel\"), \n      new Document(6L, \"darren now\"),  \n     new Document(7L, \"Sunshine Quiz! Win a super Sony\")),Document.class); \n\n```", "```scala\nDataset<Row> predictions = cvModel.transform(test); \n\n```", "```scala\nfor (Row r : predictions.select(\"id\", \"text\", \"probability\", \"prediction\").collect()) \n    { \nSystem.out.println(\"(\" + r.get(0) + \", \" + r.get(1) + \") --> prob=\" + r.get(2) + \", prediction=\" + r.get(3)); \n    }  \n\n```", "```scala\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator; \nimport org.apache.spark.ml.param.ParamMap; \nimport org.apache.spark.ml.regression.LinearRegression; \nimport org.apache.spark.ml.tuning.ParamGridBuilder; \nimport org.apache.spark.ml.tuning.TrainValidationSplit; \nimport org.apache.spark.ml.tuning.TrainValidationSplitModel; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession;\n```", "```scala\n  SparkSession spark = SparkSession \n          .builder() \n          .appName(\"TrainSplitOCR\") \n              .master(\"local[*]\") \n               .config(\"spark.sql.warehouse.dir\",  \n                 \"E:/Exp/\") \n              .getOrCreate(); \n\n```", "```scala\nDataset<Row> data = spark.read().format(\"libsvm\").load(\"input/Letterdata_libsvm.data\"); \ndata.show(false); \n\n```", "```scala\n// Prepare training and test data. \nDataset<Row>[] splits = data.randomSplit(new double[] {0.9, 0.1}, 12345); \nDataset<Row> training = splits[0]; \nDataset<Row> test = splits[1]; \n\n```", "```scala\nLinearRegression lr = new LinearRegression(); \n\n```", "```scala\nParamMap[] paramGrid = new ParamGridBuilder() \n.addGrid(hashingTF.numFeatures(), new int[] { 10, 100, 1000 }).addGrid(lr.regParam(), new double[] { 0.1, 0.01 }) \n.build(); \n\n```", "```scala\nTrainValidationSplit trainValidationSplit = new TrainValidationSplit() \n    .setEstimator(lr) \n    .setEvaluator(new MulticlassClassificationEvaluator()) \n    .setEstimatorParamMaps(paramGrid) \n    .setTrainRatio(0.7); \n\n```", "```scala\nTrainValidationSplitModel model = trainValidationSplit.fit(training); \n\n```", "```scala\nDataset<Row> per_param = model.transform(test); \nper_param.show(false);   \n\n```", "```scala\nimport org.apache.spark.api.java.JavaRDD; \nimport org.apache.spark.api.java.function.Function; \nimport org.apache.spark.ml.classification.LogisticRegression; \nimport org.apache.spark.ml.evaluation.RegressionEvaluator; \nimport org.apache.spark.ml.feature.LabeledPoint; \nimport org.apache.spark.ml.linalg.DenseVector; \nimport org.apache.spark.ml.linalg.Vector; \nimport org.apache.spark.ml.param.ParamMap; \nimport org.apache.spark.ml.tuning.ParamGridBuilder; \nimport org.apache.spark.ml.tuning.TrainValidationSplit; \nimport org.apache.spark.ml.tuning.TrainValidationSplitModel; \nimport org.apache.spark.rdd.RDD; \nimport org.apache.spark.sql.Dataset; \nimport org.apache.spark.sql.Row; \nimport org.apache.spark.sql.SparkSession; \n\n```", "```scala\nstatic SparkSession spark = SparkSession \n  .builder() \n  .appName(\"CrossValidationforSpamFiltering\") \n  .master(\"local[*]\") \n  .config(\"spark.sql.warehouse.dir\", \"C:/Exp/\"). \n  getOrCreate(); \n\n```", "```scala\nString path = \"breastcancer/input/wdbc.data\"; \nRDD<String> lines = spark.sparkContext().textFile(path, 3); \n\n```", "```scala\nJavaRDD<LabeledPoint> linesRDD = lines.toJavaRDD().map(new Function<String, LabeledPoint>() { \n      public LabeledPoint call(String lines) { \n      String[] tokens = lines.split(\",\"); \n      double[] features = new double[30]; \n      for (int i = 2; i < features.length; i++) { \n          features[i - 2] =             \n                 Double.parseDouble(tokens[i]); \n        } \n           Vector v = new DenseVector(features); \n           if (tokens[1].equals(\"B\")) { \n      return new LabeledPoint(1.0, v); // benign \n    } else { \n    return new LabeledPoint(0.0, v); // malignant \n    } \n      } \n    }); \n\n```", "```scala\nDataset<Row> data = spark.sqlContext().createDataFrame(linesRDD, LabeledPoint.class); \ndata.show(); \n\n```", "```scala\nDataset<Row>[] splits=data.randomSplit(new double[] {0.8, 0.2}); \nDataset<Row> training = splits[0]; \nDataset<Row> test = splits[1];\n```", "```scala\nLogisticRegression lr = new LogisticRegression(); \n\n```", "```scala\nParamMap[] paramGrid = new ParamGridBuilder() \n.addGrid(lr.regParam(), new double[] {0.1, 0.01}) \n.addGrid(lr.fitIntercept()) \n.addGrid(lr.elasticNetParam(), new double[] {0.0, 0.5, 1.0}) \n.build();\n```", "```scala\nTrainValidationSplit trainValidationSplit = new TrainValidationSplit() \n.setEstimator(lr) \n.setEvaluator(new RegressionEvaluator()) \n.setEstimatorParamMaps(paramGrid) \n.setTrainRatio(0.8); \n\n```", "```scala\nTrainValidationSplitModel model = trainValidationSplit.fit(training); \n\n```", "```scala\nDataset<Row> per_param = model.transform(test); \nper_param.show(); \n\n```"]