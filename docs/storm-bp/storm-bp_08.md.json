["```scala\nTwitterSpout spout = new TwitterSpout();\nStream inputStream = topology.newStream(\"nlp\", spout);\ntry {\ninputStream.each(new Fields(\"tweet\"), new TweetSplitterFunction(), new Fields(\"word\"))\n          .each(new Fields(\"searchphrase\", \"tweet\", \"word\"), new WordFrequencyFunction(), new Fields(\"baseline\"))\n          .each(new Fields(\"searchphrase\", \"tweet\", \"word\", \"baseline\"), new PersistenceFunction(), new Fields())\t\n          .partitionPersist(new DruidStateFactory(), new Fields(\"searchphrase\", \"tweet\", \"word\", \"baseline\"), new DruidStateUpdater());\n} catch (IOException e) {\nthrow new RuntimeException(e);\n}\nreturn topology.build();\n```", "```scala\n   query = new Query(SEARCH_PHRASE);\n   query.setLang(\"en\");\n   result = twitter.search(query);\n   ...\n   for (Status status : result.getTweets()) {\n       List<Object> tweets = new ArrayList<Object>();\n       tweets.add(SEARCH_PHRASE);\n       tweets.add(status.getText());\n       collector.emit(tweets);\n   }\n```", "```scala\n@Override\npublic void execute(TridentTuple tuple, TridentCollector collector) {\nString tweet = (String) tuple.getValue(0);\nLOG.debug(\"SPLITTING TWEET [\" + tweet + \"]\");\nPattern p = Pattern.compile(\"[a-zA-Z]+\");\nMatcher m = p.matcher(tweet);\nList<String> result = new ArrayList<String>();\n   while (m.find()) {\n       String word = m.group();\n       if (word.length() > 0) {\n         List<Object> newTuple = new ArrayList<Object>();\n         newTuple.add(word);\n         collector.emit(newTuple);\n       }\n   }\n}\n```", "```scala\npublic static final long DEFAULT_BASELINE = 10000;\nprivate Map<String, Long> wordLikelihoods = \nnew HashMap<String, Long>();\n\npublic WordFrequencyFunction() throws IOException {\nFile file = new File(\"src/main/resources/en.txt\");\nBufferedReader br = new BufferedReader(new FileReader(file));\nString line;\nwhile ((line = br.readLine()) != null) {\nString[] pair = line.split(\" \");\n   long baseline = Long.parseLong(pair[1]);\n   LOG.debug(\"[\" + pair[0] + \"]=>[\" + baseline + \"]\");\n   wordLikelihoods.put(pair[0].toLowerCase(), baseline);\n   i++;\n}\nbr.close();\n}\n\n@Override\npublic void execute(TridentTuple tuple,\nTridentCollector collector) {\nString word = (String) tuple.getValue(2);\nLong baseline = this.getLikelihood(word);\nList<Object> newTuple = new ArrayList<Object>();\nnewTuple.add(baseline);\ncollector.emit(newTuple);\n}\n\npublic long getLikelihood(String word){\nLong baseline = this.wordLikelihoods.get(word);\nif (baseline == null)\nreturn DEFAULT_BASELINE;\nelse\n   return baseline;\n}\n```", "```scala\nyou 4621939\nthe 3957465\ni 3476773\nto 2873389\n...\nof 1531878\nthat 1323823\nin 1295198\nis 1242191\nme 1208959\nwhat 1071825\n```", "```scala\n@Override\npublic void execute(TridentTuple tuple, \n   TridentCollector collector) {\nwriteToLog(tuple);\ncollector.emit(tuple);\n}\n\nsynchronized public void writeToLog(TridentTuple tuple) {\nDateTime dt = new DateTime();\nDateTimeFormatter fmt = ISODateTimeFormat.dateTime();\nStringBuffer sb = new StringBuffer(\"{ \");\nsb.append(String.format(\"\\\"utcdt\\\":\\\"%s\\\",\", fmt.print(dt)));\nsb.append(String.format(\"\\\"searchphrase\\\":\\\"%s\\\",\", tuple.getValue(0)));\nsb.append(String.format(\"\\\"word\\\":\\\"%s\\\",\", tuple.getValue(2)));\nsb.append(String.format(\"\\\"baseline\\\":%s\", tuple.getValue(3)));\nsb.append(\"}\");\nBufferedWriter bw;\ntry {\nbw = new BufferedWriter(new FileWriter(\"nlp.json\", true));\nbw.write(sb.toString());\n   bw.newLine();\n   bw.close();\n} catch (IOException e) {\n   throw new RuntimeException(e);\n}\n}\n```", "```scala\n{ \"utcdt\":\"2013-08-25T14:47:38.883-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"his\",\"baseline\":279134}\n{ \"utcdt\":\"2013-08-25T14:47:38.884-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"annual\",\"baseline\":839}\n{ \"utcdt\":\"2013-08-25T14:47:38.885-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"salary\",\"baseline\":1603}\n{ \"utcdt\":\"2013-08-25T14:47:38.886-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"from\",\"baseline\":285711}\n{ \"utcdt\":\"2013-08-25T14:47:38.886-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"Apple\",\"baseline\":10000}\n```", "```scala\n@Override\npublic InputRow nextRow() {\n   final Map<String, Object> theMap =\nMaps.newTreeMap(String.CASE_INSENSITIVE_ORDER);\ntry {\nTridentTuple tuple = null;\n   tuple = BLOCKING_QUEUE.poll();\n   if (tuple != null) {\nString phrase = (String) tuple.getValue(0);\n      String word = (String) tuple.getValue(2);\n      Long baseline = (Long) tuple.getValue(3);\n      theMap.put(\"searchphrase\", phrase);\n      theMap.put(\"word\", word);\n      theMap.put(\"baseline\", baseline);\n}\n\n   if (BLOCKING_QUEUE.isEmpty()) {\n      STATUS.putInLimbo(TRANSACTION_ID);\n      LIMBO_TRANSACTIONS.add(TRANSACTION_ID);\n      LOG.info(\"Batch is fully consumed by Druid. Unlocking [FINISH]\");\n      synchronized (FINISHED) {\n          FINISHED.notify();\n      }\n   }\n} catch (Exception e) {\nLOG.error(\"Error occurred in nextRow.\", e);\n}\nfinal LinkedList<String> dimensions = new LinkedList<String>();\ndimensions.add(\"searchphrase\");\ndimensions.add(\"word\");\nreturn new MapBasedInputRow(System.currentTimeMillis(), \ndimensions, theMap); \n}\n```", "```scala\ndruid.pusher.cassandra=true\ndruid.pusher.cassandra.host=localhost:9160 \ndruid.pusher.cassandra.keyspace=druid\ndruid.zk.service.host=localhost\ndruid.zk.paths.base=/druid\ndruid.host=127.0.0.1\ndruid.database.segmentTable=prod_segments\ndruid.database.user=druid\ndruid.database.password=druid\ndruid.database.connectURI=jdbc:mysql://localhost:3306/druid\ndruid.zk.paths.discoveryPath=/druid/discoveryPath\ndruid.realtime.specFile=./src/main/resources/realtime.spec\ndruid.port=7272\ndruid.request.logging.dir=/tmp/druid/realtime/log\n```", "```scala\n[{\n    \"schema\": {\n        \"dataSource\": \"nlp\",\n        \"aggregators\": [\n            { \"type\": \"count\", \"name\": \"wordcount\" },\n            { \"type\": \"max\", \"fieldName\": \"baseline\", \nname\" : \"maxbaseline\" }\n        ],\n        \"indexGranularity\": \"minute\",\n        \"shardSpec\": {\"type\": \"none\"}\n    },\n\n    \"config\": {\n        \"maxRowsInMemory\": 50000,\n        \"intermediatePersistPeriod\": \"PT30s\"\n    },\n\n    \"firehose\": {\n        \"type\": \"storm\",\n        \"sleepUsec\": 100000,\n        \"maxGeneratedRows\": 5000000,\n        \"seed\": 0,\n        \"nTokens\": 255,\n        \"nPerSleep\": 3\n    },\n\n    \"plumber\": {\n        \"type\": \"realtime\",\n        \"windowPeriod\": \"PT10s\",\n        \"segmentGranularity\": \"minute\",\n        \"basePersistDirectory\": \"/tmp/nlp/basePersist\"\n    }\n}]\n```", "```scala\n{\n     \"queryType\": \"groupBy\",\n     \"dataSource\": \"nlp\",\n     \"granularity\": \"minute\",\n     \"dimensions\": [\"searchphrase\", \"word\"],\n     \"aggregations\":[\n        { \"type\": \"longSum\", \"fieldName\":\"wordcount\", \n\"name\": \"totalcount\"},\n        { \"type\": \"max\", \"fieldName\":\"maxbaseline\", \n\"name\": \"totalbaseline\"}\n     ],\n     \"postAggregations\": [{\n       \"type\": \"arithmetic\",\n       \"name\": \"relevance\",\n       \"fn\": \"/\",\n       \"fields\": [\n            { \"type\": \"fieldAccess\", \"fieldName\": \"totalcount\" },\n            { \"type\": \"fieldAccess\", \"fieldName\": \"totalbaseline\" }\n       ]\n     }],\n     \"intervals\":[\"2012-10-01T00:00/2020-01-01T00\"]\n }\n```", "```scala\n...\nurl=\"http://localhost:7272/druid/v2/?pretty=true\"\nresponse = RestClient.post url, File.read(\"realtime_query\"), :accept => :json, :content_type => 'appplication/json'\n#puts(response)\nresult = JSON.parse(response.to_s)\n\nword_relevance = {}\nresult.each do |slice|\n  event = slice['event']\n  word_relevance[event['word']]=event['relevance']\nend\n\ncount = 0\nword_relevance.sort_by {|k,v| v}.reverse.each do |word, relevance|\n  puts(\"#{word}->#{relevance}\")\n  count=count+1\n  if(count == 20) then\n    break\n  end\nend\n```", "```scala\nclaiming->31.789579158316634\napple->27.325982081323225\npurchase->20.985449735449734\nJobs->20.618\nSteve->17.446\nshares->14.802238805970148\nrandom->13.480033984706882\ncreation->12.7524115755627\nApple->12.688\nacts->8.82582081246522\nprevent->8.702687877125618\nfarmer->8.640522875816993\ndeveloped->8.62642740619902\njobs->8.524986566362172\nbottles->8.30523560209424\ntechnology->7.535137701804368\ncurrent->7.21418826739427\nempire->6.924050632911392\n```", "```scala\ndef map(doc)\n   result = []\ndoc.split(' ').each do |word|\nresult << [word, 1]\n   end\n   return result\nend\n```", "```scala\nmap(\"the quick fox jumped over the dog over and over again\")\n => [[\"the\", 1], [\"quick\", 1], [\"fox\", 1], [\"jumped\", 1], [\"over\", 1], [\"the\", 1], [\"dog\", 1], [\"over\", 1], [\"and\", 1], [\"over\", 1], [\"again\", 1]]\n```", "```scala\ndef reduce(key, values)\n   sum = values.inject { |sum, x| sum + x }\n   return [key, sum]\nend\n```", "```scala\nreduce(\"over\", [1,1,1])\n => [\"over\", 3]\n\n```", "```scala\ndruid/druid-indexing-hadoop-0.5.39-SNAPSHOT.jar\ndruid/druid-services-0.5.39-SNAPSHOT-selfcontained.jar\ndruid/config/compute/runtime.properties\ndruid/config/master/runtime.properties\ndruid/batchConfig.json\n```", "```scala\n# Path on local FS for storage of segments; \n# dir will be created if needed\ndruid.paths.indexCache=/tmp/druid/indexCache\n# Path on local FS for storage of segment metadata; \n# dir will be created if needed\ndruid.paths.segmentInfoCache=/tmp/druid/segmentInfoCache\n```", "```scala\ndruid.port=8082\n```", "```scala\njava -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 \\\n-classpath ./druid-services-0.5.39-SNAPSHOT-selfcontained.jar:config/compute \\\ncom.metamx.druid.http.ComputeMain\n```", "```scala\njava -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 \\\n-classpath ./druid-services-0.5.39-SNAPSHOT-selfcontained.jar:config/compute \\\ncom.metamx.druid.http.ComputeMain\n```", "```scala\njava -Xmx256m -Duser.timezone=UTC -Dfile.encoding=UTF-8 \\\n-Ddruid.realtime.specFile=realtime.spec -classpath druid-services-0.5.39-SNAPSHOT-selfcontained.jar:druid-indexing-hadoop-0.5.39-SNAPSHOT.jar \\\ncom.metamx.druid.indexer.HadoopDruidIndexerMain batchConfig.json\n```", "```scala\n{\n  \"dataSource\": \"historical\",\n  \"timestampColumn\": \"utcdt\",\n  \"timestampFormat\": \"iso\",\n  \"dataSpec\": {\n    \"format\": \"json\",\n    \"dimensions\": [\"searchphrase\", \"word\"]\n  },\n  \"granularitySpec\": {\n    \"type\":\"uniform\",\n    \"intervals\":[\"2013-08-21T19/PT1H\"],\n    \"gran\":\"hour\"\n  },\n  \"pathSpec\": { \"type\": \"static\",\n                \"paths\": \"/tmp/nlp.json\" },\n  \"rollupSpec\": {\n            \"aggs\": [ { \"type\": \"count\", \"name\": \"wordcount\" },\n                         { \"type\": \"max\", \"fieldName\": \"baseline\", \n                                       \"name\" : \"maxbaseline\" } ],\n      \"rollupGranularity\": \"minute\"},\n      \"workingPath\": \"/tmp/working_path\",\n  \"segmentOutputPath\": \"/tmp/segments\",\n  \"leaveIntermediate\": \"false\",\n  \"partitionsSpec\": {\n    \"targetPartitionSize\": 5000000\n  },\n  \"updaterJobSpec\": {\n    \"type\":\"db\",\n    \"connectURI\":\"jdbc:mysql://localhost:3306/druid\",\n    \"user\":\"druid\",\n    \"password\":\"druid\",\n    \"segmentTable\":\"prod_segments\"\n  }\n}\n```", "```scala\n{ \"utcdt\":\"2013-08-25T14:47:38.883-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"his\",\"baseline\":279134}\n{ \"utcdt\":\"2013-08-25T14:47:38.884-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"annual\",\"baseline\":839}\n{ \"utcdt\":\"2013-08-25T14:47:38.885-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"salary\",\"baseline\":1603}\n{ \"utcdt\":\"2013-08-25T14:47:38.886-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"from\",\"baseline\":285711}\n{ \"utcdt\":\"2013-08-25T14:47:38.886-04:00\",\"searchphrase\":\"apple jobs\",\"word\":\"Apple\",\"baseline\":10000}\n```", "```scala\njob.setInputFormatClass(TextInputFormat.class);\njob.setMapperClass(IndexGeneratorMapper.class);\njob.setMapOutputValueClass(Text.class);\n...\njob.setReducerClass(IndexGeneratorReducer.class);\njob.setOutputKeyClass(BytesWritable.class);\njob.setOutputValueClass(Text.class);\njob.setOutputFormatClass(IndexGeneratorOutputFormat.class);\nFileOutputFormat.setOutputPath(job,config.makeIntermediatePath());\nconfig.addInputPaths(job);\nconfig.intoConfiguration(job);\n...\njob.setJarByClass(IndexGeneratorJob.class);\njob.submit();\n```", "```scala\n  @Override\n  public Job addInputPaths(HadoopDruidIndexerConfig config, \nJob job) throws IOException {\n    log.info(\"Adding paths[%s]\", paths);\n    FileInputFormat.addInputPaths(job, paths);\n    return job;\n  }\n```", "```scala\n@Override\nprotected void map(LongWritable key, Text value, Context context\n  ) throws IOException, InterruptedException\n  {\n    try {\n      final InputRow inputRow;\n      try {\n        inputRow = parser.parse(value.toString());\n      }\n      catch (IllegalArgumentException e) {\n        if (config.isIgnoreInvalidRows()) {\n          context.getCounter(HadoopDruidIndexerConfig.IndexJobCounters.INVALID_ROW_COUNTER).increment(1);\n          return; // we're ignoring this invalid row\n        } else {\n          throw e;\n        }\n      }\n      if(config.getGranularitySpec().bucketInterval(new DateTime(inputRow.getTimestampFromEpoch())).isPresent()) {\n        innerMap(inputRow, value, context);\n      }\n    }\n    catch (RuntimeException e) {\n      throw new RE(e, \"Failure on row[%s]\", value);\n    }\n  }\n```", "```scala\n@Override\nprotected void innerMap(InputRow inputRow,\n        Text text,\n        Context context\n    ) throws IOException, InterruptedException{\n\n // Group by bucket, sort by timestamp\nfinal Optional<Bucket> bucket = getConfig().getBucket(inputRow);\n\nif (!bucket.isPresent()) {\nthrow new ISE(\"WTF?! No bucket found for row: %s\", inputRow);\n}\n\ncontext.write(new SortableBytes(\n              bucket.get().toGroupKey(),\n              Longs.toByteArray(inputRow.getTimestampFromEpoch())\n          ).toBytesWritable(),text);\n}\n```", "```scala\n@Override\nprotected void reduce(BytesWritable key, Iterable<Text> values,\nfinal Context context\n    ) throws IOException, InterruptedException{\nSortableBytes keyBytes = SortableBytes.fromBytesWritable(key);\nBucket bucket = Bucket.fromGroupKey(keyBytes.getGroupKey()).lhs;\n\nfinal Interval interval =\nconfig.getGranularitySpec().bucketInterval(bucket.time).get();\nfinal DataRollupSpec rollupSpec = config.getRollupSpec();\nfinal AggregatorFactory[] aggs = rollupSpec.getAggs().toArray(\n          new AggregatorFactory[rollupSpec.getAggs().size()]);\n\nIncrementalIndex index = makeIncrementalIndex(bucket, aggs);\n...\nfor (final Text value : values) {\ncontext.progress();\n   final InputRow inputRow =\nindex.getSpatialDimensionRowFormatter()\n.formatRow(parser.parse(value.toString()));\n        allDimensionNames.addAll(inputRow.getDimensions());\n      ...\nIndexMerger.persist(index, interval, file, \nindex = makeIncrementalIndex(bucket, aggs);\n      ...\n   }\n   ...\n);\n...\nserializeOutIndex(context, bucket, mergedBase,\n Lists.newArrayList(allDimensionNames));\n...\n}\n```", "```scala\n2013-08-28 04:07:46,405 INFO [main] org.apache.hadoop.mapred.JobClient -   Map-Reduce Framework\n2013-08-28 04:07:46,405 INFO [main] org.apache.hadoop.mapred.JobClient -     Reduce input groups=1\n2013-08-28 04:07:46,405 INFO [main] org.apache.hadoop.mapred.JobClient -     Combine output records=0\n2013-08-28 04:07:46,405 INFO [main] org.apache.hadoop.mapred.JobClient -     Map input records=201363\n2013-08-28 04:07:46,405 INFO [main] org.apache.hadoop.mapred.JobClient -     Reduce shuffle bytes=0\n2013-08-28 04:07:46,406 INFO [main] org.apache.hadoop.mapred.JobClient -     Reduce output records=0\n2013-08-28 04:07:46,406 INFO [main] org.apache.hadoop.mapred.JobClient -     Spilled Records=402726\n2013-08-28 04:07:46,406 INFO [main] org.apache.hadoop.mapred.JobClient -     Map output bytes=27064165\n2013-08-28 04:07:46,406 INFO [main] org.apache.hadoop.mapred.JobClient -     Combine input records=0\n2013-08-28 04:07:46,406 INFO [main] org.apache.hadoop.mapred.JobClient -     Map output records=201363\n2013-08-28 04:07:46,406 INFO [main] org.apache.hadoop.mapred.JobClient -     Reduce input records=201363\n2013-08-28 04:07:46,433 INFO [main] com.metamx.druid.indexer.IndexGeneratorJob - Adding segment historical_2013-08-28T04:00:00.000Z_2013-08-28T05:00:00.000Z_2013-08-28T04:07:32.243Z to the list of published segments\n2013-08-28 04:07:46,708 INFO [main] com.metamx.druid.indexer.DbUpdaterJob - Published historical_2013-08-28T04:00:00.000Z_2013-08-28T05:00:00.000Z_2013-08-28T04:07:32.243Z\n2013-08-28 04:07:46,754 INFO [main] com.metamx.druid.indexer.IndexGeneratorJob - Adding segment historical_2013-08-28T04:00:00.000Z_2013-08-28T05:00:00.000Z_2013-08-28T04:07:32.243Z to the list of published segments\n2013-08-28 04:07:46,755 INFO [main] com.metamx.druid.indexer.HadoopDruidIndexerJob - Deleting path[/tmp/working_path/historical/2013-08-28T040732.243Z]\n```", "```scala\n{\n  \"version\" : \"v1\",\n  \"timestamp\" : \"2013-08-28T04:06:00.000Z\",\n  \"event\" : {\n    \"totalcount\" : 171,\n    \"totalbaseline\" : 28719.0,\n    \"searchphrase\" : \"apple jobs\",\n    \"relevance\" : 0.005954246317768724,\n    \"word\" : \"working\"\n  }\n}\n```"]