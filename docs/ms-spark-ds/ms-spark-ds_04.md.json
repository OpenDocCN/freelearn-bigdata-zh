["```scala\n$ cat 20150218230000.gkg.csv | gawk -F\"\\t\" '{print $4}' | \\ \n  sed \"s/[0-9]/9/g; s/[a-z]/a/g; s/[A-Z]/A/g\" | sort |    \\ \n  uniq -c | sort -r -n | head -20 \n\n 232 aaaa.aaa \n 195 aaaaaaaaaa.aaa \n 186 aaaaaa.aaa \n 182 aaaaaaaa.aaa \n 168 aaaaaaa.aaa \n 167 aaaaaaaaaaaa.aaa \n 167 aaaaa.aaa \n 153 aaaaaaaaaaaaa.aaa \n 147 aaaaaaaaaaa.aaa \n 120 aaaaaaaaaaaaaa.aaa \n\n```", "```scala\n\n$ # note: on a mac use gsed, on linux use sed. \n$ hdfs dfs -cat 20150218230000.gkg.csv |                 \\ \n  gawk -F\"\\t\" '{print $4}' | sed \"s/[0-9]/9/g; s/[A-Z]/A/g; \\ \n  s/[a-z]/a/g; s/a*a/a/g\"| sort | uniq -c | sort -r -n \n\n2356 a.a \n 508 a.a.a \n  83 a-a.a \n  58 a99.a \n  36 a999.a \n  24 a-9.a \n  21 99a.a \n  21 9-a.a \n  15 a9.a \n  15 999a.a \n  12 a9a.a \n  11 a99a.a \n   8 a-a.a.a \n   7 9a.a \n   3 a-a-a.a \n   2 AAA Aa     <---note here the pattern that stands out \n   2 9a99a.a \n   2 9a.a.a \n   1 a9.a.a \n   1 a.99a.a \n   1 9a9a.a \n   1 9999a.a \n\n```", "```scala\n$ # here is a Low Granularity report from bytefreq\n$ hdfs dfs \u2013cat 20150218230000.gkg.csv | \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\\\ngawk -F\"\\t\" '{print $4}' | awk -F\",\" \u2013f\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\\ ~/bytefreq/bytefreq_v1.04.awk -v header=\"0\" -v report=\"0\"\u00a0 \\\n\u00a0\u00a0-v grain=\"L\"\n\n-\u00a0 ##column_100000001\u00a0 2356\u00a0 a.a\u00a0\u00a0\u00a0 sfgate.com\n-\u00a0 ##column_100000001\u00a0 508\u00a0 a.a.a\u00a0\u00a0\u00a0 theaustralian.com.au\n-\u00a0 ##column_100000001\u00a0 109\u00a0 a9.a\u00a0\u00a0\u00a0 france24.com\n-\u00a0 ##column_100000001\u00a0 83\u00a0 a-a.a\u00a0\u00a0\u00a0 news-gazette.com\n-\u00a0 ##column_100000001\u00a0 44\u00a0 9a.a\u00a0\u00a0\u00a0 927thevan.com\n-\u00a0 ##column_100000001\u00a0 24\u00a0 a-9.a\u00a0\u00a0\u00a0 abc-7.com\n-\u00a0 ##column_100000001\u00a0 23\u00a0 a9a.a\u00a0\u00a0\u00a0 abc10up.com\n-\u00a0 ##column_100000001\u00a0 21\u00a0 9-a.a\u00a0\u00a0\u00a0 4-traders.com\n-\u00a0 ##column_100000001\u00a0 8\u00a0 a-a.a.a\u00a0 gazette-news.co.uk\n-\u00a0 ##column_100000001\u00a0 3\u00a0 9a9a.a\u00a0\u00a0\u00a0 8points9seconds.com\n-\u00a0 ##column_100000001\u00a0 3\u00a0 a-a-a.a\u00a0 the-american-interest.com\n-\u00a0 ##column_100000001\u00a0 2\u00a0 9a.a.a\u00a0\u00a0\u00a0 9news.com.au\n-\u00a0 ##column_100000001\u00a0 2\u00a0 A Aa\u00a0\u00a0\u00a0 BBC Monitoring\n-\u00a0 ##column_100000001\u00a0 1\u00a0 a.9a.a\u00a0\u00a0\u00a0 vancouver.24hrs.ca\n-\u00a0 ##column_100000001\u00a0 1\u00a0 a9.a.a\u00a0\u00a0\u00a0 guide2.co.nz\n\n$ hdfs dfs -cat 20150218230000.gkg.csv | gawk \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\\\n-F\"\\t\" '{print $4}'|gawk -F\",\" -f ~/bytefreq/bytefreq_v1.04.awk\\\n-v header=\"0\" -v report=\"2\" -v grain=\"L\" | grep \",A Aa\"\n\nBBC Monitoring,A Aa\nBBC Monitoring,A Aa\n```", "```scala\nMetric Descriptor \nSource Studied \nIngestTime \nMaskType \nFieldName \nOccurrence Count \nKeyCount   \nMaskCount \nDescription \n\n```", "```scala\nval tst = \"Andrew \"\n\ndef toCodePointVector(input: String) = input.map{\n\u00a0\u00a0\u00a0 case (i) if i > 65535 =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 val hchar = (i - 0x10000) / 0x400 + 0xD800\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 val lchar = (i - 0x10000) % 0x400 + 0xDC00\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 f\"\\\\u$hchar%04x\\\\u$lchar%04x\"\n\u00a0\u00a0\u00a0 case (i) if i > 0 => f\"\\\\u$i%04x\"\n\u00a0\u00a0\u00a0 // kudos to Ben Reich: http://k.bytefreq.com/1MjyvNz\n\u00a0\u00a0\u00a0 }\n\nval out = toCodePointVector(tst)\n\nval rows = sc.parallelize(out)\nrows.countByValue().foreach(println)\n\n// results in the following: [codepoint], [Frequency_count]\n(\\u0065,1)\n(\\u03d6,1)\n(\\u006e,1)\n(\\u0072,1)\n(\\u0077,1)\n(\\u0041,1)\n(\\u0020,2)\n(\\u6f22,1)\n(\\u0064,1)\n(\\u5b57,1)\n```", "```scala\n$ wget ftp://ftp.unicode.org/Public/UNIDATA/UnicodeData.txt      \n$ cat UnicodeData.txt | gawk -F\";\" '{OFS=\";\"} {print $1,$3,$2}' \\ \n  | sed 's/-/ /g'| gawk '{print $1,$2}'| gawk -F\";\" '{OFS=\"\\t\"} \\ \n  length($1) < 5 {print $1,$2,$3}' > codepoints.txt \n\n# use \"hdfs dfs -put\" to load codepoints.txt to hdfs, so  \n# you can use it later \n\nhead -1300 codepoints.txt | tail -4 \n0513      Ll    CYRILLIC SMALL \n0514      Lu    CYRILLIC CAPITAL \n0515      Ll    CYRILLIC SMALL \n0516      Lu    CYRILLIC CAPITAL \n\n```", "```scala\nCc  Other, Control \nCf  Other, Format \nCn  Other, Not Assigned \nCo  Other, Private Use \nCs  Other, Surrogate \nLC  Letter, Cased \nLl  Letter, Lowercase \nLm  Letter, Modifier \nLo  Letter, Other \nLt  Letter, Titlecase \nLu  Letter, Uppercase \nMc  Mark, Spacing Combining \nMe  Mark, Enclosing \nMn  Mark, Nonspacing \nNd  Number, Decimal Digit \nNl  Number, Letter \nNo  Number, Other \nPc  Punctuation, Connector \nPd  Punctuation, Dash \nPe  Punctuation, Close \nPf  Punctuation, Final quote \nPi  Punctuation, Initial quote \nPo  Punctuation, Other \nPs  Punctuation, Open \nSc  Symbol, Currency \nSk  Symbol, Modifier \nSm  Symbol, Math \nSo  Symbol, Other \nZl  Separator, Line \nZp  Separator, Paragraph \nZs  Separator, Space \n\n```", "```scala\n%dep\nz.reset\n// z.load(\"groupId>:artifactId:version\")\n```", "```scala\ngit clone https://bytesumo@bitbucket.org/gzet_io/profilers.git \nsudo cp profilers-1.0.0.jar /home/zeppelin/. \nsudo ls /home/zeppelin/  \n\n```", "```scala\n%dep \n// you need to put the profiler jar into a directory \n// that Zeppelin has access to.  \n// For example, /home/zeppelin, a non-hdfs directory on  \n// the namenode. \nz.load(\"/home/zeppelin/profilers-1.0.0.jar\") \n// you may need to restart your interpreter, then run  \n// this paragraph \n\n```", "```scala\n%sh\n# list the first two files in the directory, make sure the header file exists\n# note - a great trick is to write just the headers to a delimited file\n# that sorts to the top of your file glob, a trick that works well with\n# Spark\u2019s csv reader where headers are not on each file you\n# hold in hdfs.\n# this is a quick inspection check, see we use column and\n# colrm to format it:\n\nhdfs dfs -cat \"/user/feeds/gdelt/events/*.export.CSV\" \\\n|head -4|column -t -s $'\\t'|colrm 68\n\nGlobalEventID\u00a0 Day\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 MonthYear\u00a0 Year\u00a0 FractionDate\u00a0 Actor1Code\n610182939\u00a0\u00a0\u00a0\u00a0\u00a0 20151221\u00a0 201512\u00a0\u00a0\u00a0\u00a0 2015\u00a0 2015.9616\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n610182940\u00a0\u00a0\u00a0\u00a0\u00a0 20151221\u00a0 201512\u00a0\u00a0\u00a0\u00a0 2015\u00a0 2015.9616\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n610182941\u00a0\u00a0\u00a0\u00a0\u00a0 20151221\u00a0 201512\u00a0\u00a0\u00a0\u00a0 2015\u00a0 2015.9616\u00a0\u00a0\u00a0\u00a0 CAN\u00a0\n```", "```scala\nval YourHeader = z.select(\"YourHeaders\", Seq(  (\"true\", \"HasHeader\"), (\"false\", \"No Header\"))).toString \n\n```", "```scala\nimport io.gzet.profilers._ \nimport sys.process._ \nimport org.apache.spark.sql.SQLContext \nimport org.apache.spark.sql.functions.udf \nimport org.apache.spark.sql.types.{StructType, StructField, StringType, IntegerType} \nimport org.apache.spark.sql.SaveMode \nimport sqlContext.implicits._ \n\n```", "```scala\nval InputFilePath = YourFilePath    \n// set our input to user's file glob \nval RawData = sqlContext.read                       \n// read in tabular data \n        .option(\"header\", YourHeader)               \n// configurable headers \n        .option(\"delimiter\", YourDelimiter )        \n// configurable delimiters \n        .option(\"nullValue\", \"NULL\")                \n// set a default char if nulls seen \n        .option(\"treatEmptyValuesAsNulls\", \"true\")  \n// set to null  \n        .option(\"inferschema\", \"false\")             \n// do not infer schema, we'll discover it \n        .csv(InputFilePath)                         \n// file glob path. Can use wildcards \nRawData.registerTempTable(\"RawData\")                \n// register data for Spark SQL access to it \nRawData.cache()                                     \n// cache the file for use \nval RawLines = sc.textFile(InputFilePath)           \n// read the file lines as a string \nRawLines.toDF.registerTempTable(\"RawLines\")      \n// useful to check for schema corruption \nRawData.printSchema()                               \n// print out the schema we found \n\n// define our profiler apps \nval ASCIICLASS_HIGHGRAIN    = MaskBasedProfiler(PredefinedMasks.ASCIICLASS_HIGHGRAIN) \nval CLASS_FREQS             = MaskBasedProfiler(PredefinedMasks.CLASS_FREQS) \nval UNICODE                 = MaskBasedProfiler(PredefinedMasks.UNICODE) \nval HEX                     = MaskBasedProfiler(PredefinedMasks.HEX) \nval ASCIICLASS_LOWGRAIN     = MaskBasedProfiler(PredefinedMasks.ASCIICLASS_LOWGRAIN) \nval POPCHECKS               = MaskBasedProfiler(PredefinedMasks.POPCHECKS) \n\n// configure our profiler apps \nval Metrics_ASCIICLASS_HIGHGRAIN    = ASCIICLASS_HIGHGRAIN.profile(YourFilePath, RawData)        \nval Metrics_CLASS_FREQS             = CLASS_FREQS.profile(YourFilePath, RawLines.toDF)            \nval Metrics_UNICODE                 = UNICODE.profile(YourFilePath, RawLines.toDF)               \nval Metrics_HEX                     = HEX.profile(YourFilePath, RawLines.toDF)                   \nval Metrics_ASCIICLASS_LOWGRAIN     = ASCIICLASS_LOWGRAIN.profile(YourFilePath, RawData)         \nval Metrics_POPCHECKS               = POPCHECKS.profile(YourFilePath, RawData)\n\n// note some of the above read tabular data, some read rawlines of string data\n\n// now register the profiler output as sql accessible data frames\n\nMetrics_ASCIICLASS_HIGHGRAIN.toDF.registerTempTable(\"Metrics_ASCIICLASS_HIGHGRAIN\")\nMetrics_CLASS_FREQS.toDF.registerTempTable(\"Metrics_CLASS_FREQS\") \nMetrics_UNICODE.toDF.registerTempTable(\"Metrics_UNICODE\") \nMetrics_HEX.toDF.registerTempTable(\"Metrics_HEX\") \nMetrics_ASCIICLASS_LOWGRAIN.toDF.registerTempTable(\"Metrics_ASCIICLASS_LOWGRAIN\") \nMetrics_POPCHECKS.toDF.registerTempTable(\"Metrics_POPCHECKS\") \n\n```", "```scala\n%sql \nselect * from RawData \nlimit 10 \n\n```", "```scala\n// load the UTF lookup tables\n\n val codePointsSchema = StructType(Array(\n \u00a0\u00a0\u00a0 StructField(\"CodePoint\"\u00a0 , StringType, true),\u00a0\u00a0\u00a0\u00a0 //$1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0 StructField(\"Category\"\u00a0\u00a0 , StringType, true), \u00a0\u00a0\u00a0 //$2\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0 StructField(\"CodeDesc\"\u00a0\u00a0 , StringType, true)\u00a0 \u00a0\u00a0\u00a0 //$3\n \u00a0\u00a0\u00a0 ))\n\n val UnicodeCatSchema = StructType(Array(\n \u00a0\u00a0\u00a0 StructField(\"Category\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0 StructField(\"Description\"\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true)\u00a0 //$2\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0 ))\n\n val codePoints = sqlContext.read\n \u00a0\u00a0\u00a0 .option(\"header\", \"false\")\u00a0\u00a0\u00a0\u00a0 // configurable headers\n \u00a0\u00a0\u00a0 .schema(codePointsSchema)\n \u00a0\u00a0\u00a0 .option(\"delimiter\", \"\\t\" )\u00a0\u00a0 // configurable delimiters\n \u00a0\u00a0\u00a0 .csv(\"/user/feeds/ref/codepoints2.txt\")\u00a0 // configurable path\n\n codePoints.registerTempTable(\"codepoints\")\n codePoints.cache()\n val utfcats = sqlContext.read\n \u00a0\u00a0 \u00a0\u00a0.option(\"header\", \"false\")\u00a0\u00a0\u00a0 // configurable headers\n \u00a0\u00a0\u00a0\u00a0 .schema(UnicodeCatSchema)\n \u00a0\u00a0\u00a0\u00a0 .option(\"delimiter\", \"\\t\" )\u00a0\u00a0 // configurable delimiters\n \u00a0\u00a0\u00a0\u00a0 .csv(\"/user/feeds/ref/UnicodeCategory.txt\")\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n\n utfcats.registerTempTable(\"utfcats\")\n utfcats.cache()\n\n // Next we build the different presentation layer views for the codepoints\n val hexReport = sqlContext.sql(\"\"\"\n select\n \u00a0 r.Category\n , r.CodeDesc\n , sum(maskCount) as maskCount\n from\n \u00a0\u00a0\u00a0 ( select\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 h.*\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ,c.*\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from Metrics_HEX h\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 left outer join codepoints c\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 on ( upper(h.MaskType) = c.CodePoint)\n \u00a0\u00a0\u00a0 ) r \n group by r.Category, r.CodeDesc\n order by r.Category, r.CodeDesc, 2 DESC\n \"\"\")\n hexReport.registerTempTable(\"hexReport\")\n hexReport.cache()\n hexReport.show(10)\n +--------+-----------------+---------+\n |Category|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 CodeDesc|maskCount|\n +--------+-----------------+---------+\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Cc|\u00a0 CTRL: CHARACTER|\u00a0\u00a0 141120|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Ll|\u00a0\u00a0\u00a0\u00a0\u00a0 LATIN SMALL|\u00a0\u00a0 266070|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Lu|\u00a0\u00a0\u00a0 LATIN CAPITAL|\u00a0\u00a0 115728|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Nd|\u00a0\u00a0\u00a0\u00a0\u00a0 DIGIT EIGHT|\u00a0\u00a0\u00a0 18934|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Nd|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 DIGIT FIVE|\u00a0\u00a0\u00a0 24389|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Nd|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 DIGIT FOUR|\u00a0\u00a0\u00a0 24106|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Nd|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 DIGIT NINE|\u00a0\u00a0\u00a0 17204|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Nd|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 DIGIT ONE|\u00a0\u00a0\u00a0 61165|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Nd|\u00a0\u00a0\u00a0\u00a0\u00a0 DIGIT SEVEN|\u00a0\u00a0\u00a0 16497|\n |\u00a0\u00a0\u00a0\u00a0\u00a0 Nd|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 DIGIT SIX|\u00a0\u00a0\u00a0 31706|\n +--------+-----------------+---------+\n```", "```scala\nMetrics_POPCHECKS.toDF.show(1000, false)  \n\n```", "```scala\nval pop_qry = sqlContext.sql(\"\"\"\nselect * from (\n\u00a0\u00a0\u00a0 select\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0 fieldName as rawFieldName\n\u00a0\u00a0\u00a0 ,\u00a0\u00a0 \u00a0coalesce( cast(regexp_replace(fieldName, \"C\", \"\") as INT), fieldName) as fieldName\n\u00a0\u00a0\u00a0 ,\u00a0\u00a0 case when maskType = 0 then \"Populated\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 when maskType = 1 then \"Missing\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 end as PopulationCheck\n\u00a0\u00a0\u00a0 , \u00a0\u00a0\u00a0 coalesce(maskCount, 0) as maskCount\n\u00a0\u00a0\u00a0 ,\u00a0\u00a0 metricDescriptor as fileName\n\u00a0\u00a0\u00a0 from Metrics_POPCHECKS\n) x\norder by fieldName\n\"\"\")\nval pivot_popquery = pop_qry.groupBy(\"fileName\",\"fieldName\").pivot(\"PopulationCheck\").sum(\"maskCount\")\n pivot_popquery.registerTempTable(\"pivot_popquery\")\n val per_pivot_popquery = sqlContext.sql(\"\"\"\n Select \n x.* \n , round(Missing/(Missing + Populated)*100,2) as PercentMissing\n from\n \u00a0\u00a0\u00a0 (select \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 fieldname\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , coalesce(Missing, 0) as Missing\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , coalesce(Populated,0) as Populated\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , fileName\n \u00a0\u00a0\u00a0 from pivot_popquery) x\n order by x.fieldname ASC\n \"\"\")\n per_pivot_popquery.registerTempTable(\"per_pivot_popquery\")\n per_pivot_popquery.select(\"fieldname\",\"Missing\",\"Populated\",\"PercentMissing\",\"fileName\").show(1000,false)\n```", "```scala\nval proReport = sqlContext.sql(\"\"\"\n select * from (\n select \n \u00a0\u00a0\u00a0\u00a0 metricDescriptor as sourceStudied\n ,\u00a0\u00a0 \"ASCII_LOWGRAIN\" as metricDescriptor\n , coalesce(cast(\u00a0 regexp_replace(fieldName, \"C\", \"\") as INT),fieldname) as fieldName\n , ingestTime\n , maskType as maskInstance\n , maskCount\n , description\n from Metrics_ASCIICLASS_LOWGRAIN \n ) x\n order by fieldNAme, maskCount DESC\n \"\"\")\n proReport.show(1000, false)\n```", "```scala\n%sql\n select x.* from (\n select \n \u00a0\u00a0\u00a0\u00a0 metricDescriptor as sourceStudied\n ,\u00a0\u00a0 \"ASCII_HIGHGRAIN\" as metricDescriptor\n , coalesce(cast(\u00a0 regexp_replace(fieldName, \"C\", \"\")\n\u00a0  as INT),fieldname) as fieldName\n , ingestTime\n , maskType as maskInstance\n , maskCount\n , log(maskCount) as log_maskCount\n from Metrics_ASCIICLASS_HIGHGRAIN \n ) x\n where\u00a0 x.fieldName like '%${ColumnName}%'\n order by fieldName, maskCount DESC\n```", "```scala\nwget http://data.gdeltproject.org/documentation/GCAM-MASTER-CODEBOOK.TXT \ncat GCAM-MASTER-CODEBOOK.TXT | \\ \ngawk 'BEGIN{OFS=\"\\t\"} $4 != \"Type\" {print $4,$5}' | column -t -s $'\\t' \\  \n| sort | uniq -c | gawk ' BEGIN{print \"Lang Type Count\" }{print $3, $2,\\ $1}' | column -t -s $' ' \n\nLang  Type         Count    Annotation \nara   SCOREDVALUE  1        Arabic \ncat   SCOREDVALUE  16       Catalan \ndeu   SCOREDVALUE  1        German \neng   SCOREDVALUE  30       English \nfra   SCOREDVALUE  1        French \nglg   SCOREDVALUE  16       Galician \nhin   SCOREDVALUE  1        Hindi \nind   SCOREDVALUE  1        Indonesian \nkor   SCOREDVALUE  1        Korean \npor   SCOREDVALUE  1        Portuguese \nrus   SCOREDVALUE  1        Russian \nspa   SCOREDVALUE  29       Spanish \nurd   SCOREDVALUE  1        Urdu \nzho   SCOREDVALUE  1        Chinese \nara   WORDCOUNT    1        Arabic \ncat   WORDCOUNT    16       Catalan \ndeu   WORDCOUNT    44       German \neng   WORDCOUNT    2441     English \nfra   WORDCOUNT    78       French \nglg   WORDCOUNT    16       Galician \nhin   WORDCOUNT    1        Hindi \nhun   WORDCOUNT    36       Hungarian \nind   WORDCOUNT    1        Indonesian \nkor   WORDCOUNT    1        Korean \npor   WORDCOUNT    46       Portuguese \nrus   WORDCOUNT    65       Russian \nspa   WORDCOUNT    62       Spanish \nswe   WORDCOUNT    64       Swedish \nurd   WORDCOUNT    1        Urdu \nzho   WORDCOUNT    1        Chinese \n\n```", "```scala\nval GkgCoreSchema = StructType(Array(\n \u00a0\u00a0\u00a0 StructField(\"GkgRecordId\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0 StructField(\"V21Date\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0 StructField(\"V2SrcCollectionId\"\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0 StructField(\"V2SrcCmnName\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$4\n \u00a0\u00a0\u00a0 StructField(\"V2DocId\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$5\n \u00a0\u00a0\u00a0 StructField(\"V1Counts\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$6\n \u00a0\u00a0\u00a0 StructField(\"V21Counts\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$7\n \u00a0\u00a0\u00a0 StructField(\"V1Themes\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$8\n \u00a0\u00a0\u00a0 StructField(\"V2Themes\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$9\n \u00a0\u00a0\u00a0 StructField(\"V1Locations\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$10\n \u00a0\u00a0\u00a0 StructField(\"V2Locations\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$11\n \u00a0\u00a0\u00a0 StructField(\"V1Persons\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$12\n \u00a0\u00a0\u00a0 StructField(\"V2Persons\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$13\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0 StructField(\"V1Orgs\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$14\n \u00a0\u00a0\u00a0 StructField(\"V2Orgs\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$15\n \u00a0\u00a0\u00a0 StructField(\"V15Tone\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$16\n \u00a0\u00a0\u00a0 StructField(\"V21Dates\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$17\n \u00a0\u00a0\u00a0 StructField(\"V2GCAM\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$18\n \u00a0\u00a0\u00a0 StructField(\"V21ShareImg\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$19\n \u00a0\u00a0\u00a0 StructField(\"V21RelImg\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0, StringType, true), //$20\n \u00a0\u00a0\u00a0 StructField(\"V21SocImage\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$21\n \u00a0\u00a0\u00a0 StructField(\"V21SocVideo\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$22\n \u00a0\u00a0\u00a0 StructField(\"V21Quotations\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$23\n \u00a0\u00a0\u00a0 StructField(\"V21AllNames\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$24\n \u00a0\u00a0\u00a0 StructField(\"V21Amounts\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$25\n \u00a0\u00a0\u00a0 StructField(\"V21TransInfo\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true), //$26\n \u00a0\u00a0\u00a0 StructField(\"V2ExtrasXML\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true)\u00a0 //$27\n \u00a0\u00a0\u00a0 ))\n\nval InputFilePath = YourFilePath\n\nval GkgRawData = sqlContext.read\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .option(\"header\", \"false\")\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .schema(GkgCoreSchema)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .option(\"delimiter\", \"\\t\")\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .csv(InputFilePath) \n\nGkgRawData.registerTempTable(\"GkgRawData\")\n\n// now we register slices of the file we want to explore quickly\n\nval PreRawData = GkgRawData.select(\"GkgRecordID\",\"V21Date\",\"V2GCAM\", \"V2DocId\")\n// we select the GCAM, plus the story URLs in V2DocID, which later we can //filter on.\n\nPreRawData.registerTempTable(\"PreRawData\")\n```", "```scala\n+----+--------------+--------------------+--------------------+\n|\u00a0 ID|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 V21Date|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 V2GCAM|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 V2DocId|\n+----+--------------+--------------------+--------------------+\n|...0|20161101000000|wc:77,c12.1:2,c12...|http://www.tampab...|\n|...1|20161101000000|wc:57,c12.1:6,c12...|http://regator.co...|\n|...2|20161101000000|wc:740,c1.3:2,c12...|http://www.9news....|\n|...3|20161101000000|wc:1011,c1.3:1,c1...|http://www.gaming...|\n|...4|20161101000000|wc:260,c1.2:1,c1....|http://cnafinance...|\n+----+--------------+--------------------+--------------------+\n```", "```scala\nimport org.apache.spark.sql.functions.{Unix_timestamp, to_date}  \n\n```", "```scala\n%sql \n -- for urls containing \u201ctrump\u201d build 15min \u201celection fraud\u201d sentiment time series chart.\n select \n \u00a0 V21Date\n , regexp_replace(z.Series, \"\\\\.\", \"_\") as Series\n , sum(coalesce(z.Measure, 0) / coalesce (z.WordCount, 1)) as Sum_Normalised_Measure\n from\n (\n \u00a0\u00a0\u00a0 select\n \u00a0\u00a0\u00a0\u00a0\u00a0 GkgRecordID\n \u00a0\u00a0\u00a0 , V21Date\n \u00a0\u00a0\u00a0 , norm_array[0] as wc_norm_series\n \u00a0\u00a0\u00a0 , norm_array[1] as WordCount\n \u00a0\u00a0\u00a0 , ts_array[0] as Series\n \u00a0\u00a0\u00a0 , ts_array[1] as Measure\n \u00a0\u00a0\u00a0 from \n \u00a0\u00a0\u00a0 (\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 select\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 GkgRecordID\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ,\u00a0\u00a0 V21Date\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , split(wc_row, \":\")\u00a0\u00a0\u00a0\u00a0 as norm_array\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , split(gcam_array, \":\") as ts_array\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from\n \u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0(\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 select \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 GkgRecordID\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ,\u00a0\u00a0 V21Date\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , gcam_row[0] as wc_row\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , explode(gcam_row) as gcam_array\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 select\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 GkgRecordID \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ,\u00a0\u00a0 from_Unixtime(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Unix_timestamp(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0V21Date, \"yyyyMMddHHmmss\")\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , 'YYYY-MM-dd-HH-mm'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ) as V21Date\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0\u00a0 split(V2GCAM, \",\")\u00a0 as gcam_row\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from PreRawData\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 where length(V2GCAM) >1\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 and V2DocId like '%trump%'\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ) w\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ) x\n \u00a0\u00a0\u00a0 ) y\n ) z\n where z.Series <> \"wc\" and z.Series = 'c18.134'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  -- c18.134 is \"ELECTION_FRAUD\"\n group by z.V21Date, z.Series\n order by z.V21Date ASC\n```", "```scala\nval ExtractGcam = sqlContext.sql(\"\"\"\nselect\u00a0 \n \u00a0 a.V21Date\n, a.Series\n, Sum(a.Sum_Normalised_Measure) as Sum_Normalised_Measure\nfrom (\n \u00a0\u00a0\u00a0select \n \u00a0\u00a0\u00a0z.partitionkey\n \u00a0\u00a0\u00a0, z.V21Date\n \u00a0\u00a0\u00a0, regexp_replace(z.Series, \"\\\\.\", \"_\") as Series\n \u00a0\u00a0\u00a0, sum(coalesce(z.Measure, 0) / coalesce (z.WordCount, 1))\n\u00a0\u00a0\u00a0\u00a0 as Sum_Normalised_Measure\n \u00a0\u00a0\u00a0from\n \u00a0\u00a0\u00a0(\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 select\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 y.V21Date\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , cast(cast(round(rand(10) *1000,0) as INT) as string)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 as partitionkey\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , y.norm_array[0] as wc_norm_series\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , y.norm_array[1] as WordCount\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , y.ts_array[0] as Series\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , y.ts_array[1] as Measure\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 select\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 x.V21Date\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , split(x.wc_row, \":\")\u00a0\u00a0\u00a0\u00a0 as norm_array\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , split(x.gcam_array, \":\") as ts_array\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 select \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 w.V21Date\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0, w.gcam_row[0] as wc_row\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0, explode(w.gcam_row) as gcam_array\n \u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0from\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 select\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from_Unixtime(Unix_timestamp(V21Date,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"yyyyMMddHHmmss\"), 'YYYY-MM-dd-HH-mm')\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 as V21Date\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0,\u00a0\u00a0 split(V2GCAM, \",\")\u00a0 as gcam_row\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0from PreRawData\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0where length(V2GCAM) > 20\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0and V2DocId like '%brexit%'\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0) w\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 where gcam_row[0] like '%wc%'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 OR gcam_row[0] like '%c18.1%'\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0) x\n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0) y \n \u00a0\u00a0\u00a0) z\n \u00a0\u00a0\u00a0where z.Series <> \"wc\" \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 and \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (\u00a0\u00a0 z.Series = 'c18.134' -- Election Fraud\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 or z.Series = 'c18.101' -- Immigration\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 or z.Series = 'c18.100' -- Democracy\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 or z.Series = 'c18.140' -- Election\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )\u00a0 \n \u00a0\u00a0\u00a0group by z.partitionkey, z.V21Date, z.Series\n) a\ngroup by a.V21Date, a.Series\n\"\"\")\n```", "```scala\ngroup by z.partitionkey, z.V21Date, z.Series \n\n-- Where the partition key is: \n-- cast(cast(round(rand(10) *1000,0) as INT) as string) as partitionkey \n\n```", "```scala\nSelect\na.Time\n, a.Series\n, Sum(Sum_Normalised_Measure) as Sum_Normalised_Measure\nfrom\n(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 select\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from_Unixtime(Unix_timestamp(V21Date,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \"yyyy-MM-dd-HH-mm\"),'YYYY-MM-dd-HH')\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 as Time\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , CASE \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 when Series = 'c18_134' then 'Election Fraud'\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 when Series = 'c18_101' then 'Immigration'\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 when Series = 'c18_100' then 'Democracy'\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 when Series = 'c18_140' then 'Election'\n \u00a0\u00a0\u00a0\u00a0\u00a0 END as Series\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , Sum_Normalised_Measure\n \u00a0\u00a0\u00a0\u00a0\u00a0 from ExtractGcam \n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -- where Series = 'c18_101' or Series = 'c18_140'\n) a\ngroup by a.Time, a.Series\norder by a.Time\n```", "```scala\n// save the data as a parquet file\nval TimeSeriesParqueFile = \"/user/feeds/gdelt/datastore/BrexitTimeSeries2016.parquet\"\u00a0\u00a0 \n\n// *** uncomment to append to an existing parquet file ***\n// ExtractGcam.save(TimeSeriesParqueFile\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  //, \"parquet\" \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 //, SaveMode.Append)\n// ***************************************************************\n// *** uncomment to initially load a new parquet file ***\n \u00a0\u00a0 ExtractGcam.save(TimeSeriesParqueFile\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  , \"parquet\"\n    , SaveMode.Overwrite)\n// ***************************************************************\n```", "```scala\n            git clone https://github.com/beljun/zeppelin-plotly\n    ```", "```scala\n            cd /usr/hdp/current/zeppelin-server/lib \n\n    ```", "```scala\n    \u00a0\u00a0 ls *war\u00a0\u00a0\u00a0 # zeppelin-web-0.6.0.2.4.0.0-169.war\n    \u00a0 cp zeppelin-web-0.6.0.2.4.0.0-169.war \\\n    \u00a0\u00a0\u00a0\u00a0\u00a0 bkp_zeppelin-web-0.6.0.2.4.0.0-169.war\n    \u00a0 jar xvf zeppelin-web-0.6.0.2.4.0.0-169.war \\\n    \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 index.html\n    \u00a0 vi index.html\n    ```", "```scala\n            jar uvf zeppelin-web-0.6.0.2.4.0.0-169.war index.html \n\n    ```", "```scala\n            sudo pip install plotly \n            sudo pip install plotly --upgrade \n            sudo pip install colors \n            sudo pip install cufflinks \n            sudo pip install pandas \n            sudo pip install Ipython \n            sudo pip install -U pyOpenSSL \n            # note also install pyOpenSSL to get things running. \n\n    ```", "```scala\n%pyspark\n# Instructions here: https://github.com/beljun/zeppelin-plotly\nimport sys\nsys.path.insert(0, \"/home/zeppelin/zeppelin-plotly\")\n\nimport offline\n\nsys.modules[\"plotly\"].offline = offline\nsys.modules[\"plotly.offline\"] = offline\n\nimport cufflinks as cf\ncf.go_offline()\n\nimport plotly.plotly as py\nimport plotly.graph_objs as go\n\nimport pandas as pd\nimport numpy as np\n\n```", "```scala\n%pyspark\n\nGcamParquet = sqlContext.read.parquet(\"/user/feeds/gdelt/datastore/BrexitTimeSeries2016.parquet\")\n\n# register the content as a python data frame\nsqlContext.registerDataFrameAsTable(GcamParquet, \"BrexitTimeSeries\")\n```", "```scala\n%pyspark \nFixedExtractGcam = sqlContext.sql(\"\"\" \nselect  \n  V21Date \n, Series \n, CASE \n    when LangLen = 0 then \"eng\" \n    when LangLen > 0 then SourceLanguage \n  END as SourceLanguage \n, FIPS104Country \n, Sum_Normalised_Measure \nfrom \n(   select *,length(SourceLanguage) as LangLen \n    from BrexitTimeSeries \n    where V21Date like \"2016%\" \n) a  \n\"\"\") \n\nsqlContext.registerDataFrameAsTable(FixedExtractGcam, \"Brexit\") \n# pyspark accessible registration of the data \n\n```", "```scala\n%pyspark \n\ntimeplot = sqlContext.sql(\"\"\" \nSelect \nfrom_Unixtime(Unix_timestamp(Time, \"yyyy-MM-dd\"), 'YYYY-MM-dd HH:mm:ss.ssss') as Time \n, a.Series \n, SourceLanguage as Lang \n--, Country \n, sum(Sum_Normalised_Measure) as Sum_Normalised_Measure \nfrom \n(       select \n          from_Unixtime(Unix_timestamp(V21Date,  \n                        \"yyyy-MM-dd-HH\"), 'YYYY-MM-dd') as Time \n        , SourceLanguage \n        , CASE \n           When Series = 'c6_6' then \"Uncertainty\" \n          END as Series \n        , Sum_Normalised_Measure \n        from Brexit  \n        where Series in ('c6_6') \n        and SourceLanguage in ( 'deu', 'fra', 'ita', 'eng', 'spa', 'pol') \n        and V21Date like '2016%'   \n) a \ngroup by a.Time, a.Series, a.SourceLanguage order by a.Time, a.Series, a.SourceLanguage \n\"\"\") \n\nsqlContext.registerDataFrameAsTable(timeplot, \"timeplot\")  \n# pyspark accessible registration of the data \n\n```", "```scala\n+------------------------+-----------+----+----------------------+ \n|Time                    |Series     |Lang|Sum_Normalised_Measure| \n+------------------------+-----------+----+----------------------+ \n|2016-01-04 00:00:00.0000|Uncertainty|deu |0.0375                | \n|2016-01-04 00:00:00.0000|Uncertainty|eng |0.5603189694252122    | \n|2016-01-04 00:00:00.0000|Uncertainty|fra |0.08089269454114742   | \n+------------------------+-----------+----+----------------------+ \n\n```", "```scala\n%pyspark \nexplorer = pd.DataFrame(timeplot.collect(), columns=['Time', 'Series', 'SourceLanguage','Sum_Normalised_Measure']) \n\n```", "```scala\npexp = pd.pivot_table(explorer, values='Sum_Normalised_Measure', index=['Time'], columns=['SourceLanguage','Series'], aggfunc=np.sum, fill_value=0) \n\n```", "```scala\npexp.iplot(title=\"BREXIT: Daily GCAM Uncertainty Sentiment Measures by Language\", kind =\"bar\", barmode=\"stack\") \n\n```", "```scala\npexp.iplot(title=\"BREXIT: Daily GCAM Uncertainty by Language, 2016-01 through 2016-07\",subplots=True, shared_xaxes=True, fill=True,  kind =\"bar\") \n\n```", "```scala\n%pyspark \nmapplot = sqlContext.sql(\"\"\" \nSelect \n  CountryCode \n, sum(Sum_Normalised_Measure) as Sum_Normalised_Measure \nfrom (  select \n        from_Unixtime(Unix_timestamp(V21Date, \"yyyy-MM-dd-HH\"),  \n                                     'YYYY-MM') as Time \n        , CASE \n             when FIPS104Country = \"AF\" then \"AFB\" \n             when FIPS104Country = \"AL\" then \"ALB\" \n                -- I have excluded the full list of  \n                -- countries in this code snippet \n             when FIPS104Country = \"WI\" then \"ESH\" \n             when FIPS104Country = \"YM\" then \"YEM\" \n             when FIPS104Country = \"ZA\" then \"ZMB\" \n             when FIPS104Country = \"ZI\" then \"ZWE\" \n          END as CountryCode \n        , Sum_Normalised_Measure \n        from Brexit  \n        where Series in ('c6_6') \n        and V21Date like '2016%' \n) a \ngroup by a.CountryCode order by a.CountryCode \n\"\"\") \n\nsqlContext.registerDataFrameAsTable(mapplot, \"mapplot\") # python \n\nmapplot2 = pd.DataFrame(mapplot.collect(), columns=['Country', 'Sum_Normalised_Measure']) \n\n```", "```scala\nmapplot2.iplot( kind = 'choropleth', locations = 'Country', z = 'Sum_Normalised_Measure', text = 'Country', locationmode = 'ISO-3', showframe = True, showcoastlines = False, projection = dict(type = 'equirectangular'), colorscale = [[0,\"rgb(5, 10, 172)\"],[0.9,\"rgb(40, 60, 190)\"],[0.9,\"rgb(70, 100, 245)\"],[1,\"rgb(90, 120, 245)\"],[1,\"rgb(106, 137, 247)\"],[1,\"rgb(220, 220, 220)\"]]) \n\n```", "```scala\nval GeoGcamSchema = StructType(Array(\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"Date\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true),\u00a0 //$1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"CountryCode\"\u00a0\u00a0 , StringType, true),\u00a0 //$2\n \u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0StructField(\"Lat\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , DoubleType, true),\u00a0 //$3\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"Long\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , DoubleType, true),\u00a0 //$4\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"Geohash\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true),\u00a0 //$5\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"NewsLang\"\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true),\u00a0 //$6\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"Series\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , StringType, true),\u00a0\u00a0\u00a0 //$7\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"Value\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , DoubleType, true),\u00a0 //$8\u00a0 \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"ArticleCount\"\u00a0 , DoubleType, true),\u00a0 //$9\u00a0 \n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StructField(\"AvgTone\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , DoubleType, true) \u00a0//$10 \n \u00a0\u00a0\u00a0))\n```", "```scala\n+--------------+-------+------+--------+------------+ \n|Date          |Country|Lat   |Long    |Geohash     | \n|              |Code   |      |        |            | \n+--------------+-------+------+--------+------------+ \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n|20151109103000|CI     |-33.45|-70.6667|66j9xyw5ds13| \n+--------------+-------+------+--------+------------+ \n+----+------+-----+-------+----------------+ \n|News|Series|SUM  |Article|AvgTone         | \n|Lang|      |Value|Count  |                | \n+----+------+-----+-------+----------------+ \n|E   |c12_1 |16.0 |1.0    |0.24390243902439| \n|E   |c12_10|26.0 |1.0    |0.24390243902439| \n|E   |c12_12|12.0 |1.0    |0.24390243902439| \n|E   |c12_13|3.0  |1.0    |0.24390243902439| \n|E   |c12_14|11.0 |1.0    |0.24390243902439| \n|E   |c12_3 |4.0  |1.0    |0.24390243902439| \n|E   |c12_4 |3.0  |1.0    |0.24390243902439| \n|E   |c12_5 |10.0 |1.0    |0.24390243902439| \n|E   |c12_7 |15.0 |1.0    |0.24390243902439| \n|E   |c12_8 |6.0  |1.0    |0.24390243902439| \n+----+------+-----+-------+----------------+ \n\n```", "```scala\n// be sure to include a dependency to the geohash library\n// here in the 1st para of zeppelin:\n// z.load(\"com.github.davidmoten:geo:0.7.1\")\n// to use the geohash functionality in your code\n\nval GcamRaw = GkgFileRaw.select(\"GkgRecordID\",\"V21Date\",\"V15Tone\",\"V2GCAM\", \"V1Locations\")\n \u00a0\u00a0\u00a0GcamRaw.cache()\n \u00a0\u00a0\u00a0GcamRaw.registerTempTable(\"GcamRaw\")\n\ndef vgeoWrap (lat: Double, long: Double, len: Int): String = {\n \u00a0\u00a0\u00a0var ret = GeoHash.encodeHash(lat, long, len)\n \u00a0\u00a0\u00a0// select the length of the geohash, less than 12..\n \u00a0\u00a0\u00a0// it pulls in the library dependency from       \n    //   com.github.davidmoten:geo:0.7.1\n \u00a0\u00a0\u00a0return(ret)\n} // we wrap up the geohash function locally\n\n// we register the vGeoHash function for use in SQL \nsqlContext.udf.register(\"vGeoHash\", vgeoWrap(_:Double,_:Double,_:Int))\n\nval ExtractGcam = sqlContext.sql(\"\"\"\n\u00a0\u00a0\u00a0 select\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 GkgRecordID \n\u00a0\u00a0\u00a0 ,\u00a0\u00a0 V21Date\n\u00a0\u00a0\u00a0 ,\u00a0\u00a0 split(V2GCAM, \",\")\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 as Array\n\u00a0\u00a0\u00a0 ,\u00a0\u00a0 explode(split(V1Locations, \";\"))\u00a0\u00a0\u00a0 as LocArray\n\u00a0\u00a0\u00a0 ,\u00a0\u00a0 regexp_replace(V15Tone, \",.*$\", \"\") as V15Tone\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -- note we truncate off the other scores\n\u00a0\u00a0\u00a0 from GcamRaw \n\u00a0\u00a0\u00a0 where length(V2GCAM) >1 and length(V1Locations) >1\n\"\"\")\n\nval explodeGcamDF = ExtractGcam.explode(\"Array\", \"GcamRow\"){c: Seq[String] => c }\n\nval GcamRows = explodeGcamDF.select(\"GkgRecordID\",\"V21Date\",\"V15Tone\",\"GcamRow\", \"LocArray\")\n// note ALL the locations get repeated against\n// every GCAM sentiment row\n\n\u00a0\u00a0\u00a0 GcamRows.registerTempTable(\"GcamRows\")\n\nval TimeSeries = sqlContext.sql(\"\"\"\nselect \u00a0\u00a0-- create geohash keys\n\u00a0 d.V21Date\n, d.LocCountryCode\n, d.Lat\n, d.Long\n , vGeoHash(d.Lat, d.Long, 12) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0as GeoHash\n, 'E' as NewsLang\n, regexp_replace(Series, \"\\\\.\", \"_\") as Series\n, coalesce(sum(d.Value),0) as SumValue\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -- SQL\u2019s \"coalesce\u201d means \u201creplaces nulls with\"\n, count(distinct\u00a0 GkgRecordID ) \u00a0\u00a0\u00a0\u00a0\u00a0as ArticleCount\n, Avg(V15Tone) \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0as AvgTone\nfrom\n(\u00a0\u00a0 select\u00a0 -- build Cartesian join of the series\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -- and granular locations\n \u00a0\u00a0\u00a0 \u00a0GkgRecordID\n\u00a0\u00a0\u00a0 , V21Date\n\u00a0\u00a0\u00a0 , ts_array[0] \u00a0as Series\n\u00a0\u00a0\u00a0 , ts_array[1] \u00a0as Value\n\u00a0\u00a0\u00a0 , loc_array[0] as LocType\n\u00a0\u00a0\u00a0 , loc_array[2] as LocCountryCode\n\u00a0\u00a0\u00a0 , loc_array[4] as Lat\n\u00a0\u00a0\u00a0 , loc_array[5] as Long\n\u00a0\u00a0\u00a0 , V15Tone\n\u00a0\u00a0\u00a0 from\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (select -- isolate the data to focus on\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 GkgRecordID\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , V21Date\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , split(GcamRow,\u00a0\u00a0 \":\") as ts_array\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , split(LocArray, \u00a0\"#\") as loc_array\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 , V15Tone\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 from GcamRows\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 where length(GcamRow)>1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ) x\n\u00a0\u00a0\u00a0 where\n\u00a0\u00a0\u00a0 (loc_array[0] = 3 or\u00a0 loc_array[0] = 4) -- city level filter\n) d\ngroup by \n\u00a0 d.V21Date\n, d.LocCountryCode\n, d.Lat\n, d.Long\n, vGeoHash(d.Lat, d.Long, 12)\n, d.Series\norder by \n\u00a0 d.V21Date\n, vGeoHash(d.Lat, d.Long, 12)\n, d.Series\n\"\"\")\n```"]