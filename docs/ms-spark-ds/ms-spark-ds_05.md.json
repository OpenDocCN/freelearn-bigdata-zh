["```scala\nc9.366\u00a0 9\u00a0\u00a0 366\u00a0\u00a0 WORDCOUNT\u00a0\u00a0 eng\u00a0\u00a0 Roget's Thesaurus 1911 Edition\u00a0\u00a0 CLASS III - RELATED TO MATTER/3.2 INORGANIC MATTER/3.2.3 IMPERFECT FLUIDS/366 OIL\n\nc18.172\u00a0 18\u00a0\u00a0 172\u00a0 \u00a0\u00a0\u00a0WORDCOUNT\u00a0\u00a0 eng\u00a0\u00a0 GDELT\u00a0\u00a0 GKG\u00a0\u00a0 Themes\u00a0\u00a0 ENV_OIL\nc18.314\u00a0 18\u00a0\u00a0 314\u00a0 \u00a0\u00a0\u00a0WORDCOUNT\u00a0\u00a0 eng\u00a0\u00a0 GDELT\u00a0\u00a0 GKG\u00a0\u00a0 Themes\u00a0\u00a0 ECON_OILPRICE\n```", "```scala\n$ mkdir gdelt && cd gdelt\n$ wget http://data.gdeltproject.org/events/md5sums\n$ for file in `cat md5sums | cut -d' ' -f3 | grep '^201[56]'` ; do wget http://data.gdeltproject.org/events/$file ; done\n$ md5sum -c md5sums 2>&1 | grep '^201[56]'\n```", "```scala\n$ ls -1 *.zip | xargs -n 1 unzip\n$ rm *.zip\n$ hdfs dfs -copyFromLocal *.CSV hdfs:///data/gdelt/\n```", "```scala\nSimpleFeatureType featureType =\n\u00a0\u00a0\u00a0 buildGDELTFeatureType(featureName);\nDataStore ds = DataStoreFinder.getDataStore(dsConf);\nds.createSchema(featureType);\nrunMapReduceJob(featureName, dsConf,\n\u00a0\u00a0\u00a0 new Path(cmd.getOptionValue(INGEST_FILE)));\n```", "```scala\nspark-submit --class io.gzet.geomesa.ingest /\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 --master yarn /\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 geomesa-ingest.jar <accumulo-instance-id>\n...\n```", "```scala\nval conf = new SparkConf()\nval sc = new SparkContext(conf.setAppName(\"Geomesa Ingest\"))\n```", "```scala\nval featureType = buildGDELTFeatureType(featureName)\nval ds = DataStoreFinder\n\u00a0\u00a0 .getDataStore(dsConf)\n\u00a0\u00a0 .createSchema(featureType)\n```", "```scala\nval distDataRDD = sc.textFile(/data/gdelt/*.CSV)\n```", "```scala\nval distDataRDD = sc.textFile(/data/gdelt/*.CSV, 20)\u00a0\n```", "```scala\nval processedRDD = distDataRDD.map(s =>{\n\u00a0\u00a0 // Processing as before to build the SimpleFeatureType\n\u00a0\u00a0 (new Text, simpleFeatureType)\n})\n```", "```scala\nprocessedRDD.saveAsNewAPIHadoopFile(\"output/path\", classOf[Text], classOf[SimpleFeatureType], classOf[GeomesaOutputFormat])\n```", "```scala\n// do setup work \nval processedRDD = distDataRDD.map(s =>{ \n   // Processing as before to build the SimpleFeatureType \n   (new Text, simpleFeatureType) \n}) \n// do cleanup work \n\n```", "```scala\nval processedRDD = distDataRDD.mapPartitions { valueIterator =>\n\u00a0 \u00a0// setup code for SimpleFeatureBuilder\n\u00a0 \u00a0val transformed = valueIterator.map( . . . )\n\u00a0 \u00a0transformed\n}\n```", "```scala\nval processedRDD = distDataRDD.mapPartitions { valueIterator =>\n\u00a0 if (valueIterator.isEmpty) {\n\u00a0\u00a0\u00a0 // return an Iterator\n\u00a0 } else {\n\u00a0\u00a0\u00a0 //\u00a0 setup code for SimpleFeatureBuilder\n\u00a0\u00a0\u00a0 valueIterator.map { s =>\n// Processing as before to build the SimpleFeatureType\n\u00a0\u00a0\u00a0\u00a0\u00a0 val simpleFeature =\n\u00a0\u00a0\u00a0\u00a0\u00a0 if (!valueIterator.hasNext) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 // cleanup here\n\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0 simpleFeature\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n}\n```", "```scala\nGeohash:String \n\n```", "```scala\nsimpleFeature.setAttribute(Geomesa, calculatedGeoHash)\n```", "```scala\n./accumulo shell -u username -p password -e \"scan -t gdelt_records -np\" | wc\n```", "```scala\n/** Geohash encoding/decoding as per http://en.wikipedia.org/wiki/Geohash */\nobject Geohash {\n\n\u00a0 val LAT_RANGE = (-90.0, 90.0)\n  val LON_RANGE = (-180.0, 180.0)\n\n\u00a0 // Aliases, utility functions\n\u00a0 type Bounds = (Double, Double)\n\u00a0 private def mid(b: Bounds) = (b._1 + b._2) / 2.0\n\u00a0 implicit class BoundedNum(x: Double) { def in(b: Bounds): Boolean = x >= b._1 && x <= b._2 }\n\n\u00a0 /**\n\u00a0\u00a0 * Encode lat/long as a base32 geohash.\n\u00a0\u00a0 *\n\u00a0\u00a0 * Precision (optional) is the number of base32 chars desired; default is 12, which gives precision well under a meter.\n\u00a0\u00a0 */\n\u00a0 def encode(lat: Double, lon: Double, precision: Int=12): String = { // scalastyle:ignore\n\u00a0\u00a0\u00a0 require(lat in LAT_RANGE, \"Latitude out of range\")\n\u00a0\u00a0\u00a0 require(lon in LON_RANGE, \"Longitude out of range\")\n\u00a0\u00a0\u00a0 require(precision > 0, \"Precision must be a positive integer\")\n\u00a0\u00a0\u00a0 val rem = precision % 2 // if precision is odd, we need an extra bit so the total bits divide by 5\n\u00a0\u00a0\u00a0 val numbits = (precision * 5) / 2\n\u00a0\u00a0\u00a0 val latBits = findBits(lat, LAT_RANGE, numbits)\n\u00a0\u00a0\u00a0 val lonBits = findBits(lon, LON_RANGE, numbits + rem)\n\u00a0\u00a0\u00a0 val bits = intercalatelonBits, latBits)\n\u00a0\u00a0\u00a0 bits.grouped(5).map(toBase32).mkString // scalastyle:ignore\n\u00a0 }\n\n\u00a0 private def findBits(part: Double, bounds: Bounds, p: Int): List[Boolean] = {\n\u00a0\u00a0\u00a0 if (p == 0) Nil\n\u00a0\u00a0\u00a0 else {\n\u00a0\u00a0\u00a0\u00a0\u00a0 val avg = mid(bounds)\n\u00a0\u00a0\u00a0\u00a0\u00a0 if (part >= avg) true :: findBits(part, (avg, bounds._2), p - 1)\n// >= to match geohash.org encoding\n\u00a0\u00a0\u00a0\u00a0\u00a0 else false :: findBits(part, (bounds._1, avg), p - 1)\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n\n\u00a0 /**\n\u00a0\u00a0 * Decode a base32 geohash into a tuple of (lat, lon)\n\u00a0\u00a0 */\n\u00a0 def decode(hash: String): (Double, Double) = {\n\u00a0\u00a0\u00a0 require(isValid(hash), \"Not a valid Base32 number\")\n\u00a0\u00a0\u00a0 val (odd, even) =toBits(hash).foldRight((List[A](), List[A]())) { case (b, (a1, a2)) => (b :: a2, a1) }\n\u00a0\u00a0\u00a0 val lon = mid(decodeBits(LON_RANGE, odd))\n\u00a0\u00a0\u00a0 val lat = mid(decodeBits(LAT_RANGE, even))\n\u00a0\u00a0\u00a0 (lat, lon)\n\u00a0 }\n\n\u00a0 private def decodeBits(bounds: Bounds, bits: Seq[Boolean]) =\n\u00a0\u00a0\u00a0 bits.foldLeft(bounds)((acc, bit) => if (bit) (mid(acc), acc._2) else (acc._1, mid(acc)))\n}\n\ndef intercalate[A](a: List[A], b: List[A]): List[A] = a match {\n case h :: t => h :: intercalate(b, t)\n case _ => b\n}\n```", "```scala\n            rm /System/Library/Java/Extensions/jai_*.\n    ```", "```scala\nhttp://localhost:8080/geoserver/geomesa/wms?\n\n```", "```scala\nservice=WMS&version=1.1.0&request=GetMap& \n\n```", "```scala\nlayers=geomesa:event&styles=& \n\n```", "```scala\ntransparency=true& \n\n```", "```scala\ncql_filter=GoldsteinScale>8& \n\n```", "```scala\nbbox=-180.0,-90.0,180.0,90.0& \n\n```", "```scala\nwidth=768&height=384& \n\n```", "```scala\nsrs=EPSG:4326&format=image%2Fjpeg& \n\n```", "```scala\ntime=2016-01-01T00:00:00.000Z/2016-01-30T23:00:00.000Z \n\n```", "```scala\nhttp://localhost:8080/geoserver/wms?service=WMS&version=1.1.0&request=GetMap&layers=layer1,layer2,layer3   ...   \n\n```", "```scala\nhttp://localhost:8080/geoserver/wms?service=WMS&version=1.1.0&request=GetMap&layers=layer1,layer2,layer3&cql_filter=INCLUDE;(LAYER2_COL='value');INCLUDE...   \n\n```", "```scala\nobject CountByWeek {\n\n \u00a0 // specify the params for the datastore\n \u00a0 val params = Map(\n \u00a0\u00a0\u00a0 \"instanceId\" -> \"accumulo\",\n \u00a0\u00a0\u00a0 \"zookeepers\" -> \"127.0.0.1:2181\",\n \u00a0\u00a0\u00a0 \"user\"\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 -> \"root\",\n \u00a0\u00a0\u00a0 \"password\"\u00a0\u00a0 -> \"accumulo\",\n \u00a0\u00a0\u00a0 \"tableName\"\u00a0 -> \"gdelt\")\n\n \u00a0 // matches the params in the datastore loading code\n \u00a0 val typeName\u00a0\u00a0\u00a0\u00a0\u00a0 = \"event\"\n \u00a0 val geom\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 = \"geom\"\n \u00a0 val date\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 = \"SQLDATE\"\n \u00a0 val actor1\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 = \"Actor1Name\"\n \u00a0 val actor2\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 = \"Actor2Name\"\n \u00a0 val eventCode\u00a0\u00a0\u00a0\u00a0 = \"EventCode\"\n \u00a0 val numArticles\u00a0\u00a0 = \"NumArticles\"\n\n\u00a0  // specify the geographical bounding\n \u00a0 val bbox\u00a0\u00a0 = \"34.515610, -21.445313, 69.744748, 36.914063\"\n\n\u00a0 // specify the temporal bounding\n \u00a0val during = \"2016-01-01T00:00:00.000Z/2016-12-30T00:00:00.000Z\"\n\n\u00a0 // create the filter\n \u00a0val filter = s\"bbox($geom, $bbox) AND $date during $during\"\n\n \u00a0def main(args: Array[String]) {\n \u00a0\u00a0 // Get a handle to the data store\n \u00a0\u00a0 val ds = DataStoreFinder\n\u00a0\u00a0\u00a0\u00a0\u00a0  .getDataStore(params)\n \u00a0\u00a0\u00a0\u00a0\u00a0 .asInstanceOf[AccumuloDataStore]\n\n \u00a0\u00a0\u00a0// Construct a CQL query to filter by bounding box\n \u00a0\u00a0\u00a0val q = new Query(typeName, ECQL.toFilter(filter))\n\n \u00a0\u00a0\u00a0// Configure Spark\n \u00a0\u00a0\u00a0val sc = new SparkContext(GeoMesaSpark.init(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 new SparkConf(true), ds))\n\n \u00a0\u00a0\u00a0 // Create an RDD from the query\n \u00a0\u00a0\u00a0 val simpleFeaureRDD = GeoMesaSpark.rdd(new Configuration,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 sc, params, q)\n\n \u00a0\u00a0\u00a0 // Convert RDD[SimpleFeature] to RDD[Row] for DataFrame creation below\n \u00a0\u00a0\u00a0 val gdeltAttrRDD = simpleFeaureRDD.mapPartitions { iter =>\n \u00a0\u00a0\u00a0\u00a0\u00a0 val df = new SimpleDateFormat(\"yyyy-MM-dd\")\n \u00a0\u00a0\u00a0\u00a0\u00a0 val ff = CommonFactoryFinder.getFilterFactory2\n \u00a0\u00a0\u00a0\u00a0\u00a0 val dt = ff.property(date)\n \u00a0\u00a0\u00a0\u00a0\u00a0 val a1n = ff.property(actor1)\n \u00a0\u00a0\u00a0\u00a0\u00a0 val a2n = ff.property(actor2)\n \u00a0\u00a0\u00a0\u00a0\u00a0 val ec = ff.property(eventCode)\n \u00a0\u00a0\u00a0\u00a0\u00a0 val na = ff.property(numArticles)\n \u00a0\u00a0\u00a0\u00a0\u00a0 iter.map { f =>\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Row(\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 df.format(dt.evaluate(f).asInstanceOf[java.util.Date]),\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 a1n.evaluate(f),\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 a2n.evaluate(f),\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ec.evaluate(f),\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 na.evaluate(f)\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )\n \u00a0\u00a0\u00a0\u00a0\u00a0 }\n \u00a0\u00a0\u00a0 }\n   }\n}\n```", "```scala\ngdeltAttrRDD.saveAsTextFile(\"/data/gdelt/brent-2016-rdd-row)\n```", "```scala\nval gdeltAttrRDD = sc.textFile(\"/data/gdelt/brent-2016-rdd-row)\n```", "```scala\nvar cameoMap = scala.collection.mutable.Map[String, String]()\n\nval linesRDD = sc.textFile(\"file://CAMEO.eventcodes.txt\")\nlinesRDD.collect.foreach(line => {\n\u00a0 val splitsArr = line.split(\"\\t\")\n\u00a0 cameoMap += (splitsArr(0) -> splitsArr(1).\nreplaceAll(\"[^A-Za-z0-9 ]\", \"\"))\n})\n```", "```scala\nval bagOfWordsRDD = gdeltAttrRDD.map(f => Row(\n \u00a0 f.get(0),\n \u00a0 f.get(1).toString.replaceAll(\"\\\\s\",\"\").\n \u00a0\u00a0\u00a0 toLowerCase + \" \" + cameoMap(f.get(3).toString).\n \u00a0\u00a0\u00a0 toLowerCase + \" \" + f.get(2).toString.replaceAll(\"\\\\s\",\"\").\n \u00a0\u00a0\u00a0 toLowerCase)\n )\n\n val gdeltSentenceStruct = StructType(Array(\n \u00a0 StructField(\"Date\", StringType, true),\n \u00a0 StructField(\"sentence\", StringType, true)\n ))\n\n val gdeltSentenceDF \n spark.createDataFrame(bagOfWordsRDD,gdeltSentenceStruct)\n gdeltSentenceDF.show(false)\n\n+----------+-----------------------------------------------------+\n|Date\u00a0\u00a0\u00a0\u00a0\u00a0 |sentence\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n+----------+-----------------------------------------------------+\n|2016-01-02|president demand not specified below unitedstates \u00a0\u00a0\u00a0|\n|2016-01-02|vladimirputin engage in negotiation beijing\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\n|2016-01-02|northcarolina make pessimistic comment neighborhood \u00a0|\n+----------+-----------------------------------------------------+\n```", "```scala\nval windowAgg = gdeltSentenceDF.\n\u00a0\u00a0\u00a0 groupBy(window(gdeltSentenceDF.col(\"Date\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"7 days\", \"7 days\", \"1 day\"))\nval sentencesDF = windowAgg.agg(\n\u00a0\u00a0\u00a0 collect_list(\"sentence\") as \"sentenceArray\")\n```", "```scala\nval convertWrappedArrayToStringUDF = udf {(array: WrappedArray[String]) =>\n \u00a0array.mkString(\" \")\n }\n\nval dateConvertUDF = udf {(date: String) =>\n\u00a0 new SimpleDateFormat(\"yyyy-MM-dd\").\n\u00a0\u00a0\u00a0 format(new SimpleDateFormat(\"yyyy-MM-dd hh:mm:ss\").\n\u00a0\u00a0\u00a0\u00a0\u00a0 parse(date))\n\u00a0 }\n\nval aggSentenceDF = sentencesDF.withColumn(\"text\",\n convertWrappedArrayToStringUDF(\n\u00a0\u00a0 sentencesDF(\"sentenceArray\"))).\n\u00a0 \u00a0\u00a0\u00a0\u00a0withColumn(\"commonFriday\", dateConvertUDF(sentencesDF(\"window.end\")))\n\naggSentenceDF.show\n\n+--------------------+-----------------+--------------+-------------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 window|\u00a0\u00a0\u00a0 sentenceArray|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 text| commonFriday|\n+--------------------+-----------------+--------------+-------------+\n|[2016-09-09 00:00...|[unitedstates app|unitedstates a|\u00a0\u00a0 2016-09-16|\n|[2016-06-24 00:00...|[student make emp|student make e|\u00a0\u00a0 2016-07-01|\n|[2016-03-04 00:00...|[american provide|american provi|\u00a0\u00a0 2016-03-11|\n+--------------------+-----------------+--------------+-------------+\n```", "```scala\n// define a function to reformat the date field\ndef convert(date:String) : String = {\n\u00a0 val dt = new SimpleDateFormat(\"dd/MM/yyyy\").parse(date)\n\u00a0 new SimpleDateFormat(\"yyyy-MM-dd\").format(dt)\n}\n\nval oilPriceDF = spark\n\u00a0 .read\n\u00a0 .option(\"header\",\"true\")\n\u00a0 .option(\"inferSchema\", \"true\")\n\u00a0 .csv(\"oil-prices.csv\")\n\n// create a User Defined Function for the date changes\nval convertDateUDF = udf {(Date: String) => convert(Date)}\n\nval oilPriceDatedDF = oilPriceDF.withColumn(\"DATE\", convertDateUDF(oilPriceDF(\"DATE\")))\n\n// offset to start at beginning of week, 4 days in this case\nval windowDF = oilPriceDatedDF.groupBy(window(oilPriceDatedDF.col(\"DATE\"),\"7 days\", \"7 days\", \"4 days\"))\n\n// find the last value in each window, this is the trading close price for that week\nval windowLastDF = windowDF.agg(last(\"PRICE\") as \"last(PRICE)\"\n).sort(\"window\")\n\nwindowLastDF.show(20, false)\n```", "```scala\n+---------------------------------------------+-----------+\n|window\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|last(PRICE)|\n+---------------------------------------------+-----------+\n|[2011-11-21 00:00:00.0,2011-11-28 00:00:00.0]|106.08\u00a0\u00a0\u00a0\u00a0 |\n|[2011-11-28 00:00:00.0,2011-12-05 00:00:00.0]|109.59\u00a0\u00a0\u00a0\u00a0 |\n|[2011-12-05 00:00:00.0,2011-12-12 00:00:00.0]|107.91\u00a0\u00a0\u00a0\u00a0 |\n|[2011-12-12 00:00:00.0,2011-12-19 00:00:00.0]|104.0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n+---------------------------------------------+-----------+\n```", "```scala\nval sortedWindow = Window.orderBy(\"window.start\")\n\n// add the previous last value to each row\nval lagLastCol = lag(col(\"last(PRICE)\"), 1).over(sortedWindow)\nval lagLastColDF = windowLastDF.withColumn(\"lastPrev(PRICE)\", lagLastCol)\n\n// create a UDF to calculate the price rise or fall\nval simplePriceChangeFunc = udf{(last : Double, prevLast : Double) =>\n\u00a0 var change = ((last - prevLast) compare 0).signum\n\u00a0 if(change == -1)\n\u00a0\u00a0\u00a0 change = 0\n\u00a0 change.toDouble\n}\n\n// create a UDF to calculate the date of the Friday for that week\nval findDateTwoDaysAgoUDF = udf{(date: String) =>\n\u00a0 val dateFormat = new SimpleDateFormat( \"yyyy-MM-dd\" )\n\u00a0 val cal = Calendar.getInstance\n\u00a0 cal.setTime( dateFormat.parse(date))\n\u00a0 cal.add( Calendar.DATE, -3 )\n\u00a0 dateFormat.format(cal.getTime)\n}\n\nval oilPriceChangeDF = lagLastColDF.withColumn(\"label\", simplePriceChangeFunc(\n\u00a0 lagLastColDF(\"last(PRICE)\"),\n\u00a0 lagLastColDF(\"lastPrev(PRICE)\")\n)).withColumn(\"commonFriday\", findDateTwoDaysAgoUDF(lagLastColDF(\"window.end\"))\n\noilPriceChangeDF.show(20, false)\n\n+--------------------+-----------+---------------+-----+------------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 window|last(PRICE)|lastPrev(PRICE)|label|commonFriday|\n+--------------------+-----------+---------------+-----+------------+\n|[2015-12-28 00:00...|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 36.4|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 null| null|\u00a0 2016-01-01|\n|[2016-01-04 00:00...|\u00a0\u00a0\u00a0\u00a0\u00a0 31.67|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 36.4|\u00a0 0.0|\u00a0 2016-01-08|\n|[2016-01-11 00:00...|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 28.8|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 31.67|\u00a0 0.0|\u00a0 2016-01-15|\n+--------------------+-----------+---------------+-----+------------+\n```", "```scala\nval changeJoinDF = aggSentenceDF\n .drop(\"window\")\n .drop(\"sentenceArray\")\n .join(oilPriceChangeDF, Seq(\"commonFriday\"))\n .withColumn(\"id\", monotonicallyIncreasingId)\n```", "```scala\nchangeJoinDF,show\n+------------+---------+---------+-----------+---------+-----+------+\n|commonFriday|\u00a0 \u00a0\u00a0\u00a0text|\u00a0\u00a0 window|last(PRICE)| lastPrev|label|\u00a0\u00a0 \u00a0id|\n+------------+---------+---------+-----------+---------+-----+------+\n|\u00a0 2016-09-16|unitedsta|[2016-09-|\u00a0\u00a0\u00a0\u00a0\u00a0 45.26|\u00a0\u00a0\u00a0 48.37|\u00a0 0.0| \u00a0\u00a0121|\n| \u00a02016-07-01|student m|[2016-06-|\u00a0\u00a0\u00a0\u00a0\u00a0 47.65|\u00a0\u00a0\u00a0 46.69|\u00a0 1.0| \u00a0\u00a0783|\n|\u00a0 2016-03-11|american |[2016-03-|\u00a0\u00a0\u00a0\u00a0\u00a0 39.41|\u00a0\u00a0\u00a0 37.61|\u00a0 1.0| \u00a0\u00a0356|\n+------------+---------+---------+-----------+---------+-----+------+\n```", "```scala\nval tokenizer = new Tokenizer().\n \u00a0 setInputCol(\"text\").\n \u00a0 setOutputCol(\"words\")\n val hashingTF = new HashingTF().\n \u00a0 setNumFeatures(10000).\n \u00a0 setInputCol(tokenizer.getOutputCol).\n \u00a0 setOutputCol(\"rawFeatures\")\n```", "```scala\nval idf = new IDF().\n\u00a0 setInputCol(hashingTF.getOutputCol).\n\u00a0 setOutputCol(\"features\")\n```", "```scala\nval nb = new NaiveBayes() \n\n```", "```scala\nval pipeline = new Pipeline().\n \u00a0setStages(Array(tokenizer, hashingTF, idf, nb))\n```", "```scala\nval splitDS = changeJoinDF.randomSplit(Array(0.75,0.25))\nval (trainingDF,testDF) = (splitDS(0),splitDS(1))\n```", "```scala\nval model = pipeline.fit(trainingDF)\n```", "```scala\nmodel.save(\"/data/models/gdelt-naivebayes-2016\") \nval naivebayesModel = PipelineModel.load(\"/data/models/Gdelt-naivebayes-2016\") \n\n```", "```scala\nmodel\n\u00a0 .transform(testDF)\n\u00a0 .select(\"id\", \"prediction\", \"label\").\n\u00a0 .collect()\n\u00a0 .foreach {\n\u00a0\u00a0\u00a0 case Row(id: Long, pred: Double, label: Double) =>\n \u00a0 \u00a0\u00a0\u00a0\u00a0println(s\"$id --> prediction=$pred --> should be: $label\")\n \u00a0}\n```", "```scala\n8847632629761 --> prediction=1.0 --> should be: 1.0\n1065151889408 --> prediction=0.0 --> should be: 0.0\n1451698946048 --> prediction=1.0 --> should be: 1.0\n```", "```scala\n+--------------------+--------------------+----------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 rawPrediction|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 probability|prediction|\n+--------------------+--------------------+----------+\n|[-6487.5367247911...|[2.26431216092671...|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.0|\n|[-8366.2851849035...|[2.42791395068146...|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.0|\n|[-4309.9770937765...|[3.18816589322004...|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 1.0|\n+--------------------+--------------------+----------+\n```"]