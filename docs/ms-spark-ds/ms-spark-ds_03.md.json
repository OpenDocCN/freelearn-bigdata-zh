["```scala\nhttp://data.gdeltproject.org/gdeltv2/masterfilelist.txt\n```", "```scala\nwget http://data.gdeltproject.org/gdeltv2/20150218230000.gkg.csv.zip -o log.txt  \nunzip 20150218230000.gkg.csv.zip \nhdfs dfs -put 20150218230000.gkg.csv /data/gdelt/gkg/2015/02/21/ \n\n```", "```scala\nwc:125,c2.21:4,c10.1:40,v10.1:3.21111111\n\n```", "```scala\ncase class Grouped(locations:Array[String], people:Array[String]) \n\nval group = Grouped(Array(\"USA\",\"France\",\"GB\"), \n       Array(\"Barack Obama\",\"David Cameron\", \"Francois Hollande\")) \n\nval ds = Seq(group).toDS \n\nds.show \n\n+-----------------+--------------------+ \n|        locations|              people| \n+-----------------+--------------------+ \n|[USA, France, GB]|[Barack Obama, Da...| \n+-----------------+--------------------+ \n\nval flatLocs = ds.withColumn(\"locations\",explode($\"locations\")) \nflatLocs.show \n\n+---------+--------------------+ \n|Locations|              People| \n+---------+--------------------+ \n|      USA|[Barack Obama, Da...| \n|   France|[Barack Obama, Da...| \n|       GB|[Barack Obama, Da...| \n+---------+--------------------+ \n\nval flatFolk = flatLocs.withColumn(\"people\",explode($\"people\")) \nflatFolk.show \n\n+---------+-----------------+ \n|Locations|           People| \n+---------+-----------------+ \n|      USA|     Barack Obama| \n|      USA|    David Cameron| \n|      USA|Francois Hollande| \n|   France|     Barack Obama| \n|   France|    David Cameron| \n|   France|Francois Hollande| \n|       GB|     Barack Obama| \n|       GB|    David Cameron| \n|       GB|Francois Hollande| \n+---------+-----------------+ \n\n```", "```scala\nwc:125,c2.21:4,c10.1:40,v10.1:3.21111111\n\n```", "```scala\nWordCount:125, General_Inquirer_Bodypt:4, SentiWordNet:40, SentiWordNet average: v10.1:3.21111111\n\n```", "```scala\n\nimport org.apache.spark.sql.functions._      \n\nval rdd = rawDS map GdeltParser.toCaseClass    \nval ds = rdd.toDS()     \n\n// DataFrame-style API \nds.agg(avg(\"goldstein\")).as(\"goldstein\").show()    \n\n// Dataset-style API \nds.groupBy(_.eventCode).count().show() \n\n```", "```scala\nimport org.apache.spark.sql.types._ \n\nval schema = StructType(Array( \n    StructField(\"GkgRecordId\"           , StringType, true), \n    StructField(\"V21Date\"               , StringType, true), \n    StructField(\"V2SrcCollectionId\"     , StringType, true),        \n    StructField(\"V2SrcCmnName\"          , StringType, true),  \n    StructField(\"V2DocId\"               , StringType, true),  \n    StructField(\"V1Counts\"              , StringType, true),  \n    StructField(\"V21Counts\"             , StringType, true),  \n    StructField(\"V1Themes\"              , StringType, true),  \n    StructField(\"V2Themes\"              , StringType, true),  \n    StructField(\"V1Locations\"           , StringType, true),  \n    StructField(\"V2Locations\"           , StringType, true),  \n    StructField(\"V1Persons\"             , StringType, true),  \n    StructField(\"V2Persons\"             , StringType, true),  \n    StructField(\"V1Orgs\"                , StringType, true),  \n    StructField(\"V2Orgs\"                , StringType, true),  \n    StructField(\"V15Tone\"               , StringType, true),  \n    StructField(\"V21Dates\"              , StringType, true),  \n    StructField(\"V2GCAM\"                , StringType, true),  \n    StructField(\"V21ShareImg\"           , StringType, true),  \n    StructField(\"V21RelImg\"             , StringType, true),  \n    StructField(\"V21SocImage\"           , StringType, true), \n    StructField(\"V21SocVideo\"           , StringType, true),  \n    StructField(\"V21Quotations\"         , StringType, true),  \n    StructField(\"V21AllNames\"           , StringType, true),  \n    StructField(\"V21Amounts\"            , StringType, true), \n    StructField(\"V21TransInfo\"          , StringType, true),  \n    StructField(\"V2ExtrasXML\"           , StringType, true)   \n)) \n\nval filename=\"path_to_your_gkg_files\"  \n\nval df = spark \n   .read \n   .option(\"header\", \"false\") \n   .schema(schema) \n   .option(\"delimiter\", \"t\") \n   .csv(filename) \n\ndf.createOrReplaceTempView(\"GKG\") \n\n```", "```scala\nspark.sql(\"SELECT V2GCAM FROM GKG LIMIT 5\").show \nspark.sql(\"SELECT AVG(GOLDSTEIN) AS GOLDSTEIN FROM GKG WHERE GOLDSTEIN IS NOT NULL\").show() \n\n```", "```scala\nval ds = df.as[GdeltEntity] \n\n```", "```scala\n\ndf.describe(\"V1Themes\").show \n\ndf.stat.freqItems(Array(\"V2Persons\")).show \n\ndf.stat.crosstab(\"V2Persons\",\"V2Locations\").show \n\n```", "```scala\n\n<dependency> \n    <groupId>com.databricks</groupId> \n    <artifactId>spark-avro_2.11</artifactId> \n    <version>3.1.0</version> \n</dependency> \n\n```", "```scala\nval GkgSchema = StructType(Array(\n \u00a0 StructField(\"GkgRecordId\", GkgRecordIdStruct, true), \n \u00a0 StructField(\"V21Date\", LongType, true), \n \u00a0 StructField(\"V2SrcCollectionId\", StringType, true), \n \u00a0 StructField(\"V2SrcCmnName\", StringType, true), \n \u00a0 StructField(\"V2DocId\", StringType, true), \n \u00a0 StructField(\"V1Counts\", ArrayType(V1CountStruct), true),\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0 StructField(\"V21Counts\", ArrayType(V21CountStruct), true),\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0 StructField(\"V1Themes\", ArrayType(StringType), true),\n \u00a0 StructField(\"V2EnhancedThemes\",ArrayType(EnhancedThemes),true),\u00a0\u00a0\u00a0 \n \u00a0 StructField(\"V1Locations\", ArrayType(V1LocationStruct), true),\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0 StructField(\"V2Locations\", ArrayType(EnhancedLocations), true), \n \u00a0 StructField(\"V1Persons\", ArrayType(StringType), true), \n \u00a0 StructField(\"V2Persons\", ArrayType(EnhancedPersonStruct), true),\u00a0\u00a0 \n \u00a0 StructField(\"V1Orgs\", ArrayType(StringType), true), \n \u00a0 StructField(\"V2Orgs\", ArrayType(EnhancedOrgStruct), true),\u00a0\u00a0\u00a0\u00a0\u00a0 \n \u00a0 StructField(\"V1Stone\", V1StoneStruct, true), \n \u00a0 StructField(\"V21Dates\", ArrayType(V21EnhancedDateStruct), true),\u00a0\u00a0\u00a0 \n \u00a0 StructField(\"V2GCAM\", ArrayType(V2GcamStruct), true), \n \u00a0 StructField(\"V21ShareImg\", StringType, true), \n \u00a0 StructField(\"V21RelImg\", ArrayType(StringType), true), \n \u00a0 StructField(\"V21SocImage\", ArrayType(StringType), true), \n \u00a0 StructField(\"V21SocVideo\", ArrayType(StringType), true), \n \u00a0 StructField(\"V21Quotations\", ArrayType(QuotationStruct), true), \n \u00a0 StructField(\"V21AllNames\", ArrayType(V21NameStruct), true), \n \u00a0 StructField(\"V21Amounts\", ArrayType(V21AmountStruct), true), \n \u00a0 StructField(\"V21TransInfo\", V21TranslationInfoStruct, true), \n \u00a0 StructField(\"V2ExtrasXML\", StringType, true) \n ))\n```", "```scala\nval GkgRecordIdStruct = StructType(Array(\n\u00a0 StructField(\"Date\", LongType),\n\u00a0 StructField(\"TransLingual\", BooleanType), \u00a0\u00a0\u00a0\u00a0\n\u00a0 StructField(\"NumberInBatch\";, IntegerType)\n))\n```", "```scala\nval gdeltRDD = sparkContext.textFile(\"20160101020000.gkg.csv\")\n\nval gdeltRowOfRowsRDD = gdeltRDD.map(_.split(\"\\t\"))\n\u00a0\u00a0 .map(attributes =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 Row(\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0createGkgRecordID(attributes(0)),\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0attributes(1).toLong,\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0createSourceCollectionIdentifier(attributes(2),\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0attributes(3),\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0attributes(4),\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0createV1Counts(attributes(5),\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0createV21Counts(attributes(6),\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0.\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0.\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0.\n\u00a0\u00a0\u00a0\u00a0\u00a0 )\n\u00a0\u00a0 ))\n```", "```scala\ndef createGkgRecordID(str: String): Row = {\n \u00a0 if (str != \"\") {\n \u00a0\u00a0\u00a0 val split = str.split(\"-\")\n \u00a0\u00a0\u00a0 if (split(1).length > 1) {\n \u00a0\u00a0\u00a0\u00a0\u00a0 Row(split(0).toLong, true, split(1).substring(1).toInt)\n \u00a0\u00a0\u00a0 }\n \u00a0\u00a0\u00a0 else {\n \u00a0\u00a0\u00a0\u00a0\u00a0 Row(split(0).toLong, false, split(1).toInt)\n \u00a0\u00a0\u00a0 }\n \u00a0 }\n \u00a0 else {\n \u00a0\u00a0\u00a0 Row(0L, false, 0)\n \u00a0 }\n }\n```", "```scala\nimport org.apache.spark.sql.types._\nimport com.databricks.spark.avro._\nimport org.apache.spark.sql.Row\n\nval df = spark.createDataFrame(gdeltRowOfRowsRDD, GkgSchema)\n\ndf.write.avro(\"/path/to/avro/output\")\n```", "```scala\nval avroDF = spark\n\u00a0 .read\n\u00a0 .format(\"com.databricks.spark.avro\")\n\u00a0 .load(\"/path/to/avro/output\")\n```", "```scala\n<dependency>\u00a0\u00a0\n\u00a0\u00a0 <groupId>org.apache.avro</groupId>\u00a0\u00a0\n\u00a0\u00a0 <artifactId>avro</artifactId>\u00a0\u00a0\n\u00a0\u00a0 <version>1.7.7</version>\n</dependency>\n\n<plugin>\u00a0\u00a0\n\u00a0\u00a0 <groupId>org.apache.avro</groupId>\u00a0\u00a0\n\u00a0\u00a0 <artifactId>avro-maven-plugin</artifactId>\u00a0\u00a0\n\u00a0\u00a0 <version>1.7.7</version>\u00a0\u00a0\n\u00a0\u00a0 <executions>\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0 <execution>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 <phase>generate-sources</phase>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 <goals>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 <goal>schema</goal>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 </goals>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 <configuration>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0<sourceDirectory>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0${project.basedir}/src/main/avro/\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0</sourceDirectory>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0<outputDirectory>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0${project.basedir}/src/main/java/\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0</outputDirectory>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 </configuration>\u00a0\u00a0\u00a0\u00a0\n\u00a0\u00a0\u00a0\u00a0\u00a0 </execution>\u00a0\u00a0\n\u00a0\u00a0 </executions>\n</plugin>\n```", "```scala\n+----------------+-------------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0primitive|\u00a0\u00a0\u00a0 \u00a0\u00a0complex|\n+----------------+-------------+\n|null\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0|\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0record|\n|Boolean\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0|\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0enum|\n|int\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0array|\n|long\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0map|\n|float\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0|\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0union|\n|double \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0fixed|\n|bytes\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0|\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\n|string\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0|\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\n+----------------+-------------+\nAvro provides an extensible type system that supports **custom types**. It's also modular and offers namespaces, so that we can add new types and reuse custom types as the schema evolves. In the preceding example, we can see primitive types extensively used, but also custom objects such as `org.io.gzet.gdelt.gkg.v1.Location`.To create Avro files, we can use the following code (full example in our code repository):\n\n```", "```scala\n\nThe `Specification` object is created for us once we compile our IDL (using the maven plugin). It contains all of the methods required to access the Avro model, for example `setV2EnhancedLocations`. We are then left with creating the functions to parse our GKG data; two examples are shown, as follows:\n\n```", "```scala\n\nThis approach creates the required Avro files, but it is shown here to demonstrate how Avro works. As it stands, this code does not operate in parallel and, therefore, should not be used on big data. If we wanted to parallelize it, we could create a custom `InputFormat`, wrap the raw data into an RDD, and perform the processing on that basis. Fortunately, we don't have to, as `spark-avro` has already done it for us.\n```", "```scala\n+--------------------------+--------------+ \n|                 File Type|          Size| \n+--------------------------+--------------+ \n|20160101020000.gkg.csv    |      20326266| \n|20160101020000.gkg.avro   |      13557119| \n|20160101020000.gkg.parquet|       6567110| \n|20160101020000.gkg.csv.bz2|       4028862| \n+--------------------------+--------------+ \n\n```", "```scala\nval gdeltAvroDF = spark\u00a0\n\u00a0\u00a0\u00a0 .read\n\u00a0\u00a0\u00a0 .format(\"com.databricks.spark.avro\")\n\u00a0\u00a0\u00a0 .load(\"/path/to/avro/output\")\n\ngdeltAvroDF.write.parquet(\"/path/to/parquet/output\")\n```", "```scala\nval inputFile = new File(\"(\"/path/to/avro/output \")\n val outputFile = new Path(\"/path/to/parquet/output\")\n\n val schema = Specification.getClassSchema\n val reader =\u00a0 new GenericDatumReader[IndexedRecord](schema)\n val avroFileReader = DataFileReader.openReader(inputFile, reader)\n\n val parquetWriter =\n\u00a0\u00a0\u00a0\u00a0 new AvroParquetWriter[IndexedRecord](outputFile, schema)\n\n while(avroFileReader.hasNext)\u00a0 {\n \u00a0\u00a0 \u00a0parquetWriter.write(dataFileReader.next())\n }\n\n dataFileReader.close()\n parquetWriter.close()\n\n```"]