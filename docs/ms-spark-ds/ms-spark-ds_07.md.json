["```scala\nval processor = new CoreNLPProcessor()\nval document = processor.annotate(text)\n\ndocument.sentences foreach { sentence =>\n\u00a0 println(sentence.syntacticTree.get)\n}\n\n/*\n(NNP Yoko)\n(NNP Ono)\n(VBD said)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (PRP she)\n\u00a0\u00a0\u00a0\u00a0\u00a0 (CC and)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (JJ late)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (NN husband)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0(NNP John)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0(NNP Lennon)\n\u00a0\u00a0\u00a0\u00a0\u00a0 (VBD shared)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (DT a)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (JJ close)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (NN relationship)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (IN with)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (NNP David)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (NNP Bowie)\n*/\n```", "```scala\ndef buildTuples(p: Array[String]): Array[(String, String)] = {\n\u00a0\u00a0\u00a0 for(i <- 0 to p.length - 2; j <- i + 1 to p.length - 1) yield {\n\u00a0\u00a0\u00a0\u00a0\u00a0 (p(i), p(j))\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n```", "```scala\n<dependency>\n\u00a0 <groupId>org.elasticsearch</groupId>\n\u00a0 <artifactId>elasticsearch-spark_2.11</artifactId>\n\u00a0 <version>2.4.0<version>\n</dependency>\n```", "```scala\nval spark = SparkSession\n\u00a0 .builder()\n\u00a0 .config(\"es.nodes\", \"localhost\")\n\u00a0 .config(\"es.port\", \"9200\")\n\u00a0 .appName(\"communities-es-download\")\n\u00a0 .getOrCreate()\n\nspark\n\u00a0 .read\n\u00a0 .format(\"org.elasticsearch.spark.sql\")\n\u00a0 .load(\"gzet/news\")\n\u00a0 .select(\"title\", \"url\")\n\u00a0 .show(5)\n\n+--------------------+--------------------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 title|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 url|\n+--------------------+--------------------+\n|Sonia Meets Mehbo...|http://www.newind...|\n|\"A Job Well Done ...|http://daphneanso...|\n|New reading progr...|http://www.mailtr...|\n|Barrie fire servi...|http://www.simcoe...|\n|Paris police stat...|http://www.dailym...|\n+--------------------+--------------------+\n```", "```scala\n<dependency>\n\u00a0 <groupId>org.json4s</groupId>\n\u00a0 <artifactId>json4s-native_2.11</artifactId>\n\u00a0 <version>3.2.11</version>\n</dependency>\n```", "```scala\nimport org.elasticsearch.spark._\nimport org.json4s.native.JsonMethods._\nimport org.json4s.DefaultFormats\n\ndef readFromES(query: String = \"?q=*\"): RDD[Array[String]] = {\n\n\u00a0 sc.esJsonRDD(\"gzet/news\", query)\n\u00a0\u00a0\u00a0 .values\n\u00a0\u00a0\u00a0 . map {\n\u00a0\u00a0\u00a0\u00a0\u00a0 jsonStr =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0implicit val format = DefaultFormats\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0val json = parse(jsonStr)\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0(json \\ \"persons\").extract[Array[String]]\n\u00a0 \u00a0\u00a0}\n\n}\n\nreadFromEs(\"?persons='david bowie'\")\n\u00a0\u00a0 .map(_.mkString(\",\"))\n\u00a0\u00a0 .take(3)\n\u00a0\u00a0 .foreach(println)\n\n/*\ndavid bowie,yoko ono,john lennon,paul mc cartney\nduncan jones,david bowie,tony visconti\ndavid bowie,boris johnson,david cameron\n*/\n```", "```scala\nval personRdd = readFromES()\nval tupleRdd = personRdd flatMap buildTuples\n```", "```scala\n# set up some users\ncreateuser matt\ncreateuser ant\ncreateuser dave\ncreateuser andy\n\n# create the persons table\ncreatetable persons\n\n# switch to the persons table\ntable persons\n\n# ensure all of the users can access the table\ngrant -s System.READ_TABLE -u matt\ngrant -s System.READ_TABLE -u ant\ngrant -s System.READ_TABLE -u dave\ngrant -s System.READ_TABLE -u andy\n\n# allocate security labels to the users\naddauths -s unclassified,secret,topsecret -u matt\naddauths -s unclassified,secret -u ant\naddauths -s unclassified,topsecret -u dave\naddauths -s unclassified -u andy\n\n# display user auths\ngetauths -u matt\n\n# create a server side iterator to sum values\nsetiter -t persons -p 10 -scan -minc -majc -n sumCombiner -class\norg.apache.accumulo.core.iterators.user.SummingCombiner\n\n# list iterators in use\nlistiter \u2013all\n\n# once the table contains some records ...\nuser matt\n\n# we'll see all of the records that match security labels for the user\nscan\n```", "```scala\nsecret&topsecret (secret AND topsecret)\nsecret|topsecret (secret OR topsecret)\nunclassified&(secret|topsecret) (unclassified AND secret, or unclassified AND topsecret)\n```", "```scala\n<dependency>\n\u00a0 <groupId>org.apache.accumulo</groupId>\n\u00a0 <artifactId>accumulo-core</artifactId>\n\u00a0 <version>1.7.0<version>\n</dependency>\n```", "```scala\nval spark = SparkSession\n\u00a0 .builder()\n\u00a0 .appName(\"communities-loader\")\n\u00a0 .getOrCreate()\n\nval sc = spark.sparkContext\nval hdpConf = sc.hadoopConfiguration\n\n// set the ES entry points\nhdpConf.set(\"es.nodes\", \"localhost:9200\")\nhdpConf.set(\"es.resource\", \"gzet/articles\")\n\n// Read map writable objects\nimport org.apache.hadoop.io.Text\nimport org.apache.hadoop.io.MapWritable\nimport org.elasticsearch.hadoop.mr.EsInputFormat\n\nval esRDD: RDD[MapWritable] = sc.newAPIHadoopRDD(\n\u00a0 hdpConf,\n\u00a0 classOf[EsInputFormat[Text, MapWritable]],\n\u00a0 classOf[Text],\n\u00a0 classOf[MapWritable]\n).values\n```", "```scala\ndef buildMutations(value: MapWritable) = {\n\n\u00a0 // Extract list of persons\n\u00a0 val people = value\n\u00a0\u00a0\u00a0 .get(\"person\")\n\u00a0\u00a0\u00a0 .asInstanceOf[ArrayWritable]\n\u00a0\u00a0\u00a0 .get()\n\u00a0\u00a0\u00a0 .map(_.asInstanceOf[Text])\n\u00a0\u00a0\u00a0 .map(_.toString)\n\n\u00a0 // Use a default Visibility\n\u00a0 val visibility = new ColumnVisibility(\"unclassified\")\n\n\u00a0 // Build mutation on tuples\n\u00a0 buildTuples(people.toArray)\n\u00a0\u00a0\u00a0 .map {\n\u00a0\u00a0\u00a0 \u00a0\u00a0case (src, dst) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0val mutation = new Mutation(src)\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0mutation.put(\"associated\", dst, visibility, \"1\")\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0(new Text(accumuloTable), mutation)\n\u00a0 \u00a0\u00a0}\n```", "```scala\n// Build Mutations\nval accumuloRDD = esRDD flatMap buildMutations\n\n// Save Mutations to Accumulo\naccumuloRDD.saveAsNewAPIHadoopFile(\n\u00a0 \"\",\n\u00a0 classOf[Text],\n\u00a0 classOf[Mutation],\n\u00a0 classOf[AccumuloOutputFormat]\n)\n```", "```scala\ndef read(\n\u00a0 sc: SparkContext,\n\u00a0 accumuloTable: String,\n\u00a0 authorization: Option[String] = None\n)\n```", "```scala\nval hdpConf = sc.hadoopConfiguration\nval job = Job.getInstance(hdpConf)\n\nval clientConfig = new ClientConfiguration()\n\u00a0 .withInstance(accumuloInstance)\n\u00a0 .withZkHosts(zookeeperHosts)\n\nAbstractInputFormat.setConnectorInfo(\n\u00a0 job,\n\u00a0 accumuloUser,\n\u00a0 new PasswordToken(accumuloPassword)\n)\n\nAbstractInputFormat.setZooKeeperInstance(\n\u00a0 job,\n\u00a0 clientConfig\n)\n\nif(authorization.isDefined) {\n\u00a0 AbstractInputFormat.setScanAuthorizations(\n\u00a0 \u00a0\u00a0job,\n\u00a0\u00a0\u00a0 new Authorizations(authorization.get)\n\u00a0 )\n}\n\nInputFormatBase.addIterator(job, is)\nInputFormatBase.setInputTableName(job, accumuloTable)\n```", "```scala\nval is = new IteratorSetting(\n\u00a0 1,\n\u00a0 \"summingCombiner\",\n\u00a0 \"org.apache.accumulo.core.iterators.user.SummingCombiner\"\n)\n\nis.addOption(\"all\", \"\")\nis.addOption(\"columns\", \"associated\")\nis.addOption(\"lossy\", \"TRUE\")\nis.addOption(\"type\", \"STRING\")\n```", "```scala\nval edgeWritableRdd: RDD[EdgeWritable] = sc.newAPIHadoopRDD(\n\u00a0 job.getConfiguration,\n\u00a0 classOf[AccumuloGraphxInputFormat],\n\u00a0 classOf[NullWritable],\n\u00a0 classOf[EdgeWritable]\n) values\n```", "```scala\nval dictionary = edgeWritableRdd\n\u00a0 .flatMap {\n\u00a0\u00a0\u00a0 edge =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0List(edge.getSourceVertex, edge.getDestVertex)\n\u00a0 }\n\u00a0 .distinct()\n\u00a0 .zipWithIndex()\n\u00a0 .mapValues {\n\u00a0\u00a0\u00a0 index =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0index + 1L\n\u00a0 }\n}\n\ndictionary.cache()\ndictionary.count()\n\ndictionary\n\u00a0 .take(3)\n\u00a0 .foreach(println)\n\n/*\n(david bowie, 1L)\n(yoko ono, 2L)\n(john lennon, 3L)\n*/\n```", "```scala\nval vertices = dictionary.map(_.swap)\n\nval edges = edgeWritableRdd\n\u00a0 .map {\n\u00a0\u00a0\u00a0 edge =>\n\u00a0 \u00a0\u00a0\u00a0\u00a0(edge.getSourceVertex, edge)\n\u00a0 }\n\u00a0 .join(dictionary)\n\u00a0 .map {\n\u00a0\u00a0\u00a0 case (from, (edge, fromId)) =>\n\u00a0 \u00a0\u00a0\u00a0\u00a0(edge.getDestVertex, (fromId, edge))\n\u00a0 }\n\u00a0 .join(dictionary)\n\u00a0 .map {\n\u00a0\u00a0\u00a0 case (to, ((fromId, edge), toId)) =>\n\u00a0 \u00a0\u00a0\u00a0\u00a0Edge(fromId, toId, edge.getCount.toLong)\n\u00a0 }\n\nval personGraph = Graph.apply(vertices, edges)\n\npersonGraph.cache()\npersonGraph.vertices.count()\n\npersonGraph\n\u00a0 .triplets\n\u00a0 .take(2)\n\u00a0 .foreach(println)\n\n/*\n((david bowie,1),(yoko ono,2),1)\n((david bowie,1),(john lennon,3),1)\n((yoko ono,2),(john lennon,3),1)\n*/\n```", "```scala\nclass VState extends Serializable {\n\u00a0 var vId = -1L\n\u00a0 var cId = -1L\n\u00a0 var changed = false\n\u00a0 var txV = 0\n\u00a0 var txC = 0\n\u00a0 var vtxV = 0\n\u00a0 var vtxV_C = 0\n\u00a0 var wcc = 0.0d\n}\n```", "```scala\nval cEdges: RDD[Edge[ED]] = graph.edges\n\u00a0 .map { e =>\n\u00a0 \u00a0\u00a0if(e.srcId > e.dstId) {\n\u00a0\u00a0\u00a0 \u00a0\u00a0Edge(e.dstId, e.srcId, e.attr)\n\u00a0 \u00a0\u00a0} else e\n\u00a0 }\n\nval canonicalGraph = Graph\n\u00a0 .apply(graph.vertices, cEdges)\n\u00a0 .partitionBy(PartitionStrategy.EdgePartition2D)\n\ncanonicalGraph.cache()\ncanonicalGraph.vertices.count()\n```", "```scala\nval triGraph = graph.triangleCount()\nval neighborRdd = graph.collectNeighborIds(EdgeDirection.Either)\n\nval subGraph = triGraph.outerJoinVertices(neighborRdd)({ (vId, triangle, neighbors) =>\n\u00a0 (triangle, neighbors.getOrElse(Array()))\n}).subgraph((t: EdgeTriplet[(Int, Array[Long]), ED]) => {\n\u00a0 t.srcAttr._2.intersect(t.dstAttr._2).nonEmpty\n}, (vId: VertexId, vStats: (Int, Array[Long])) => {\n\u00a0 vStats._1 > 0\n})\n```", "```scala\nval initGraph: Graph[VState, ED] = subGraph.outerJoinVertices(subGraph.degrees)((vId, vStat, degrees) => {\n\u00a0 val state = new VState()\n\u00a0 state.vId = vId\n\u00a0 state.cId = vId\n\u00a0 state.changed = true\n\u00a0 state.txV = vStat._1\n\u00a0 state.vtxV = degrees.getOrElse(0)\n\u00a0 state.wcc = degrees.getOrElse(0).toDouble / vStat._1 \n\u00a0 state\n})\n\ninitGraph.cache()\ninitGraph.vertices.count()\n\ncanonicalGraph.unpersist(blocking = false)\n```", "```scala\ndef getBestCid(v: VState, msgs: Array[VState]): VertexId = {\n\n\u00a0 val candidates = msgs filter {\n\n\u00a0\u00a0\u00a0 msg =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0msg.wcc > v.wcc ||\n\u00a0\u00a0\u00a0 \u00a0\u00a0(msg.wcc == v.wcc && msg.vtxV > v.vtxV) ||\n\u00a0\u00a0\u00a0 \u00a0\u00a0(msg.wcc == v.wcc && msg.vtxV > v.vtxV && msg.cId > v.cId)\n\u00a0 \u00a0\u00a0}\n\n\u00a0 if(candidates.isEmpty) {\n\n\u00a0\u00a0\u00a0 v.cId\n\n\u00a0 } else {\n\n\u00a0\u00a0\u00a0 candidates\n\u00a0\u00a0\u00a0\u00a0 .sortBy {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 msg =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0(msg.wcc, msg.vtxV, msg.cId)\n\u00a0\u00a0\u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0 .last\n\u00a0\u00a0\u00a0\u00a0\u00a0 .cId\n\u00a0 }\n\n}\n\ndef sendMsg = (ctx: EdgeContext[VState, ED, Array[VState]]) => {\n\n\u00a0 ctx.sendToDst(\n\u00a0\u00a0\u00a0 Array(ctx.srcAttr)\n\u00a0 )\n\n\u00a0 ctx.sendToSrc(\n\u00a0\u00a0\u00a0 Array(ctx.dstAttr)\n\u00a0 )\n}\n\ndef mergeMsg = (m1: Array[VState], m2: Array[VState]) => {\n\u00a0 m1 ++ m2\n}\n\ndef msgs = subGraph.aggregateMessages(sendMsg, mergeMsg)\n\nval initCIdGraph = subGraph.outerJoinVertices(msgs)((vId, vData, msgs) => {\n\u00a0 val newCId = getBestCid(vData, msgs.getOrElse(Array()))\n\u00a0 vData.cId = newCId\n\u00a0 vData\n})\n\ninitCIdGraph.cache()\ninitCIdGraph.vertices.count()\ninitGraph.unpersist(blocking = false)\n```", "```scala\nval initialMsg = new VState() \n\n```", "```scala\nimplicit val VSOrdering: Ordering[VState] = Ordering.by({ state =>\n\u00a0 (state.wcc, state.vtxV, state.vId)\n})\n\ndef compareState(c1: VState, c2: VState) = {\n\u00a0 List(c1, c2).sorted(VStateOrdering.reverse)\n}\n\nval mergeMsg = (c1: VState, c2: VState) => {\n\u00a0 compareState(c1, c2).head\n}\n```", "```scala\ndef sendMsg = (t: EdgeTriplet[VState, ED]) => {\n\n\u00a0 val messages = mutable.Map[Long, VState]()\n\u00a0 val sorted = compareState(t.srcAttr, t.dstAttr)\n\u00a0 val (fromNode, toNode) = (sorted.head, sorted.last)\n\u00a0 if (fromNode.changed) {\n\u00a0\u00a0\u00a0 messages.put(fromNode.vId, fromNode)\n\u00a0 \u00a0\u00a0messages.put(toNode.vId, fromNode)\n\u00a0 }\n\n\u00a0 messages.toIterator\n\n}\n```", "```scala\ndef vprog = (vId: VertexId, state: VState, message: VState) => {\n\n\u00a0 if (message.vId >= 0L) {\n\n\u00a0 \u00a0\u00a0// message comes from myself\n\u00a0 \u00a0\u00a0// I stop spamming people\n\u00a0 \u00a0\u00a0if (message.vId == vId) {\n\u00a0\u00a0\u00a0 \u00a0\u00a0state.changed = false\n\u00a0 \u00a0\u00a0}\n\n\u00a0 \u00a0\u00a0// Sender is a center of its own community\n\u00a0 \u00a0\u00a0// I become a border node of its community\n\u00a0\u00a0\u00a0 if (message.cId == message.vId) {\n\u00a0\u00a0\u00a0 \u00a0\u00a0state.changed = false\n\u00a0\u00a0\u00a0 \u00a0\u00a0state.cId = message.cId\n\u00a0 \u00a0\u00a0}\n\n\u00a0 \u00a0\u00a0// Sender is a border node of a foreign community\n\u00a0 \u00a0\u00a0// I become a center of my own community\n\u00a0 \u00a0\u00a0// I broadcast this change downstream\n\u00a0 \u00a0\u00a0if (message.cId != message.vId) {\n\u00a0 \u00a0\u00a0\u00a0\u00a0state.changed = true\n\u00a0\u00a0\u00a0 \u00a0\u00a0state.cId = vId\n\u00a0 \u00a0\u00a0}\n\n\u00a0 }\n\u00a0 state\n\n}\n```", "```scala\nval pregelGraph: Graph[VState, ED] = Pregel.apply(\n\u00a0 initCIdGraph, \n\u00a0 initialMsg, \n\u00a0 Int.MaxValue \n)(\n\u00a0 vprog,\n\u00a0 sendMsg,\n\u00a0 mergeMsg\n)\n\npregelGraph.cache()\npregelGraph.vertices.count()\n```", "```scala\ndef repartition[ED: ClassTag](graph: Graph[VState, ED]) = {\n\n\u00a0 val partitionedEdges = graph\n\u00a0\u00a0\u00a0 .triplets\n\u00a0\u00a0\u00a0 .map {\n\u00a0\u00a0\u00a0\u00a0\u00a0 e =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 val cId1 = e.srcAttr.cId\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 val cId2 = e.dstAttr.cId\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0val hash = math.abs((cId1, cId2).hashCode())\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0val partition = hash % partitions\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0(partition, e)\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 .partitionBy(new HashPartitioner(partitions))\n\u00a0\u00a0\u00a0 .map {\n\u00a0\u00a0\u00a0\u00a0\u00a0 pair =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0Edge(pair._2.srcId, pair._2.dstId, pair._2.attr)\n\u00a0\u00a0\u00a0 }\n\n\u00a0 Graph(graph.vertices, partitionedEdges)\n\n}\n```", "```scala\ncase class CommunityStats(\n\u00a0\u00a0 r: Int,\n\u00a0\u00a0 d: Double,\n\u00a0\u00a0 b: Int\n)\n\ndef getCommunityStats[ED: ClassTag](graph: Graph[VState, ED]) = {\n\n\u00a0 val cVert = graph\n\u00a0\u00a0\u00a0 .vertices\n\u00a0\u00a0\u00a0 .map(_._2.cId -> 1)\n\u00a0\u00a0\u00a0 .reduceByKey(_+_)\n\u00a0\u00a0\u00a0 .collectAsMap()\n\n\u00a0 val cEdges = graph\n\u00a0\u00a0\u00a0 .triplets\n\u00a0\u00a0\u00a0 .flatMap { t =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0if(t.srcAttr.cId == t.dstAttr.cId){\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0Iterator(((\"I\", t.srcAttr.cId), 1))\n\u00a0\u00a0\u00a0 \u00a0\u00a0} else {\n\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0Iterator(\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0((\"O\", t.srcAttr.cId), 1), \n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0((\"O\", t.dstAttr.cId), 1)\n\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0)\n\u00a0\u00a0\u00a0 \u00a0\u00a0}\n\u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0 .reduceByKey(_+_)\n\u00a0\u00a0\u00a0 .collectAsMap()\n\n\u00a0 cVert.map {\n\u00a0\u00a0\u00a0 case (cId, cCount) =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0val intEdges = cEdges.getOrElse((\"I\", cId), 0)\n\u00a0\u00a0\u00a0 \u00a0\u00a0val extEdges = cEdges.getOrElse((\"O\", cId), 0)\n\u00a0\u00a0\u00a0 \u00a0\u00a0val density = 2 * intEdges / math.pow(cCount, 2)\n\u00a0\u00a0\u00a0 \u00a0\u00a0(cId, CommunityStats(cCount, density, extEdges))\n\u00a0 } \n\n}\n```", "```scala\nvar communityStats = getCommunityStats(pregelGraph)\nval bCommunityStats = sc.broadcast(communityStats)\n```", "```scala\ndef collectCommunityEdges[ED: ClassTag](graph: Graph[VState, ED]) = {\n\n\u00a0 graph.outerJoinVertices(graph.aggregateMessages((e: EdgeContext[VState, ED, Array[VertexId]]) => {\n\u00a0\u00a0\u00a0 if(e.dstAttr.cId == e.srcAttr.cId){\n\u00a0\u00a0\u00a0\u00a0\u00a0 e.sendToDst(Array(e.srcId))\n\u00a0\u00a0\u00a0\u00a0\u00a0 e.sendToSrc(Array(e.dstId))\n\u00a0\u00a0\u00a0 }\n\u00a0 }, (e1: Array[VertexId], e2: Array[VertexId]) => {\n\u00a0\u00a0\u00a0 e1 ++ e2\n\u00a0 }))((vid, vState, vNeighbours) => {\n\u00a0\u00a0\u00a0 (vState, vNeighbours.getOrElse(Array()))\n\u00a0 })\n\n}\n```", "```scala\ndef collectCommunityTriangles[ED: ClassTag](graph: Graph[(VState, Array[Long]), ED]) = {\n\n\u00a0 graph.aggregateMessages((ctx: EdgeContext[(VState, Array[Long]), ED, Int]) => {\n\u00a0\u00a0\u00a0 if(ctx.srcAttr._1.cId == ctx.dstAttr._1.cId){\n\u00a0\u00a0\u00a0\u00a0\u00a0 val (smallSet, largeSet) = if (ctx.srcAttr._2.length < ctx.dstAttr._2.length) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (ctx.srcAttr._2.toSet, ctx.dstAttr._2.toSet)\n\u00a0\u00a0\u00a0\u00a0\u00a0 } else {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (ctx.dstAttr._2.toSet, ctx.srcAttr._2.toSet)\n\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0 val it = smallSet.iterator\n\u00a0\u00a0\u00a0\u00a0\u00a0 var counter: Int = 0\n\u00a0 \u00a0\u00a0\u00a0\u00a0while (it.hasNext) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 val vid = it.next()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if (\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 vid != ctx.srcId &&\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 vid != ctx.dstId &&\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 largeSet.contains(vid)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 ) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 counter += 1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 ctx.sendToSrc(counter)\n\u00a0\u00a0\u00a0\u00a0\u00a0 ctx.sendToDst(counter)\n\n\u00a0\u00a0\u00a0 }\n\u00a0 }, (e1: Int, e2: Int) => (e1 + e2))\n\n}\n```", "```scala\ndef updateGraph[ED: ClassTag](graph: Graph[VState, ED], stats: Broadcast[Map[VertexId, CommunityStats]]) = {\n\n\u00a0 val cNeighbours = collectCommunityEdges(graph)\n\u00a0 val cTriangles = collectCommunityTriangles(cNeighbours)\n\n\u00a0 cNeighbours.outerJoinVertices(cTriangles)(\n\u00a0\u00a0\u00a0 (vId, vData, tri) => {\n\u00a0\u00a0\u00a0 \u00a0\u00a0val s = vData._1\n\u00a0\u00a0\u00a0 \u00a0\u00a0val r = stats.value.get(s.cId).get.r\n\n\u00a0\u00a0\u00a0 \u00a0\u00a0// Core equation: compute WCC(v,C)\n\u00a0\u00a0\u00a0\u00a0\u00a0 val a = s.txC * s.vtxV\n\u00a0\u00a0\u00a0\u00a0\u00a0 val b = (s.txV * (r - 1 + s.vtxV_C).toDouble) \n\u00a0\u00a0\u00a0 \u00a0\u00a0val wcc = a / b\n\n\u00a0\u00a0\u00a0 \u00a0\u00a0val vtxC = vData._2.length\n\u00a0\u00a0\u00a0 \u00a0\u00a0s.vtxV_C = s.vtxV \u2013 vtxC\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 // Triangles are counted twice (incoming / outgoing)\n\u00a0\u00a0\u00a0 \u00a0\u00a0s.txC = tri.getOrElse(0) / 2\n\u00a0\u00a0\u00a0 \u00a0\u00a0s.wcc = wcc\n\u00a0\u00a0\u00a0 \u00a0\u00a0s\n\u00a0 })\n\n}\n\nval wccGraph = updateGraph(pregelGraph, bCommunityStats)\n```", "```scala\ndef computeWCC[ED: ClassTag](graph: Graph[VState, ED], cStats: Broadcast[Map[VertexId, CommunityStats]]): Double = {\n\n\u00a0 val total = graph.vertices\n\u00a0\u00a0\u00a0 .map {\n\u00a0\u00a0\u00a0\u00a0\u00a0 case (vId, vState) =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0(vState.cId, vState.wcc)\n\u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0 .reduceByKey(_+_)\n\u00a0\u00a0\u00a0 .map {\n\u00a0\u00a0\u00a0\u00a0\u00a0 case (cId, wcc) =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0cStats.value.get(cId).get.r * wcc\n\u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0 .sum\n\n\u00a0 total / graph.vertices.count\n\n}\n\nval wcc = computeWCC(wccGraph, bCommunityStats)\nval bWcc = sc.broadCast(wcc)\n```", "```scala\nval cDegrees = itGraph.aggregateMessages((ctx: EdgeContext[VState, ED, Map[VertexId, Int]]) => {\n\n\u00a0 ctx.sendToDst(\n\u00a0\u00a0\u00a0 Map(ctx.srcAttr.cId -> 1)\n\u00a0 )\n\n\u00a0 ctx.sendToSrc(\n\u00a0\u00a0\u00a0 Map(ctx.dstAttr.cId -> 1)\n\u00a0 )\n\n}, (e1: Map[VertexId, Int], e2: Map[VertexId, Int]) => {\n\u00a0 e1 |+| e2\n})\n```"]