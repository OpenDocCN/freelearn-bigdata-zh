["```scala\n<dependency>\u00a0\n \u00a0<groupId>com.google.code.gson</groupId>\n \u00a0<artifactId>gson</artifactId>\n \u00a0<version>2.3.1</version>\n</dependency>\n```", "```scala\nval sparkConf \u00a0= new SparkConf().setAppName(\"Twitter Extractor\")\nval sc = new SparkContext(sparkConf)\nval ssc = new StreamingContext(sc, Minutes(5))\n\nval filter = args\n\nval twitterStream = createTwitterStream(ssc, filter)\n\u00a0 .mapPartitions { it =>\n \u00a0\u00a0 \u00a0val gson = new GsonBuilder().create()\n \u00a0\u00a0 \u00a0it.map { s: Status =>\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Try(gson.toJson(s)).toOption\n \u00a0\u00a0 \u00a0}\n \u00a0}\n```", "```scala\ntwitterStream\n \u00a0.filter(_.isSuccess)\n \u00a0.map(_.get)\n \u00a0.saveAsTextFiles(\"/path/to/twitter\")\n```", "```scala\nssc.start()\nssc.awaitTermination()\n```", "```scala\n<dependency>\n \u00a0<groupId>org.twitter4j</groupId>\n \u00a0<artifactId>twitter4j-core</artifactId>\n \u00a0<version>4.0.4</version>\n</dependency>\n```", "```scala\nConfigurationBuilder builder = new ConfigurationBuilder();\nbuilder.setOAuthConsumerKey(apiKey);\nbuilder.setOAuthConsumerSecret(apiSecret);\nConfiguration configuration = builder.build();\n\nAccessToken token = new AccessToken(\n\u00a0 accessToken,\n\u00a0 accessTokenSecret\n);\n\nTwitter twitter =\n\u00a0 new TwitterFactory(configuration)\n\u00a0\u00a0\u00a0\u00a0  .getInstance(token);\n```", "```scala\nQuery q = new Query(filter);\nq.setSince(fromDate);\nq.setUntil(toDate);\nq.setCount(400);\n\nQueryResult r = twitter.search(q);\nList<Status> tweets = r.getTweets();\n```", "```scala\nMap<String, RateLimitStatus> rls = twitter.getRateLimitStatus(\"search\");\nSystem.out.println(rls.get(\"/search/tweets\"));\n\n/*\nRateLimitStatusJSONImpl{remaining=179, limit=180, resetTimeInSeconds=1482102697, secondsUntilReset=873}\n*/\n```", "```scala\nRateLimitStatus strl = rls.get(\"/search/tweets\");\nint totalTweets = 0;\nlong maxID = -1;\nfor (int i = 0; i < 400; i++) {\n\n \u00a0// throttling\n \u00a0if (strl.getRemaining() == 0)\n\u00a0\u00a0  Thread.sleep(strl.getSecondsUntilReset() * 1000L);\n\n \u00a0Query q = new Query(filter);\n \u00a0q.setSince(fromDate);\n \u00a0q.setUntil(toDate);\n \u00a0q.setCount(100);\n\n \u00a0// paging\n \u00a0if (maxID != -1) q.setMaxId(maxID - 1);\n\n \u00a0QueryResult r = twitter.search(q);\n \u00a0for (Status s: r.getTweets()) {\n \u00a0\u00a0 totalTweets++;\n \u00a0\u00a0\u00a0if (maxID == -1 || s.getId() < maxID)\n\u00a0\u00a0\u00a0\u00a0 maxID = s.getId();\n \u00a0\u00a0\u00a0\u00a0writer.println(gson.toJson(s));\n \u00a0}\n \u00a0strl = r.getRateLimitStatus();\n}\n```", "```scala\njava -Dtwitter.properties=twitter.properties /\n\u00a0 -jar trump-1.0.jar #maga 2016-11-08 2016-11-09 /\n\u00a0\u00a0/path/to/twitter-maga.json\n```", "```scala\ntwitter.token = XXXXXXXXXXXXXX\ntwitter.token.secret = XXXXXXXXXXXXXX\ntwitter.api.key = XXXXXXXXXXXXXX\ntwitter.api.secret = XXXXXXXXXXXXXX\n```", "```scala\n<dependency>\n\u00a0\u00a0<groupId>com.kcthota</groupId>\n\u00a0\u00a0<artifactId>emoji4j</artifactId>\n\u00a0\u00a0<version>5.0</version>\n</dependency>\n```", "```scala\nimport emoji4j.EmojiUtils\n\ndef clean = {\n\u00a0 var text = tweet.toLowerCase()\n\u00a0 text = text.replaceAll(\"https?:\\\\/\\\\/\\\\S+\", \"\")\n\u00a0 text = StringUtils.stripAccents(text)\n\u00a0 EmojiUtils.removeAllEmojis(text)\n\u00a0\u00a0\u00a0 .trim\n\u00a0\u00a0\u00a0 .toLowerCase()\n\u00a0\u00a0\u00a0 .replaceAll(\"rt\\\\s+\", \"\")\n\u00a0\u00a0\u00a0 .replaceAll(\"@[\\\\w\\\\d-_]+\", \"\")\n\u00a0\u00a0\u00a0 .replaceAll(\"[^\\\\w#\\\\[\\\\]:'\\\\.!\\\\?,]+\", \" \")\n\u00a0\u00a0\u00a0 .replaceAll(\"\\\\s+([:'\\\\.!\\\\?,])\\\\1\", \"$1\")\n \u00a0\u00a0 .replaceAll(\"[\\\\s\\\\t]+\", \" \")\n\u00a0\u00a0\u00a0 .replaceAll(\"[\\\\r\\\\n]+\", \". \")\n \u00a0\u00a0 .replaceAll(\"(\\\\w)\\\\1{2,}\", \"$1$1\") // avoid looooool \n \u00a0\u00a0 .replaceAll(\"#\\\\W\", \"\")\n \u00a0\u00a0 .replaceAll(\"[#':,;\\\\.]$\", \"\")\n \u00a0\u00a0 .trim\n}\n```", "```scala\nval eR = \"(:\\\\w+:)\".r\n\ndef emojis = {\n\u00a0 var text = tweet.toLowerCase()\n\u00a0 text = text.replaceAll(\"https?:\\\\/\\\\/\\\\S+\", \"\")\n\u00a0 eR.findAllMatchIn(EmojiUtils.shortCodify(text))\n\u00a0\u00a0  .map(_.group(1))\n\u00a0\u00a0  .filter { emoji =>\n\u00a0\u00a0\u00a0\u00a0  EmojiUtils.isEmoji(emoji)\n\u00a0\u00a0\u00a0 }.map(_.replaceAll(\"\\\\W\", \"\"))\n\u00a0\u00a0  .toArray\n}\n```", "```scala\n<dependency>\n \u00a0<groupId>edu.stanford.nlp</groupId>\n \u00a0<artifactId>stanford-corenlp</artifactId>\n \u00a0<version>3.5.0</version>\n \u00a0<classifier>models</classifier>\n</dependency>\n\n<dependency>\n \u00a0<groupId>edu.stanford.nlp</groupId>\n \u00a0<artifactId>stanford-corenlp</artifactId>\n \u00a0<version>3.5.0</version>\n</dependency>\n```", "```scala\ndef getAnnotator: StanfordCoreNLP = {\n  val p = new Properties()\n  p.setProperty(\n\u00a0\u00a0\u00a0 \"annotators\",\n\u00a0\u00a0\u00a0 \"tokenize, ssplit, pos, lemma, parse, sentiment\"\n  )\n \u00a0new StanfordCoreNLP(pipelineProps)\n}\n\ndef lemmatize(text: String,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 annotator: StanfordCoreNLP = getAnnotator) = {\n\n \u00a0val annotation = annotator.process(text.clean)\n \u00a0val sentences = annotation.get(classOf[SentencesAnnotation])\n \u00a0  sentences.flatMap { sentence =>\n \u00a0\u00a0 sentence.get(classOf[TokensAnnotation])\n\u00a0 .map { token =>\n \u00a0\u00a0\u00a0token.get(classOf[LemmaAnnotation])\n\u00a0 }\n\u00a0 .mkString(\" \")\n}\n\nval text = \"If you're bashing Trump and his voters and calling them a variety of hateful names, aren't you doing exactly what you accuse them?\"\n\nprintln(lemmatize(text))\n\n/*\nif you be bash trump and he voter and call they a variety of hateful name, be not you do exactly what you accuse they\n*/\n```", "```scala\ndef sentiment(coreMap: CoreMap) = {\n\n coreMap.get(classOf[SentimentCoreAnnotations.ClassName].match {\n \u00a0\u00a0 \u00a0case \"Very negative\" => 0\n \u00a0\u00a0 \u00a0case \"Negative\" => 1\n \u00a0\u00a0 \u00a0case \"Neutral\" => 2\n \u00a0\u00a0\u00a0 case \"Positive\" => 3\n \u00a0\u00a0\u00a0 case \"Very positive\" => 4\n \u00a0\u00a0\u00a0 case _ =>\n\u00a0\u00a0\u00a0\u00a0\u00a0  throw new IllegalArgumentException(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  s\"Could not get sentiment for [${coreMap.toString}]\"\n\u00a0\u00a0\u00a0\u00a0\u00a0  )\n  }\n}\n\ndef extractSentiment(text: String,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 annotator: StanfordCoreNLP = getSentimentAnnotator) = {\n\n \u00a0val annotation = annotator.process(text)\n \u00a0val sentences = annotation.get(classOf[SentencesAnnotation])\n \u00a0val totalScore = sentences map sentiment\n\n\u00a0 if (sentences.nonEmpty) {\n\u00a0\u00a0\u00a0 totalScore.sum / sentences.size()\n\u00a0\u00a0}\u00a0else {\n\u00a0\u00a0\u00a0\u00a02.0f\n\u00a0 }\n\n}\n\nextractSentiment(\"God bless America. Thank you Donald Trump!\")\n // 2.5\n\nextractSentiment(\"This is the most horrible day ever\")\n // 1.0\n```", "```scala\ncase class Tweet(\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0date: Long,\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0body: String,\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sentiment: Float,\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0state: Option[String],\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0geoHash: Option[String],\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0emojis: Array[String]\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0)\n```", "```scala\nval analyzeJson = (it: Iterator[String]) => {\n\n \u00a0implicit val format = DefaultFormats\n \u00a0val annotator = getAnnotator\n\u00a0 val sdf = new SimpleDateFormat(\"MMM d, yyyy hh:mm:ss a\")\n\n \u00a0it.map { tweet =>\n\n \u00a0\u00a0 val json = parse(tweet)\n\u00a0\u00a0 \u00a0val dateStr = (json \\ \"createdAt\").extract[String]\n\u00a0\u00a0\u00a0 val date = Try(\n\u00a0\u00a0\u00a0\u00a0\u00a0 sdf.parse(dateStr).getTime\n\u00a0\u00a0\u00a0 )\n \u00a0\u00a0\u00a0 .getOrElse(0L)\n\n\u00a0\u00a0\u00a0 val text = (json \\ \"text\").extract[String] \n\u00a0\u00a0\u00a0 val location = Try(\n\u00a0\u00a0\u00a0\u00a0\u00a0 (json \\ \"user\" \\ \"location\").extract[String]\n\u00a0\u00a0\u00a0 )\n\u00a0\u00a0\u00a0\u00a0 .getOrElse(\"\")\n\u00a0\u00a0\u00a0\u00a0 .toLowerCase()\n\n \u00a0\u00a0\u00a0 val state = Try {\n\u00a0\u00a0\u00a0\u00a0\u00a0  location.split(\"\\\\s\")\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0.map(_.toUpperCase())\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .filter { s =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0states.contains(s)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .head\n\u00a0\u00a0\u00a0  }\n\u00a0\u00a0\u00a0\u00a0 .toOption\n\n\u00a0\u00a0\u00a0 val cleaned = text.clean\n\n\u00a0\u00a0\u00a0 Tweet(\n\u00a0\u00a0\u00a0\u00a0 date,\n\u00a0\u00a0\u00a0\u00a0 cleaned.lemmatize(annotator),\n\u00a0\u00a0\u00a0\u00a0 cleaned.sentiment(annotator),\n\u00a0\u00a0\u00a0\u00a0 state, \n\u00a0\u00a0\u00a0\u00a0 text.emojis\n\u00a0\u00a0\u00a0 )\n\u00a0 }\n}\n\nval tweetJsonRDD = sc.textFile(\"/path/to/twitter\")\nval tweetRDD = twitterJsonRDD mapPartitions analyzeJson\ntweetRDD.toDF().show(5)\n\n/*\n+-------------+---------------+---------+--------+----------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 date|\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0body|sentiment|\u00a0\u00a0 state|\u00a0\u00a0\u00a0 emojis|\n+-------------+---------------+---------+--------+----------+\n|1478557859000|happy halloween| \u00a0\u00a0\u00a0\u00a0\u00a02.0|\u00a0\u00a0\u00a0 None [ghost]\u00a0\u00a0 |\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n|1478557860000|slave to the gr|\u00a0\u00a0\u00a0\u00a0\u00a0 2.5|\u00a0\u00a0\u00a0 None|[]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \n|1478557862000|why be he so pe|\u00a0\u00a0\u00a0\u00a0 \u00a03.0|Some(MD)|[]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n|1478557862000|marcador sentim|\u00a0\u00a0 \u00a0\u00a0\u00a02.0|\u00a0\u00a0\u00a0 None|[]\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 |\n|1478557868000|you mindset tow|\u00a0\u00a0\u00a0\u00a0\u00a0 2.0|\u00a0\u00a0\u00a0 None|[sparkles]|\n+-------------+---------------+---------+--------+----------+\n*/\n```", "```scala\ncase class Metric(name: String,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0time: Long,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0value: Double,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tags: Map[String, String],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0viz: Option[String] = None\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )\n```", "```scala\ndef expandedTweets = rdd.flatMap { tweet =>\n\u00a0 List(\"trump\", \"clinton\") filter { f =>\n\u00a0\u00a0\u00a0 tweet.body.contains(f)\n\u00a0\u00a0} map { tag =>\n\u00a0\u00a0\u00a0 (tag, tweet)\n\u00a0\u00a0}\n}\n```", "```scala\ndef buildViz(tone: Float) = {\n\u00a0 if (tone <= 1.5f) Some(\"SECRET\") else None: Option[String]\n}\n```", "```scala\ndef sentimentByState = {\n\u00a0 expandedTweets.map { case (tag, tweet) =>\n\u00a0\u00a0\u00a0 ((tag, tweet.date, tweet.state), tweet.sentiment)\n\u00a0\u00a0}\n\u00a0\u00a0.groupByKey()\n\u00a0\u00a0.mapValues { f =>\n\u00a0\u00a0\u00a0\u00a0f.sum / f.size\n\u00a0\u00a0}\n\u00a0\u00a0.map { case ((tag, date, state), sentiment) =>\n\u00a0\u00a0\u00a0\u00a0val viz = buildViz(sentiment)\n\u00a0\u00a0\u00a0\u00a0val meta = Map(\"state\" -> state) \n\u00a0\u00a0\u00a0\u00a0Metric(\"io.gzet.state.$tag\", date, sentiment, meta, viz)\n\u00a0\u00a0}\n}\n```", "```scala\ndef toPut = {\n\n\u00a0 val vizMap = if(viz.isDefined) {\n\u00a0\u00a0 \u00a0List(\"viz\" -> viz.get)\n\u00a0 } else {\n\u00a0\u00a0 \u00a0List[(String, String)]()\n\u00a0 }\n\n\u00a0\u00a0val strTags = vizMap\n\u00a0 \u00a0\u00a0.union(tags.toList)\n\u00a0 \u00a0\u00a0.map { case (k, v) => s\"$k=$v\" }\n\u00a0\u00a0\u00a0 .mkString(\" \")\n\n\u00a0 s\"put $name $time $value $strTags\"\n}\n\nimplicit class Metrics(rdd: RDD[Metric]) {\n\n\u00a0 def publish = {\n\n\u00a0\u00a0 \u00a0rdd.foreachPartition { it: Iterator[Metric] =>\n\n\u00a0\u00a0\u00a0 \u00a0\u00a0val sock = new Socket(timelyHost, timelyPort)\n\u00a0\u00a0\u00a0\u00a0 \u00a0val writer = new PrintStream(\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0sock.getOutputStream,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0true,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0StandardCharsets.UTF_8.name\n\u00a0\u00a0\u00a0\u00a0\u00a0 )\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 it.foreach { metric =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0writer.println(metric.toPut)\n\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0 writer.flush()\n\u00a0\u00a0\u00a0 }\n\n\u00a0 }\n}\n\ntweetRDD.sentimentByState.publish\n```", "```scala\n// Read \u00a0metrics from Timely\nval conf = AccumuloConfig(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"GZET\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"alice\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"alice\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"localhost:2181\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )\n\nval metricsRDD = sc.timely(conf, Some(\"io.gzet.state.*\"))\n```", "```scala\nimport org.apache.spark.mllib.feature.Word2Vec\n\nval corpusRDD = tweetRDD\n\u00a0\u00a0\u00a0.map(_.body.split(\"\\\\s\").toSeq)\n\u00a0\u00a0\u00a0.filter(_.distinct.length >= 4)\n\nval model = new Word2Vec().fit(corpusRDD)\n```", "```scala\nmodel.findSynonyms(\"#lockherup\", 10).foreach(println)\n\n/*\n(#hillaryforprison,2.3266071900089313)\n(#neverhillary,2.2890002973310066)\n(#draintheswamp,2.2440446323298175)\n(#trumppencelandslide,2.2392471034643604)\n(#womenfortrump,2.2331140131326874)\n(#trumpwinsbecause,2.2182999853485454)\n(#imwithhim,2.1950198833564563)\n(#deplorable,2.1570936207197016)\n(#trumpsarmy,2.155859656266577)\n(#rednationrising,2.146132149205829)\n*/ \u00a0\n```", "```scala\n[KING] is at [MAN] what [QUEEN] is at [?????] \n\n```", "```scala\nVKING - VQUEEN = VMAN - V???? \nV???? = VMAN - VKING + VQUEEN \n\n```", "```scala\nimport org.apache.spark.mllib.linalg.Vectors\n\ndef association(word1: String, word2: String, word3: String) = {\n\n\u00a0 val isTo = model\n\u00a0\u00a0  .getVectors\n\u00a0\u00a0  .get(word2)\n\u00a0\u00a0  .get\n\u00a0\u00a0  .zip(model.getVectors.get(word1).get)\n\u00a0\u00a0  .map(t => t._1 - t._2)\n\n\u00a0val what = model\n\u00a0\u00a0 .getVectors\n\u00a0\u00a0 .get(word3)\n\u00a0\u00a0 .get\n\n\u00a0val vec = isTo\n\u00a0\u00a0 .zip(what)\n\u00a0\u00a0 .map(t => t._1 + t._2)\n\u00a0\u00a0 .map(_.toDouble)\n\n Vectors.dense(vec)\n\n}\n\nval assoc = association(\"trump\", \"republican\", \"clinton\")\n\nmodel.findSynonyms(assoc, 1)\n\u00a0\u00a0\u00a0\u00a0 .foreach(println)\n\n*// (democrat,1.6838367309269164)* \n\n```", "```scala\nmodel.save(sc, \"/path/to/word2vec\")\n\nval retrieved = Word2VecModel.load(sc, \"/path/to/word2vec\")\n```", "```scala\nval bModel = sc.broadcast(model)\nval bDictionary = sc.broadcast(\n\u00a0 model.getVectors\n\u00a0\u00a0\u00a0 .keys\n\u00a0\u00a0\u00a0 .toList\n\u00a0\u00a0\u00a0 .zipWithIndex\n\u00a0\u00a0\u00a0 .map(l => (l._1, l._2.toLong + 1L))\n\u00a0\u00a0\u00a0 .toMap\n)\n\nimport org.apache.spark.graphx._\n\nval wordRDD = sc.parallelize(\n\u00a0 model.getVectors\n\u00a0\u00a0\u00a0 .keys\n\u00a0\u00a0\u00a0 .toSeq\n\u00a0\u00a0\u00a0 .filter(s => s.length > 3)\n)\n\nval word2EdgeRDD = wordRDD.mapPartitions { it =>\n\u00a0 val model = bModel.value\n\u00a0 val dictionary = bDictionary.value\n\n\u00a0 it.flatMap { from =>\n\u00a0\u00a0 \u00a0val synonyms = model.findSynonyms(from, 5)\n\u00a0\u00a0\u00a0 val tot = synonyms.map(_._2).sum\n\u00a0\u00a0\u00a0 synonyms.map { case (to, sim) =>\n\u00a0\u00a0\u00a0\u00a0 \u00a0val norm = sim / tot\n\u00a0\u00a0\u00a0\u00a0\u00a0 Edge(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0dictionary.get(from).get,\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dictionary.get(to).get,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0norm\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0)\n\u00a0\u00a0\u00a0\u00a0 \u00a0}\n\u00a0\u00a0 }\n}\n\nval word2Graph = Graph.fromEdges(word2EdgeRDD, 0L)\n\nword2Graph.cache()\nword2Graph.vertices.count()\n```", "```scala\nval cc = word2Graph\n\u00a0 .connectedComponents()\n\u00a0\u00a0.vertices\n\u00a0\u00a0.values\n\u00a0\u00a0.distinct\n\u00a0\u00a0.count\n\nprintln(s\"Do we still have faith in humanity? ${cc > 1L}\")\n// false\n```", "```scala\nimport org.apache.spark.graphx.lib.ShortestPaths\n\nval shortestPaths = ShortestPaths.run(graph, Seq(godwin))\n```", "```scala\nval depth = sc.broadcast(\n\u00a0\u00a0shortestPaths.vertices\n\u00a0\u00a0\u00a0\u00a0.values\n\u00a0\u00a0\u00a0\u00a0.filter(_.nonEmpty)\n\u00a0\u00a0\u00a0\u00a0.map(_.values.min)\n\u00a0\u00a0\u00a0\u00a0.max()\n)\n\nlogInfo(s\"Godwin depth is [${depth.value}]\")\n// 16\n\nshortestPaths.vertices.map { case (vid, hops) =>\n\u00a0\u00a0if(hops.nonEmpty) {\n\u00a0\u00a0\u00a0 val godwin = Option(\n\u00a0\u00a0\u00a0\u00a0\u00a0 math.min(hops.values.min / depth.value.toDouble, 1.0)\n\u00a0\u00a0\u00a0 )\n\u00a0\u00a0\u00a0\u00a0(vid, godwin)\n\u00a0\u00a0\u00a0} else {\n\u00a0\u00a0\u00a0\u00a0\u00a0(vid, None: Option[Double])\n\u00a0\u00a0\u00a0}\n}\n.filter(_._2.isDefined)\n.map { case (vid, distance) =>\n\u00a0 (vid, distance.get)\n}\n.collectAsMap()\n```", "```scala\nval size = sc.broadcast(\n\u00a0\u00a0word2Graph\n\u00a0\u00a0\u00a0 .vertices\n\u00a0\u00a0\u00a0\u00a0.count()\n\u00a0\u00a0\u00a0\u00a0.toInt\n)\n\nval indexedRowRDD = word2Graph.edges\n\u00a0 .map { case edge =>\n\u00a0\u00a0\u00a0 (edge.srcId,(edge.dstId.toInt, edge.attr))\n\u00a0 }\n\u00a0 .groupByKey()\n\u00a0 .map { case (id, it) =>\n\u00a0\u00a0\u00a0 new IndexedRow(id, Vectors.sparse(size.value, it.toSeq))\n\u00a0 }\n\nval m1 = new IndexedRowMatrix(indexedRowRDD)\nval m3 = m1.multiply(m2)\n```", "```scala\nGodwin.randomWalks(graph, \"love\", 100) \n\n```", "```scala\ncase class Word2Score(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 trump: Double,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0clinton: Double,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0love: Double,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hate: Double\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )\n\ndef cosineSimilarity(x: Array[Float],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0y: Array[Float]): Double = {\n\n\u00a0 val dot = x.zip(y).map(a => a._1 * a._2).sum\n\u00a0 val magX = math.sqrt(x.map(i => i*i).sum)\n\u00a0 val magY = math.sqrt(y.map(i => i*i).sum)\n\n\u00a0 dot / (magX * magY)\n}\n\nval trump = model.getVectors.get(\"trump\").get\nval clinton = model.getVectors.get(\"clinton\").get\nval love = model.getVectors.get(\"love\").get\nval hate = model.getVectors.get(\"hate\").get\n\nval word2Score = sc.broadcast(\n\u00a0\u00a0\u00a0model.getVectors.map { case (word, vector) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0val scores = Word2Score(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 cosineSimilarity(vector, trump),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cosineSimilarity(vector, clinton),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cosineSimilarity(vector, love),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cosineSimilarity(vector, hate)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )\n\u00a0\u00a0\u00a0\u00a0\u00a0(word, scores)\n\u00a0\u00a0\u00a0}\n)\n```", "```scala\nimport org.apache.spark.sql.functions._\nimport collection.mutable.WrappedArray\n\nval featureTrump = udf((words:WrappedArray[String]) => {\n\u00a0 words.map(word2Score.value.get)\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0.map(_.get.trump)\n\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0.sum / words.length\n})\n\nval featureClinton = udf((words:WrappedArray[String]) => {\n\u00a0 words.map(word2Score.value.get)\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0.map(_.get.clinton)\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0.sum / words.length\n})\n\nval featureLove = udf((words:WrappedArray[String]) => {\n\u00a0 words.map(word2Score.value.get)\n\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0.map(_.get.love)\n\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0.sum / words.length\n})\n\nval featureHate = udf((words:WrappedArray[String]) => {\n\u00a0 words.map(word2Score.value.get)\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0.map(_.get.hate)\n\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0.sum / words.length\n})\n```", "```scala\nval lov = sc.broadcast(\n\u00a0 Set(\"heart\", \"heart_eyes\", \"kissing_heart\", \"hearts\", \"kiss\")\n)\n\nval joy = sc.broadcast(\n\u00a0 Set(\"joy\", \"grin\", \"laughing\", \"grinning\", \"smiley\", \"clap\", \"sparkles\")\n)\n\nval jok = sc.broadcast(\n\u00a0 Set(\"wink\", \"stuck_out_tongue_winking_eye\", \"stuck_out_tongue\")\n)\n\nval sad = sc.broadcast(\n\u00a0 Set(\"weary\", \"tired_face\", \"unamused\", \"frowning\", \"grimacing\", \"disappointed\")\n)\n\nval cry = sc.broadcast(\n\u00a0 Set(\"sob\", \"rage\", \"cry\", \"scream\", \"fearful\", \"broken_heart\")\n)\n\nval allEmojis = sc.broadcast(\n\u00a0 lov.value ++ joy.value ++ jok.value ++ sad.value ++ cry.value\n)\n```", "```scala\nval buildVector = udf((sentiment: Double, tone: Double, trump: Double, clinton: Double, love: Double, hate: Double, godwin: Double) => {\n\u00a0 Vectors.dense(\n\u00a0\u00a0\u00a0\u00a0Array(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sentiment,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0tone,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trump,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0clinton,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0love,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hate,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0godwin\n\u00a0\u00a0\u00a0\u00a0)\n\u00a0\u00a0)\n})\n\nval featureTweetDF = tweetRDD.toDF\n  .withColumn(\"words\", extractWords($\"body\"))\n  .withColumn(\"tone\", featureEmojis($\"emojis\"))\n  .withColumn(\"trump\", featureTrump($\"body\"))\n  .withColumn(\"clinton\", featureClinton($\"body\"))\n  .withColumn(\"godwin\", featureGodwin($\"body\"))\n  .withColumn(\"love\", featureLove($\"words\"))\n  .withColumn(\"hate\", featureHate($\"words\"))\n  .withColumn(\"features\",\n\u00a0\u00a0\u00a0 buildVector(\n\u00a0\u00a0\u00a0\u00a0\u00a0 $\"sentiment\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 $\"tone\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 $\"trump\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 $\"clinton\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 $\"love\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 $\"hate\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 $\"godwin\")\n\u00a0\u00a0\u00a0 )\n\nimport org.apache.spark.ml.feature.Normalizer\n\nval normalizer = new Normalizer()\n\u00a0 .setInputCol(\"features\")\n\u00a0 .setOutputCol(\"vector\")\n\u00a0 .setP(1.0)\n```", "```scala\nimport org.apache.spark.ml.clustering.KMeans\n\nval kmeansModel = new KMeans()\n\u00a0 .setFeaturesCol(\"vector\")\n\u00a0 .setPredictionCol(\"cluster\")\n\u00a0 .setK(5)\n\u00a0 .setMaxIter(Int.MaxValue)\n\u00a0 .setInitMode(\"k-means||\")\n\u00a0 .setInitSteps(10)\n\u00a0 .setTol(0.01)\n\u00a0 .fit(vectorTweetDF)\n```", "```scala\nimport org.apache.spark.ml.clustering.KMeansModel\n\nval centers = sc.broadcast(kmeansModel.clusterCenters)\nimport org.apache.spark.mllib.linalg.Vector\n\nval euclidean = udf((v: Vector, cluster: Int) => {\n \u00a0 math.sqrt(centers.value(cluster).toArray.zip(v.toArray).map {\n\u00a0\u00a0\u00a0 case (x1, x2) => math.pow(x1 - x2, 2)\n\u00a0 }\n\u00a0 .sum)\n})\n```", "```scala\nval predictionDF = kmeansModel\n \u00a0 .transform(vectorTweetDF)\n \u00a0 .withColumn(\"distance\", euclidean($\"vector\", $\"cluster\"))\n\npredictionDF.write.saveAsTable(\"twitter\")\n```"]