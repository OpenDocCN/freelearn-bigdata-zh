["```scala\nL1 cache \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 0.5 ns\n```", "```scala\nscala> val i = Integer.MAX_VALUE\ni: Int = 2147483647\n\nscala> i + 1\nres1: Int = -2147483648\n```", "```scala\nscala> val distanceBetweenStars = Seq(2147483647, 2147483647)\ndistanceBetweenStars: Seq[Int] = List(2147483647, 2147483647)\n\nscala> val rdd = spark.sparkContext.parallelize(distanceBetweenStars)\nrdd: org.apache.spark.rdd.RDD[Int] =\u00a0 ...\n\nscala> rdd.reduce(_+_)\nres1: Int = -2\n```", "```scala\nscala> distanceBetweenStars.toDS.reduce(_+_)\nres2: Int = -2\n```", "```scala\nscala> val bigNumber = Float.MaxValue\nbigNumber: Float = 3.4028235E38\n\nscala> val verySmall = Int.MaxValue / bigNumber\nverySmall: Float = 6.310888E-30\n\nscala> val almostAsBig = bigNumber - verySmall\nalmostAsBig: Float = 3.4028235E38\n\nscala> bigNumber - almostAsBig\nres2: Float = 0.0\n```", "```scala\n\u00a0\u00a0 def merge(self, value):\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 delta = value - self.mu\n \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0self.n += 1\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self.mu += delta / self.n\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self.m2 += delta * (value - self.mu)\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self.maxValue = maximum(self.maxValue, value)\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 self.minValue = minimum(self.minValue, value)\n```", "```scala\ncontent\n\u00a0 .as[Content]\n\u00a0 .map{\n\u00a0\u00a0\u00a0 _.body match {\n\u00a0\u00a0\u00a0\u00a0\u00a0 case b if b.isEmpty\u00a0 => (\"NOT FOUND\",1)\n\u00a0\u00a0\u00a0\u00a0\u00a0 case _ => (\"FOUND\",1)\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n\u00a0 .groupByKey(_._1)\n\u00a0 .reduceGroups {\n\u00a0\u00a0\u00a0\u00a0 (v1,v2) => (v1._1, v1._2 + v2._2)\n\u00a0 }\n\u00a0 .toDF(\"NEWS ARTICLE\",\"COUNT\")\n\u00a0 .show\n```", "```scala\n+------------+------+\n|NEWS ARTICLE| COUNT|\n+------------+------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 FOUND|154572|\n|\u00a0\u00a0 NOT FOUND|190285|\n+------------+------+\n```", "```scala\ncase class PersonTone(article: String, name: String, tone: Double)\n\nobject Brexit {\n\u00a0 def averageNewsSentiment(df: DataFrame): Dataset[(String,Double)] = ???\n}\n```", "```scala\n            <dependency>\n    \u00a0         <groupId>com.github.marklister</groupId>\n    \u00a0         <artifactId>product-\n              collections_${scala.binary.version}</artifactId>\n    \u00a0         <version>1.4.5</version>\n            <scope>test</scope>\n            </dependency>\n\n            <dependency>\n    \u00a0        <groupId>org.scalatest</groupId>\n    \u00a0        <artifactId>scalatest_${scala.binary.version}  </artifactId>\n    \u00a0        <scope>test</scope>\n            </dependency>\n    ```", "```scala\nimport java.io.StringReader\nimport io.gzet.test.SparkFunSuite\nimport org.scalatest.Matchers\nimport com.github.marklister.collections.io._\n\nclass RegressionTest extends SparkFunSuite with Matchers {\n\n\u00a0 localTest(\"should compute average sentiment\") { spark =>\n\n\u00a0\u00a0\u00a0 // given\n\u00a0\u00a0\u00a0 val input = CsvParser(PersonTone)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .parse(new StringReader(\n\"\"\"http://www.ibtimes.co.uk/...,Nigel Farage,-2.4725485679183\nhttp://www.computerweekly.co.uk/...,Iain Duncan-Smith,1.95886385896181\nhttp://www.guardian.com/...,Nigel Farage,3.79346680716544\nhttp://nbc-2.com/...,David Cameron,0.195886385896181\nhttp://dailyamerican.com/...,David Cameron,-5.82329317269076\"\"\"))\n\n\u00a0\u00a0\u00a0 val expectedOutput = Array(\n\u00a0\u00a0\u00a0\u00a0\u00a0 (\"Nigel Farage\", 1.32091823925),\n\u00a0\u00a0\u00a0\u00a0\u00a0 (\"Iain Duncan-Smith\",1.95886385896181),\n\u00a0\u00a0\u00a0\u00a0\u00a0 (\"David Cameron\",-5.62740678679))\n\n\u00a0\u00a0\u00a0 // when\n\u00a0\u00a0\u00a0 val actualOutput =\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Brexit.averageNewsSentiment(input.toDS).collect()\n\n\u00a0\u00a0\u00a0 // test\n\u00a0\u00a0\u00a0 actualOutput should have length expectedOutput.length\n\u00a0\u00a0\u00a0 actualOutput.toSet should be (expectedOutput.toSet)\n\u00a0 }\n}\n```", "```scala\npersonDS                             personRDD\n\u00a0 .groupBy($\"name\")                  \u00a0 .map(p => (p.person,1)) \n\u00a0 .count                             \u00a0 .reduceByKey(_+_)\n\u00a0 .sort($\"count\".desc)               \u00a0 .sortBy(_._2,false)\n\u00a0 .show\n\n36 seconds (Dataset API)             99 seconds (RDD API)\n```", "```scala\nRDDs (using ds.rdd) when you need the flexibility to compute something not available on the higher level API.\n```", "```scala\n            val sortedPairs = rdd.sortByKey() \n            sortedPairs.filterByRange(lower, upper) \n\n    ```", "```scala\n            SparkSession\n    \u00a0         .builder()\n    \u00a0         .config(\"spark.io.compression.codec\", \"lzf\")\n    ```", "```scala\nval toBeBroadcast = smallDataset.collect\nval bv = spark.sparkContext.broadcast(toBeBroadcast)\n```", "```scala\nds.mapPartitions { partition =>\n\n\u00a0\u00a0\u00a0 val smallDataset = bv.value\n\u00a0\u00a0\u00a0 partition map { r => f(r, bv.value) }\n}\n```", "```scala\nbv.destroy() \n\n```", "```scala\nSparkSession\n\u00a0 .builder()\n\u00a0 .config(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\")\n```", "```scala\n// (person:String, article:Array[String])\nval rdd:RDD[(String,Array[String])] = ...\n```", "```scala\ncase class ArticleStats(minLength:Long,maxLength:Long,mfuWord:(String,Int))\n```", "```scala\nval init = (a:Array[String]) => {\n\u00a0 ArticleStats(a)\n}\n\nval update = (stats:ArticleStats, a:Array[String]) => {\n\u00a0 stats |+| ArticleStats(a)\n}\n\nval merge = (s1:ArticleStats,s2:ArticleStats) => {\n\u00a0 s1 |+| s2\n}\n```", "```scala\nobject ArticleStats {\n\u00a0 def apply(a:Array[String]) =\n\u00a0\u00a0\u00a0 new ArticleStats(calcMin(a),calcMax(a),findMFUWord(a))\n\u00a0...\n}\n\nimplicit object statsSemiGroup extends SemiGroup[ArticleStats] {\n\u00a0 def append(a: ArticleStats, b: ArticleStats) : ArticleStats = ???\n}\n```", "```scala\nrdd.mapValues { case value => init(value) }\n\u00a0\u00a0 .groupByKey()\n\u00a0\u00a0 .mapValues { case list => list.fold(merge) } // note: update not used\n```", "```scala\nrdd.map(init(_._2)).reduceByKey(merge)\n```", "```scala\nrdd.aggregateByKey(ArticleStats())(update,merge) \n\nupdate and merge being called, however init is not used directly. Instead, an empty container, in the form of a blank ArticleStats object, is provided explicitly for the purposes of initialization. This syntax is closer to that of fold, so it's useful if you're more familiar with that style.\n```", "```scala\nrdd.combineByKey(init,update,merge)\n```", "```scala\n--num-executors (YARN-only setting [as of Spark 2.0])\n--executor-cores\n--executor-memory\n--total-executor-cores\n```", "```scala\nds.rdd.getNumPartitions()\n```", "```scala\nds.repartition(400)\n\u00a0 .groupByKey($\"key\")\n\u00a0 .reduceGroups(f)\n\u00a0 .explain\n\n...\n\n+- Exchange RoundRobinPartitioning(400)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 +- *BatchedScan parquet\n```", "```scala\nrdd.reduceByKey(f,400)\n\u00a0\u00a0\u00a0 .toDebugString\n\nres1: String =\n(400) ShuffledRDD[11] at reduceByKey at <console>:26 []\n\u00a0 +-(7) MapPartitionsRDD[10] at map at <console>:26 []\n\u00a0\u00a0\u00a0\u00a0 |\u00a0 MapPartitionsRDD[6] at rdd at <console>:26 []\n\u00a0\u00a0\u00a0\u00a0 |\u00a0 MapPartitionsRDD[5] at rdd at <console>:26 []\n\u00a0\u00a0\u00a0\u00a0 |\u00a0 MapPartitionsRDD[4] at rdd at <console>:26 []\n\u00a0\u00a0\u00a0\u00a0 |\u00a0 FileScanRDD[3] at rdd at <console>:26 []\n```", "```scala\nrdd filter {\n\u00a0\u00a0 case (k,v) => isPopular(k)\n}\n.map {\n\u00a0\u00a0 case (k,v) => (k + r.nextInt(n), v)\n}\n```", "```scala\nrdd.reduceByKey(_+_).sortBy(_._2,false) // inefficient for large groups\n```", "```scala\ncase class Mention(name:String, article:String, published:Long) \n\n```", "```scala\ncase class SortKey(name:String, published:Long)\n```", "```scala\nclass GroupingPartitioner(partitions: Int) extends Partitioner {\n\n \u00a0\u00a0 override def numPartitions: Int = partitions\n\n\u00a0\u00a0\u00a0 override def getPartition(key: Any): Int = {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 val groupBy = key.asInstanceOf[SortKey]\n\u00a0\u00a0\u00a0\u00a0\u00a0 groupBy.name.hashCode() % numPartitions\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n```", "```scala\nimplicit val sortBy: Ordering[SortKey] = Ordering.by(m => m.published)\n```", "```scala\nval pairs = mentions.rdd.keyBy(m => SortKey(m.name, m.published))\npairs.repartitionAndSortWithinPartitions(new GroupingPartitioner(n))\n```", "```scala\nrdd.countApprox() \nrdd.countByValueApprox() \nrdd.countApproxDistinct() \n\n```", "```scala\nds.cache \nds.count \n\n```"]