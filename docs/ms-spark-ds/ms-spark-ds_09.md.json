["```scala\n$ wget https://archive.org/download/stackexchange/beer.stackexchange.com.7z\n$ 7z e beer.stackexchange.com.7z\n```", "```scala\nimport io.gzet.tagging.stackoverflow.StackBootstraping\n\nval spark = SparkSession.builder()\n\u00a0 .appName(\"StackExchange\")\n\u00a0 .getOrCreate()\n\nval sc = spark.sparkContext\nval rdd = sc.textFile(\"/path/to/posts.xml\")\nval brewing = StackBootstraping.parse(rdd)\n\nbrewing.show(5)\n\n+--------------------+--------------------+\n|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 body|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 tags|\n+--------------------+--------------------+\n|I was offered a b...|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [hops]|\n|As far as we know...|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [history]|\n|How is low/no alc...|\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 [brewing]|\n|In general, what'...|[serving, tempera...|\n|Currently I am st...| [pilsener, storage]|\n+--------------------+--------------------+\n```", "```scala\nval labelMap = brewing\n\u00a0 .select(\"tags\")\n\u00a0 .withColumn(\"tag\", explode(brewing(\"tags\")))\n\u00a0 .select(\"tag\")\n\u00a0 .distinct()\n\u00a0 .rdd\n\u00a0 .map(_.getString(0)).zipWithIndex()\n\u00a0 .mapValues(_.toDouble + 1.0d)\nlabelMap.take(5).foreach(println)\n\n/*\n(imperal-stout,1.0)\n(malt,2.0)\n(lent,3.0)\n(production,4.0)\n(local,5.0)\n*/\n```", "```scala\n<dependency>\n \u00a0 <groupId>org.apache.lucene</groupId>\n \u00a0 <artifactId>lucene-analyzers-common</artifactId>\n \u00a0 <version>4.10.1</version>\n </dependency>\n```", "```scala\ndef stem(rdd: RDD[(String, Array[String])]) = {\n\n \u00a0val replacePunc = \"\"\"\\\\W\"\"\".r\n \u00a0val replaceDigitOnly = \"\"\"\\\\s\\\\d+\\\\s\"\"\".r\n\n \u00a0rdd mapPartitions { it =>\n\n \u00a0\u00a0\u00a0val analyzer = new EnglishAnalyzer\n \u00a0\u00a0\u00a0it map { case (body, tags) =>\n \u00a0\u00a0\u00a0\u00a0 val content1 = replacePunc.replaceAllIn(body, \" \")\n \u00a0\u00a0\u00a0\u00a0\u00a0val content = replaceDigitOnly.replaceAllIn(content1, \" \")\n \u00a0\u00a0\u00a0\u00a0 val tReader = new StringReader(content)\n \u00a0\u00a0\u00a0\u00a0 val tStream = analyzer.tokenStream(\"contents\", tReader)\n \u00a0\u00a0\u00a0\u00a0 val term = tStream.addAttribute(classOf[CharTermAttribute])\n \u00a0\u00a0\u00a0\u00a0\u00a0 tStream.reset()\n \u00a0\u00a0\u00a0\u00a0 val terms = collection.mutable.MutableList[String]()\n \u00a0\u00a0\u00a0\u00a0 while (tStream.incrementToken) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0  val clean = term.toString\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if (!clean.matches(\".*\\\\d.*\") && clean.length > 3) {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 terms += clean\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n \u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0 tStream.close()\n \u00a0\u00a0\u00a0\u00a0\u00a0(terms.toArray, tags)\n \u00a0\u00a0\u00a0 }\n\n \u00a0}\n```", "```scala\nval hashingTf = new HashingTF()\nval normalizer = new Normalizer()\n\nval labeledCorpus = stem(df map { row =>\n\u00a0 val body = row.getString(0)\n\u00a0 val tags = row.getAs[mutable.WrappedArray[String]](1)\n\u00a0 (body, tags)\n})\n\nval labeledPoints = labeledCorpus flatMap { case (corpus, tags) =>\n\u00a0 val vector = hashingTf.transform(corpus)\n\u00a0 val normVector = normalizer.transform(vector)\n\u00a0 tags map { tag =>\n  \u00a0 val label = bLabelMap.value.getOrElse(tag, 0.0d)\n\u00a0\u00a0\u00a0 LabeledPoint(label, normVector)\n\u00a0 }\n}\n```", "```scala\nlabeledPoints.cache()\nval model: NaiveBayesModel = NaiveBayes.train(labeledPoints)\nlabeledPoints.unpersist(blocking = false)\n\nmodel\n\u00a0 .predict(testPoints)\n\u00a0 .map { prediction =>\n \u00a0 \u00a0\u00a0bLabelMap.value.map(_.swap).get(prediction).get\n \u00a0 }\n\u00a0 .zip(testLabels)\n\u00a0 .toDF(\"predicted\",\"original\")\n\u00a0 .show(5)\n\n+---------+-----------+\n| original|\u00a0 predicted|\n+---------+-----------+\n|\u00a0 brewing|\u00a0\u00a0\u00a0 brewing|\n|\u00a0\u00a0\u00a0\u00a0\u00a0 ipa|\u00a0\u00a0 hangover|\n| hangover|\u00a0\u00a0 hangover|\n| drinking|\u00a0\u00a0 drinking|\n| pilsener|\u00a0\u00a0 pilsener|\n+---------+-----------+\n```", "```scala\ndef train(rdd: RDD[(String, Array[String])]): ClassifierModel\ndef predict(rdd: RDD[String]): RDD[String]\n```", "```scala\nval sparkConf = new SparkConf().setAppName(\"GZET\")\nval ssc = new StreamingContext(sparkConf, Minutes(15))\nval sc = ssc.sparkContext\n```", "```scala\n$ kafka-server-start /usr/local/etc/kafka/server.properties > /var/log/kafka/kafka-server.log 2>&1 &\n\n$ kafka-topics --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic gzet\n```", "```scala\n$ cat ${FILE} | awk '{print $NF}' | sort | uniq | kafka-console-producer --broker-list localhost:9092 --topic gzet\n```", "```scala\n<dependency>\n \u00a0 <groupId>org.apache.spark</groupId>\n \u00a0 <artifactId>spark-streaming-kafka-0-8_2.11</artifactId>\n\u00a0  <version>2.0.0</version>\n</dependency>\n```", "```scala\ndef createGdeltStream(ssc: StreamingContext) = {\n \u00a0 KafkaUtils.createStream(\n \u00a0\u00a0\u00a0 ssc,\n \u00a0\u00a0\u00a0 \"localhost:2181\",\n \u00a0\u00a0\u00a0 \"gzet\",\n \u00a0\u00a0\u00a0 Map(\"gzet\" -> 10)\n \u00a0 ).values\n }\n\nval gdeltUrlStream: DStream[String] = createGdeltStream(ssc)\n```", "```scala\n<dependency>\n \u00a0 <groupId>org.apache.bahir</groupId>\n \u00a0 <artifactId>spark-streaming-twitter_2.11</artifactId>\n\u00a0  <version>2.0.0</version>\n</dependency>\n```", "```scala\nimport twitter4j.auth.OAuthAuthorization\nimport twitter4j.conf.ConfigurationBuilder\n\ndef getTwitterConfiguration = {\n\n\u00a0 val builder = new ConfigurationBuilder()\n\n\u00a0 builder.setOAuthConsumerKey(\"XXXXXXXXXXXXXXX\")\n\u00a0 builder.setOAuthConsumerSecret(\"XXXXXXXXXXXX\")\n\u00a0\u00a0builder.setOAuthAccessToken(\"XXXXXXXXXXXXXXX\")\n\u00a0 builder.setOAuthAccessTokenSecret(\"XXXXXXXXX\")\n\n\u00a0 val configuration = builder.build()\n\u00a0 Some(new OAuthAuthorization(configuration))\n\n}\n```", "```scala\ndef createTwitterStream(ssc: StreamingContext) = {\n \u00a0 TwitterUtils.createStream(\n \u00a0\u00a0\u00a0 ssc,\n \u00a0\u00a0\u00a0 getTwitterConfiguration,\n \u00a0\u00a0\u00a0 Array[String]()\n \u00a0 )\n}\n\nval twitterStream: DStream[Status] = createTwitterStream(ssc)\ngetText method that returns the tweet body:\n```", "```scala\nval body: String = status.getText()\nval user: User = status.getUser()\nval contributors: Array[Long] = status.getContributors()\nval createdAt: Long = status.getCreatedAt()\n../..\n```", "```scala\ndef extractTags(tweet: String) = {\n \u00a0StringUtils.stripAccents(tweet.toLowerCase())\n \u00a0\u00a0\u00a0.split(\"\\\\s\")\n \u00a0\u00a0\u00a0.filter { word =>\n \u00a0\u00a0\u00a0\u00a0 word.startsWith(\"#\") &&\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0word.length > minHashTagLength &&\n \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0word.matches(\"#[a-z]+\")\n \u00a0\u00a0\u00a0}\n}\n\ndef extractUrls(tweet: String) = {\n \u00a0tweet.split(\"\\\\s\")\n \u00a0\u00a0\u00a0.filter(_.startsWith(\"http\"))\n \u00a0\u00a0\u00a0.map(_.trim)\n \u00a0\u00a0\u00a0.filter(url => Try(new URL(url)).isSuccess)\n}\n\ndef getLabeledUrls(twitterStream: DStream[Status]) = {\n\u00a0 twitterStream flatMap { tweet =>\n\u00a0 \u00a0\u00a0val tags = extractTags(tweet.getText)\n\u00a0\u00a0\u00a0 val urls = extractUrls(tweet.getText)\n\u00a0\u00a0\u00a0 urls map { url =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 (url, tags)\n\u00a0 \u00a0\u00a0}\n\u00a0 }\n}\n\nval labeledUrls = getLabeledUrls(twitterStream)\n```", "```scala\ndef getTrends(twitterStream: DStream[Status]) = {\n\n\u00a0 val stream = twitterStream\n\u00a0\u00a0\u00a0 .flatMap { tweet =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0extractTags(tweet.getText)\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 .map(_ -> 1)\n\u00a0\u00a0\u00a0 .reduceByKeyAndWindow(_ + _, Minutes(windowSize))\n\n\u00a0 stream.foreachRDD { rdd =>\n\u00a0   val top10 = rdd.sortBy(_._2, ascending = false).take(10)\n\u00a0\u00a0\u00a0 top10.foreach { case (hashTag, count) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 println(s\"[$hashTag] - $count\")\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n\n\u00a0 stream\n}\n\nval twitterTrend = getTrends(twitterStream)\n```", "```scala\nval joinFunc = (labeledUrls: RDD[(String, Array[String])], twitterTrend: RDD[(String, Int)]) => {\n\n \u00a0 val sc = twitterTrend.sparkContext\n\u00a0  val leaderBoard = twitterTrend\n \u00a0\u00a0\u00a0 .sortBy(_._2, ascending = false)\n \u00a0\u00a0\u00a0 .take(100)\n \u00a0\u00a0\u00a0 .map(_._1)\n\n   val bLeaderBoard = sc.broadcast(leaderBoard)\n\n \u00a0 labeledUrls\n\u00a0\u00a0\u00a0  .flatMap { case (url, tags) =>\n \u00a0\u00a0\u00a0  \u00a0tags map (tag => (url, tag))\n \u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0  .filter { case (url, tag) =>\n \u00a0\u00a0\u00a0 \u00a0\u00a0bLeaderBoard.value.contains(tag)\n \u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0  .groupByKey()\n  \u00a0\u00a0 .mapValues(_.toArray.distinct)\n\n }\n\n val labeledTrendUrls = labeledUrls\n\u00a0  .transformWith(twitterTrend, joinFunc)\n```", "```scala\ndef expandUrl(url: String) : String = {\n\n \u00a0var connection: HttpURLConnection = null\n \u00a0try {\n\n \u00a0\u00a0\u00a0connection = new URL(url)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .openConnection\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .asInstanceOf[HttpURLConnection]\n\n \u00a0\u00a0\u00a0connection.setInstanceFollowRedirects(false)\n \u00a0\u00a0\u00a0connection.setUseCaches(false)\n \u00a0\u00a0\u00a0connection.setRequestMethod(\"GET\")\n \u00a0\u00a0\u00a0connection.connect()\n\n \u00a0\u00a0\u00a0val redirectedUrl = connection.getHeaderField(\"Location\")\n\n\u00a0\u00a0\u00a0 if(StringUtils.isNotEmpty(redirectedUrl)){\n \u00a0\u00a0\u00a0\u00a0\u00a0 redirectedUrl\n \u00a0\u00a0\u00a0 } else {\n \u00a0\u00a0\u00a0\u00a0\u00a0 url\n \u00a0\u00a0\u00a0 }\n\n \u00a0 } catch {\n \u00a0\u00a0\u00a0 case e: Throwable => url\n \u00a0 } finally {\n \u00a0\u00a0\u00a0 if(connection != null)\n \u00a0\u00a0\u00a0\u00a0\u00a0 connection.disconnect()\n \u00a0 }\n }\n\n def expandUrls(tStream: DStream[(String, Array[String])]) = {\n \u00a0 tStream\n\u00a0\u00a0\u00a0  .map { case (url, tags) =>\n \u00a0\u00a0\u00a0 \u00a0\u00a0(HtmlHandler.expandUrl(url), tags)\n \u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0  .filter { case (url, tags) =>\n \u00a0\u00a0\u00a0 \u00a0\u00a0!untrustedSources.value.contains(url)\n \u00a0 \u00a0\u00a0}\n}\n\nval expandedUrls = expandUrls(labeledTrendUrls)\n```", "```scala\ndef fetchHtmlContent(tStream: DStream[(String, Array[String])]) = {\n\n \u00a0tStream\n\u00a0\u00a0\u00a0 .reduceByKey(_++_.distinct)\n\u00a0\u00a0\u00a0 .mapPartitions { it =>\n\n\u00a0\u00a0\u00a0 \u00a0\u00a0val htmlFetcher = new HtmlHandler()\n \u00a0\u00a0\u00a0 \u00a0val goose = htmlFetcher.getGooseScraper\n \u00a0\u00a0\u00a0 \u00a0val sdf = new SimpleDateFormat(\"yyyyMMdd\")\n\n \u00a0\u00a0\u00a0 \u00a0it.map { case (url, tags) =>\n \u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0val content = htmlFetcher.fetchUrl(goose, url, sdf)\n \u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0(content, tags)\n \u00a0\u00a0\u00a0 \u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0 .filter { case (contentOpt, tags) =>\n \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0contentOpt.isDefined &&\n \u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0contentOpt.get.body.isDefined &&\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0contentOpt.get.body.get.split(\"\\\\s+\").length >= 500\n \u00a0\u00a0\u00a0 \u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0 .map { case (contentOpt, tags) =>\n \u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0(contentOpt.get.body.get, tags)\n \u00a0\u00a0\u00a0 \u00a0}\n\n}\n\nval twitterContent = fetchHtmlContent(expandedUrls)\n```", "```scala\ndef fetchHtmlContent(urlStream: DStream[String]) = {\n\n \u00a0urlStream\n\u00a0\u00a0\u00a0 .map(_ -> 1)\n\u00a0\u00a0\u00a0 .reduceByKey()\n\u00a0\u00a0\u00a0 .keys\n\u00a0\u00a0\u00a0 .mapPartitions { urls =>\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 val sdf = new SimpleDateFormat(\"yyyyMMdd\")\n \u00a0\u00a0\u00a0 \u00a0val htmlHandler = new HtmlHandler()\n \u00a0\u00a0\u00a0 \u00a0val goose = htmlHandler.getGooseScraper\n \u00a0\u00a0\u00a0 \u00a0urls.map { url =>\n \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 htmlHandler.fetchUrl(goose, url, sdf)\n \u00a0\u00a0\u00a0 \u00a0}\n\n \u00a0 \u00a0}\n\u00a0\u00a0\u00a0 .filter { content =>\n \u00a0\u00a0\u00a0 \u00a0content.isDefined &&\n \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0content.get.body.isDefined &&\n \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0content.get.body.get.split(\"\\\\s+\").length > 500\n \u00a0 \u00a0}\n\u00a0\u00a0\u00a0 .map(_.get)\n}\n\nval gdeltContent = fetchHtmlContent(gdeltUrlStream)\n```", "```scala\n$ curl -XPUT 'http://localhost:9200/gzet'\n$ curl -XPUT 'http://localhost:9200/gzet/_mapping/twitter' -d '\n{\n \u00a0\u00a0\u00a0\"_ttl\" : {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\"enabled\" : true\n \u00a0 \u00a0},\n \u00a0\u00a0\u00a0\"properties\": {\n \u00a0\u00a0\u00a0\u00a0\u00a0\"body\": {\n \u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\"type\": \"string\"\n \u00a0\u00a0\u00a0\u00a0\u00a0},\n \u00a0\u00a0\u00a0\u00a0\u00a0\"time\": {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"date\",\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"format\": \"yyyy-MM-dd HH:mm:ss\"\n \u00a0\u00a0\u00a0\u00a0\u00a0},\n \u00a0\u00a0\u00a0\u00a0\u00a0\"tags\": {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"string\",\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"index\": \"not_analyzed\"\n \u00a0\u00a0\u00a0\u00a0\u00a0},\n \u00a0\u00a0\u00a0\u00a0\u00a0\"batch\": {\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"type\": \"integer\"\n \u00a0\u00a0\u00a0\u00a0\u00a0}\n \u00a0\u00a0\u00a0}\n}'\n```", "```scala\nimport org.elasticsearch.spark._\nimport org.elasticsearch.spark.rdd.Metadata._\n\ndef saveCurrentBatch(twitterContent: RDD[(String, Array[String])]) = {\n \u00a0twitterContent mapPartitions { it =>\n \u00a0\u00a0\u00a0val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n \u00a0\u00a0\u00a0it map { case (content, tags) =>\n \u00a0\u00a0\u00a0\u00a0\u00a0val data = Map(\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"time\" -> sdf.format(new Date()),\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"body\" -> content,\n \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"tags\" -> tags\n \u00a0\u00a0\u00a0\u00a0\u00a0)\n\u00a0\u00a0\u00a0\u00a0\u00a0 val metadata = Map(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 TTL -> \"172800s\"\n\u00a0\u00a0\u00a0\u00a0\u00a0 )\n \u00a0\u00a0\u00a0\u00a0\u00a0(metadata, data)\n \u00a0\u00a0\u00a0 }\n \u00a0 } saveToEsWithMeta \"gzet/twitter\"\n }\n```", "```scala\n$ while true ; do TTL=`curl -XGET 'http://localhost:9200/gzet/twitter/AVRr9LaCoYjYhZG9lvBl' 2>/dev/null | jq \"._ttl\"`; echo \"TTL is $TTL\"; sleep 1; done\n\n../..\nTTL is 48366081\nTTL is 48365060\nTTL is 48364038\nTTL is 48363016\n../..\n```", "```scala\nimport org.elasticsearch.spark._\nimport org.json4s.DefaultFormats\nimport org.json4s.jackson.JsonMethods._\n\ndef getOnlineRecords(sc: SparkContext) = {\n \u00a0sc.esJsonRDD(\"gzet/twitter\").values map { jsonStr =>\n \u00a0\u00a0\u00a0implicit val format = DefaultFormats\n \u00a0\u00a0\u00a0 val json = parse(jsonStr)\n \u00a0\u00a0\u00a0 val tags = (json \\ \"tags\").extract[Array[String]]\n \u00a0\u00a0\u00a0 val body = (json \\ \"body\").extract[String]\n \u00a0\u00a0\u00a0 (body, tags)\n \u00a0 }\n }\n```", "```scala\ntwitterContent foreachRDD { batch =>\n\n \u00a0val sc = batch.sparkContext \n \u00a0batch.cache()\n\n \u00a0if(batch.count() > 0) {\n\u00a0\u00a0\u00a0 val window = getOnlineRecords(sc)\n\u00a0\u00a0\u00a0 saveCurrentBatch(batch)\n\u00a0\u00a0\u00a0 val trainingSet = batch.union(window)\n\u00a0 \u00a0\u00a0//Train method described hereafter\n\u00a0\u00a0\u00a0 trainAndSave(trainingSet, modelOutputDir)\n\u00a0 }\n\n\u00a0 batch.unpersist(blocking = false)\n}\n```", "```scala\ncase class ClassifierModel(\n\u00a0 model: NaiveBayesModel,\n\u00a0 labels: Map[String, Double]\n) {\n\n \u00a0 def predictProbabilities(vectors: RDD[Vector]) = {\n \u00a0\u00a0\u00a0 val sc = vectors.sparkContext\n \u00a0\u00a0\u00a0 val bLabels = sc.broadcast(labels.map(_.swap))\n \u00a0\u00a0\u00a0 model.predictProbabilities(vectors).map { vector =>\n \u00a0\u00a0\u00a0\u00a0\u00a0 bLabels.value\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 .toSeq\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 .sortBy(_._1)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 .map(_._2)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 .zip(vector.toArray)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0 .toMap\n \u00a0\u00a0\u00a0 }\n \u00a0 }\n\n \u00a0 def save(sc: SparkContext, outputDir: String) = {\n \u00a0\u00a0\u00a0 model.save(sc, s\"$outputDir/model\")\n \u00a0\u00a0\u00a0 sc.parallelize(labels.toSeq)\n\u00a0\u00a0\u00a0\u00a0\u00a0  .saveAsObjectFile(s\"$outputDir/labels\")\n \u00a0 }\n\n}\n```", "```scala\nval model = NaiveBayes.train(points)\nvectors.map { vector =>\n \u00a0model.predict(vector)\n }\n```", "```scala\nval model = NaiveBayes.train(points)\nval bcModel = sc.broadcast(model)\nvectors mapPartitions { it =>\n \u00a0val model = bcModel.value\n \u00a0it.map { vector =>\n \u00a0\u00a0\u00a0model.predict(vector)\n \u00a0}\n}\n```", "```scala\nvar model = None: Option[ClassifierModel]\n\ndef train(rdd: RDD[(String, Array[String])]): ClassifierModel = {\n \u00a0val labeledPoints = buildLabeledPoints(rdd)\n \u00a0val labels = getLabels(rdd)\n \u00a0labeledPoints.cache()\n \u00a0val nbModel = NaiveBayes.train(labeledPoints)\n \u00a0labeledPoints.unpersist(blocking = false)\n \u00a0val cModel = ClassifierModel(nbModel, labels)\n \u00a0model = Some(cModel)\n \u00a0cModel\n}\n```", "```scala\ndef trainAndSave(trainingSet: RDD[(String, Array[String])], \u00a0modelOutputDir: String) = {\n\u00a0 Classifier\n\u00a0\u00a0\u00a0  .train(trainingSet)\n\u00a0\u00a0\u00a0  .save(batch.sparkContext, modelOutputDir)\n}\n```", "```scala\ngdeltContent.foreachRDD { batch =>\n\n \u00a0val textRdd = batch.map(_.body.get)\n \u00a0val predictions = Classifier.predictProbabilities(textRdd)\n\n \u00a0batch.zip(predictions).map { case (content, dist) =>\n\u00a0\u00a0\u00a0 val hashTags = dist.filter { case (hashTag, proba) =>\n \u00a0\u00a0\u00a0\u00a0\u00a0proba > 0.25d\n \u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0 .toSeq\n\u00a0\u00a0\u00a0 .map(_._1)\n \u00a0\u00a0\u00a0(content, hashTags)\n \u00a0}\n\u00a0 .map { case (content, hashTags) =>\n\u00a0\u00a0\u00a0 val sdf = new SimpleDateFormat(\"yyyy-MM-dd HH:mm:ss\")\n \u00a0\u00a0\u00a0Map(\n \u00a0\u00a0\u00a0\u00a0\u00a0\"time\" \u00a0-> sdf.format(new Date()),\n \u00a0\u00a0\u00a0\u00a0\u00a0\"body\" \u00a0-> content.body.get,\n \u00a0\u00a0\u00a0\u00a0\u00a0\"url\" \u00a0\u00a0-> content.url,\n \u00a0\u00a0\u00a0\u00a0\u00a0\"tags\" \u00a0-> hashTags,\n \u00a0\u00a0\u00a0\u00a0 \"title\" -> content.title\n \u00a0\u00a0\u00a0)\n\u00a0 }\n\u00a0 .saveToEs(\"gzet/gdelt\")\n\n}\n```"]