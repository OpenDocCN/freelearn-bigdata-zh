["```scala\nArray(\"Hello Spark\", \"Hello Hadoop\", \"Hello Spark\")\n \u00a0.groupBy(a => Integer.toBinaryString(a.hashCode))\n \u00a0.foreach(println)\n\n11001100010111100111000111001111 List(Hello Spark, Hello Spark)\n10101011110110000110101101110011 List(Hello Hadoop)\n```", "```scala\nArray(\"AaAa\", \"BBBB\", \"AaBB\", \"BBAa\")\n \u00a0.groupBy(a => Integer.toBinaryString(a.hashCode))\n \u00a0.foreach(Sprintln)\n\n11111000000001000000 List(AaAa, BBBB, AaBB, BBAa)\n```", "```scala\nArray(\"Hello, Spark\", \"Hello Spark\")\n\u00a0 .groupBy(a => Integer.toBinaryString(a.hashCode))\n\u00a0 .foreach(println)\n\n11100001101000010101000011010111\u00a0 List(Hello, Spark)\n11001100010111100111000111001111\u00a0 List(Hello Spark)\n```", "```scala\ndef shingles(content: String) = {\n\u00a0 content.replaceAll(\"\\\\s+\", \"\")\n\u00a0\u00a0\u00a0 .sliding(2)\n\u00a0\u00a0\u00a0 .map(s => s.mkString(\"\"))\n\u00a0\u00a0\u00a0 .map(s => (s, s.hashCode)) \n}\n\nimplicit class BitOperations(i1: Int) {\n\u00a0 def toHashString: String = {\n\u00a0\u00a0\u00a0 String.format(\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"%32s\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 Integer.toBinaryString(i1)\n\u00a0\u00a0\u00a0 ).replace(\" \", \"0\")\n\u00a0 }\n}\n\nshingles(\"spark\").foreach { case (shingle, hash) =>\n\u00a0 println(\"[\" + shingle + \"]\\t\" + hash.toHashString)\n}\n\n[sp]\u00a0 00000000000000000000111001011101\n[pa]\u00a0 00000000000000000000110111110001\n[ar]\u00a0 00000000000000000000110000110001\n[rk]\u00a0 00000000000000000000111000111001\n```", "```scala\nimplicit class BitOperations(i1: Int) {\n\n\u00a0 // ../..\u00a0\n\n  def isBitSet(bit: Int): Boolean = {\n\u00a0\u00a0\u00a0 ((i1 >> bit) & 1) == 1\n\u00a0 }\n}\n\nimplicit class Simhash(content: String) {\n\n\u00a0 def simhash = {\n\u00a0\u00a0\u00a0 val aggHash = shingles(content).flatMap{ hash =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 Range(0, 32).map { bit =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 (bit, if (hash.isBitSet(bit)) 1 else -1)\n\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 .groupBy(_._1)\n\u00a0\u00a0\u00a0 .mapValues(_.map(_._2).sum > 0)\n\u00a0\u00a0\u00a0 .toArray\n\n\u00a0\u00a0\u00a0 buildSimhash(0, aggHash)\n\u00a0 }\n\n private def buildSimhash(\n\u00a0\u00a0\u00a0\u00a0\u00a0 simhash: Int,\n \u00a0\u00a0\u00a0\u00a0 aggBit: Array[(Int, Boolean)]\n\u00a0\u00a0\u00a0\u00a0 ): Int = {\n\n\u00a0\u00a0\u00a0 if(aggBit.isEmpty) return simhash\n\u00a0\u00a0\u00a0 val (bit, isSet) = aggBit.head\n\u00a0\u00a0\u00a0 val newSimhash = if(isSet) {\n\u00a0\u00a0\u00a0\u00a0\u00a0 simhash | (1 << bit)\n\u00a0\u00a0\u00a0 } else {\n\u00a0\u00a0\u00a0\u00a0\u00a0 simhash\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 buildSimhash(newSimhash, aggBit.tail)\n\n\u00a0 }\n}\n\nval s = \"mastering spark for data science\"\nprintln(toHashString(s.simhash))\n\n00000000000000000000110000110001\n```", "```scala\nimplicit class BitOperations(i1: Int) {\n\n\u00a0 // ../..\n\n\u00a0 def distance(i2: Int) = {\n\u00a0\u00a0\u00a0 Integer.bitCount(i1 ^ i2) \n\u00a0 }\n}\n\nval s1 = \"hello simhash\"\nval s2 = \"hello minhash\"\nval dist = s1.simhash.distance(s2.simhash)\n```", "```scala\nval gdeltInputDir = args.head\nval gkgRDD = sc.textFile(gdeltInputDir)\n\u00a0 .map(GKGParser.toJsonGKGV2)\n\u00a0 .map(GKGParser.toCaseClass2)\n\nval urlRDD = gkgRDD.map(g => g.documentId.getOrElse(\"NA\"))\n\u00a0 .filter(url => Try(new URL(url)).isSuccess)\n\u00a0 .distinct()\n\u00a0 .repartition(partitions)\n\nval contentRDD = urlRDD mapPartitions { it =>\n\u00a0 val html = new HtmlFetcher()\n\u00a0 it map html.fetch\n}\n```", "```scala\n<dependency>\n\u00a0 <groupId>org.apache.lucene</groupId>\n\u00a0 <artifactId>lucene-analyzers-common</artifactId>\n\u00a0 <version>4.10.1</version>\n</dependency>\n```", "```scala\nimport io.gzet.story.simhash.SimhashUtils._\nval simhashRDD = corpusRDD.mapValues(_.simhash)\n```", "```scala\nhamming match {\n\u00a0 case 0 => // identical articles - true-duplicate\n\u00a0 case 1 => // near-duplicate (mainly typo errors)\n\u00a0 case 2 => // near-duplicate (minor difference in style)\n\u00a0 case _ => // different articles\n}\n```", "```scala\ndef oneBitMasks: Set[Int] = {\n\u00a0 (0 to 31).map(offset => 1 << offset).toSet\n}\n\n00000000000000000000000000000001\n00000000000000000000000000000010\n00000000000000000000000000000100\n00000000000000000000000000001000\n...\n```", "```scala\nval s = 23423\noneBitMasks foreach { mask =>\n\u00a0 println((mask ^ s).toHashString)\n}\n\n00000000000000000101101101111111\n00000000000000000101101101111110\n00000000000000000101101101111101\n00000000000000000101101101111011\n...\n```", "```scala\ndef twoBitsMasks: Set[Int] = {\n\u00a0 val masks = oneBitMasks\n\u00a0 masks flatMap { e1 =>\n\u00a0\u00a0\u00a0 masks.filter( e2 => e1 != e2) map { e2 =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 e1 | e2\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n}\n\n00000000000000000000000000000011\n00000000000000000000000000000101\n00000000000000000000000000000110\n00000000000000000000000000001001\n...\n```", "```scala\nval searchmasks = twoBitsMasks ++ oneBitMasks ++ Set(0) \n\n```", "```scala\nval duplicateTupleRDD = simhashRDD.flatMap {\n\u00a0 case ((id, _), simhash) =>\n\u00a0 \u00a0\u00a0searchmasks.map { mask =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0(simhash ^ mask, id)\n\u00a0 \u00a0\u00a0}\n}\n.groupByKey()\n```", "```scala\nval edgeRDD = duplicateTupleRDD\n\u00a0 .values\n\u00a0 .flatMap { it =>\n\u00a0 \u00a0\u00a0val list = it.toList\n\u00a0 \u00a0\u00a0for (x <- list; y <- list) yield (x, y)\n\u00a0 }\n\u00a0 .filter { case (x, y) =>\n\u00a0 \u00a0\u00a0x != y\n\u00a0 }\n\u00a0 .distinct()\n\u00a0 .map {case (x, y) =>\n\u00a0 \u00a0\u00a0Edge(x, y, 0)\n\u00a0 }\n\nval duplicateRDD = Graph.fromEdges(edgeRDD, 0L)\n\u00a0 .connectedComponents()\n\u00a0 .vertices\n\u00a0 .join(simhashRDD.keys)\n\u00a0 .values\n```", "```scala\n<dependency>\n\u00a0 <groupId>com.datastax.spark</groupId>\n\u00a0 <artifactId>spark-cassandra-connector_2.11</artifactId>\n</dependency>\n```", "```scala\nCREATE TABLE gzet.articles (\n\u00a0 simhash int PRIMARY KEY,\n\u00a0 url text,\n\u00a0 title text,\n\u00a0 body text\n);\n```", "```scala\nimport com.datastax.spark.connector._\n\ncorpusRDD.map { case (content, simhash) =>\n\u00a0 Article(\n\u00a0\u00a0\u00a0 simhash,\n\u00a0\u00a0\u00a0 content.body,\n\u00a0\u00a0\u00a0 content.title,\n\u00a0\u00a0\u00a0 content.url\n\u00a0 )\n}\n.saveToCassandra(cassandraKeyspace, cassandraTable)\n```", "```scala\n<packaging>play2</packaging>\n\n<dependencies>\n\u00a0 <dependency>\n\u00a0 \u00a0\u00a0<groupId>com.typesafe.play</groupId>\n\u00a0\u00a0\u00a0 <artifactId>play_2.11</artifactId>\n\u00a0 </dependency>\n\u00a0 <dependency>\n\u00a0\u00a0\u00a0 <groupId>com.datastax.cassandra</groupId>\n\u00a0\u00a0\u00a0 <artifactId>cassandra-driver-core</artifactId>\n\u00a0 </dependency>\n</dependencies>\n```", "```scala\nclass CassandraDao() {\n\n\u00a0 private val session = Cluster.builder()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0.addContactPoint(cassandraHost)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .withPort(cassandraPort)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .build()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 .connect()\n\n\u00a0 def findDuplicates(hash: Int): List[Article] = {\n\u00a0\u00a0\u00a0 searchmasks.map { mask =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 val searchHash = mask ^ hash\n\u00a0\u00a0\u00a0\u00a0\u00a0 val stmt = s\"SELECT simhash, url, title, body FROM gzet.articles WHERE simhash = $searchHash;\"\n\u00a0\u00a0\u00a0\u00a0\u00a0 val results = session.execute(stmt).all()\n\u00a0\u00a0\u00a0\u00a0\u00a0 results.map { row =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Article(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0row.getInt(\"simhash\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 row.getString(\"body\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 row.getString(\"title\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 row.getString(\"url\")\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )\n\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0\u00a0\u00a0 .head\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 .toList\n\u00a0 }\n}\n```", "```scala\nobject Simhash extends Controller {\n\n\u00a0 val dao = new CassandraDao()\n\u00a0 val goose = new HtmlFetcher()\n\n\u00a0 def detect = Action { implicit request =>\n\u00a0\u00a0\u00a0 val url = request.getQueryString(\"url\").getOrElse(\"NA\")\n\u00a0\u00a0\u00a0 val article = goose.fetch(url)\n\u00a0\u00a0\u00a0 val hash = Tokenizer.lucene(article.body).simhash\n\u00a0\u00a0\u00a0 val related = dao.findDuplicates(hash)\n\u00a0\u00a0\u00a0 Ok(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Json.toJson(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Duplicate(\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hash,\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0article.body,\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0article.title,\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0url,\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0related\n\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0)\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0)\n\u00a0\u00a0\u00a0 )\n\u00a0 }\n}\n```", "```scala\nGET /simhash io.gzet.story.web.controllers.Simhash.detect \n\n```", "```scala\ncurl -XGET 'localhost:9000/simhash?url= http://www.detroitnews.com/story/tech/2016/10/12/samsung-damage/91948802/'\n\n{\n\u00a0 \"simhash\": 1822083259,\n\u00a0 \"body\": \"Seoul, South Korea - The fiasco of Samsung's [...]\n\u00a0 \"title\": \"Fiasco leaves Samsung's smartphone brand [...]\",\n\u00a0 \"url\": \"http://www.detroitnews.com/story/tech/2016/[...]\",\n\u00a0 \"related\": [\n\u00a0\u00a0\u00a0 {\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"hash\": 1821919419,\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"body\": \"SEOUL, South Korea - The fiasco of [...]\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"title\": \"Note 7 fiasco leaves Samsung's [...]\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"url\": \"http://www.chron.com/business/technology/[...]\"\n\u00a0\u00a0\u00a0 },\n\u00a0\u00a0\u00a0 {\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"hash\": -325433157,\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"body\": \"The fiasco of Samsung's fire-prone [...]\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"title\": \"Samsung's Smartphone Brand [...]\",\n\u00a0\u00a0\u00a0\u00a0\u00a0 \"url\": \"http://www.toptechnews.com/[...]\"\n\u00a0\u00a0\u00a0 }\n\u00a0 ]\n}\n```", "```scala\nimplicit class Simhash(content: String) {\n\n\u00a0 // ../..\n\n\u00a0 def weightedSimhash = {\n\n\u00a0\u00a0\u00a0 val features = shingles(content)\n\u00a0\u00a0\u00a0 val totalWords = features.length\n\u00a0\u00a0\u00a0 val aggHashWeight = features.zipWithIndex\n\u00a0\u00a0\u00a0\u00a0\u00a0 .map {case (hash, id) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0(hash, 1.0 - id / totalWords.toDouble)\n\u00a0\u00a0\u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0 .flatMap { case (hash, weight) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0Range(0, 32).map { bit =>\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0(bit, if(hash.isBitSet(bit)) weight else -weight)\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0 .groupBy(_._1)\n\u00a0\u00a0\u00a0\u00a0\u00a0 .mapValues(_.map(_._2).sum > 0)\n\u00a0\u00a0\u00a0\u00a0\u00a0 .toArray\n\n\u00a0\u00a0\u00a0 buildSimhash(0, aggHashWeight)\n \u00a0}\n\n}\n```", "```scala\nval tfRDD = documentRDD.flatMap { case (docId, body) =>\n\u00a0 body.split(\"\\\\s\").map { word =>\n\u00a0\u00a0\u00a0 ((docId, word), 1)\n\u00a0 }\n}\n.reduceByKey(_+_)\n.map { case ((docId, word), tf) =>\n\u00a0 (docId, (word, tf))\n}\n```", "```scala\nval n = sc.broadcast(documentRDD.count())\nval dfMap = sc.broadcast(\n\u00a0 tfRDD.map { case (docId, (word, _)) =>\n\u00a0\u00a0\u00a0 (docId, word)\n\u00a0 }\n\u00a0 .distinct()\n\u00a0 .values\n\u00a0 .map { word =>\n\u00a0\u00a0\u00a0 (word, 1)\n\u00a0 }\n\u00a0 .reduceByKey(_+_)\n\u00a0 .collectAsMap()\n)\n\nval tfIdfRDD = tfRDD.mapValues { case (word, tf) =>\n\u00a0 val df = dfMap.value.get(word).get\n\u00a0 val idf = math.log((n.value + 1) / (df + 1))\n\u00a0 (word, tf * idf)\n}\n```", "```scala\nval numFeatures = 256\n\nval vectorRDD = tfIdfRDD.mapValues { case (word, tfIdf) =>\n\u00a0 val rawMod = word.hashCode % numFeatures\n\u00a0 rawMod + (if (rawMod < 0) numFeatures else 0)\n\u00a0 (word.hashCode / numFeatures, tfIdf)\n}\n.groupByKey()\n.values\n.map { it =>\n\u00a0 Vectors.sparse(numFeatures, it.toSeq)\n}\n```", "```scala\nval tfModel = new HashingTF(1 << 20)\nval tfRDD = documentRDD.values.map { body =>\n\u00a0 tfModel.transform(body.split(\"\\\\s\"))\n}\n\nval idfModel = new IDF().fit(tfRDD)\nval tfIdfRDD = idfModel.transform(tfRDD)\nval normalizer = new Normalizer()\nval sparseVectorRDD = tfIdfRDD map normalizer.transform\n```", "```scala\nval embedding = Embedding(Embedding.MEDIUM_DIMENSIONAL_RI)\nval denseVectorRDD = sparseVectorRDD map embedding.embed\ndenseVectorRDD.cache()\n```", "```scala\ndef euclidean(xs: Array[Double], ys: Array[Double]) = {\n\u00a0 require(xs.length == ys.length)\n\u00a0 math.sqrt((xs zip ys)\n\u00a0\u00a0\u00a0 .map { case (x, y) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 math.pow(y - x, 2)\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 .sum\n\u00a0 )\n}\n\ndef cosine(xs: Array[Double], ys: Array[Double]) = {\n\n\u00a0 require(xs.length == ys.length)\n\u00a0 val magX = math.sqrt(xs.map(i => i * i).sum)\n\u00a0 val magY = math.sqrt(ys.map(i => i * i).sum)\n\u00a0 val dotP = (xs zip ys).map { case (x, y) =>\n\u00a0\u00a0\u00a0 x * y\n\u00a0 }.sum\n\n\u00a0 dotP / (magX * magY)\n}\n```", "```scala\nval model: KMeansModel = new KMeans()\n\u00a0 .setEpsilon(0.01)\n\u00a0 .setK(numberOfClusters)\n\u00a0 .setMaxIterations(1000)\n\u00a0 .run(denseVectorRDD)\n```", "```scala\nval wsse = model.computeCost(denseVectorRDD) \n\n```", "```scala\nval clusterTitleRDD = articleRDD\n\u00a0 .zip(denseVectorRDD)\n\u00a0 .map { case ((id, article), vector) =>\n\u00a0 \u00a0\u00a0(model.predict(vector), article.title)\n\u00a0\u00a0}\n```", "```scala\nnifi.remote.input.socket.port=8055 \n\n```", "```scala\n<dependency>\n\u00a0 <groupId>org.apache.nifi</groupId>\n\u00a0 <artifactId>nifi-spark-receiver</artifactId>\n\u00a0 <version>0.6.1</version>\n</dependency>\n```", "```scala\ndef readFromNifi(ssc: StreamingContext): DStream[String] = {\n\n\u00a0 val nifiConf = new SiteToSiteClient.Builder()\n\u00a0\u00a0\u00a0 .url(\"http://localhost:8090/nifi\")\n\u00a0\u00a0\u00a0 .portName(\"Send_To_Spark\")\n\u00a0\u00a0\u00a0 .buildConfig()\n\n\u00a0 val receiver = new NiFiReceiver(nifiConf, StorageLevel.MEMORY_ONLY)\n\u00a0 ssc.receiverStream(receiver) map {packet =>\n\u00a0\u00a0\u00a0 new String(packet.getContent, StandardCharsets.UTF_8)\n\u00a0 }\n}\n```", "```scala\nval ssc = new StreamingContext(sc, Minutes(15)) \nval gdeltStream: DStream[String] = readFromNifi(ssc) \nval gkgStream = parseGkg(gdeltStream) \n\n```", "```scala\nval extractUrlsFromRDD = (rdd: RDD[GkgEntity2]) => {\n\u00a0 rdd.map { gdelt =>\n\u00a0\u00a0\u00a0 gdelt.documentId.getOrElse(\"NA\")\n\u00a0 }\n\u00a0 .distinct()\n}\nval urlStream = gkgStream.transform(extractUrlsFromRDD)\nval contentStream = fetchHtml(urlStream)\n```", "```scala\nval buildVectors = (rdd: RDD[Content]) => {\n\n\u00a0 val corpusRDD = rdd.map(c => (c, Tokenizer.stem(c.body)))\n\n\u00a0 val tfModel = new HashingTF(1 << 20)\n\u00a0 val tfRDD = corpusRDD mapValues tfModel.transform\n\n\u00a0 val idfModel = new IDF() fit tfRDD.values\n\u00a0 val idfRDD = tfRDD mapValues idfModel.transform\n\n\u00a0 val normalizer = new Normalizer()\n\u00a0 val sparseRDD = idfRDD mapValues normalizer.transform\n\n\u00a0 val embedding = Embedding(Embedding.MEDIUM_DIMENSIONAL_RI)\n\u00a0 val denseRDD = sparseRDD mapValues embedding.embed\n\n\u00a0 denseRDD\n}\n\nval vectorStream = contentStream transform buildVectors\n```", "```scala\nval model = new StreamingKMeans()\n\u00a0 .setK(15)\n\u00a0 .setRandomCenters(256, 0.0)\n\u00a0 .setHalfLife(2, \"batches\")\n\nmodel.trainOn(vectorStream.map(_._2))\n```", "```scala\nval storyStream = model predictOnValues vectorStream  \n\n```", "```scala\nMap(\n\u00a0 \"uuid\" -> gkg.gkgId,\n\u00a0 \"topic\" -> clusterId,\n\u00a0 \"batch\" -> batchId,\n\u00a0 \"simhash\" -> content.body.simhash, \n\u00a0 \"date\" -> gkg.date,\n\u00a0 \"url\" -> content.url,\n\u00a0 \"title\" -> content.title,\n\u00a0 \"body\" -> content.body,\n\u00a0 \"tone\" -> gkg.tones.get.averageTone,\n\u00a0 \"country\" -> gkg.v2Locations,\n\u00a0 \"theme\" -> gkg.v2Themes,\n\u00a0 \"person\" -> gkg.v2Persons,\n\u00a0 \"organization\" -> gkg.v2Organizations,\n\u00a0 \"vector\" -> v.toArray.mkString(\",\")\n)\n```", "```scala\nimport org.json4s.DefaultFormats\nimport org.json4s.native.JsonMethods._\n\nval defaultVector = Array.fill[Double](256)(0.0d).mkString(\",\")\nval minBatchQuery = batchId - 4\nval query = \"{\"query\":{\"range\":{\"batch\":{\"gte\": \" + minBatchQuery + \",\"lte\": \" + batchId + \"}}}}\"\nval nodesDrift = sc.esJsonRDD(esArticles, query)\n\u00a0 .values\n\u00a0 .map { strJson =>\n\u00a0 \u00a0\u00a0implicit val format = DefaultFormats\n\u00a0 \u00a0\u00a0val json = parse(strJson)\n\u00a0 \u00a0\u00a0val vectorStr = (json \\ \"vector\").extractOrElse[String](defaultVector)\n\u00a0 \u00a0\u00a0val vector = Vectors.dense(vectorStr.split(\",\").map(_.toDouble))\n\u00a0 \u00a0\u00a0val previousCluster = (json \\ \"topic\").extractOrElse[Int](-1)\n\u00a0 \u00a0\u00a0val newCluster = model.latestModel().predict(vector)\n\u00a0 \u00a0\u00a0((previousCluster, newCluster), 1)\n\u00a0 }\n\u00a0 .reduceByKey(_ + _)\n```", "```scala\nval latest = model.latestModel()\nval topTitles = rdd.values\n\u00a0 .map { case ((content, v, cId), gkg) =>\n\u00a0 \u00a0\u00a0val dist = euclidean(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 latest.clusterCenters(cId).toArray,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 v.toArray\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 )\n\u00a0 \u00a0\u00a0(cId, (content.title, dist))\n\u00a0 }\n\u00a0 .groupByKey()\n\u00a0 .mapValues { it =>\n\u00a0 \u00a0\u00a0Try(it.toList.sortBy(_._2).map(_._1).head).toOption\n\u00a0 }\n\u00a0 .collectAsMap()\n```"]