["```scala\n<dependency>\n\u00a0 <groupId>com.gravity</groupId>\n\u00a0 <artifactId>goose</artifactId>\n\u00a0 <version>2.1.23</version>\n</dependency>\n```", "```scala\ndef getGooseScraper(): Goose = {\n\u00a0 val conf: Configuration = new Configuration\n\u00a0 conf.setEnableImageFetching(false)\n\u00a0 conf.setBrowserUserAgent(userAgent)\n\u00a0 conf.setConnectionTimeout(connectionTimeout)\n\u00a0 conf.setSocketTimeout(socketTimeout)\n\u00a0 new Goose(conf)\n}\n\nval url = \"http://www.bbc.co.uk/news/entertainment-arts-35278872\"\nval goose: Goose = getGooseScraper()\nval article: Article = goose.extractContent(url)\n```", "```scala\nval cleanedBody: String = article.cleanedArticleText\nval title: String = article.title\nval description: String = article.metaDescription\nval keywords: String = article.metaKeywords\nval domain: String = article.domain\nval date: Date = article.publishDate\nval tags: Set[String] = article.tags\n\n/*\nBody: Singer David Bowie, one of the most influential musicians...\nTitle: David Bowie dies of cancer aged 69\nDescription: Tributes are paid to David Bowie...\nDomain: www.bbc.co.uk\n*/\n```", "```scala\n$ git clone git@bitbucket.org:gzet_io/goose.git\n$ cd goose && mvn clean install\n```", "```scala\n<dependency>\n \u00a0<groupId>com.gravity</groupId>\n \u00a0<artifactId>goose_2.11</artifactId>\n \u00a0<version>2.1.30</version>\n</dependency>\n```", "```scala\nval goose = getGooseScraper()\ndef fetchArticles(urlRdd: RDD[String]): RDD[Article] = {\n\u00a0 urlRdd.map(goose.extractContent)\n}\n```", "```scala\ndef fechArticles(urlRdd: RDD[String]): RDD[Article] = {\n\u00a0 urlRdd map { url =>\n\u00a0\u00a0\u00a0 val goose = getGooseScraper()\n\u00a0\u00a0\u00a0 goose.extractContent(url)\n\u00a0 }\n}\n```", "```scala\ndef fetchArticles(urlRdd: RDD[String]): RDD[Article] = {\n\u00a0 urlRdd mapPartitions { urls =>\n\u00a0\u00a0\u00a0 val goose = getGooseScraper()\n\u00a0\u00a0\u00a0 urls map goose.extractContent\n\u00a0 }\n}\n```", "```scala\ncase class Content(\n\u00a0 \u00a0\u00a0\u00a0url: String,\n\u00a0 \u00a0\u00a0\u00a0title: Option[String],\n\u00a0 \u00a0\u00a0\u00a0description: Option[String],\n\u00a0 \u00a0\u00a0\u00a0body: Option[String],\n\u00a0 \u00a0\u00a0\u00a0publishDate: Option[String]\n)\n\ndef fetchArticles(urlRdd: RDD[String]): RDD[Content] = {\n\n\u00a0 urlRdd mapPartitions { urls =>\n\n\u00a0\u00a0\u00a0 val sdf = new SimpleDateFormat(\"yyyy-MM-dd'T'HH:mm:ssZ\")\n\u00a0\u00a0\u00a0 val goose = getGooseScraper()\n\n\u00a0\u00a0\u00a0 urls map { url =>\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 try {\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 val article = goose.extractContent(url)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 var body = None: Option[String]\n\u00a0 \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0var title = None: Option[String]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 var description = None: Option[String]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 var publishDate = None: Option[String]\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if (StringUtils.isNotEmpty(article.cleanedArticleText))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 body = Some(article.cleanedArticleText)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0if (StringUtils.isNotEmpty(article.title))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 title = Some(article.title)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if (StringUtils.isNotEmpty(article.metaDescription))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 description = Some(article.metaDescription)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 if (article.publishDate != null)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 publishDate = Some(sdf.format(article.publishDate))\n\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 Content(url, title, description, body, publishDate)\n\n\u00a0\u00a0\u00a0\u00a0\u00a0 } catch {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 case e: Throwable => Content(url, None, None, None, None)\n\u00a0\u00a0\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n\n}\n```", "```scala\nval urlRdd = getDistinctUrls(gdeltRdd).repartition(200)\nurlRdd.cache()\nurlRdd.count()\n\nval contentRdd: RDD[Content] = fetchArticles(urlRdd)\ncontentRdd.persist(StorageLevel.DISK_ONLY)\ncontentRdd.count()\n```", "```scala\n<dependency>\n\u00a0 <groupId>org.clulab</groupId>\n\u00a0 <artifactId>processors-corenlp_2.11</artifactId>\n\u00a0 <version>6.0.1</version>\n</dependency>\n\n<dependency>\n\u00a0 <groupId>org.clulab</groupId>\n\u00a0 <artifactId>processors-main_2.11</artifactId>\n\u00a0 <version>6.0.1</version>\n</dependency>\n\n<dependency>\n\u00a0 <groupId>org.clulab</groupId>\n\u00a0 <artifactId>processors-models_2.11</artifactId>\n\u00a0 <version>6.0.1</version>\n</dependency>\n```", "```scala\ncase class Entity(eType: String, eVal: String)\n\ndef processSentence(sentence: Sentence): List[Entity] = {\n\u00a0 val entities = sentence.lemmas.get\n\u00a0\u00a0\u00a0 .zip(sentence.entities.get)\n\u00a0\u00a0\u00a0 .map {\n\u00a0\u00a0\u00a0\u00a0\u00a0 case (eVal, eType) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0Entity(eType, eVal)\n\u00a0\u00a0\u00a0 }\n}\n\ndef extractEntities(processor: Processor, corpus: String) = {\n\u00a0 val doc = processor.annotate(corpus)\n\u00a0 doc.sentences map processSentence\n}\n\nval t = \"David Bowie was born in London\"\nval processor: Processor = new FastNLPProcessor()\nval sentences = extractEntities(processor, t)\n\nsentences foreach { sentence =>\n\u00a0 sentence foreach println\n}\n\n/*\nEntity(David,PERSON)\nEntity(Bowie,PERSON)\nEntity(was,O)\nEntity(born,O)\nEntity(in,O) \nEntity(London,LOCATION) \n*/\n```", "```scala\ndef aggregate(entities: Array[Entity]) = {\n\u00a0 aggregateEntities(entities.head, entities.tail, List())\n}\n\ndef aggregateEntity(e1: Entity, e2: Entity) = {\n\u00a0 Entity(e1.eType, e1.eVal + \" \" + e2.eVal)\n}\n\ndef aggEntities(current: Entity, entities: Array[Entity], processed : List[Entity]): List[Entity] = {\n\u00a0 if(entities.isEmpty) {\n// End of recusion, no additional entity to process\n\u00a0\u00a0\u00a0 // Append our last un-processed entity to our list\n\u00a0\u00a0\u00a0 current :: processed\n\u00a0 } else {\n\u00a0\u00a0\u00a0 val entity = entities.head\n\u00a0\u00a0\u00a0 if(entity.eType == current.eType) {\n // Aggregate consecutive values only of a same entity type\u00a0\u00a0\u00a0\u00a0\u00a0 val aggEntity = aggregateEntity(current, entity)\n*\u00a0\u00a0 \u00a0\u00a0 // Process next record*\n\u00a0 \u00a0\u00a0\u00a0\u00a0aggEntities(aggEntity, entities.tail, processed)\n\u00a0\u00a0\u00a0 } else {\n// Add current entity as a candidate for a next aggregation\n\u00a0\u00a0\u00a0\u00a0\u00a0 // Append our previous un-processed entity to our list\u00a0\u00a0\u00a0\u00a0\u00a0 aggEntities(entity, entities.tail, current :: processed)\n\u00a0\u00a0\u00a0 }\n\u00a0 }\n}\n\ndef processSentence(sentence: Sentence): List[Entity] = {\n\u00a0 val entities = sentence.lemmas.get\n\u00a0\u00a0\u00a0 .zip(sentence.entities.get)\n\u00a0\u00a0\u00a0 .map {\n\u00a0\u00a0\u00a0\u00a0\u00a0 case (eVal, eType) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0Entity(eType, eVal)\n\u00a0\u00a0\u00a0 }\n\u00a0 aggregate(entities)\n}\n```", "```scala\n/*\n(PERSON,David Bowie)\n(O,was born in)\n(LOCATION,London) \n*/\n```", "```scala\ncase class Entities(sentences: Array[List[(String, String)]])\n\u00a0{\n\n\u00a0 def getSentences = sentences\n\n\u00a0 def getEntities(entity: String) = {\n\u00a0\u00a0\u00a0 sentences flatMap { sentence =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 sentence\n\u00a0\u00a0\u00a0 } filter { case (entityType, entityValue) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 entityType == entity\n\u00a0\u00a0\u00a0 } map { case (entityType, entityValue) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 entityValue\n\u00a0\u00a0\u00a0 } toSeq\n\u00a0 }\n```", "```scala\ndef extract(corpusRdd: RDD[String]): RDD[Entities] = {\n\u00a0 corpusRdd mapPartitions {\n\u00a0\u00a0\u00a0 case it=>\n\u00a0\u00a0\u00a0 \u00a0\u00a0val processor = new FastNLPProcessor()\n\u00a0\u00a0\u00a0 \u00a0\u00a0it map {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 corpus =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0val entities = extractEntities(processor, corpus)\n\u00a0\u00a0\u00a0\u00a0\u00a0 \u00a0\u00a0\u00a0\u00a0new Entities(entities)\n\u00a0\u00a0\u00a0 \u00a0\u00a0}\n\u00a0 \u00a0\u00a0}\n\u00a0 }\n```", "```scala\nval entityRdd: RDD[Entities] = extract(corpusRdd)\nentityRdd.persist(StorageLevel.DISK_ONLY)\nentityRdd.count()\n\nval perRdd = entityRdd.map(_.getEntities(\"PERSON\"))\nval locRdd = entityRdd.map(_.getEntities(\"LOCATION\"))\nval orgRdd = entityRdd.map(_.getEntities(\"ORGANIZATION\"))\n```", "```scala\nval corpusRdd: RDD[String] = inputRdd.repartition(120)\ncorpusRdd.cache()\ncorpusRdd.count()\n\nval entityRdd: RDD[Entities] = extract(corpusRdd)\n```", "```scala\ncase class GeoName(\n\u00a0 geoId: Long,\n\u00a0 name: String,\n\u00a0 altNames: Array[String],\n\u00a0 country: Option[String],\n\u00a0 adminCode: Option[String],\n\u00a0 featureClass: Char,\n\u00a0 featureCode: String,\n\u00a0 population: Long,\n\u00a0 timezone: Array[String],\n\u00a0 geoPoint: GeoPoint\n)\n\ncase class GeoPoint(\n\u00a0 lat: Double,\n\u00a0 lon: Double\n)\n```", "```scala\nval geoNameRdd: RDD[GeoName] = GeoNameLookup.load(\n\u00a0 sc,\n\u00a0 adminCodesPath,\n\u00a0 allCountriesPath\n)\n```", "```scala\nval geoAltNameRdd = geoNameRdd.flatMap {\n\u00a0 geoName =>\n\u00a0 \u00a0\u00a0altNames map { altName =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 (clean(altName), geoName)\n\u00a0\u00a0\u00a0 }\n} filter { case (altName, geoName) =>\n\u00a0 StringUtils.isNotEmpty(altName.length)\n} distinct()\n\nval inputNameRdd = inputRdd.map { name =>\n\u00a0 (clean(name), name)\n} filter { case (cleanName, place) =>\n\u00a0 StringUtils.*isNotEmpty*(cleanName.length)\n }\n```", "```scala\ndef geoLookup(\n\u00a0 inputNameRdd: RDD[(String, String)],\n\u00a0 geoNameRdd: RDD[(String, GeoName)]\n): RDD[(String, Array[GeoName])] = {\n\n\u00a0 inputNameRdd\n\u00a0\u00a0\u00a0 .join(geoNameRdd)\n\u00a0\u00a0 \u00a0.map { case (key, (name, geo)) =>\n\u00a0\u00a0\u00a0 \u00a0\u00a0(name, geo)\n\u00a0 \u00a0\u00a0}\n\u00a0\u00a0\u00a0 .groupByKey()\n\u00a0\u00a0\u00a0 .mapValues(_.toSet)\n\n}\n```", "```scala\n<dependency>\n\u00a0 <groupId>org.scalanlp</groupId>\n\u00a0 <artifactId>breeze_2.11</artifactId>\n\u00a0 <version>0.12</version>\n</dependency>\n```", "```scala\nval bfSize = inputRdd.count()\nval bf: BloomFilter[String] = inputRdd.mapPartitions { it =>\n\u00a0 val bf = BloomFilter.optimallySized[String](bfSize, 0.001)\n\u00a0 it.foreach { cleanName =>\n\u00a0\u00a0\u00a0 bf += cleanName\n\u00a0 }\n\u00a0 Iterator(bf)\n} reduce(_ | _)\n```", "```scala\nval geoNameFilterRdd = geoAltNameRdd filter {\n\u00a0 case(name, geo) =>\n\u00a0 \u00a0\u00a0bf.contains(name)\n}\n\nval resultRdd = geoLookup(inputNameRdd, geoNameFilterRdd)\n```", "```scala\nval geoAltNamePartitionRdd = geoAltNameRdd.partitionBy(\n\u00a0 new HashPartitioner(100)\n).cache()\n\ngeoAltNamePartitionRdd.count()\nval resultRdd = geoLookup(inputNameRdd, geoAltNamePartitionRdd)\n```", "```scala\n<dependency>\n\u00a0 <groupId>org.scalaz</groupId>\n\u00a0 <artifactId>scalaz-core_2.11</artifactId>\n\u00a0 <version>7.2.0</version>\n</dependency>\n```", "```scala\npersonRDD.take(8).foreach(println)\n\n/*\nDavid Bowie\ndavid bowie\ndavid#Bowie\nDavid Bowie\ndavid bowie\nDavid Bowie\nDavid Bowie\nZiggy Stardust\n*/\n```", "```scala\nval wcRDD = personRDD\n\u00a0 .map(_ -> 1)\n\u00a0 .reduceByKey(_+_)\n\nwcRDD.collect.foreach(println)\n/*\n(David Bowie, 4)\n(david bowie, 2)\n(david#Bowie, 1)\n(Ziggy Stardust, 1)\n*/\n```", "```scala\nval lcRDD = wcRDD.map { case (p, tf) => \n  (p.lowerCase(), tf) \n} \n.reduceByKey(_+_) \n\nlcRDD.collect.foreach(println) \n\n/* \n(david bowie, 6) \n(david#bowie, 1) \n(ziggy stardust, 1) \n*/ \n\n```", "```scala\nval reRDD = lcRDD.map { case (p, tf) =>\n\u00a0\u00a0(p.replaceAll(\"[^a-z]\", \"\"), tf)\n}\n.reduceByKey(_+_)\n\nreRDD.collect.foreach(println)\n\n/*\n(david bowie, 7)\n(ziggy stardust, 1)\n*/\n```", "```scala\nimport scalaz.Scalaz._\n\ndef initialize(rdd: RDD[String]) = {\n\u00a0 rdd.map(s => (s, Map(s -> 1)))\n\u00a0\u00a0\u00a0\u00a0 .reduceByKey(_ |+| _)\n}\n\ndef lcDedup(rdd: RDD[(String, Map[String, Int])]) = {\n\u00a0 rdd.map { case (name, tf) =>\n\u00a0\u00a0\u00a0 (name.toLowerCase(), tf)\n\u00a0 }\n\u00a0 .reduceByKey(_ |+| _)\n}\n\ndef reDedup(rdd: RDD[(String, Map[String, Int])]) = {\n\u00a0 rdd.map { case (name, tf) =>\n\u00a0\u00a0\u00a0 (name.replaceAll(\"\\\\W\", \"\"), tf)\n\u00a0 }\n\u00a0 .reduceByKey(_ |+| _)\n}\n\nval wcTfRdd = initialize(personRDD)\nval lcTfRdd = lcDedup(wcTfRdd)\nval reTfRdd = reDedup(lcTfRdd)\n\nreTfRdd.values.collect.foreach(println)\n\n/*\nMap(David Bowie -> 4, david bowie -> 2, david#Bowie -> 1)\nMap(ziggy stardust -> 1)\n*/\n```", "```scala\nval dicRDD = fuTfRdd.values.flatMap {\n\u00a0 alternatives =>\n\u00a0\u00a0\u00a0 val top = alternatives.toList.sortBy(_._2).last._1\n\u00a0\u00a0\u00a0 tf.filter(_._1 != top).map { case (alternative, tf) =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 (alternative, top)\n\u00a0\u00a0\u00a0 }\n}\n\ndicRDD.collect.foreach(println)\n\n/*\ndavid bowie, David Bowie\ndavid#Bowie, David Bowie\n*/\n```", "```scala\ndef clean(name: String, stopWords: Set[String]) = {\n\n\u00a0 StringUtils.stripAccents(name)\n\u00a0\u00a0\u00a0 .split(\"\\\\W+\").map(_.trim).filter { case part =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 !stopWords.contains(part.toLowerCase())\n\u00a0\u00a0\u00a0 }\n\u00a0\u00a0\u00a0 .mkString(\" \")\n\u00a0\u00a0\u00a0 .split(\"(?<=[a-z])(?=[A-Z])\")\n\u00a0\u00a0\u00a0 .filter(_.length >= 2)\n\u00a0\u00a0\u00a0 .mkString(\" \")\n\u00a0\u00a0\u00a0 .toLowerCase()\n\n}\n\ndef simpleDedup(rdd: RDD[(String, Map[String, Int])], stopWords: Set[String]) = {\n\n\u00a0 rdd.map { case (name, tf) =>\n\u00a0\u00a0\u00a0 (clean(name, stopWords), tf)\n\u00a0 }\n\u00a0 .reduceByKey(_ |+| _)\n\n}\n```", "```scala\ndef metaphone(name: String) = {\n\u00a0 val dm = new DoubleMetaphone()\n\u00a0 name.split(\"\\\\s\")\n\u00a0\u00a0\u00a0 .map(dm.doubleMetaphone)\n\u00a0\u00a0\u00a0 .mkString(\"#\")\n}\n\ndef metaphoneDedup(rdd: RDD[(String, Map[String, Int])]) = {\n\u00a0 rdd.map { case (name, tf) =>\n\u00a0\u00a0\u00a0 (metaphone(name), tf)\n\u00a0 }\n\u00a0 .reduceByKey(_ |+| _)\n}\n```", "```scala\npersons.foreach(p => println(p + \"\\t\" + metaphoneAndNickNames(p))\n\n/*\nDavid Bowie\u00a0 TFT#P\nDavid Bowi\u00a0\u00a0 TFT#P\nDave Bowie\u00a0\u00a0 TFT#P\n*/\n```", "```scala\ndef getBestNameRdd(rdd: RDD[(String, Map[String, Int])]) = {\n\u00a0 rdd.flatMap { case (key, tf) =>\n\u00a0\u00a0\u00a0 val bestName = tf.toSeq.sortBy(_._2).last._1\n\u00a0\u00a0\u00a0 tf.keySet.map { altName =>\n\u00a0\u00a0\u00a0\u00a0\u00a0 (altName, bestName)\n\u00a0\u00a0\u00a0 } \n\u00a0 }\n}\n\nval bestNameRdd = getBestNameRdd(nameTfRdd)\n\nval dedupRdd = nameRdd\n\u00a0 .map(_ -> 1)\n\u00a0 .leftOuterJoin(bestNameRdd)\n\u00a0 .map { case (name, (dummy, optBest)) =>\n\u00a0\u00a0\u00a0 optBest.getOrElse(name)\n\u00a0 }\n```"]